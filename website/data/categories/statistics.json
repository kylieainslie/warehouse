[
  {
    "id": 21999,
    "package_name": "rstan",
    "title": "R Interface to Stan",
    "description": "User-facing R functions are provided to parse, compile,\ntest, estimate, and analyze Stan models by accessing the\nheader-only Stan library provided by the 'StanHeaders' package.\nThe Stan project develops a probabilistic programming language\nthat implements full Bayesian statistical inference via Markov\nChain Monte Carlo, rough Bayesian inference via 'variational'\napproximation, and (optionally penalized) maximum likelihood\nestimation via optimization. In all three cases, automatic\ndifferentiation is used to quickly and accurately evaluate\ngradients without burdening the user with the need to derive\nthe partial derivatives.",
    "version": "2.36.0.9000",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "url": "https://mc-stan.org/rstan/, https://discourse.mc-stan.org",
    "exports": [
      ["As.mcmc.list"],
      ["check_divergences"],
      ["check_energy"],
      ["check_hmc_diagnostics"],
      ["check_treedepth"],
      ["constrain_pars"],
      ["cpp_object_initializer"],
      ["ess_bulk"],
      ["ess_tail"],
      ["expose_stan_functions"],
      ["extract"],
      ["extract_sparse_parts"],
      ["get_adaptation_info"],
      ["get_bfmi"],
      ["get_cppcode"],
      ["get_cppo_mode"],
      ["get_cxxflags"],
      ["get_divergent_iterations"],
      ["get_elapsed_time"],
      ["get_inits"],
      ["get_logposterior"],
      ["get_low_bfmi_chains"],
      ["get_max_treedepth_iterations"],
      ["get_num_divergent"],
      ["get_num_leapfrog_per_iteration"],
      ["get_num_max_treedepth"],
      ["get_num_upars"],
      ["get_posterior_mean"],
      ["get_rng"],
      ["get_sampler_params"],
      ["get_seed"],
      ["get_seeds"],
      ["get_stancode"],
      ["get_stanmodel"],
      ["get_stream"],
      ["gqs"],
      ["grad_log_prob"],
      ["log_prob"],
      ["loo"],
      ["loo_moment_match"],
      ["lookup"],
      ["makeconf_path"],
      ["monitor"],
      ["nlist"],
      ["optimizing"],
      ["OUT"],
      ["plot"],
      ["quietgg"],
      ["read_rdump"],
      ["read_stan_csv"],
      ["Rhat"],
      ["RNG"],
      ["rstan_gg_options"],
      ["rstan_ggtheme_options"],
      ["rstan_options"],
      ["rstan.package.skeleton"],
      ["sampling"],
      ["sbc"],
      ["set_cppo"],
      ["sflist2stanfit"],
      ["show"],
      ["stan"],
      ["stan_ac"],
      ["stan_demo"],
      ["stan_dens"],
      ["stan_diag"],
      ["stan_ess"],
      ["stan_hist"],
      ["stan_mcse"],
      ["stan_model"],
      ["stan_par"],
      ["stan_plot"],
      ["stan_rdump"],
      ["stan_rhat"],
      ["stan_scat"],
      ["stan_trace"],
      ["stan_version"],
      ["stanc"],
      ["stanc_builder"],
      ["summary"],
      ["traceplot"],
      ["unconstrain_pars"],
      ["vb"]
    ],
    "topics": [
      ["bayesian-data-analysis"],
      ["bayesian-inference"],
      ["bayesian-statistics"],
      ["mcmc"],
      ["stan"],
      ["cpp"]
    ],
    "score": 20.4517,
    "stars": 1069
  },
  {
    "id": 9091,
    "package_name": "bayesplot",
    "title": "Plotting for Bayesian Models",
    "description": "Plotting functions for posterior analysis, MCMC\ndiagnostics, prior and posterior predictive checks, and other\nvisualizations to support the applied Bayesian workflow\nadvocated in Gabry, Simpson, Vehtari, Betancourt, and Gelman\n(2019) <doi:10.1111/rssa.12378>. The package is designed not\nonly to provide convenient functionality for users, but also a\ncommon set of functions that can be easily used by developers\nworking on a variety of R packages for Bayesian modeling,\nparticularly (but not exclusively) packages interfacing with\n'Stan'.",
    "version": "1.15.0.9000",
    "maintainer": "Jonah Gabry <jgabry@gmail.com>",
    "url": "https://mc-stan.org/bayesplot/",
    "exports": [
      ["abline_01"],
      ["available_mcmc"],
      ["available_ppc"],
      ["available_ppd"],
      ["bayesplot_grid"],
      ["bayesplot_theme_get"],
      ["bayesplot_theme_replace"],
      ["bayesplot_theme_set"],
      ["bayesplot_theme_update"],
      ["color_scheme_get"],
      ["color_scheme_set"],
      ["color_scheme_view"],
      ["example_group_data"],
      ["example_mcmc_draws"],
      ["example_x_data"],
      ["example_y_data"],
      ["example_yrep_draws"],
      ["facet_bg"],
      ["facet_text"],
      ["grid_lines"],
      ["hline_0"],
      ["hline_at"],
      ["lbub"],
      ["legend_move"],
      ["legend_none"],
      ["legend_text"],
      ["log_posterior"],
      ["mcmc_acf"],
      ["mcmc_acf_bar"],
      ["mcmc_areas"],
      ["mcmc_areas_data"],
      ["mcmc_areas_ridges"],
      ["mcmc_areas_ridges_data"],
      ["mcmc_combo"],
      ["mcmc_dens"],
      ["mcmc_dens_chains"],
      ["mcmc_dens_chains_data"],
      ["mcmc_dens_overlay"],
      ["mcmc_hex"],
      ["mcmc_hist"],
      ["mcmc_hist_by_chain"],
      ["mcmc_intervals"],
      ["mcmc_intervals_data"],
      ["mcmc_neff"],
      ["mcmc_neff_data"],
      ["mcmc_neff_hist"],
      ["mcmc_nuts_acceptance"],
      ["mcmc_nuts_divergence"],
      ["mcmc_nuts_energy"],
      ["mcmc_nuts_stepsize"],
      ["mcmc_nuts_treedepth"],
      ["mcmc_pairs"],
      ["mcmc_parcoord"],
      ["mcmc_parcoord_data"],
      ["mcmc_rank_ecdf"],
      ["mcmc_rank_hist"],
      ["mcmc_rank_overlay"],
      ["mcmc_recover_hist"],
      ["mcmc_recover_intervals"],
      ["mcmc_recover_scatter"],
      ["mcmc_rhat"],
      ["mcmc_rhat_data"],
      ["mcmc_rhat_hist"],
      ["mcmc_scatter"],
      ["mcmc_trace"],
      ["mcmc_trace_data"],
      ["mcmc_trace_highlight"],
      ["mcmc_violin"],
      ["neff_ratio"],
      ["nuts_params"],
      ["overlay_function"],
      ["pairs_condition"],
      ["pairs_style_np"],
      ["panel_bg"],
      ["param_glue"],
      ["param_range"],
      ["parcoord_style_np"],
      ["plot_bg"],
      ["pp_check"],
      ["ppc_bars"],
      ["ppc_bars_data"],
      ["ppc_bars_grouped"],
      ["ppc_boxplot"],
      ["ppc_data"],
      ["ppc_dens"],
      ["ppc_dens_overlay"],
      ["ppc_dens_overlay_grouped"],
      ["ppc_dots"],
      ["ppc_ecdf_overlay"],
      ["ppc_ecdf_overlay_grouped"],
      ["ppc_error_binned"],
      ["ppc_error_data"],
      ["ppc_error_hist"],
      ["ppc_error_hist_grouped"],
      ["ppc_error_scatter"],
      ["ppc_error_scatter_avg"],
      ["ppc_error_scatter_avg_grouped"],
      ["ppc_error_scatter_avg_vs_x"],
      ["ppc_freqpoly"],
      ["ppc_freqpoly_grouped"],
      ["ppc_hist"],
      ["ppc_intervals"],
      ["ppc_intervals_data"],
      ["ppc_intervals_grouped"],
      ["ppc_km_overlay"],
      ["ppc_km_overlay_grouped"],
      ["ppc_loo_intervals"],
      ["ppc_loo_pit"],
      ["ppc_loo_pit_data"],
      ["ppc_loo_pit_ecdf"],
      ["ppc_loo_pit_overlay"],
      ["ppc_loo_pit_qq"],
      ["ppc_loo_ribbon"],
      ["ppc_pit_ecdf"],
      ["ppc_pit_ecdf_grouped"],
      ["ppc_ribbon"],
      ["ppc_ribbon_data"],
      ["ppc_ribbon_grouped"],
      ["ppc_rootogram"],
      ["ppc_scatter"],
      ["ppc_scatter_avg"],
      ["ppc_scatter_avg_data"],
      ["ppc_scatter_avg_grouped"],
      ["ppc_scatter_data"],
      ["ppc_stat"],
      ["ppc_stat_2d"],
      ["ppc_stat_data"],
      ["ppc_stat_freqpoly"],
      ["ppc_stat_freqpoly_grouped"],
      ["ppc_stat_grouped"],
      ["ppc_violin_grouped"],
      ["ppd_boxplot"],
      ["ppd_data"],
      ["ppd_dens"],
      ["ppd_dens_overlay"],
      ["ppd_dots"],
      ["ppd_ecdf_overlay"],
      ["ppd_freqpoly"],
      ["ppd_freqpoly_grouped"],
      ["ppd_hist"],
      ["ppd_intervals"],
      ["ppd_intervals_data"],
      ["ppd_intervals_grouped"],
      ["ppd_ribbon"],
      ["ppd_ribbon_data"],
      ["ppd_ribbon_grouped"],
      ["ppd_stat"],
      ["ppd_stat_2d"],
      ["ppd_stat_data"],
      ["ppd_stat_freqpoly"],
      ["ppd_stat_freqpoly_grouped"],
      ["ppd_stat_grouped"],
      ["rhat"],
      ["scatter_style_np"],
      ["theme_default"],
      ["trace_style_np"],
      ["vars"],
      ["vline_0"],
      ["vline_at"],
      ["xaxis_text"],
      ["xaxis_ticks"],
      ["xaxis_title"],
      ["yaxis_text"],
      ["yaxis_ticks"],
      ["yaxis_title"]
    ],
    "topics": [
      ["bayesian"],
      ["ggplot2"],
      ["mcmc"],
      ["pandoc"],
      ["stan"],
      ["statistical-graphics"],
      ["visualization"]
    ],
    "score": 18.5236,
    "stars": 440
  },
  {
    "id": 26065,
    "package_name": "xts",
    "title": "eXtensible Time Series",
    "description": "Provide for uniform handling of R's different time-based\ndata classes by extending zoo, maximizing native format\ninformation preservation and allowing for user level\ncustomization and extension, while simplifying cross-class\ninteroperability.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 18.47,
    "stars": 0
  },
  {
    "id": 16672,
    "package_name": "loo",
    "title": "Efficient Leave-One-Out Cross-Validation and WAIC for Bayesian\nModels",
    "description": "Efficient approximate leave-one-out cross-validation (LOO)\nfor Bayesian models fit using Markov chain Monte Carlo, as\ndescribed in Vehtari, Gelman, and Gabry (2017)\n<doi:10.1007/s11222-016-9696-4>. The approximation uses Pareto\nsmoothed importance sampling (PSIS), a new procedure for\nregularizing importance weights.  As a byproduct of the\ncalculations, we also obtain approximate standard errors for\nestimated predictive errors and for the comparison of\npredictive errors between models. The package also provides\nmethods for using stacking and other model weighting techniques\nto average Bayesian predictive distributions.",
    "version": "2.9.0.9000",
    "maintainer": "Jonah Gabry <jgabry@gmail.com>",
    "url": "https://mc-stan.org/loo/, https://discourse.mc-stan.org",
    "exports": [
      [".compute_point_estimate"],
      [".ndraws"],
      [".thin_draws"],
      ["compare"],
      ["crps"],
      ["E_loo"],
      ["elpd"],
      ["example_loglik_array"],
      ["example_loglik_matrix"],
      ["extract_log_lik"],
      ["find_model_names"],
      ["gpdfit"],
      ["is.kfold"],
      ["is.loo"],
      ["is.psis"],
      ["is.psis_loo"],
      ["is.sis"],
      ["is.tis"],
      ["is.waic"],
      ["kfold"],
      ["kfold_split_grouped"],
      ["kfold_split_random"],
      ["kfold_split_stratified"],
      ["loo"],
      ["loo_approximate_posterior"],
      ["loo_approximate_posterior.array"],
      ["loo_approximate_posterior.function"],
      ["loo_approximate_posterior.matrix"],
      ["loo_compare"],
      ["loo_crps"],
      ["loo_i"],
      ["loo_model_weights"],
      ["loo_model_weights.default"],
      ["loo_moment_match"],
      ["loo_moment_match.default"],
      ["loo_predictive_metric"],
      ["loo_scrps"],
      ["loo_subsample"],
      ["loo_subsample.function"],
      ["loo.array"],
      ["loo.function"],
      ["loo.matrix"],
      ["mcse_loo"],
      ["nlist"],
      ["obs_idx"],
      ["pareto_k_ids"],
      ["pareto_k_influence_values"],
      ["pareto_k_table"],
      ["pareto_k_values"],
      ["pointwise"],
      ["print_dims"],
      ["pseudobma_weights"],
      ["psis"],
      ["psis_n_eff_values"],
      ["psislw"],
      ["relative_eff"],
      ["scrps"],
      ["sis"],
      ["stacking_weights"],
      ["tis"],
      ["waic"],
      ["waic.array"],
      ["waic.function"],
      ["waic.matrix"],
      ["weights.importance_sampling"]
    ],
    "topics": [
      ["bayes"],
      ["bayesian"],
      ["bayesian-data-analysis"],
      ["bayesian-inference"],
      ["bayesian-methods"],
      ["bayesian-statistics"],
      ["cross-validation"],
      ["information-criterion"],
      ["model-comparison"],
      ["stan"]
    ],
    "score": 18.3467,
    "stars": 155
  },
  {
    "id": 20642,
    "package_name": "quantmod",
    "title": "Quantitative Financial Modelling Framework",
    "description": "Specify, build, trade, and analyse quantitative financial\ntrading strategies.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 17.4142,
    "stars": 0
  },
  {
    "id": 15622,
    "package_name": "insight",
    "title": "Easy Access to Model Information for Various Model Objects",
    "description": "A tool to provide an easy, intuitive and consistent access\nto information contained in various R models, like model\nformulas, model terms, information about random effects, data\nthat was used to fit the model or data from response variables.\n'insight' mainly revolves around two types of functions:\nFunctions that find (the names of) information, starting with\n'find_', and functions that get the underlying data, starting\nwith 'get_'. The package has a consistent syntax and works with\nmany different model objects, where otherwise functions to\naccess these information are missing.",
    "version": "1.4.4",
    "maintainer": "Daniel Lüdecke <officialeasystats@gmail.com>",
    "url": "https://easystats.github.io/insight/",
    "exports": [
      ["all_models_equal"],
      ["all_models_same_class"],
      ["broom_columns"],
      ["check_if_installed"],
      ["clean_names"],
      ["clean_parameters"],
      ["color_if"],
      ["color_text"],
      ["color_theme"],
      ["colour_if"],
      ["colour_text"],
      ["compact_character"],
      ["compact_list"],
      ["display"],
      ["download_model"],
      ["easystats_columns"],
      ["ellipsis_info"],
      ["export_table"],
      ["find_algorithm"],
      ["find_auxiliary"],
      ["find_formula"],
      ["find_interactions"],
      ["find_offset"],
      ["find_parameters"],
      ["find_predictors"],
      ["find_random"],
      ["find_random_slopes"],
      ["find_response"],
      ["find_smooth"],
      ["find_statistic"],
      ["find_terms"],
      ["find_transformation"],
      ["find_variables"],
      ["find_weights"],
      ["format_alert"],
      ["format_bf"],
      ["format_capitalize"],
      ["format_ci"],
      ["format_error"],
      ["format_message"],
      ["format_number"],
      ["format_p"],
      ["format_pd"],
      ["format_percent"],
      ["format_rope"],
      ["format_string"],
      ["format_table"],
      ["format_value"],
      ["format_warning"],
      ["formula_ok"],
      ["get_auxiliary"],
      ["get_call"],
      ["get_correlation_slope_intercept"],
      ["get_correlation_slopes"],
      ["get_data"],
      ["get_datagrid"],
      ["get_deviance"],
      ["get_df"],
      ["get_dispersion"],
      ["get_family"],
      ["get_intercept"],
      ["get_loglikelihood"],
      ["get_loglikelihood_adjustment"],
      ["get_mixed_info"],
      ["get_model"],
      ["get_modelmatrix"],
      ["get_parameters"],
      ["get_predicted"],
      ["get_predicted_ci"],
      ["get_predictors"],
      ["get_priors"],
      ["get_random"],
      ["get_residuals"],
      ["get_response"],
      ["get_sigma"],
      ["get_statistic"],
      ["get_transformation"],
      ["get_varcov"],
      ["get_variance"],
      ["get_variance_dispersion"],
      ["get_variance_distribution"],
      ["get_variance_fixed"],
      ["get_variance_intercept"],
      ["get_variance_random"],
      ["get_variance_residual"],
      ["get_variance_slope"],
      ["get_weights"],
      ["has_intercept"],
      ["has_single_value"],
      ["is_bayesian_model"],
      ["is_converged"],
      ["is_empty_object"],
      ["is_gam_model"],
      ["is_mixed_model"],
      ["is_model"],
      ["is_model_supported"],
      ["is_multivariate"],
      ["is_nested_models"],
      ["is_nullmodel"],
      ["is_regression_model"],
      ["link_function"],
      ["link_inverse"],
      ["loglikelihood"],
      ["model_info"],
      ["model_name"],
      ["n_grouplevels"],
      ["n_obs"],
      ["n_parameters"],
      ["n_unique"],
      ["null_model"],
      ["object_has_names"],
      ["object_has_rownames"],
      ["print_color"],
      ["print_colour"],
      ["print_html"],
      ["print_md"],
      ["print_parameters"],
      ["safe_deparse"],
      ["safe_deparse_symbol"],
      ["standardize_column_order"],
      ["standardize_names"],
      ["supported_models"],
      ["text_remove_backticks"],
      ["trim_ws"],
      ["validate_argument"]
    ],
    "topics": [
      ["easystats"],
      ["hacktoberfest"],
      ["insight"],
      ["models"],
      ["names"],
      ["predictors"],
      ["random"]
    ],
    "score": 17.4073,
    "stars": 426
  },
  {
    "id": 7305,
    "package_name": "StanHeaders",
    "title": "C++ Header Files for Stan",
    "description": "The C++ header files of the Stan project are provided by\nthis package, but it contains little R code or documentation.\nThe main reference is the vignette. There is a shared object\ncontaining part of the 'CVODES' library, but its functionality\nis not accessible from R. 'StanHeaders' is primarily useful for\ndevelopers who want to utilize the 'LinkingTo' directive of\ntheir package's DESCRIPTION file to build on the Stan library\nwithout incurring unnecessary dependencies. The Stan project\ndevelops a probabilistic programming language that implements\nfull or approximate Bayesian statistical inference via Markov\nChain Monte Carlo or 'variational' methods and implements\n(optionally penalized) maximum likelihood estimation via\noptimization. The Stan library includes an advanced automatic\ndifferentiation scheme, 'templated' statistical and linear\nalgebra functions that can handle the automatically\n'differentiable' scalar types (and doubles, 'ints', etc.), and\na parser for the Stan language. The 'rstan' package provides\nuser-facing R functions to parse, compile, test, estimate, and\nanalyze Stan models.",
    "version": "2.36.0.9000",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "url": "https://mc-stan.org/",
    "exports": [
      ["stanFunction"]
    ],
    "topics": [
      ["bayesian-data-analysis"],
      ["bayesian-inference"],
      ["bayesian-statistics"],
      ["mcmc"],
      ["stan"]
    ],
    "score": 17.4008,
    "stars": 1069
  },
  {
    "id": 20045,
    "package_name": "posterior",
    "title": "Tools for Working with Posterior Distributions",
    "description": "Provides useful tools for both users and developers of\npackages for fitting Bayesian models or working with output\nfrom Bayesian models. The primary goals of the package are to:\n(a) Efficiently convert between many different useful formats\nof draws (samples) from posterior or prior distributions. (b)\nProvide consistent methods for operations commonly performed on\ndraws, for example, subsetting, binding, or mutating draws. (c)\nProvide various summaries of draws in convenient formats. (d)\nProvide lightweight implementations of state of the art\nposterior inference diagnostics. References: Vehtari et al.\n(2021) <doi:10.1214/20-BA1221>.",
    "version": "1.6.1.9000",
    "maintainer": "Paul-Christian Bürkner <paul.buerkner@gmail.com>",
    "url": "https://mc-stan.org/posterior/, https://discourse.mc-stan.org/",
    "exports": [
      ["%**%"],
      ["%in%"],
      ["as_draws"],
      ["as_draws_array"],
      ["as_draws_df"],
      ["as_draws_list"],
      ["as_draws_matrix"],
      ["as_draws_rvars"],
      ["as_rvar"],
      ["as_rvar_factor"],
      ["as_rvar_integer"],
      ["as_rvar_logical"],
      ["as_rvar_numeric"],
      ["as_rvar_ordered"],
      ["autocorrelation"],
      ["autocovariance"],
      ["bind_draws"],
      ["cdf"],
      ["chain_ids"],
      ["default_convergence_measures"],
      ["default_mcse_measures"],
      ["default_summary_measures"],
      ["diag"],
      ["dissent"],
      ["draw_ids"],
      ["draws_array"],
      ["draws_df"],
      ["draws_list"],
      ["draws_matrix"],
      ["draws_of"],
      ["draws_of<-"],
      ["draws_rvars"],
      ["drop"],
      ["E"],
      ["entropy"],
      ["ess_basic"],
      ["ess_bulk"],
      ["ess_mean"],
      ["ess_median"],
      ["ess_quantile"],
      ["ess_sd"],
      ["ess_tail"],
      ["example_draws"],
      ["extract_list_of_variable_arrays"],
      ["extract_variable"],
      ["extract_variable_array"],
      ["extract_variable_matrix"],
      ["for_each_draw"],
      ["is_constant"],
      ["is_draws"],
      ["is_draws_array"],
      ["is_draws_df"],
      ["is_draws_list"],
      ["is_draws_matrix"],
      ["is_draws_rvars"],
      ["is_rvar"],
      ["is_rvar_factor"],
      ["is_rvar_ordered"],
      ["iteration_ids"],
      ["mad"],
      ["match"],
      ["mcse_mean"],
      ["mcse_median"],
      ["mcse_quantile"],
      ["mcse_sd"],
      ["merge_chains"],
      ["modal_category"],
      ["mutate_variables"],
      ["nchains"],
      ["ndraws"],
      ["niterations"],
      ["nvariables"],
      ["order_draws"],
      ["pareto_convergence_rate"],
      ["pareto_diags"],
      ["pareto_khat"],
      ["pareto_khat_threshold"],
      ["pareto_min_ss"],
      ["pareto_smooth"],
      ["pit"],
      ["Pr"],
      ["ps_convergence_rate"],
      ["ps_khat_threshold"],
      ["ps_min_ss"],
      ["ps_tail"],
      ["ps_tail_length"],
      ["quantile2"],
      ["r_scale"],
      ["rdo"],
      ["rename_variables"],
      ["repair_draws"],
      ["resample_draws"],
      ["reserved_variables"],
      ["rfun"],
      ["rhat"],
      ["rhat_basic"],
      ["rhat_nested"],
      ["rstar"],
      ["rvar"],
      ["rvar_all"],
      ["rvar_any"],
      ["rvar_apply"],
      ["rvar_factor"],
      ["rvar_ifelse"],
      ["rvar_is_finite"],
      ["rvar_is_infinite"],
      ["rvar_is_na"],
      ["rvar_is_nan"],
      ["rvar_mad"],
      ["rvar_max"],
      ["rvar_mean"],
      ["rvar_median"],
      ["rvar_min"],
      ["rvar_ordered"],
      ["rvar_prod"],
      ["rvar_quantile"],
      ["rvar_range"],
      ["rvar_rng"],
      ["rvar_sd"],
      ["rvar_sum"],
      ["rvar_var"],
      ["sd"],
      ["set_variables"],
      ["split_chains"],
      ["subset_draws"],
      ["summarise_draws"],
      ["summarize_draws"],
      ["thin_draws"],
      ["u_scale"],
      ["var"],
      ["variables"],
      ["variables<-"],
      ["variance"],
      ["weight_draws"],
      ["z_scale"]
    ],
    "topics": [
      ["bayes"],
      ["bayesian"],
      ["mcmc"]
    ],
    "score": 17.0374,
    "stars": 168
  },
  {
    "id": 9097,
    "package_name": "bayestestR",
    "title": "Understand and Describe Bayesian Models and Posterior\nDistributions",
    "description": "Provides utilities to describe posterior distributions and\nBayesian models. It includes point-estimates such as Maximum A\nPosteriori (MAP), measures of dispersion (Highest Density\nInterval - HDI; Kruschke, 2015 <doi:10.1016/C2012-0-00477-2>)\nand indices used for null-hypothesis testing (such as ROPE\npercentage, pd and Bayes factors). References: Makowski et al.\n(2021) <doi:10.21105/joss.01541>.",
    "version": "0.17.0.1",
    "maintainer": "Dominique Makowski <officialeasystats@gmail.com>",
    "url": "https://easystats.github.io/bayestestR/",
    "exports": [
      ["area_under_curve"],
      ["auc"],
      ["bayesfactor"],
      ["bayesfactor_inclusion"],
      ["bayesfactor_models"],
      ["bayesfactor_parameters"],
      ["bayesfactor_pointnull"],
      ["bayesfactor_restricted"],
      ["bayesfactor_rope"],
      ["bayesian_as_frequentist"],
      ["bcai"],
      ["bci"],
      ["bf_inclusion"],
      ["bf_models"],
      ["bf_parameters"],
      ["bf_pointnull"],
      ["bf_restricted"],
      ["bf_rope"],
      ["bic_to_bf"],
      ["check_prior"],
      ["ci"],
      ["contr.bayes"],
      ["contr.equalprior"],
      ["contr.equalprior_deviations"],
      ["contr.equalprior_pairs"],
      ["contr.orthonorm"],
      ["convert_bayesian_as_frequentist"],
      ["convert_p_to_pd"],
      ["convert_pd_to_p"],
      ["density_at"],
      ["describe_posterior"],
      ["describe_prior"],
      ["diagnostic_draws"],
      ["diagnostic_posterior"],
      ["display"],
      ["distribution"],
      ["distribution_beta"],
      ["distribution_binom"],
      ["distribution_binomial"],
      ["distribution_cauchy"],
      ["distribution_chisq"],
      ["distribution_chisquared"],
      ["distribution_custom"],
      ["distribution_gamma"],
      ["distribution_gaussian"],
      ["distribution_mixture_normal"],
      ["distribution_nbinom"],
      ["distribution_normal"],
      ["distribution_poisson"],
      ["distribution_student"],
      ["distribution_student_t"],
      ["distribution_t"],
      ["distribution_tweedie"],
      ["distribution_uniform"],
      ["effective_sample"],
      ["equivalence_test"],
      ["estimate_density"],
      ["eti"],
      ["hdi"],
      ["map_estimate"],
      ["mcse"],
      ["mediation"],
      ["model_to_priors"],
      ["overlap"],
      ["p_direction"],
      ["p_map"],
      ["p_pointnull"],
      ["p_rope"],
      ["p_significance"],
      ["p_to_bf"],
      ["p_to_pd"],
      ["pd"],
      ["pd_to_p"],
      ["point_estimate"],
      ["print_html"],
      ["print_md"],
      ["reshape_draws"],
      ["reshape_iterations"],
      ["rope"],
      ["rope_range"],
      ["sensitivity_to_prior"],
      ["sexit"],
      ["sexit_thresholds"],
      ["si"],
      ["simulate_correlation"],
      ["simulate_difference"],
      ["simulate_prior"],
      ["simulate_simpson"],
      ["simulate_ttest"],
      ["spi"],
      ["unupdate"],
      ["weighted_posteriors"]
    ],
    "topics": [
      ["bayes-factors"],
      ["bayesfactor"],
      ["bayesian"],
      ["bayesian-framework"],
      ["credible-interval"],
      ["easystats"],
      ["hacktoberfest"],
      ["hdi"],
      ["map"],
      ["posterior-distributions"],
      ["rope"]
    ],
    "score": 16.9979,
    "stars": 588
  },
  {
    "id": 12350,
    "package_name": "effectsize",
    "title": "Indices of Effect Size",
    "description": "Provide utilities to work with indices of effect size for\na wide variety of models and hypothesis tests (see list of\nsupported models using the function\n'insight::supported_models()'), allowing computation of and\nconversion between indices such as Cohen's d, r, odds, etc.\nReferences: Ben-Shachar et al. (2020)\n<doi:10.21105/joss.02815>.",
    "version": "1.0.1.2",
    "maintainer": "Mattan S. Ben-Shachar <mattansb@msbstats.info>",
    "url": "https://easystats.github.io/effectsize/",
    "exports": [
      [".es_aov_simple"],
      [".es_aov_strata"],
      [".es_aov_table"],
      ["arr"],
      ["arr_to_logoddsratio"],
      ["arr_to_nnt"],
      ["arr_to_oddsratio"],
      ["arr_to_probs"],
      ["arr_to_riskratio"],
      ["c_to_w"],
      ["chisq_to_cohens_w"],
      ["chisq_to_cramers_v"],
      ["chisq_to_fei"],
      ["chisq_to_pearsons_c"],
      ["chisq_to_phi"],
      ["chisq_to_tschuprows_t"],
      ["cliffs_delta"],
      ["cohens_d"],
      ["cohens_f"],
      ["cohens_f_squared"],
      ["cohens_g"],
      ["cohens_h"],
      ["cohens_u1"],
      ["cohens_u2"],
      ["cohens_u3"],
      ["cohens_w"],
      ["cov_pooled"],
      ["cramers_v"],
      ["d_to_logoddsratio"],
      ["d_to_oddsratio"],
      ["d_to_overlap"],
      ["d_to_p_superiority"],
      ["d_to_r"],
      ["d_to_u1"],
      ["d_to_u2"],
      ["d_to_u3"],
      ["display"],
      ["effectsize"],
      ["epsilon_squared"],
      ["equivalence_test"],
      ["eta_squared"],
      ["eta_squared_posterior"],
      ["eta2_to_f"],
      ["eta2_to_f2"],
      ["F_to_d"],
      ["F_to_epsilon2"],
      ["f_to_eta2"],
      ["F_to_eta2"],
      ["F_to_eta2_adj"],
      ["F_to_f"],
      ["F_to_f2"],
      ["F_to_omega2"],
      ["F_to_r"],
      ["f2_to_eta2"],
      ["fei"],
      ["fei_to_w"],
      ["format_standardize"],
      ["get_effectsize_label"],
      ["get_effectsize_name"],
      ["glass_delta"],
      ["hedges_g"],
      ["interpret"],
      ["interpret_agfi"],
      ["interpret_bf"],
      ["interpret_cfi"],
      ["interpret_cohens_d"],
      ["interpret_cohens_g"],
      ["interpret_cramers_v"],
      ["interpret_direction"],
      ["interpret_epsilon_squared"],
      ["interpret_ess"],
      ["interpret_eta_squared"],
      ["interpret_fei"],
      ["interpret_gfi"],
      ["interpret_glass_delta"],
      ["interpret_hedges_g"],
      ["interpret_icc"],
      ["interpret_ifi"],
      ["interpret_kendalls_w"],
      ["interpret_nfi"],
      ["interpret_nnfi"],
      ["interpret_oddsratio"],
      ["interpret_omega_squared"],
      ["interpret_p"],
      ["interpret_pd"],
      ["interpret_phi"],
      ["interpret_pnfi"],
      ["interpret_r"],
      ["interpret_r2"],
      ["interpret_r2_semipartial"],
      ["interpret_rank_biserial"],
      ["interpret_rfi"],
      ["interpret_rhat"],
      ["interpret_rmsea"],
      ["interpret_rope"],
      ["interpret_srmr"],
      ["interpret_vif"],
      ["is_effectsize_name"],
      ["is.rules"],
      ["kendalls_w"],
      ["logoddsratio_to_arr"],
      ["logoddsratio_to_d"],
      ["logoddsratio_to_nnt"],
      ["logoddsratio_to_probs"],
      ["logoddsratio_to_r"],
      ["logoddsratio_to_riskratio"],
      ["mad_pooled"],
      ["mahalanobis_d"],
      ["means_ratio"],
      ["nnt"],
      ["nnt_to_arr"],
      ["nnt_to_logoddsratio"],
      ["nnt_to_oddsratio"],
      ["nnt_to_probs"],
      ["nnt_to_riskratio"],
      ["odds_to_probs"],
      ["oddsratio"],
      ["oddsratio_to_arr"],
      ["oddsratio_to_d"],
      ["oddsratio_to_nnt"],
      ["oddsratio_to_probs"],
      ["oddsratio_to_r"],
      ["oddsratio_to_riskratio"],
      ["omega_squared"],
      ["p_overlap"],
      ["p_superiority"],
      ["pearsons_c"],
      ["phi"],
      ["phi_to_chisq"],
      ["print_html"],
      ["print_md"],
      ["probs_to_odds"],
      ["r_to_d"],
      ["r_to_logoddsratio"],
      ["r_to_oddsratio"],
      ["r2_semipartial"],
      ["rank_biserial"],
      ["rank_epsilon_squared"],
      ["rank_eta_squared"],
      ["rb_to_p_superiority"],
      ["rb_to_vda"],
      ["rb_to_wmw_odds"],
      ["repeated_measures_d"],
      ["riskratio"],
      ["riskratio_to_arr"],
      ["riskratio_to_logoddsratio"],
      ["riskratio_to_nnt"],
      ["riskratio_to_oddsratio"],
      ["riskratio_to_probs"],
      ["rm_d"],
      ["rules"],
      ["sd_pooled"],
      ["standardise"],
      ["standardize"],
      ["standardize_info"],
      ["standardize_parameters"],
      ["standardize_posteriors"],
      ["t_to_d"],
      ["t_to_epsilon2"],
      ["t_to_eta2"],
      ["t_to_eta2_adj"],
      ["t_to_f"],
      ["t_to_f2"],
      ["t_to_omega2"],
      ["t_to_r"],
      ["t_to_v"],
      ["t_to_w"],
      ["tschuprows_t"],
      ["v_to_t"],
      ["v_to_w"],
      ["vd_a"],
      ["w_to_c"],
      ["w_to_fei"],
      ["w_to_t"],
      ["w_to_v"],
      ["wmw_odds"],
      ["z_to_d"],
      ["z_to_r"]
    ],
    "topics": [
      ["anova"],
      ["cohens-d"],
      ["compute"],
      ["conversion"],
      ["correlation"],
      ["effect-size"],
      ["effectsize"],
      ["hacktoberfest"],
      ["hedges-g"],
      ["interpretation"],
      ["standardization"],
      ["standardized"],
      ["statistics"]
    ],
    "score": 16.9112,
    "stars": 347
  },
  {
    "id": 9667,
    "package_name": "brms",
    "title": "Bayesian Regression Models using 'Stan'",
    "description": "Fit Bayesian generalized (non-)linear multivariate\nmultilevel models using 'Stan' for full Bayesian inference. A\nwide range of distributions and link functions are supported,\nallowing users to fit -- among others -- linear, robust linear,\ncount data, survival, response times, ordinal, zero-inflated,\nhurdle, and even self-defined mixture models all in a\nmultilevel context. Further modeling options include both\ntheory-driven and data-driven non-linear terms,\nauto-correlation structures, censoring and truncation,\nmeta-analytic standard errors, and quite a few more. In\naddition, all parameters of the response distribution can be\npredicted in order to perform distributional regression. Prior\nspecifications are flexible and explicitly encourage users to\napply prior distributions that actually reflect their prior\nknowledge. Models can easily be evaluated and compared using\nseveral methods assessing posterior or prior predictions.\nReferences: Bürkner (2017) <doi:10.18637/jss.v080.i01>; Bürkner\n(2018) <doi:10.32614/RJ-2018-017>; Bürkner (2021)\n<doi:10.18637/jss.v100.i05>; Carpenter et al. (2017)\n<doi:10.18637/jss.v076.i01>.",
    "version": "2.23.1",
    "maintainer": "Paul-Christian Bürkner <paul.buerkner@gmail.com>",
    "url": "https://github.com/paul-buerkner/brms,\nhttps://discourse.mc-stan.org/, https://paulbuerkner.com/brms/",
    "exports": [
      ["acat"],
      ["acformula"],
      ["add_criterion"],
      ["add_ic"],
      ["add_ic<-"],
      ["add_loo"],
      ["add_rstan_model"],
      ["add_waic"],
      ["ar"],
      ["arma"],
      ["as_draws"],
      ["as_draws_array"],
      ["as_draws_df"],
      ["as_draws_list"],
      ["as_draws_matrix"],
      ["as_draws_rvars"],
      ["as.brmsprior"],
      ["as.mcmc"],
      ["asym_laplace"],
      ["autocor"],
      ["bayes_factor"],
      ["bayes_R2"],
      ["bernoulli"],
      ["Beta"],
      ["beta_binomial"],
      ["bf"],
      ["bridge_sampler"],
      ["brm"],
      ["brm_multiple"],
      ["brmsfamily"],
      ["brmsfit_needs_refit"],
      ["brmsformula"],
      ["brmsterms"],
      ["car"],
      ["categorical"],
      ["combine_models"],
      ["compare_ic"],
      ["conditional_effects"],
      ["conditional_smooths"],
      ["constant"],
      ["control_params"],
      ["cor_ar"],
      ["cor_arma"],
      ["cor_arr"],
      ["cor_bsts"],
      ["cor_car"],
      ["cor_cosy"],
      ["cor_errorsar"],
      ["cor_fixed"],
      ["cor_icar"],
      ["cor_lagsar"],
      ["cor_ma"],
      ["cor_sar"],
      ["cosy"],
      ["cox"],
      ["cratio"],
      ["cs"],
      ["cse"],
      ["cumulative"],
      ["custom_family"],
      ["dasym_laplace"],
      ["data_predictor"],
      ["data_response"],
      ["dbeta_binomial"],
      ["ddirichlet"],
      ["default_prior"],
      ["density_ratio"],
      ["dexgaussian"],
      ["dfrechet"],
      ["dgen_extreme_value"],
      ["dhurdle_gamma"],
      ["dhurdle_lognormal"],
      ["dhurdle_negbinomial"],
      ["dhurdle_poisson"],
      ["dinv_gaussian"],
      ["dirichlet"],
      ["dirichlet_multinomial"],
      ["dlogistic_normal"],
      ["dmulti_normal"],
      ["dmulti_student_t"],
      ["do_call"],
      ["dshifted_lnorm"],
      ["dskew_normal"],
      ["dstudent_t"],
      ["dvon_mises"],
      ["dwiener"],
      ["dzero_inflated_beta"],
      ["dzero_inflated_beta_binomial"],
      ["dzero_inflated_binomial"],
      ["dzero_inflated_negbinomial"],
      ["dzero_inflated_poisson"],
      ["empty_prior"],
      ["exgaussian"],
      ["exponential"],
      ["expose_functions"],
      ["expp1"],
      ["extract_draws"],
      ["fcor"],
      ["fixef"],
      ["frechet"],
      ["gen_extreme_value"],
      ["geometric"],
      ["get_dpar"],
      ["get_prior"],
      ["get_y"],
      ["gp"],
      ["gr"],
      ["horseshoe"],
      ["hurdle_cumulative"],
      ["hurdle_gamma"],
      ["hurdle_lognormal"],
      ["hurdle_negbinomial"],
      ["hurdle_poisson"],
      ["hypothesis"],
      ["inits"],
      ["inv_logit_scaled"],
      ["is.brmsfit"],
      ["is.brmsfit_multiple"],
      ["is.brmsformula"],
      ["is.brmsprior"],
      ["is.brmsterms"],
      ["is.cor_arma"],
      ["is.cor_brms"],
      ["is.cor_car"],
      ["is.cor_cosy"],
      ["is.cor_fixed"],
      ["is.cor_sar"],
      ["is.mvbrmsformula"],
      ["is.mvbrmsterms"],
      ["kfold"],
      ["kfold_predict"],
      ["lasso"],
      ["lf"],
      ["log_lik"],
      ["log_posterior"],
      ["logistic_normal"],
      ["logit_scaled"],
      ["logm1"],
      ["lognormal"],
      ["loo"],
      ["LOO"],
      ["loo_compare"],
      ["loo_epred"],
      ["loo_linpred"],
      ["loo_model_weights"],
      ["loo_moment_match"],
      ["loo_predict"],
      ["loo_predictive_interval"],
      ["loo_R2"],
      ["loo_subsample"],
      ["ma"],
      ["make_conditions"],
      ["make_stancode"],
      ["make_standata"],
      ["marginal_effects"],
      ["marginal_smooths"],
      ["mcmc_plot"],
      ["me"],
      ["mi"],
      ["mixture"],
      ["mm"],
      ["mmc"],
      ["mo"],
      ["model_weights"],
      ["multinomial"],
      ["mvbf"],
      ["mvbind"],
      ["mvbrmsformula"],
      ["nchains"],
      ["ndraws"],
      ["neff_ratio"],
      ["negbinomial"],
      ["ngrps"],
      ["niterations"],
      ["nlf"],
      ["nsamples"],
      ["nuts_params"],
      ["nvariables"],
      ["opencl"],
      ["parnames"],
      ["parse_bf"],
      ["pasym_laplace"],
      ["pbeta_binomial"],
      ["pexgaussian"],
      ["pfrechet"],
      ["pgen_extreme_value"],
      ["phurdle_gamma"],
      ["phurdle_lognormal"],
      ["phurdle_negbinomial"],
      ["phurdle_poisson"],
      ["pinv_gaussian"],
      ["post_prob"],
      ["posterior_average"],
      ["posterior_epred"],
      ["posterior_interval"],
      ["posterior_linpred"],
      ["posterior_predict"],
      ["posterior_samples"],
      ["posterior_smooths"],
      ["posterior_summary"],
      ["posterior_table"],
      ["pp_average"],
      ["pp_check"],
      ["pp_expect"],
      ["pp_mixture"],
      ["predictive_error"],
      ["predictive_interval"],
      ["prepare_predictions"],
      ["prior"],
      ["prior_"],
      ["prior_draws"],
      ["prior_samples"],
      ["prior_string"],
      ["prior_summary"],
      ["pshifted_lnorm"],
      ["psis"],
      ["pskew_normal"],
      ["pstudent_t"],
      ["pvon_mises"],
      ["pzero_inflated_beta"],
      ["pzero_inflated_beta_binomial"],
      ["pzero_inflated_binomial"],
      ["pzero_inflated_negbinomial"],
      ["pzero_inflated_poisson"],
      ["qasym_laplace"],
      ["qfrechet"],
      ["qgen_extreme_value"],
      ["qshifted_lnorm"],
      ["qskew_normal"],
      ["qstudent_t"],
      ["R2D2"],
      ["ranef"],
      ["rasym_laplace"],
      ["rbeta_binomial"],
      ["rdirichlet"],
      ["read_csv_as_stanfit"],
      ["recompile_model"],
      ["reloo"],
      ["rename_pars"],
      ["resp_bhaz"],
      ["resp_cat"],
      ["resp_cens"],
      ["resp_dec"],
      ["resp_index"],
      ["resp_mi"],
      ["resp_rate"],
      ["resp_se"],
      ["resp_subset"],
      ["resp_thres"],
      ["resp_trials"],
      ["resp_trunc"],
      ["resp_vint"],
      ["resp_vreal"],
      ["resp_weights"],
      ["restructure"],
      ["rexgaussian"],
      ["rfrechet"],
      ["rgen_extreme_value"],
      ["rhat"],
      ["rinv_gaussian"],
      ["rlogistic_normal"],
      ["rmulti_normal"],
      ["rmulti_student_t"],
      ["rows2labels"],
      ["rshifted_lnorm"],
      ["rskew_normal"],
      ["rstudent_t"],
      ["rvon_mises"],
      ["rwiener"],
      ["s"],
      ["sar"],
      ["save_pars"],
      ["set_mecor"],
      ["set_nl"],
      ["set_prior"],
      ["set_rescor"],
      ["shifted_lognormal"],
      ["skew_normal"],
      ["sratio"],
      ["stancode"],
      ["standata"],
      ["stanplot"],
      ["stanvar"],
      ["student"],
      ["t2"],
      ["theme_black"],
      ["theme_default"],
      ["threading"],
      ["unstr"],
      ["update_adterms"],
      ["validate_newdata"],
      ["validate_prior"],
      ["VarCorr"],
      ["variables"],
      ["von_mises"],
      ["waic"],
      ["WAIC"],
      ["weibull"],
      ["wiener"],
      ["xbeta"],
      ["zero_inflated_beta"],
      ["zero_inflated_beta_binomial"],
      ["zero_inflated_binomial"],
      ["zero_inflated_negbinomial"],
      ["zero_inflated_poisson"],
      ["zero_one_inflated_beta"]
    ],
    "topics": [
      ["bayesian-inference"],
      ["brms"],
      ["multilevel-models"],
      ["stan"],
      ["statistical-models"]
    ],
    "score": 16.8292,
    "stars": 1371
  },
  {
    "id": 1587,
    "package_name": "DESeq2",
    "title": "Differential gene expression analysis based on the negative\nbinomial distribution",
    "description": "Estimate variance-mean dependence in count data from\nhigh-throughput sequencing assays and test for differential\nexpression based on a model using the negative binomial\ndistribution.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 16.6574,
    "stars": 0
  },
  {
    "id": 19527,
    "package_name": "performance",
    "title": "Assessment of Regression Models Performance",
    "description": "Utilities for computing measures to assess model quality,\nwhich are not directly provided by R's 'base' or 'stats'\npackages. These include e.g. measures like r-squared,\nintraclass correlation coefficient (Nakagawa, Johnson &\nSchielzeth (2017) <doi:10.1098/rsif.2017.0213>), root mean\nsquared error or functions to check models for overdispersion,\nsingularity or zero-inflation and more. Functions apply to a\nlarge variety of regression models, including generalized\nlinear models, mixed effects models and Bayesian models.\nReferences: Lüdecke et al. (2021) <doi:10.21105/joss.03139>.",
    "version": "0.15.3.1",
    "maintainer": "Daniel Lüdecke <officialeasystats@gmail.com>",
    "url": "https://easystats.github.io/performance/",
    "exports": [
      ["as.dag"],
      ["binned_residuals"],
      ["check_autocorrelation"],
      ["check_clusterstructure"],
      ["check_collinearity"],
      ["check_concurvity"],
      ["check_convergence"],
      ["check_dag"],
      ["check_distribution"],
      ["check_factorstructure"],
      ["check_group_variation"],
      ["check_heterogeneity_bias"],
      ["check_heteroscedasticity"],
      ["check_heteroskedasticity"],
      ["check_homogeneity"],
      ["check_itemscale"],
      ["check_kmo"],
      ["check_model"],
      ["check_multimodal"],
      ["check_normality"],
      ["check_outliers"],
      ["check_overdispersion"],
      ["check_predictions"],
      ["check_residuals"],
      ["check_singularity"],
      ["check_sphericity"],
      ["check_sphericity_bartlett"],
      ["check_symmetry"],
      ["check_zeroinflation"],
      ["compare_performance"],
      ["cronbachs_alpha"],
      ["display"],
      ["icc"],
      ["item_alpha"],
      ["item_difficulty"],
      ["item_discrimination"],
      ["item_intercor"],
      ["item_omega"],
      ["item_reliability"],
      ["item_split_half"],
      ["item_totalcor"],
      ["looic"],
      ["mae"],
      ["model_performance"],
      ["mse"],
      ["multicollinearity"],
      ["performance"],
      ["performance_accuracy"],
      ["performance_aic"],
      ["performance_aicc"],
      ["performance_cv"],
      ["performance_dvour"],
      ["performance_hosmer"],
      ["performance_logloss"],
      ["performance_mae"],
      ["performance_mse"],
      ["performance_pcp"],
      ["performance_reliability"],
      ["performance_rmse"],
      ["performance_roc"],
      ["performance_rse"],
      ["performance_score"],
      ["print_html"],
      ["print_md"],
      ["r2"],
      ["r2_bayes"],
      ["r2_coxsnell"],
      ["r2_efron"],
      ["r2_ferrari"],
      ["r2_kullback"],
      ["r2_loo"],
      ["r2_loo_posterior"],
      ["r2_mcfadden"],
      ["r2_mckelvey"],
      ["r2_mlm"],
      ["r2_nagelkerke"],
      ["r2_nakagawa"],
      ["r2_posterior"],
      ["r2_somers"],
      ["r2_tjur"],
      ["r2_xu"],
      ["r2_zeroinflated"],
      ["rmse"],
      ["simulate_residuals"],
      ["test_bf"],
      ["test_likelihoodratio"],
      ["test_lrt"],
      ["test_performance"],
      ["test_vuong"],
      ["test_wald"],
      ["variance_decomposition"]
    ],
    "topics": [
      ["aic"],
      ["easystats"],
      ["hacktoberfest"],
      ["loo"],
      ["machine-learning"],
      ["mixed-models"],
      ["models"],
      ["performance"],
      ["r2"],
      ["statistics"]
    ],
    "score": 16.5507,
    "stars": 1119
  },
  {
    "id": 10557,
    "package_name": "collapse",
    "title": "Advanced and Fast Data Transformation",
    "description": "A large C/C++-based package for advanced data\ntransformation and statistical computing in R that is extremely\nfast, class-agnostic, robust, and programmer friendly. Core\nfunctionality includes a rich set of S3 generic grouped and\nweighted statistical functions for vectors, matrices and data\nframes, which provide efficient low-level vectorizations,\nOpenMP multithreading, and skip missing values by default.\nThese are integrated with fast grouping and ordering algorithms\n(also callable from C), and efficient data manipulation\nfunctions. The package also provides a flexible and rigorous\napproach to time series and panel data in R, fast functions for\ndata transformation and common statistical procedures, detailed\n(grouped, weighted) summary statistics, powerful tools to work\nwith nested data, fast data object conversions, functions for\nmemory efficient R programming, and helpers to effectively deal\nwith variable labels, attributes, and missing data. It\nseamlessly supports base R objects/classes as well as 'units',\n'integer64', 'xts'/ 'zoo', 'tibble', 'grouped_df',\n'data.table', 'sf', and 'pseries'/'pdata.frame'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 15.8595,
    "stars": 0
  },
  {
    "id": 22000,
    "package_name": "rstanarm",
    "title": "Bayesian Applied Regression Modeling via Stan",
    "description": "Estimates previously compiled regression models using the\n'rstan' package, which provides the R interface to the Stan C++\nlibrary for Bayesian estimation. Users specify models via the\ncustomary R syntax with a formula and data.frame plus some\nadditional arguments for priors.",
    "version": "2.36.0.9000",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "url": "https://mc-stan.org/rstanarm/, https://discourse.mc-stan.org",
    "exports": [
      ["as_draws"],
      ["as_draws_array"],
      ["as_draws_df"],
      ["as_draws_list"],
      ["as_draws_matrix"],
      ["as_draws_rvars"],
      ["bayes_R2"],
      ["cauchy"],
      ["compare_models"],
      ["decov"],
      ["default_prior_coef"],
      ["default_prior_intercept"],
      ["dirichlet"],
      ["exponential"],
      ["fixef"],
      ["get_surv"],
      ["get_x"],
      ["get_y"],
      ["get_z"],
      ["hs"],
      ["hs_plus"],
      ["invlogit"],
      ["kfold"],
      ["laplace"],
      ["lasso"],
      ["launch_shinystan"],
      ["lkj"],
      ["log_lik"],
      ["logit"],
      ["loo"],
      ["loo_compare"],
      ["loo_linpred"],
      ["loo_model_weights"],
      ["loo_predict"],
      ["loo_predictive_interval"],
      ["loo_R2"],
      ["neg_binomial_2"],
      ["ngrps"],
      ["normal"],
      ["nsamples"],
      ["pairs_condition"],
      ["pairs_style_np"],
      ["plot_nonlinear"],
      ["plot_stack_jm"],
      ["posterior_epred"],
      ["posterior_interval"],
      ["posterior_linpred"],
      ["posterior_predict"],
      ["posterior_survfit"],
      ["posterior_traj"],
      ["posterior_vs_prior"],
      ["pp_check"],
      ["pp_validate"],
      ["predictive_error"],
      ["predictive_interval"],
      ["prior_options"],
      ["prior_summary"],
      ["product_normal"],
      ["ps_check"],
      ["R2"],
      ["ranef"],
      ["se"],
      ["sigma"],
      ["stan_aov"],
      ["stan_betareg"],
      ["stan_betareg.fit"],
      ["stan_biglm"],
      ["stan_biglm.fit"],
      ["stan_clogit"],
      ["stan_gamm4"],
      ["stan_glm"],
      ["stan_glm.fit"],
      ["stan_glm.nb"],
      ["stan_glmer"],
      ["stan_glmer.nb"],
      ["stan_jm"],
      ["stan_lm"],
      ["stan_lm.fit"],
      ["stan_lm.wfit"],
      ["stan_lmer"],
      ["stan_mvmer"],
      ["stan_nlmer"],
      ["stan_polr"],
      ["stan_polr.fit"],
      ["stan_surv"],
      ["stanjm_list"],
      ["stanmvreg_list"],
      ["stanreg_list"],
      ["student_t"],
      ["Surv"],
      ["tve"],
      ["VarCorr"],
      ["waic"]
    ],
    "topics": [
      ["bayesian"],
      ["bayesian-data-analysis"],
      ["bayesian-inference"],
      ["bayesian-methods"],
      ["bayesian-statistics"],
      ["multilevel-models"],
      ["rstan"],
      ["rstanarm"],
      ["stan"],
      ["statistical-modeling"],
      ["cpp"]
    ],
    "score": 15.7929,
    "stars": 400
  },
  {
    "id": 19279,
    "package_name": "parameters",
    "title": "Processing of Model Parameters",
    "description": "Utilities for processing the parameters of various\nstatistical models. Beyond computing p values, CIs, and other\nindices for a wide variety of models (see list of supported\nmodels using the function 'insight::supported_models()'), this\npackage implements features like bootstrapping or simulating of\nparameters and models, feature reduction (feature extraction\nand variable selection) as well as functions to describe data\nand variable characteristics (e.g. skewness, kurtosis,\nsmoothness or distribution).",
    "version": "0.28.3",
    "maintainer": "Daniel Lüdecke <officialeasystats@gmail.com>",
    "url": "https://easystats.github.io/parameters/",
    "exports": [
      ["bootstrap_model"],
      ["bootstrap_parameters"],
      ["ci"],
      ["ci_betwithin"],
      ["ci_kenward"],
      ["ci_ml1"],
      ["ci_satterthwaite"],
      ["closest_component"],
      ["cluster_analysis"],
      ["cluster_centers"],
      ["cluster_discrimination"],
      ["cluster_meta"],
      ["cluster_performance"],
      ["compare_models"],
      ["compare_parameters"],
      ["confidence_curve"],
      ["consonance_function"],
      ["convert_efa_to_cfa"],
      ["degrees_of_freedom"],
      ["demean"],
      ["describe_distribution"],
      ["display"],
      ["dof"],
      ["dof_betwithin"],
      ["dof_kenward"],
      ["dof_ml1"],
      ["dof_satterthwaite"],
      ["dominance_analysis"],
      ["efa_to_cfa"],
      ["equivalence_test"],
      ["factor_analysis"],
      ["factor_scores"],
      ["format_df_adjust"],
      ["format_order"],
      ["format_p_adjust"],
      ["format_parameters"],
      ["get_scores"],
      ["kurtosis"],
      ["model_parameters"],
      ["n_clusters"],
      ["n_clusters_dbscan"],
      ["n_clusters_elbow"],
      ["n_clusters_gap"],
      ["n_clusters_hclust"],
      ["n_clusters_silhouette"],
      ["n_components"],
      ["n_factors"],
      ["n_parameters"],
      ["p_calibrate"],
      ["p_direction"],
      ["p_function"],
      ["p_significance"],
      ["p_value"],
      ["p_value_betwithin"],
      ["p_value_kenward"],
      ["p_value_ml1"],
      ["p_value_satterthwaite"],
      ["parameters"],
      ["parameters_type"],
      ["pool_parameters"],
      ["principal_components"],
      ["print_html"],
      ["print_md"],
      ["random_parameters"],
      ["reduce_data"],
      ["reduce_parameters"],
      ["rescale_weights"],
      ["reshape_loadings"],
      ["rotated_data"],
      ["se_kenward"],
      ["se_satterthwaite"],
      ["select_parameters"],
      ["simulate_model"],
      ["simulate_parameters"],
      ["skewness"],
      ["sort_parameters"],
      ["standard_error"],
      ["standardise_info"],
      ["standardise_parameters"],
      ["standardise_posteriors"],
      ["standardize_info"],
      ["standardize_names"],
      ["standardize_parameters"],
      ["standardize_posteriors"],
      ["supported_models"],
      ["visualisation_recipe"]
    ],
    "topics": [
      ["beta"],
      ["bootstrap"],
      ["ci"],
      ["confidence-intervals"],
      ["data-reduction"],
      ["easystats"],
      ["fa"],
      ["feature-extraction"],
      ["feature-reduction"],
      ["hacktoberfest"],
      ["parameters"],
      ["pca"],
      ["pvalues"],
      ["regression-models"],
      ["robust-statistics"],
      ["standardize"],
      ["standardized-estimates"],
      ["statistical-models"]
    ],
    "score": 15.724,
    "stars": 476
  },
  {
    "id": 17658,
    "package_name": "mlr3",
    "title": "Machine Learning in R - Next Generation",
    "description": "Efficient, object-oriented programming on the building\nblocks of machine learning. Provides 'R6' objects for tasks,\nlearners, resamplings, and measures. The package is geared\ntowards scalability and larger datasets by supporting\nparallelization and out-of-memory data-backends like databases.\nWhile 'mlr3' focuses on the core computational operations,\nadd-on packages provide additional functionality.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 15.4443,
    "stars": 0
  },
  {
    "id": 1615,
    "package_name": "DHARMa",
    "title": "Residual Diagnostics for Hierarchical (Multi-Level / Mixed)\nRegression Models",
    "description": "The 'DHARMa' package uses a simulation-based approach to\ncreate readily interpretable scaled (quantile) residuals for\nfitted (generalized) linear mixed models. Currently supported\nare linear and generalized linear (mixed) models from 'lme4'\n(classes 'lmerMod', 'glmerMod'), 'glmmTMB', 'GLMMadaptive', and\n'spaMM'; phylogenetic linear models from 'phylolm' (classes\n'phylolm' and 'phyloglm'); generalized additive models ('gam'\nfrom 'mgcv'); 'glm' (including 'negbin' from 'MASS', but\nexcluding quasi-distributions) and 'lm' model classes.\nMoreover, externally created simulations, e.g. posterior\npredictive simulations from Bayesian software such as 'JAGS',\n'STAN', or 'BUGS' can be processed as well. The resulting\nresiduals are standardized to values between 0 and 1 and can be\ninterpreted as intuitively as residuals from a linear\nregression. The package also provides a number of plot and test\nfunctions for typical model misspecification problems, such as\nover/underdispersion, zero-inflation, and residual spatial,\nphylogenetic and temporal autocorrelation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 15.2132,
    "stars": 0
  },
  {
    "id": 16943,
    "package_name": "marginaleffects",
    "title": "Predictions, Comparisons, Slopes, Marginal Means, and Hypothesis\nTests",
    "description": "Compute and plot predictions, slopes, marginal means, and\ncomparisons (contrasts, risk ratios, odds, etc.) for over 100\nclasses of statistical and machine learning models in R.\nConduct linear and non-linear hypothesis tests, or equivalence\ntests. Calculate uncertainty estimates using the delta method,\nbootstrapping, or simulation-based inference. Details can be\nfound in Arel-Bundock, Greifer, and Heiss (2024)\n<doi:10.18637/jss.v111.i09>.",
    "version": "0.31.0.4",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://marginaleffects.com/",
    "exports": [
      ["autodiff"],
      ["avg_comparisons"],
      ["avg_predictions"],
      ["avg_slopes"],
      ["comparisons"],
      ["components"],
      ["datagrid"],
      ["expect_comparisons"],
      ["expect_hypotheses"],
      ["expect_margins"],
      ["expect_predictions"],
      ["expect_slopes"],
      ["get_coef"],
      ["get_dataset"],
      ["get_draws"],
      ["get_group_names"],
      ["get_model_matrix"],
      ["get_predict"],
      ["get_vcov"],
      ["glance"],
      ["hypotheses"],
      ["inferences"],
      ["plot_comparisons"],
      ["plot_predictions"],
      ["plot_slopes"],
      ["posterior_draws"],
      ["predictions"],
      ["prune"],
      ["refit"],
      ["set_coef"],
      ["slopes"],
      ["tidy"]
    ],
    "topics": [],
    "score": 15.0743,
    "stars": 573
  },
  {
    "id": 22941,
    "package_name": "shinystan",
    "title": "Interactive Visual and Numerical Diagnostics and Posterior\nAnalysis for Bayesian Models",
    "description": "A graphical user interface for interactive Markov chain\nMonte Carlo (MCMC) diagnostics and plots and tables helpful for\nanalyzing a posterior sample. The interface is powered by the\n'Shiny' web application framework from 'RStudio' and works with\nthe output of MCMC programs written in any programming language\n(and has extended functionality for 'Stan' models fit using the\n'rstan' and 'rstanarm' packages).",
    "version": "2.7.0.9000",
    "maintainer": "Jonah Gabry <jgabry@gmail.com>",
    "url": "https://mc-stan.org/shinystan/, https://discourse.mc-stan.org",
    "exports": [
      ["as.shinystan"],
      ["deploy_shinystan"],
      ["drop_parameters"],
      ["generate_quantity"],
      ["is.shinystan"],
      ["launch_shinystan"],
      ["launch_shinystan_demo"],
      ["model_code"],
      ["model_name"],
      ["notes"],
      ["rename_model"],
      ["retrieve"],
      ["sso_info"],
      ["update_sso"]
    ],
    "topics": [
      ["bayesian"],
      ["bayesian-data-analysis"],
      ["bayesian-inference"],
      ["bayesian-methods"],
      ["bayesian-statistics"],
      ["mcmc"],
      ["shiny-apps"],
      ["stan"],
      ["statistical-graphics"]
    ],
    "score": 14.9612,
    "stars": 200
  },
  {
    "id": 11375,
    "package_name": "datawizard",
    "title": "Easy Data Wrangling and Statistical Transformations",
    "description": "A lightweight package to assist in key steps involved in\nany data analysis workflow: (1) wrangling the raw data to get\nit in the needed form, (2) applying preprocessing steps and\nstatistical transformations, and (3) compute statistical\nsummaries of data properties and distributions. It is also the\ndata wrangling backend for packages in 'easystats' ecosystem.\nReferences: Patil et al. (2022) <doi:10.21105/joss.04684>.",
    "version": "1.3.0",
    "maintainer": "Etienne Bacher <etienne.bacher@protonmail.com>",
    "url": "https://easystats.github.io/datawizard/",
    "exports": [
      ["adjust"],
      ["as.prop.table"],
      ["assign_labels"],
      ["categorize"],
      ["center"],
      ["centre"],
      ["change_scale"],
      ["coef_var"],
      ["coerce_to_numeric"],
      ["colnames_to_row"],
      ["column_as_rownames"],
      ["contr.deviation"],
      ["convert_na_to"],
      ["convert_to_na"],
      ["data_addprefix"],
      ["data_addsuffix"],
      ["data_adjust"],
      ["data_arrange"],
      ["data_codebook"],
      ["data_duplicated"],
      ["data_extract"],
      ["data_filter"],
      ["data_group"],
      ["data_join"],
      ["data_match"],
      ["data_merge"],
      ["data_modify"],
      ["data_partition"],
      ["data_peek"],
      ["data_read"],
      ["data_relocate"],
      ["data_remove"],
      ["data_rename"],
      ["data_rename_rows"],
      ["data_reorder"],
      ["data_replicate"],
      ["data_restoretype"],
      ["data_rotate"],
      ["data_seek"],
      ["data_select"],
      ["data_separate"],
      ["data_summary"],
      ["data_tabulate"],
      ["data_to_long"],
      ["data_to_wide"],
      ["data_transpose"],
      ["data_ungroup"],
      ["data_unique"],
      ["data_unite"],
      ["data_write"],
      ["degroup"],
      ["demean"],
      ["describe_distribution"],
      ["detrend"],
      ["display"],
      ["distribution_coef_var"],
      ["distribution_mode"],
      ["empty_columns"],
      ["empty_rows"],
      ["extract_column_names"],
      ["find_columns"],
      ["kurtosis"],
      ["labels_to_levels"],
      ["mean_sd"],
      ["means_by_group"],
      ["median_mad"],
      ["normalize"],
      ["print_html"],
      ["print_md"],
      ["ranktransform"],
      ["recode_into"],
      ["recode_values"],
      ["remove_empty"],
      ["remove_empty_columns"],
      ["remove_empty_rows"],
      ["replace_nan_inf"],
      ["rescale"],
      ["rescale_weights"],
      ["reshape_ci"],
      ["reshape_longer"],
      ["reshape_wider"],
      ["reverse"],
      ["reverse_scale"],
      ["row_count"],
      ["row_means"],
      ["row_sums"],
      ["row_to_colnames"],
      ["rowid_as_column"],
      ["rownames_as_column"],
      ["skewness"],
      ["slide"],
      ["smoothness"],
      ["standardise"],
      ["standardize"],
      ["text_concatenate"],
      ["text_format"],
      ["text_fullstop"],
      ["text_lastchar"],
      ["text_paste"],
      ["text_remove"],
      ["text_wrap"],
      ["to_factor"],
      ["to_numeric"],
      ["unnormalize"],
      ["unstandardise"],
      ["unstandardize"],
      ["visualisation_recipe"],
      ["weighted_mad"],
      ["weighted_mean"],
      ["weighted_median"],
      ["weighted_sd"],
      ["winsorize"]
    ],
    "topics": [
      ["data"],
      ["dplyr"],
      ["hacktoberfest"],
      ["janitor"],
      ["manipulation"],
      ["reshape"],
      ["tidyr"],
      ["wrangling"]
    ],
    "score": 14.8074,
    "stars": 232
  },
  {
    "id": 10902,
    "package_name": "countrycode",
    "title": "Convert Country Names and Country Codes",
    "description": "Standardize country names, convert them into one of 40\ndifferent coding schemes, convert between coding schemes, and\nassign region descriptors.",
    "version": "1.6.1",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://vincentarelbundock.github.io/countrycode/",
    "exports": [
      ["countrycode"],
      ["countryname"],
      ["get_dictionary"],
      ["guess_field"]
    ],
    "topics": [],
    "score": 14.677,
    "stars": 358
  },
  {
    "id": 21370,
    "package_name": "report",
    "title": "Automated Reporting of Results and Statistical Models",
    "description": "The aim of the 'report' package is to bridge the gap\nbetween R’s output and the formatted results contained in your\nmanuscript. This package converts statistical models and data\nframes into textual reports suited for publication, ensuring\nstandardization and quality in results reporting.",
    "version": "0.6.2.1",
    "maintainer": "Rémi Thériault <remi.theriault@mail.mcgill.ca>",
    "url": "https://easystats.github.io/report/",
    "exports": [
      ["as.report"],
      ["as.report_effectsize"],
      ["as.report_info"],
      ["as.report_intercept"],
      ["as.report_model"],
      ["as.report_parameters"],
      ["as.report_performance"],
      ["as.report_priors"],
      ["as.report_random"],
      ["as.report_statistics"],
      ["as.report_table"],
      ["as.report_text"],
      ["cite_citation"],
      ["cite_easystats"],
      ["cite_packages"],
      ["clean_citation"],
      ["display"],
      ["format_algorithm"],
      ["format_citation"],
      ["format_formula"],
      ["format_model"],
      ["is.report"],
      ["print_html"],
      ["print_md"],
      ["report"],
      ["report_date"],
      ["report_effectsize"],
      ["report_info"],
      ["report_intercept"],
      ["report_model"],
      ["report_packages"],
      ["report_parameters"],
      ["report_participants"],
      ["report_performance"],
      ["report_priors"],
      ["report_random"],
      ["report_s"],
      ["report_sample"],
      ["report_statistics"],
      ["report_story"],
      ["report_system"],
      ["report_table"],
      ["report_text"]
    ],
    "topics": [
      ["anovas"],
      ["apa"],
      ["automated-report-generation"],
      ["automatic"],
      ["bayesian"],
      ["describe"],
      ["easystats"],
      ["hacktoberfest"],
      ["manuscript"],
      ["models"],
      ["report"],
      ["reporting"],
      ["reports"],
      ["scientific"],
      ["statsmodels"]
    ],
    "score": 14.6385,
    "stars": 710
  },
  {
    "id": 10857,
    "package_name": "correlation",
    "title": "Methods for Correlation Analysis",
    "description": "Lightweight package for computing different kinds of\ncorrelations, such as partial correlations, Bayesian\ncorrelations, multilevel correlations, polychoric correlations,\nbiweight correlations, distance correlations and more. Part of\nthe 'easystats' ecosystem. References: Makowski et al. (2020)\n<doi:10.21105/joss.02306>.",
    "version": "0.8.8.1",
    "maintainer": "Brenton M. Wiernik <brenton@wiernik.org>",
    "url": "https://easystats.github.io/correlation/",
    "exports": [
      ["cor_lower"],
      ["cor_smooth"],
      ["cor_sort"],
      ["cor_test"],
      ["cor_text"],
      ["cor_to_ci"],
      ["cor_to_cov"],
      ["cor_to_p"],
      ["cor_to_pcor"],
      ["cor_to_spcor"],
      ["cormatrix_to_excel"],
      ["correlation"],
      ["display"],
      ["distance_mahalanobis"],
      ["is_positive_definite"],
      ["is.cor"],
      ["is.positive_definite"],
      ["isSquare"],
      ["matrix_inverse"],
      ["pcor_to_cor"],
      ["print_html"],
      ["print_md"],
      ["simulate_simpson"],
      ["standardize_names"],
      ["visualisation_recipe"],
      ["z_fisher"]
    ],
    "topics": [
      ["bayesian"],
      ["bayesian-correlations"],
      ["biserial"],
      ["cor"],
      ["correlation"],
      ["correlation-analysis"],
      ["correlations"],
      ["easystats"],
      ["gamma"],
      ["gaussian-graphical-models"],
      ["hacktoberfest"],
      ["matrix"],
      ["multilevel-correlations"],
      ["outliers"],
      ["partial"],
      ["partial-correlations"],
      ["regression"],
      ["robust"],
      ["spearman"]
    ],
    "score": 14.4934,
    "stars": 447
  },
  {
    "id": 22003,
    "package_name": "rstantools",
    "title": "Tools for Developing R Packages Interfacing with 'Stan'",
    "description": "Provides various tools for developers of R packages\ninterfacing with 'Stan' <https://mc-stan.org>, including\nfunctions to set up the required package structure, S3 generics\nand default methods to unify function naming across\n'Stan'-based R packages, and vignettes with recommendations for\ndevelopers.",
    "version": "2.6.0.9000",
    "maintainer": "Jonah Gabry <jgabry@gmail.com>",
    "url": "https://mc-stan.org/rstantools/, https://discourse.mc-stan.org/",
    "exports": [
      ["bayes_R2"],
      ["init_cpp"],
      ["log_lik"],
      ["loo_epred"],
      ["loo_linpred"],
      ["loo_pit"],
      ["loo_predict"],
      ["loo_predictive_interval"],
      ["loo_R2"],
      ["nsamples"],
      ["posterior_epred"],
      ["posterior_interval"],
      ["posterior_linpred"],
      ["posterior_predict"],
      ["predictive_error"],
      ["predictive_interval"],
      ["prior_summary"],
      ["rstan_config"],
      ["rstan_create_package"],
      ["rstantools_load_code"],
      ["use_rstan"]
    ],
    "topics": [
      ["bayesian-data-analysis"],
      ["bayesian-statistics"],
      ["developer-tools"],
      ["stan"]
    ],
    "score": 14.375,
    "stars": 50
  },
  {
    "id": 24497,
    "package_name": "texreg",
    "title": "Conversion of R Regression Output to LaTeX or HTML Tables",
    "description": "Converts coefficients, standard errors, significance\nstars, and goodness-of-fit statistics of statistical models\ninto LaTeX tables or HTML tables/MS Word documents or to nicely\nformatted screen output for the R console for easy model\ncomparison. A list of several models can be combined in a\nsingle table. The output is highly customizable. New model\ntypes can be easily implemented. Details can be found in\nLeifeld (2013), JStatSoft <doi:10.18637/jss.v055.i08>.)",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 14.2163,
    "stars": 0
  },
  {
    "id": 17349,
    "package_name": "mets",
    "title": "Analysis of Multivariate Event Times",
    "description": "Implementation of various statistical models for\nmultivariate event history data\n<doi:10.1007/s10985-013-9244-x>. Including multivariate\ncumulative incidence models <doi:10.1002/sim.6016>, and\nbivariate random effects probit models (Liability models)\n<doi:10.1016/j.csda.2015.01.014>. Modern methods for survival\nanalysis, including regression modelling (Cox, Fine-Gray,\nGhosh-Lin, Binomial regression) with fast computation of\ninfluence functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 14.125,
    "stars": 0
  },
  {
    "id": 17582,
    "package_name": "mixOmics",
    "title": "Omics Data Integration Project",
    "description": "Multivariate methods are well suited to large omics data\nsets where the number of variables (e.g. genes, proteins,\nmetabolites) is much larger than the number of samples\n(patients, cells, mice). They have the appealing properties of\nreducing the dimension of the data by using instrumental\nvariables (components), which are defined as combinations of\nall variables. Those components are then used to produce useful\ngraphical outputs that enable better understanding of the\nrelationships and correlation structures between the different\ndata sets that are integrated. mixOmics offers a wide range of\nmultivariate methods for the exploration and integration of\nbiological datasets with a particular focus on variable\nselection. The package proposes several sparse multivariate\nmodels we have developed to identify the key variables that are\nhighly correlated, and/or explain the biological outcome of\ninterest. The data that can be analysed with mixOmics may come\nfrom high throughput sequencing technologies, such as omics\ndata (transcriptomics, metabolomics, proteomics, metagenomics\netc) but also beyond the realm of omics (e.g. spectral\nimaging). The methods implemented in mixOmics can also handle\nmissing values without having to delete entire rows with\nmissing data. A non exhaustive list of methods include variants\nof generalised Canonical Correlation Analysis, sparse Partial\nLeast Squares and sparse Discriminant Analysis. Recently we\nimplemented integrative methods to combine multiple data sets:\nN-integration with variants of Generalised Canonical\nCorrelation Analysis and P-integration with variants of\nmulti-group Partial Least Squares.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.9812,
    "stars": 0
  },
  {
    "id": 16464,
    "package_name": "limma",
    "title": "Linear Models for Microarray and Omics Data",
    "description": "Data analysis, linear models and differential expression\nfor omics data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.8852,
    "stars": 0
  },
  {
    "id": 23260,
    "package_name": "smooth",
    "title": "Forecasting Using State Space Models",
    "description": "Functions implementing Single Source of Error state space\nmodels for purposes of time series analysis and forecasting.\nThe package includes ADAM (Svetunkov, 2023,\n<https://openforecast.org/adam/>), Exponential Smoothing\n(Hyndman et al., 2008, <doi: 10.1007/978-3-540-71918-2>),\nSARIMA (Svetunkov & Boylan, 2019 <doi:\n10.1080/00207543.2019.1600764>), Complex Exponential Smoothing\n(Svetunkov & Kourentzes, 2018, <doi:\n10.13140/RG.2.2.24986.29123>), Simple Moving Average (Svetunkov\n& Petropoulos, 2018 <doi: 10.1080/00207543.2017.1380326>) and\nseveral simulation functions. It also allows dealing with\nintermittent demand based on the iETS framework (Svetunkov &\nBoylan, 2019, <doi: 10.13140/RG.2.2.35897.06242>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.8334,
    "stars": 0
  },
  {
    "id": 10488,
    "package_name": "cobalt",
    "title": "Covariate Balance Tables and Plots",
    "description": "Generate balance tables and plots for covariates of groups\npreprocessed through matching, weighting or subclassification,\nfor example, using propensity scores. Includes integration with\n'MatchIt', 'WeightIt', 'MatchThem', 'twang', 'Matching',\n'optmatch', 'CBPS', 'ebal', 'cem', 'sbw', and 'designmatch' for\nassessing balance on the output of their preprocessing\nfunctions. Users can also specify data for balance assessment\nnot generated through the above packages. Also included are\nmethods for assessing balance in clustered or multiply imputed\ndata sets or data sets with multi-category, continuous, or\nlongitudinal treatments.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.6193,
    "stars": 0
  },
  {
    "id": 17783,
    "package_name": "modelsummary",
    "title": "Summary Tables and Plots for Statistical Models and Data:\nBeautiful, Customizable, and Publication-Ready",
    "description": "Create beautiful and customizable tables to summarize\nseveral statistical models side-by-side. Draw coefficient\nplots, multi-level cross-tabs, dataset summaries, balance\ntables (a.k.a. \"Table 1s\"), and correlation matrices. This\npackage supports dozens of statistical models, and it can\nproduce tables in HTML, LaTeX, Word, Markdown, PDF, PowerPoint,\nExcel, RTF, JPG, or PNG. Tables can easily be embedded in\n'Rmarkdown' or 'knitr' dynamic documents. Details can be found\nin Arel-Bundock (2022) <doi:10.18637/jss.v103.i01>.",
    "version": "2.5.0.4",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://modelsummary.com",
    "exports": [
      ["All"],
      ["AllObs"],
      ["Arguments"],
      ["coef_rename"],
      ["colLabels"],
      ["config_modelsummary"],
      ["datasummary"],
      ["datasummary_balance"],
      ["datasummary_correlation"],
      ["datasummary_correlation_format"],
      ["datasummary_crosstab"],
      ["datasummary_df"],
      ["datasummary_skim"],
      ["DropEmpty"],
      ["dsummary"],
      ["dvnames"],
      ["Factor"],
      ["fmt_decimal"],
      ["fmt_equivalence"],
      ["fmt_sci"],
      ["fmt_significant"],
      ["fmt_sprintf"],
      ["fmt_statistic"],
      ["fmt_term"],
      ["Format"],
      ["get_estimates"],
      ["get_gof"],
      ["glance"],
      ["glance_custom"],
      ["gof_map"],
      ["Heading"],
      ["Histogram"],
      ["labelSubset"],
      ["Max"],
      ["Mean"],
      ["Median"],
      ["Min"],
      ["modelplot"],
      ["modelsummary"],
      ["msummary"],
      ["Multicolumn"],
      ["N"],
      ["Ncol"],
      ["NPercent"],
      ["NUnique"],
      ["P0"],
      ["P100"],
      ["P25"],
      ["P50"],
      ["P75"],
      ["Paste"],
      ["Percent"],
      ["PercentMissing"],
      ["PlusMinus"],
      ["RowFactor"],
      ["rowLabels"],
      ["RowNum"],
      ["SD"],
      ["supported_models"],
      ["tidy"],
      ["tidy_custom"],
      ["update_modelsummary"],
      ["Var"]
    ],
    "topics": [],
    "score": 13.5012,
    "stars": 945
  },
  {
    "id": 22561,
    "package_name": "see",
    "title": "Model Visualisation Toolbox for 'easystats' and 'ggplot2'",
    "description": "Provides plotting utilities supporting packages in the\n'easystats' ecosystem\n(<https://github.com/easystats/easystats>) and some extra\nthemes, geoms, and scales for 'ggplot2'. Color scales are based\non <https://materialui.co/>. References: Lüdecke et al. (2021)\n<doi:10.21105/joss.03393>.",
    "version": "0.12.0.1",
    "maintainer": "Indrajeet Patil <patilindrajeet.science@gmail.com>",
    "url": "https://easystats.github.io/see/",
    "exports": [
      ["add_plot_attributes"],
      ["bluebrown_colors"],
      ["coord_radar"],
      ["data_plot"],
      ["flat_colors"],
      ["geom_binomdensity"],
      ["geom_count_borderless"],
      ["geom_count2"],
      ["geom_from_list"],
      ["geom_jitter_borderless"],
      ["geom_jitter2"],
      ["geom_point_borderless"],
      ["geom_point2"],
      ["geom_pointrange_borderless"],
      ["geom_pointrange2"],
      ["geom_pooljitter"],
      ["geom_poolpoint"],
      ["geom_violindot"],
      ["geom_violinhalf"],
      ["geoms_from_list"],
      ["golden_ratio"],
      ["material_colors"],
      ["metro_colors"],
      ["oi_colors"],
      ["okabeito_colors"],
      ["palette_bluebrown"],
      ["palette_colorhex"],
      ["palette_flat"],
      ["palette_material"],
      ["palette_metro"],
      ["palette_oi"],
      ["palette_okabeito"],
      ["palette_pizza"],
      ["palette_see"],
      ["palette_social"],
      ["pizza_colors"],
      ["plots"],
      ["scale_color_bluebrown"],
      ["scale_color_bluebrown_c"],
      ["scale_color_bluebrown_d"],
      ["scale_color_colorhex"],
      ["scale_color_colorhex_c"],
      ["scale_color_colorhex_d"],
      ["scale_color_flat"],
      ["scale_color_flat_c"],
      ["scale_color_flat_d"],
      ["scale_color_material"],
      ["scale_color_material_c"],
      ["scale_color_material_d"],
      ["scale_color_metro"],
      ["scale_color_metro_c"],
      ["scale_color_metro_d"],
      ["scale_color_oi"],
      ["scale_color_okabeito"],
      ["scale_color_pizza"],
      ["scale_color_pizza_c"],
      ["scale_color_pizza_d"],
      ["scale_color_see"],
      ["scale_color_see_c"],
      ["scale_color_see_d"],
      ["scale_color_social"],
      ["scale_color_social_c"],
      ["scale_color_social_d"],
      ["scale_colour_bluebrown"],
      ["scale_colour_bluebrown_c"],
      ["scale_colour_bluebrown_d"],
      ["scale_colour_colorhex"],
      ["scale_colour_colorhex_c"],
      ["scale_colour_colorhex_d"],
      ["scale_colour_flat"],
      ["scale_colour_flat_c"],
      ["scale_colour_flat_d"],
      ["scale_colour_material"],
      ["scale_colour_material_c"],
      ["scale_colour_material_d"],
      ["scale_colour_metro"],
      ["scale_colour_metro_c"],
      ["scale_colour_metro_d"],
      ["scale_colour_oi"],
      ["scale_colour_okabeito"],
      ["scale_colour_pizza"],
      ["scale_colour_pizza_c"],
      ["scale_colour_pizza_d"],
      ["scale_colour_see"],
      ["scale_colour_see_c"],
      ["scale_colour_see_d"],
      ["scale_colour_social"],
      ["scale_colour_social_c"],
      ["scale_colour_social_d"],
      ["scale_fill_bluebrown"],
      ["scale_fill_bluebrown_c"],
      ["scale_fill_bluebrown_d"],
      ["scale_fill_colorhex"],
      ["scale_fill_colorhex_c"],
      ["scale_fill_colorhex_d"],
      ["scale_fill_flat"],
      ["scale_fill_flat_c"],
      ["scale_fill_flat_d"],
      ["scale_fill_material"],
      ["scale_fill_material_c"],
      ["scale_fill_material_d"],
      ["scale_fill_metro"],
      ["scale_fill_metro_c"],
      ["scale_fill_metro_d"],
      ["scale_fill_oi"],
      ["scale_fill_okabeito"],
      ["scale_fill_pizza"],
      ["scale_fill_pizza_c"],
      ["scale_fill_pizza_d"],
      ["scale_fill_see"],
      ["scale_fill_see_c"],
      ["scale_fill_see_d"],
      ["scale_fill_social"],
      ["scale_fill_social_c"],
      ["scale_fill_social_d"],
      ["see_colors"],
      ["social_colors"],
      ["theme_abyss"],
      ["theme_azurelight"],
      ["theme_blackboard"],
      ["theme_lucid"],
      ["theme_modern"],
      ["theme_radar"],
      ["theme_radar_dark"]
    ],
    "topics": [
      ["data-visualization"],
      ["easystats"],
      ["ggplot2"],
      ["hacktoberfest"],
      ["plotting"],
      ["see"],
      ["statistics"],
      ["visualisation"],
      ["visualization"]
    ],
    "score": 13.495,
    "stars": 937
  },
  {
    "id": 12315,
    "package_name": "edgeR",
    "title": "Empirical Analysis of Digital Gene Expression Data in R",
    "description": "Differential expression analysis of sequence count data.\nImplements a range of statistical methodology based on the\nnegative binomial distributions, including empirical Bayes\nestimation, exact tests, generalized linear models,\nquasi-likelihood, and gene set enrichment. Can perform\ndifferential analyses of any type of omics data that produces\nread counts, including RNA-seq, ChIP-seq, ATAC-seq,\nBisulfite-seq, SAGE, CAGE, metabolomics, or proteomics spectral\ncounts. RNA-seq analyses can be conducted at the gene or\nisoform level, and tests can be conducted for differential exon\nor transcript usage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.3067,
    "stars": 0
  },
  {
    "id": 19410,
    "package_name": "pcaMethods",
    "title": "A collection of PCA methods",
    "description": "Provides Bayesian PCA, Probabilistic PCA, Nipals PCA,\nInverse Non-Linear PCA and the conventional SVD PCA. A cluster\nbased method for missing value estimation is included for\ncomparison. BPCA, PPCA and NipalsPCA may be used to perform PCA\non incomplete data as well as for accurate missing value\nestimation. A set of methods for printing and plotting the\nresults is also provided. All PCA methods make use of the same\ndata structure (pcaRes) to provide a common interface to the\nPCA results. Initiated at the Max-Planck Institute for\nMolecular Plant Physiology, Golm, Germany.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 13.212,
    "stars": 0
  },
  {
    "id": 17772,
    "package_name": "modelbased",
    "title": "Estimation of Model-Based Predictions, Contrasts and Means",
    "description": "Implements a general interface for model-based estimations\nfor a wide variety of models, used in the computation of\nmarginal means, contrast analysis and predictions. For a list\nof supported models, see 'insight::supported_models()'.",
    "version": "0.13.1",
    "maintainer": "Dominique Makowski <officialeasystats@gmail.com>",
    "url": "https://easystats.github.io/modelbased/",
    "exports": [
      ["describe_nonlinear"],
      ["display"],
      ["estimate_contrasts"],
      ["estimate_expectation"],
      ["estimate_grouplevel"],
      ["estimate_link"],
      ["estimate_means"],
      ["estimate_prediction"],
      ["estimate_relation"],
      ["estimate_slopes"],
      ["estimate_smooth"],
      ["find_inversions"],
      ["get_emcontrasts"],
      ["get_emmeans"],
      ["get_emtrends"],
      ["get_marginalcontrasts"],
      ["get_marginalmeans"],
      ["get_marginaltrends"],
      ["pool_contrasts"],
      ["pool_predictions"],
      ["pool_slopes"],
      ["print_html"],
      ["print_md"],
      ["reshape_grouplevel"],
      ["residualize_over_grid"],
      ["smoothing"],
      ["standardize"],
      ["unstandardize"],
      ["visualisation_recipe"],
      ["zero_crossings"]
    ],
    "topics": [
      ["contrast-analysis"],
      ["contrasts"],
      ["easystats"],
      ["estimate"],
      ["ggplot2"],
      ["hacktoberfest"],
      ["marginal"],
      ["marginal-effects"],
      ["means"],
      ["predict"]
    ],
    "score": 13.1094,
    "stars": 257
  },
  {
    "id": 12212,
    "package_name": "easystats",
    "title": "Framework for Easy Statistical Modeling, Visualization, and\nReporting",
    "description": "A meta-package that installs and loads a set of packages\nfrom 'easystats' ecosystem in a single step. This collection of\npackages provide a unifying and consistent framework for\nstatistical modeling, visualization, and reporting.\nAdditionally, it provides articles targeted at instructors for\nteaching 'easystats', and a dashboard targeted at new R users\nfor easily conducting statistical analysis by accessing summary\nresults, model fit indices, and visualizations with minimal\nprogramming.",
    "version": "0.7.5.2",
    "maintainer": "Daniel Lüdecke <d.luedecke@uke.de>",
    "url": "https://easystats.github.io/easystats/",
    "exports": [
      ["display"],
      ["easystats_citations"],
      ["easystats_downloads"],
      ["easystats_packages"],
      ["easystats_update"],
      ["easystats_zen"],
      ["install_latest"],
      ["install_suggested"],
      ["model_dashboard"],
      ["print_html"],
      ["print_md"],
      ["show_reverse_dependencies"],
      ["show_suggested"]
    ],
    "topics": [
      ["dataanalytics"],
      ["datascience"],
      ["easystats"],
      ["hacktoberfest"],
      ["models"],
      ["performance-metrics"],
      ["regression-models"],
      ["statistics"]
    ],
    "score": 13.0522,
    "stars": 1149
  },
  {
    "id": 24784,
    "package_name": "tinytable",
    "title": "Simple and Configurable Tables in 'HTML', 'LaTeX', 'Markdown',\n'Word', 'PNG', 'PDF', and 'Typst' Formats",
    "description": "Create highly customized tables with this simple and\ndependency-free package. Data frames can be converted to\n'HTML', 'LaTeX', 'Markdown', 'Word', 'PNG', 'PDF', or 'Typst'\ntables. The user interface is minimalist and easy to learn. The\nsyntax is concise. 'HTML' tables can be customized using the\nflexible 'Bootstrap' framework, and 'LaTeX' code with the\n'tabularray' package.",
    "version": "0.15.2.4",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://vincentarelbundock.github.io/tinytable/",
    "exports": [
      ["colnames"],
      ["colnames<-"],
      ["format_tt"],
      ["format_vector"],
      ["group_tt"],
      ["knit_print.tinytable"],
      ["pkgdown_print.tinytable"],
      ["plot_tt"],
      ["plot_vector"],
      ["rbind2"],
      ["save_tt"],
      ["style_tt"],
      ["style_vector"],
      ["theme_default"],
      ["theme_empty"],
      ["theme_grid"],
      ["theme_html"],
      ["theme_latex"],
      ["theme_markdown"],
      ["theme_revealjs"],
      ["theme_rotate"],
      ["theme_striped"],
      ["theme_tinytable"],
      ["theme_tt"],
      ["theme_typst"],
      ["tt"],
      ["tt_format"],
      ["tt_group"],
      ["tt_plot"],
      ["tt_save"],
      ["tt_style"]
    ],
    "topics": [],
    "score": 12.9383,
    "stars": 319
  },
  {
    "id": 14126,
    "package_name": "ggdag",
    "title": "Analyze and Create Elegant Directed Acyclic Graphs",
    "description": "Tidy, analyze, and plot directed acyclic graphs (DAGs).\n'ggdag' is built on top of 'dagitty', an R package that uses\nthe 'DAGitty' web tool (<https://dagitty.net/>) for creating\nand analyzing DAGs. 'ggdag' makes it easy to tidy and plot\n'dagitty' objects using 'ggplot2' and 'ggraph', as well as\ncommon analytic and graphical functions, such as determining\nadjustment sets and node relationships.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.8901,
    "stars": 0
  },
  {
    "id": 18851,
    "package_name": "olsrr",
    "title": "Tools for Building OLS Regression Models",
    "description": "Tools designed to make it easier for users, particularly\nbeginner/intermediate R users to build ordinary least squares\nregression models. Includes comprehensive regression output,\nheteroskedasticity tests, collinearity diagnostics, residual\ndiagnostics, measures of influence, model fit assessment and\nvariable selection procedures.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.5152,
    "stars": 0
  },
  {
    "id": 15537,
    "package_name": "imputeTS",
    "title": "Time Series Missing Value Imputation",
    "description": "Imputation (replacement) of missing values in univariate\ntime series. Offers several imputation functions and missing\ndata plots. Available imputation algorithms include: 'Mean',\n'LOCF', 'Interpolation', 'Moving Average', 'Seasonal\nDecomposition', 'Kalman Smoothing on Structural Time Series\nmodels', 'Kalman Smoothing on ARIMA models'. Published in\nMoritz and Bartz-Beielstein (2017) <doi:10.32614/RJ-2017-009>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.4819,
    "stars": 0
  },
  {
    "id": 17671,
    "package_name": "mlr3learners",
    "title": "Recommended Learners for 'mlr3'",
    "description": "Recommended Learners for 'mlr3'. Extends 'mlr3' with\ninterfaces to essential machine learning packages on CRAN.\nThis includes, but is not limited to: (penalized) linear and\nlogistic regression, linear and quadratic discriminant\nanalysis, k-nearest neighbors, naive Bayes, support vector\nmachines, and gradient boosting.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.2782,
    "stars": 0
  },
  {
    "id": 10445,
    "package_name": "cmdstanr",
    "title": "R Interface to 'CmdStan'",
    "description": "A lightweight interface to 'Stan' <https://mc-stan.org>.\nThe 'CmdStanR' interface is an alternative to 'RStan' that\ncalls the command line interface for compilation and running\nalgorithms instead of interfacing with C++ via 'Rcpp'. This has\nmany benefits including always being compatible with the latest\nversion of Stan, fewer installation errors, fewer unexpected\ncrashes in RStudio, and a more permissive license.",
    "version": "0.9.0",
    "maintainer": "Andrew Johnson <andrew.johnson@arjohnsonau.com>",
    "url": "https://mc-stan.org/cmdstanr/, https://discourse.mc-stan.org",
    "exports": [
      ["as_cmdstan_fit"],
      ["as_draws"],
      ["as_mcmc.list"],
      ["as.CmdStanDiagnose"],
      ["as.CmdStanGQ"],
      ["as.CmdStanLaplace"],
      ["as.CmdStanMCMC"],
      ["as.CmdStanMLE"],
      ["as.CmdStanPathfinder"],
      ["as.CmdStanVB"],
      ["check_cmdstan_toolchain"],
      ["cmdstan_default_install_path"],
      ["cmdstan_default_path"],
      ["cmdstan_make_local"],
      ["cmdstan_model"],
      ["cmdstan_path"],
      ["cmdstan_version"],
      ["cmdstanr_example"],
      ["draws_to_csv"],
      ["eng_cmdstan"],
      ["install_cmdstan"],
      ["num_threads"],
      ["print_example_program"],
      ["read_cmdstan_csv"],
      ["read_sample_csv"],
      ["rebuild_cmdstan"],
      ["register_knitr_engine"],
      ["set_cmdstan_path"],
      ["set_num_threads"],
      ["write_stan_file"],
      ["write_stan_json"],
      ["write_stan_tempfile"]
    ],
    "topics": [
      ["bayes"],
      ["bayesian"],
      ["markov-chain-monte-carlo"],
      ["maximum-likelihood"],
      ["mcmc"],
      ["stan"],
      ["variational-inference"]
    ],
    "score": 12.2642,
    "stars": 155
  },
  {
    "id": 9651,
    "package_name": "brglm2",
    "title": "Bias Reduction in Generalized Linear Models",
    "description": "Estimation and inference from generalized linear models\nbased on various methods for bias reduction and maximum\npenalized likelihood with powers of the Jeffreys prior as\npenalty. The 'brglmFit()' fitting method can achieve reduction\nof estimation bias by solving either the mean bias-reducing\nadjusted score equations in Firth (1993)\n<doi:10.1093/biomet/80.1.27> and Kosmidis and Firth (2009)\n<doi:10.1093/biomet/asp055>, or the median bias-reducing\nadjusted score equations in Kenne et al. (2017)\n<doi:10.1093/biomet/asx046>, or through the direct subtraction\nof an estimate of the bias of the maximum likelihood estimator\nfrom the maximum likelihood estimates as in Cordeiro and\nMcCullagh (1991) <https://www.jstor.org/stable/2345592>. See\nKosmidis et al (2020) <doi:10.1007/s11222-019-09860-6> for more\ndetails. Estimation in all cases takes place via a quasi Fisher\nscoring algorithm, and S3 methods for the construction of of\nconfidence intervals for the reduced-bias estimates are\nprovided. In the special case of generalized linear models for\nbinomial and multinomial responses (both ordinal and nominal),\nthe adjusted score approaches to mean and media bias reduction\nhave been found to return estimates with improved frequentist\nproperties, that are also always finite, even in cases where\nthe maximum likelihood estimates are infinite (e.g. complete\nand quasi-complete separation; see Kosmidis and Firth, 2020\n<doi:10.1093/biomet/asaa052>, for a proof for mean bias\nreduction in logistic regression). The 'mdyplFit()' fitting\nmethod fits logistic regression models using maximum\nDiaconis-Ylvisaker prior penalized likelihood, which also\nguarantees finite estimates. High-dimensionality corrections\nunder proportional asymptotics can be applied to the resulting\nobjects; see Sterzinger and Kosmidis (2024)\n<doi:10.48550/arXiv.2311.07419> for details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.2122,
    "stars": 0
  },
  {
    "id": 14410,
    "package_name": "glmGamPoi",
    "title": "Fit a Gamma-Poisson Generalized Linear Model",
    "description": "Fit linear models to overdispersed count data. The package\ncan estimate the overdispersion and fit repeated models for\nmatrix input. It is designed to handle large input datasets as\nthey typically occur in single cell RNA-seq experiments.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.174,
    "stars": 0
  },
  {
    "id": 25475,
    "package_name": "variancePartition",
    "title": "Quantify and interpret drivers of variation in multilevel gene\nexpression experiments",
    "description": "Quantify and interpret multiple sources of biological and\ntechnical variation in gene expression experiments. Uses a\nlinear mixed model to quantify variation in gene expression\nattributable to individual, tissue, time point, or technical\nvariables.  Includes dream differential expression analysis for\nrepeated measures.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 12.1627,
    "stars": 0
  },
  {
    "id": 8031,
    "package_name": "WeightIt",
    "title": "Weighting for Covariate Balance in Observational Studies",
    "description": "Generates balancing weights for causal effect estimation\nin observational studies with binary, multi-category, or\ncontinuous point or longitudinal treatments by easing and\nextending the functionality of several R packages and providing\nin-house estimation methods. Available methods include those\nthat rely on parametric modeling, optimization, and machine\nlearning. Also allows for assessment of weights and checking of\ncovariate balance by interfacing directly with the 'cobalt'\npackage. Methods for estimating weighted regression models that\ntake into account uncertainty in the estimation of the weights\nvia M-estimation or bootstrapping are available. See the\nvignette \"Installing Supporting Packages\" for instructions on\nhow to install any package 'WeightIt' uses, including those\nthat may not be on CRAN.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.9946,
    "stars": 0
  },
  {
    "id": 12075,
    "package_name": "dtwclust",
    "title": "Time Series Clustering Along with Optimizations for the Dynamic\nTime Warping Distance",
    "description": "Time series clustering along with optimized techniques\nrelated to the Dynamic Time Warping distance and its\ncorresponding lower bounds. Implementations of partitional,\nhierarchical, fuzzy, k-Shape and TADPole clustering are\navailable. Functionality can be easily extended with custom\ndistance measures and centroid definitions. Implementations of\nDTW barycenter averaging, a distance based on global alignment\nkernels, and the soft-DTW distance and centroid routines are\nalso provided. All included distance functions have custom\nloops optimized for the calculation of cross-distance matrices,\nincluding parallelization support. Several cluster validity\nindices are included.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.9453,
    "stars": 0
  },
  {
    "id": 19990,
    "package_name": "pomp",
    "title": "Statistical Inference for Partially Observed Markov Processes",
    "description": "Tools for data analysis with partially observed Markov\nprocess (POMP) models (also known as stochastic dynamical\nsystems, hidden Markov models, and nonlinear, non-Gaussian,\nstate-space models).  The package provides facilities for\nimplementing POMP models, simulating them, and fitting them to\ntime series data by a variety of frequentist and Bayesian\nmethods.  It is also a versatile platform for implementation of\ninference methods for general POMP models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.8057,
    "stars": 0
  },
  {
    "id": 3531,
    "package_name": "KFAS",
    "title": "Kalman Filter and Smoother for Exponential Family State Space\nModels",
    "description": "State space modelling is an efficient and flexible\nframework for statistical inference of a broad class of time\nseries and other data. KFAS includes computationally efficient\nfunctions for Kalman filtering, smoothing, forecasting, and\nsimulation of multivariate exponential family state space\nmodels, with observations from Gaussian, Poisson, binomial,\nnegative binomial, and gamma distributions. See the paper by\nHelske (2017) <doi:10.18637/jss.v078.i10> for details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.7429,
    "stars": 0
  },
  {
    "id": 25123,
    "package_name": "tsfeatures",
    "title": "Time Series Feature Extraction",
    "description": "Methods for extracting various features from time series\ndata. The features provided are those from Hyndman, Wang and\nLaptev (2013) <doi:10.1109/ICDMW.2015.104>, Kang, Hyndman and\nSmith-Miles (2017) <doi:10.1016/j.ijforecast.2016.09.004> and\nfrom Fulcher, Little and Jones (2013)\n<doi:10.1098/rsif.2013.0048>. Features include spectral\nentropy, autocorrelations, measures of the strength of\nseasonality and trend, and so on. Users can also define their\nown feature functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.5834,
    "stars": 0
  },
  {
    "id": 4090,
    "package_name": "MOFA2",
    "title": "Multi-Omics Factor Analysis v2",
    "description": "The MOFA2 package contains a collection of tools for\ntraining and analysing multi-omic factor analysis (MOFA). MOFA\nis a probabilistic factor model that aims to identify principal\naxes of variation from data sets that can comprise multiple\nomic layers and/or groups of samples. Additional time or space\ninformation on the samples can be incorporated using the\nMEFISTO framework, which is part of MOFA2. Downstream analysis\nfunctions to inspect molecular features underlying each factor,\nvizualisation, imputation etc are available.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.2301,
    "stars": 0
  },
  {
    "id": 22542,
    "package_name": "seasonal",
    "title": "R Interface to X-13-ARIMA-SEATS",
    "description": "Easy-to-use interface to X-13-ARIMA-SEATS, the seasonal\nadjustment software by the US Census Bureau. It offers full\naccess to almost all options and outputs of X-13, including\nX-11 and SEATS, automatic ARIMA model search, outlier detection\nand support for user defined holiday variables, such as Chinese\nNew Year or Indian Diwali. A graphical user interface can be\nused through the 'seasonalview' package. Uses the X-13-binaries\nfrom the 'x13binary' package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.2227,
    "stars": 0
  },
  {
    "id": 20284,
    "package_name": "projpred",
    "title": "Projection Predictive Feature Selection",
    "description": "Performs projection predictive feature selection for\ngeneralized linear models (Piironen, Paasiniemi, and Vehtari,\n2020, <doi:10.1214/20-EJS1711>) with or without multilevel or\nadditive terms (Catalina, Bürkner, and Vehtari, 2022,\n<https://proceedings.mlr.press/v151/catalina22a.html>), for\nsome ordinal and nominal regression models (Weber, Glass, and\nVehtari, 2025, <doi:10.1007/s00180-024-01506-0>), and for many\nother regression models (using the latent projection by\nCatalina, Bürkner, and Vehtari, 2021,\n<doi:10.48550/arXiv.2109.04702>, which can also be applied to\nmost of the former models). The package is compatible with the\n'rstanarm' and 'brms' packages, but other reference models can\nalso be used. See the vignettes and the documentation for more\ninformation and examples.",
    "version": "2.10.0.9000",
    "maintainer": "Osvaldo Martin <aloctavodia@gmail.com>",
    "url": "https://mc-stan.org/projpred/, https://discourse.mc-stan.org",
    "exports": [
      ["augdat_ilink_binom"],
      ["augdat_link_binom"],
      ["break_up_matrix_term"],
      ["cl_agg"],
      ["cv_folds"],
      ["cv_ids"],
      ["cv_proportions"],
      ["cv_varsel"],
      ["cvfolds"],
      ["do_call"],
      ["extend_family"],
      ["force_search_terms"],
      ["get_refmodel"],
      ["init_refmodel"],
      ["performances"],
      ["predictor_terms"],
      ["proj_linpred"],
      ["proj_predict"],
      ["project"],
      ["ranking"],
      ["run_cvfun"],
      ["solution_terms"],
      ["Student_t"],
      ["suggest_size"],
      ["varsel"],
      ["y_wobs_offs"]
    ],
    "topics": [
      ["bayes"],
      ["bayesian"],
      ["bayesian-inference"],
      ["rstanarm"],
      ["stan"],
      ["statistics"],
      ["variable-selection"],
      ["openblas"],
      ["cpp"]
    ],
    "score": 11.1603,
    "stars": 113
  },
  {
    "id": 4874,
    "package_name": "OpenML",
    "title": "Open Machine Learning and Open Data Platform",
    "description": "We provide an R interface to 'OpenML.org' which is an\nonline machine learning platform where researchers can access\nopen data, download and upload data sets, share their machine\nlearning tasks and experiments and organize them online to work\nand collaborate with other researchers. The R interface allows\nto query for data sets with specific properties, and allows the\ndownloading and uploading of data sets, tasks, flows and runs.\nSee <https://www.openml.org/guide/api> for more information.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.1486,
    "stars": 0
  },
  {
    "id": 15579,
    "package_name": "infercnv",
    "title": "Infer Copy Number Variation from Single-Cell RNA-Seq Data",
    "description": "Using single-cell RNA-Seq expression to visualize CNV in\ncells.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.0613,
    "stars": 0
  },
  {
    "id": 4616,
    "package_name": "NNS",
    "title": "Nonlinear Nonparametric Statistics",
    "description": "NNS (Nonlinear Nonparametric Statistics) leverages partial\nmoments – the fundamental elements of variance that\nasymptotically approximate the area under f(x) – to provide a\nrobust foundation for nonlinear analysis while maintaining\nlinear equivalences.  NNS delivers a comprehensive suite of\nadvanced statistical techniques, including: Numerical\nintegration, Numerical differentiation, Clustering,\nCorrelation, Dependence, Causal analysis, ANOVA, Regression,\nClassification, Seasonality, Autoregressive modeling,\nNormalization, Stochastic dominance and Advanced Monte Carlo\nsampling.  All routines based on: Viole, F. and Nawrocki, D.\n(2013), Nonlinear Nonparametric Statistics: Using Partial\nMoments (ISBN: 1490523995).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.0388,
    "stars": 0
  },
  {
    "id": 3879,
    "package_name": "MARSS",
    "title": "Multivariate Autoregressive State-Space Modeling",
    "description": "The MARSS package provides maximum-likelihood parameter\nestimation for constrained and unconstrained linear\nmultivariate autoregressive state-space (MARSS) models,\nincluding partially deterministic models. MARSS models are a\nclass of dynamic linear model (DLM) and vector autoregressive\nmodel (VAR) model. Fitting available via\nExpectation-Maximization (EM), BFGS (using optim), and 'TMB'\n(using the 'marssTMB' companion package). Functions are\nprovided for parametric and innovations bootstrapping, Kalman\nfiltering and smoothing, model selection criteria including\nbootstrap AICb, confidences intervals via the Hessian\napproximation or bootstrapping, and all conditional residual\ntypes. See the user guide for examples of dynamic factor\nanalysis, dynamic linear models, outlier and shock detection,\nand multivariate AR-p models. Online workshops (lectures,\neBook, and computer labs) at <https://atsa-es.github.io/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 11.0282,
    "stars": 0
  },
  {
    "id": 19338,
    "package_name": "pastecs",
    "title": "Package for Analysis of Space-Time Ecological Series",
    "description": "Regularisation, decomposition and analysis of space-time\nseries. The pastecs R package is a PNEC-Art4 and IFREMER\n(Benoit Beliaeff <Benoit.Beliaeff@ifremer.fr>) initiative to\nbring PASSTEC 2000 functionalities to R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.9365,
    "stars": 0
  },
  {
    "id": 74,
    "package_name": "ALDEx2",
    "title": "Analysis Of Differential Abundance Taking Sample and Scale\nVariation Into Account",
    "description": "A differential abundance analysis for the comparison of\ntwo or more conditions. Useful for analyzing data from standard\nRNA-seq or meta-RNA-seq assays as well as selected and\nunselected values from in-vitro sequence selections. Uses a\nDirichlet-multinomial model to infer abundance from counts,\noptimized for three or more experimental replicates. The method\ninfers biological and sampling variation to calculate the\nexpected false discovery rate, given the variation, based on a\nWilcoxon Rank Sum test and Welch's t-test (via aldex.ttest), a\nKruskal-Wallis test (via aldex.kw), a generalized linear model\n(via aldex.glm), or a correlation test (via aldex.corr). All\ntests report predicted p-values and posterior\nBenjamini-Hochberg corrected p-values. ALDEx2 also calculates\nexpected standardized effect sizes for paired or unpaired study\ndesigns. ALDEx2 can now be used to estimate the effect of scale\non the results and report on the scale-dependent robustness of\nresults.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.9223,
    "stars": 0
  },
  {
    "id": 20223,
    "package_name": "priorsense",
    "title": "Prior Diagnostics and Sensitivity Analysis",
    "description": "Provides functions for prior and likelihood sensitivity\nanalysis in Bayesian models. Currently it implements methods to\ndetermine the sensitivity of the posterior to power-scaling\nperturbations of the prior and likelihood.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.7666,
    "stars": 0
  },
  {
    "id": 372,
    "package_name": "BAS",
    "title": "Bayesian Variable Selection and Model Averaging using Bayesian\nAdaptive Sampling",
    "description": "Package for Bayesian Variable Selection and Model\nAveraging in linear models and generalized linear models using\nstochastic or deterministic sampling without replacement from\nposterior distributions.  Prior distributions on coefficients\nare from Zellner's g-prior or mixtures of g-priors\ncorresponding to the Zellner-Siow Cauchy Priors or the mixture\nof g-priors from Liang et al (2008)\n<DOI:10.1198/016214507000001337> for linear models or mixtures\nof g-priors from Li and Clyde (2019)\n<DOI:10.1080/01621459.2018.1469992> in generalized linear\nmodels. Other model selection criteria include AIC, BIC and\nEmpirical Bayes estimates of g. Sampling probabilities may be\nupdated based on the sampled models using sampling w/out\nreplacement or an efficient MCMC algorithm which samples models\nusing a tree structure of the model space as an efficient hash\ntable.  See Clyde, Ghosh and Littman (2010)\n<DOI:10.1198/jcgs.2010.09049> for details on the sampling\nalgorithms. Uniform priors over all models or beta-binomial\nprior distributions on model size are allowed, and for large p\ntruncated priors on the model space may be used to enforce\nsampling models that are full rank. The user may force\nvariables to always be included in addition to imposing\nconstraints that higher order interactions are included only if\ntheir parents are included in the model. This material is based\nupon work supported by the National Science Foundation under\nDivision of Mathematical Sciences grant 1106891. Any opinions,\nfindings, and conclusions or recommendations expressed in this\nmaterial are those of the author(s) and do not necessarily\nreflect the views of the National Science Foundation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.7545,
    "stars": 0
  },
  {
    "id": 20390,
    "package_name": "psycho",
    "title": "Efficient and Publishing-Oriented Workflow for Psychological\nScience",
    "description": "The main goal of the psycho package is to provide tools\nfor psychologists, neuropsychologists and neuroscientists, to\nfacilitate and speed up the time spent on data analysis. It\naims at supporting best practices and tools to format the\noutput of statistical methods to directly paste them into a\nmanuscript, ensuring statistical reporting standardization and\nconformity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.6102,
    "stars": 0
  },
  {
    "id": 24942,
    "package_name": "tradeSeq",
    "title": "trajectory-based differential expression analysis for sequencing\ndata",
    "description": "tradeSeq provides a flexible method for fitting regression\nmodels that can be used to find genes that are differentially\nexpressed along one or multiple lineages in a trajectory. Based\non the fitted models, it uses a variety of tests suited to\nanswer different questions of interest, e.g. the discovery of\ngenes for which expression is associated with pseudotime, or\nwhich are differentially expressed (in a specific region) along\nthe trajectory. It fits a negative binomial generalized\nadditive model (GAM) for each gene, and performs inference on\nthe parameters of the GAM.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.5022,
    "stars": 0
  },
  {
    "id": 10038,
    "package_name": "celda",
    "title": "CEllular Latent Dirichlet Allocation",
    "description": "Celda is a suite of Bayesian hierarchical models for\nclustering single-cell RNA-sequencing (scRNA-seq) data. It is\nable to perform \"bi-clustering\" and simultaneously cluster\ngenes into gene modules and cells into cell subpopulations. It\nalso contains DecontX, a novel Bayesian method to\ncomputationally estimate and remove RNA contamination in\nindividual cells without empty droplet information. A variety\nof scRNA-seq data visualization functions is also included.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.4886,
    "stars": 0
  },
  {
    "id": 19237,
    "package_name": "pammtools",
    "title": "Piece-Wise Exponential Additive Mixed Modeling Tools for\nSurvival Analysis",
    "description": "The Piece-wise exponential (Additive Mixed) Model (PAMM;\nBender and others (2018) <doi: 10.1177/1471082X17748083>) is a\npowerful model class for the analysis of survival (or\ntime-to-event) data, based on Generalized Additive (Mixed)\nModels (GA(M)Ms). It offers intuitive specification and robust\nestimation of complex survival models with stratified baseline\nhazards, random effects, time-varying effects, time-dependent\ncovariates and cumulative effects (Bender and others (2019)),\nas well as support for left-truncated data as well as competing\nrisks, recurrent events and multi-state settings. pammtools\nprovides tidy workflow for survival analysis with PAMMs,\nincluding data simulation, transformation and other functions\nfor data preprocessing and model post-processing as well as\nvisualization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.4018,
    "stars": 0
  },
  {
    "id": 1932,
    "package_name": "DoubleML",
    "title": "Double Machine Learning in R",
    "description": "Implementation of the double/debiased machine learning\nframework of Chernozhukov et al. (2018)\n<doi:10.1111/ectj.12097> for partially linear regression\nmodels, partially linear instrumental variable regression\nmodels, interactive regression models and interactive\ninstrumental variable regression models. 'DoubleML' allows\nestimation of the nuisance parts in these models by machine\nlearning methods and computation of the Neyman orthogonal score\nfunctions. 'DoubleML' is built on top of 'mlr3' and the 'mlr3'\necosystem. The object-oriented implementation of 'DoubleML'\nbased on the 'R6' package is very flexible. More information\navailable in the publication in the Journal of Statistical\nSoftware: <doi:10.18637/jss.v108.i03>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.2194,
    "stars": 0
  },
  {
    "id": 374,
    "package_name": "BASiCS",
    "title": "Bayesian Analysis of Single-Cell Sequencing data",
    "description": "Single-cell mRNA sequencing can uncover novel cell-to-cell\nheterogeneity in gene expression levels in seemingly\nhomogeneous populations of cells. However, these experiments\nare prone to high levels of technical noise, creating new\nchallenges for identifying genes that show genuine\nheterogeneous expression within the population of cells under\nstudy. BASiCS (Bayesian Analysis of Single-Cell Sequencing\ndata) is an integrated Bayesian hierarchical model to perform\nstatistical analyses of single-cell RNA sequencing datasets in\nthe context of supervised experiments (where the groups of\ncells of interest are known a priori, e.g. experimental\nconditions or cell types). BASiCS performs built-in data\nnormalisation (global scaling) and technical noise\nquantification (based on spike-in genes). BASiCS provides an\nintuitive detection criterion for highly (or lowly) variable\ngenes within a single group of cells. Additionally, BASiCS can\ncompare gene expression patterns between two or more\npre-specified groups of cells. Unlike traditional differential\nexpression tools, BASiCS quantifies changes in expression that\nlie beyond comparisons of means, also allowing the study of\nchanges in cell-to-cell heterogeneity. The latter can be\nquantified via a biological over-dispersion parameter that\nmeasures the excess of variability that is observed with\nrespect to Poisson sampling noise, after normalisation and\ntechnical noise removal. Due to the strong mean/over-dispersion\nconfounding that is typically observed for scRNA-seq datasets,\nBASiCS also tests for changes in residual over-dispersion,\ndefined by residual values with respect to a global\nmean/over-dispersion trend.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.1532,
    "stars": 0
  },
  {
    "id": 15980,
    "package_name": "jstable",
    "title": "Create Tables from Different Types of Regression",
    "description": "Create regression tables from generalized linear\nmodel(GLM), generalized estimating equation(GEE), generalized\nlinear mixed-effects model(GLMM), Cox proportional hazards\nmodel, survey-weighted generalized linear model(svyglm) and\nsurvey-weighted Cox model results for publication.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 10.1384,
    "stars": 0
  },
  {
    "id": 13039,
    "package_name": "fastverse",
    "title": "A Suite of High-Performance Packages for Statistics and Data\nManipulation",
    "description": "Easy installation, loading and management, of\nhigh-performance packages for statistical computing and data\nmanipulation in R. The core 'fastverse' consists of 4 packages:\n'data.table', 'collapse', 'kit' and 'magrittr', that jointly\nonly depend on 'Rcpp'. The 'fastverse' can be freely and\npermanently extended with additional packages, both globally or\nfor individual projects. Separate package verses can also be\ncreated. Fast packages for many common tasks such as time\nseries, dates and times, strings, spatial data, statistics,\ndata serialization, larger-than-memory processing, and\ncompilation of R code are listed in the README file:\n<https://github.com/fastverse/fastverse#suggested-extensions>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.9375,
    "stars": 0
  },
  {
    "id": 19686,
    "package_name": "phyr",
    "title": "Model Based Phylogenetic Analysis",
    "description": "A collection of functions to do model-based phylogenetic\nanalysis. It includes functions to calculate community\nphylogenetic diversity, to estimate correlations among\nfunctional traits while accounting for phylogenetic\nrelationships, and to fit phylogenetic generalized linear mixed\nmodels. The Bayesian phylogenetic generalized linear mixed\nmodels are fitted with the 'INLA' package\n(<https://www.r-inla.org>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.7113,
    "stars": 0
  },
  {
    "id": 7973,
    "package_name": "WDI",
    "title": "World Development Indicators and Other World Bank Data",
    "description": "Search and download data from over 40 databases hosted by\nthe World Bank, including the World Development Indicators\n('WDI'), International Debt Statistics, Doing Business, Human\nCapital Index, and Sub-national Poverty indicators.",
    "version": "2.7.9",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://vincentarelbundock.github.io/WDI/",
    "exports": [
      ["languages_supported"],
      ["WDI"],
      ["WDIbulk"],
      ["WDIcache"],
      ["WDIsearch"]
    ],
    "topics": [],
    "score": 9.6967,
    "stars": 233
  },
  {
    "id": 1139,
    "package_name": "Cardinal",
    "title": "A mass spectrometry imaging toolbox for statistical analysis",
    "description": "Implements statistical & computational tools for analyzing\nmass spectrometry imaging datasets, including methods for\nefficient pre-processing, spatial segmentation, and\nclassification.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.6897,
    "stars": 0
  },
  {
    "id": 22675,
    "package_name": "seqHMM",
    "title": "Mixture Hidden Markov Models for Social Sequence Data and Other\nMultivariate, Multichannel Categorical Time Series",
    "description": "Designed for estimating variants of hidden (latent) Markov\nmodels (HMMs), mixture HMMs, and non-homogeneous HMMs (NHMMs)\nfor social sequence data and other categorical time series.\nSpecial cases include feedback-augmented NHMMs, Markov models\nwithout latent layer, mixture Markov models, and latent class\nmodels. The package supports models for one or multiple\nsubjects with one or multiple parallel sequences (channels).\nExternal covariates can be added to explain cluster membership\nin mixture models as well as initial, transition and emission\nprobabilities in NHMMs. The package provides functions for\nevaluating and comparing models, as well as functions for\nvisualizing of multichannel sequence data and HMMs. For NHMMs,\nmethods for computing average causal effects and marginal state\nand emission probabilities are available. Models are estimated\nusing maximum likelihood via the EM algorithm or direct\nnumerical maximization with analytical gradients. Documentation\nis available via several vignettes, and Helske and Helske\n(2019, <doi:10.18637/jss.v088.i03>). For methodology behind the\nNHMMs, see Helske (2025, <doi:10.48550/arXiv.2503.16014>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.6318,
    "stars": 0
  },
  {
    "id": 21158,
    "package_name": "readabs",
    "title": "Download and Tidy Time Series Data from the Australian Bureau of\nStatistics",
    "description": "Downloads, imports, and tidies time series data from the\nAustralian Bureau of Statistics <https://www.abs.gov.au/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.5037,
    "stars": 0
  },
  {
    "id": 23185,
    "package_name": "sl3",
    "title": "Pipelines for Machine Learning and Super Learning",
    "description": "A modern implementation of the Super Learner prediction\nalgorithm, coupled with a general purpose framework for\ncomposing arbitrary pipelines for machine learning tasks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.4075,
    "stars": 0
  },
  {
    "id": 14835,
    "package_name": "hBayesDM",
    "title": "Hierarchical Bayesian Modeling of Decision-Making Tasks",
    "description": "Fit an array of decision-making tasks with computational\nmodels in a hierarchical Bayesian framework. Can perform\nhierarchical Bayesian analysis of various computational models\nwith a single line of coding (Ahn et al., 2017)\n<doi:10.1162/CPSY_a_00002>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.3427,
    "stars": 0
  },
  {
    "id": 18606,
    "package_name": "nonlinearTseries",
    "title": "Nonlinear Time Series Analysis",
    "description": "Functions for nonlinear time series analysis. This package\npermits the computation of the most-used nonlinear\nstatistics/algorithms including generalized correlation\ndimension, information dimension, largest Lyapunov exponent,\nsample entropy and Recurrence Quantification Analysis (RQA),\namong others. Basic routines for surrogate data testing are\nalso included. Part of this work was based on the book\n\"Nonlinear time series analysis\" by Holger Kantz and Thomas\nSchreiber (ISBN: 9780521529020).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 9.0769,
    "stars": 0
  },
  {
    "id": 8142,
    "package_name": "aaltobda",
    "title": "Functionality and Data for the Aalto Course on Bayesian Data\nAnalysis",
    "description": "Functionality and Data for the Aalto University Course on\nBayesian Data Analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.9832,
    "stars": 0
  },
  {
    "id": 22408,
    "package_name": "sccomp",
    "title": "Differential Composition and Variability Analysis for\nSingle-Cell Data",
    "description": "Comprehensive R package for differential composition and\nvariability analysis in single-cell RNA sequencing, CyTOF, and\nmicrobiome data. Provides robust Bayesian modeling with outlier\ndetection, random effects, and advanced statistical methods for\ncell type proportion analysis. Features include probabilistic\noutlier identification, mixed-effect modeling, differential\nvariability testing, and comprehensive visualization tools.\nPerfect for cancer research, immunology, developmental biology,\nand single-cell genomics applications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.9713,
    "stars": 0
  },
  {
    "id": 16885,
    "package_name": "manymome",
    "title": "Mediation, Moderation and Moderated-Mediation After Model\nFitting",
    "description": "Computes indirect effects, conditional effects, and\nconditional indirect effects in a structural equation model or\npath model after model fitting, with no need to define any user\nparameters or label any paths in the model syntax, using the\napproach presented in Cheung and Cheung (2024)\n<doi:10.3758/s13428-023-02224-z>. Can also form bootstrap\nconfidence intervals by doing bootstrapping only once and\nreusing the bootstrap estimates in all subsequent computations.\nSupports bootstrap confidence intervals for standardized\n(partially or completely) indirect effects, conditional\neffects, and conditional indirect effects as described in\nCheung (2009) <doi:10.3758/BRM.41.2.425> and Cheung, Cheung,\nLau, Hui, and Vong (2022) <doi:10.1037/hea0001188>. Model\nfitting can be done by structural equation modeling using\nlavaan() or regression using lm().",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.755,
    "stars": 0
  },
  {
    "id": 15699,
    "package_name": "investr",
    "title": "Inverse Estimation/Calibration Functions",
    "description": "Functions to facilitate inverse estimation (e.g.,\ncalibration) in linear, generalized linear, nonlinear, and\n(linear) mixed-effects models. A generic function is also\nprovided for plotting fitted regression models with or without\nconfidence/prediction bands that may be of use to the general\nuser. For a general overview of these methods, see Greenwell\nand Schubert Kabban (2014) <doi:10.32614/RJ-2014-009>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.7131,
    "stars": 0
  },
  {
    "id": 8963,
    "package_name": "bambu",
    "title": "Context-Aware Transcript Quantification from Long Read RNA-Seq\ndata",
    "description": "bambu is a R package for multi-sample transcript discovery\nand quantification using long read RNA-Seq data. You can use\nbambu after read alignment to obtain expression estimates for\nknown and novel transcripts and genes. The output from bambu\ncan directly be used for visualisation and downstream analysis\nsuch as differential gene expression or transcript usage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.61,
    "stars": 0
  },
  {
    "id": 8616,
    "package_name": "apeglm",
    "title": "Approximate posterior estimation for GLM coefficients",
    "description": "apeglm provides Bayesian shrinkage estimators for effect\nsizes for a variety of GLM models, using approximation of the\nposterior for individual coefficients.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.5246,
    "stars": 0
  },
  {
    "id": 11151,
    "package_name": "ctsem",
    "title": "Continuous Time Structural Equation Modelling",
    "description": "Hierarchical continuous (and discrete) time state space\nmodelling, for linear and nonlinear systems measured by\ncontinuous variables, with limited support for binary data. The\nsubject specific dynamic system is modelled as a stochastic\ndifferential equation (SDE) or difference equation, measurement\nmodels are typically multivariate normal factor models. Linear\nmixed effects SDE's estimated via maximum likelihood and\noptimization are the default. Nonlinearities, (state dependent\nparameters) and random effects on all parameters are possible,\nusing either max likelihood / max a posteriori optimization\n(with optional importance sampling) or Stan's Hamiltonian Monte\nCarlo sampling. See\n<https://github.com/cdriveraus/ctsem/raw/master/vignettes/hierarchicalmanual.pdf>\nfor details. See <https://osf.io/preprints/psyarxiv/4q9ex_v2>\nfor a detailed tutorial. Priors may be used. For the conceptual\noverview of the hierarchical Bayesian linear SDE approach, see\n<https://www.researchgate.net/publication/324093594_Hierarchical_Bayesian_Continuous_Time_Dynamic_Modeling>.\nExogenous inputs may also be included, for an overview of such\npossibilities see\n<https://www.researchgate.net/publication/328221807_Understanding_the_Time_Course_of_Interventions_with_Continuous_Time_Dynamic_Models>\n. <https://cdriver.netlify.app/> contains some tutorial blog\nposts.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.4423,
    "stars": 0
  },
  {
    "id": 23632,
    "package_name": "splitTools",
    "title": "Tools for Data Splitting",
    "description": "Fast, lightweight toolkit for data splitting. Data sets\ncan be partitioned into disjoint groups (e.g. into training,\nvalidation, and test) or into (repeated) k-folds for subsequent\ncross-validation. Besides basic splits, the package supports\nstratified, grouped as well as blocked splitting. Furthermore,\ncross-validation folds for time series data can be created. See\ne.g. Hastie et al. (2001) <doi:10.1007/978-0-387-84858-7> for\nthe basic background on data partitioning and cross-validation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.4306,
    "stars": 0
  },
  {
    "id": 23014,
    "package_name": "simDAG",
    "title": "Simulate Data from a (Time-Dependent) Causal DAG",
    "description": "Simulate complex data from a given directed acyclic graph\nand information about each individual node. Root nodes are\nsimply sampled from the specified distribution. Child Nodes are\nsimulated according to one of many implemented regressions,\nsuch as logistic regression, linear regression, poisson\nregression or any other function. Also includes a comprehensive\nframework for discrete-time simulation, discrete-event\nsimulation, and networks-based simulation which can generate\neven more complex longitudinal and dependent data. For more\ndetails, see Robin Denz, Nina Timmesfeld (2025)\n<doi:10.48550/arXiv.2506.01498>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.4212,
    "stars": 0
  },
  {
    "id": 7552,
    "package_name": "TRONCO",
    "title": "TRONCO, an R package for TRanslational ONCOlogy",
    "description": "The TRONCO (TRanslational ONCOlogy) R package collects\nalgorithms to infer progression models via the approach of\nSuppes-Bayes Causal Network, both from an ensemble of tumors\n(cross-sectional samples) and within an individual patient\n(multi-region or single-cell samples). The package provides\nparallel implementation of algorithms that process binary\nmatrices where each row represents a tumor sample and each\ncolumn a single-nucleotide or a structural variant driving the\nprogression; a 0/1 value models the absence/presence of that\nalteration in the sample. The tool can import data from plain,\nMAF or GISTIC format files, and can fetch it from the\ncBioPortal for cancer genomics. Functions for data manipulation\nand visualization are provided, as well as functions to\nimport/export such data to other bioinformatics tools for, e.g,\nclustering or detection of mutually exclusive alterations.\nInferred models can be visualized and tested for their\nconfidence via bootstrap and cross-validation. TRONCO is used\nfor the implementation of the Pipeline for Cancer Inference\n(PICNIC).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.4028,
    "stars": 0
  },
  {
    "id": 5830,
    "package_name": "RJDemetra",
    "title": "Interface to 'JDemetra+' Seasonal Adjustment Software",
    "description": "Interface around 'JDemetra+'\n(<https://github.com/jdemetra/jdemetra-app>), the seasonal\nadjustment software officially recommended to the members of\nthe European Statistical System (ESS) and the European System\nof Central Banks. It offers full access to all options and\noutputs of 'JDemetra+', including the two leading seasonal\nadjustment methods TRAMO/SEATS+ and X-12ARIMA/X-13ARIMA-SEATS.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.3604,
    "stars": 0
  },
  {
    "id": 24350,
    "package_name": "targeted",
    "title": "Targeted Inference",
    "description": "Various methods for targeted and semiparametric inference\nincluding augmented inverse probability weighted (AIPW)\nestimators for missing data and causal inference (Bang and\nRobins (2005) <doi:10.1111/j.1541-0420.2005.00377.x>), variable\nimportance and conditional average treatment effects (CATE)\n(van der Laan (2006) <doi:10.2202/1557-4679.1008>), estimators\nfor risk differences and relative risks (Richardson et al.\n(2017) <doi:10.1080/01621459.2016.1192546>), assumption lean\ninference for generalized linear model parameters (Vansteelandt\net al. (2022) <doi:10.1111/rssb.12504>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.3239,
    "stars": 0
  },
  {
    "id": 3513,
    "package_name": "JointAI",
    "title": "Joint Analysis and Imputation of Incomplete Data",
    "description": "Joint analysis and imputation of incomplete data in the\nBayesian framework, using (generalized) linear (mixed) models\nand extensions there of, survival models, or joint models for\nlongitudinal and survival data, as described in Erler,\nRizopoulos and Lesaffre (2021) <doi:10.18637/jss.v100.i20>.\nIncomplete covariates, if present, are automatically imputed.\nThe package performs some preprocessing of the data and creates\na 'JAGS' model, which will then automatically be passed to\n'JAGS' <https://mcmc-jags.sourceforge.io/> with the help of the\npackage 'rjags'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.3187,
    "stars": 0
  },
  {
    "id": 4733,
    "package_name": "NormalyzerDE",
    "title": "Evaluation of normalization methods and calculation of\ndifferential expression analysis statistics",
    "description": "NormalyzerDE provides screening of normalization methods\nfor LC-MS based expression data. It calculates a range of\nnormalized matrices using both existing approaches and a novel\ntime-segmented approach, calculates performance measures and\ngenerates an evaluation report. Furthermore, it provides an\neasy utility for Limma- or ANOVA- based differential expression\nanalysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.29,
    "stars": 0
  },
  {
    "id": 9353,
    "package_name": "biobroom",
    "title": "Turn Bioconductor objects into tidy data frames",
    "description": "This package contains methods for converting standard\nobjects constructed by bioinformatics packages, especially\nthose in Bioconductor, and converting them to tidy data. It\nthus serves as a complement to the broom package, and follows\nthe same the tidy, augment, glance division of tidying methods.\nTidying data makes it easy to recombine, reshape and visualize\nbioinformatics analyses.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2405,
    "stars": 0
  },
  {
    "id": 21236,
    "package_name": "reda",
    "title": "Recurrent Event Data Analysis",
    "description": "Contains implementations of recurrent event data analysis\nroutines including (1) survival and recurrent event data\nsimulation from stochastic process point of view by the\nthinning method proposed by Lewis and Shedler (1979)\n<doi:10.1002/nav.3800260304> and the inversion method\nintroduced in Cinlar (1975, ISBN:978-0486497976), (2) the mean\ncumulative function (MCF) estimation by the Nelson-Aalen\nestimator of the cumulative hazard rate function, (3)\ntwo-sample recurrent event responses comparison with the\npseudo-score tests proposed by Lawless and Nadeau (1995)\n<doi:10.2307/1269617>, (4) gamma frailty model with spline rate\nfunction following Fu, et al. (2016)\n<doi:10.1080/10543406.2014.992524>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.2253,
    "stars": 0
  },
  {
    "id": 17677,
    "package_name": "mlr3proba",
    "title": "Probabilistic Supervised Learning for 'mlr3'",
    "description": "Provides extensions for probabilistic supervised learning\nfor 'mlr3'. This currently includes survival analysis,\nprobabilistic regression and density estimation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.189,
    "stars": 0
  },
  {
    "id": 12004,
    "package_name": "dreamlet",
    "title": "Scalable differential expression analysis of single cell\ntranscriptomics datasets with complex study designs",
    "description": "Recent advances in single cell/nucleus transcriptomic\ntechnology has enabled collection of cohort-scale datasets to\nstudy cell type specific gene expression differences associated\ndisease state, stimulus, and genetic regulation. The scale of\nthese data, complex study designs, and low read count per cell\nmean that characterizing cell type specific molecular\nmechanisms requires a user-frieldly, purpose-build analytical\nframework. We have developed the dreamlet package that applies\na pseudobulk approach and fits a regression model for each gene\nand cell cluster to test differential expression across\nindividuals associated with a trait of interest. Use of\nprecision-weighted linear mixed models enables accounting for\nrepeated measures study designs, high dimensional batch\neffects, and varying sequencing depth or observed cells per\nbiosample.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.1888,
    "stars": 0
  },
  {
    "id": 2267,
    "package_name": "ExploreModelMatrix",
    "title": "Graphical Exploration of Design Matrices",
    "description": "Given a sample data table and a design formula,\nExploreModelMatrix generates an interactive application for\nexploration of the resulting design matrix. This can be helpful\nfor interpreting model coefficients and constructing\nappropriate contrasts in (generalized) linear models. Static\nvisualizations can also be generated.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.1818,
    "stars": 0
  },
  {
    "id": 1599,
    "package_name": "DEqMS",
    "title": "a tool to perform statistical analysis of differential protein\nexpression for quantitative proteomics data.",
    "description": "DEqMS is developped on top of Limma. However, Limma\nassumes same prior variance for all genes. In proteomics, the\naccuracy of protein abundance estimates varies by the number of\npeptides/PSMs quantified in both label-free and labelled data.\nProteins quantification by multiple peptides or PSMs are more\naccurate. DEqMS package is able to estimate different prior\nvariances for proteins quantified by different number of\nPSMs/peptides, therefore acchieving better accuracy. The\npackage can be applied to analyze both label-free and labelled\nproteomics data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.1589,
    "stars": 0
  },
  {
    "id": 24112,
    "package_name": "survex",
    "title": "Explainable Machine Learning in Survival Analysis",
    "description": "Survival analysis models are commonly used in medicine and\nother areas. Many of them are too complex to be interpreted by\nhuman. Exploration and explanation is needed, but standard\nmethods do not give a broad enough picture. 'survex' provides\neasy-to-apply methods for explaining survival models, both\ncomplex black-boxes and simpler statistical models. They\ninclude methods specific to survival analysis such as\nSurvSHAP(t) introduced in Krzyzinski et al., (2023)\n<doi:10.1016/j.knosys.2022.110234>, SurvLIME described in\nKovalev et al., (2020) <doi:10.1016/j.knosys.2020.106164> as\nwell as extensions of existing ones described in Biecek et al.,\n(2021) <doi:10.1201/9780429027192>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.1175,
    "stars": 0
  },
  {
    "id": 22323,
    "package_name": "scDD",
    "title": "Mixture modeling of single-cell RNA-seq data to identify genes\nwith differential distributions",
    "description": "This package implements a method to analyze single-cell\nRNA- seq Data utilizing flexible Dirichlet Process mixture\nmodels. Genes with differential distributions of expression are\nclassified into several interesting patterns of differences\nbetween two conditions. The package also includes functions for\nsimulating data with these patterns from negative binomial\ndistributions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.1096,
    "stars": 0
  },
  {
    "id": 23401,
    "package_name": "spEDM",
    "title": "Spatial Empirical Dynamic Modeling",
    "description": "Inferring causation from spatial cross-sectional data\nthrough empirical dynamic modeling (EDM), with methodological\nextensions including geographical convergent cross mapping from\nGao et al. (2023) <doi:10.1038/s41467-023-41619-6>, as well as\nthe spatial causality test following the approach of Herrera et\nal. (2016) <doi:10.1111/pirs.12144>, together with geographical\npattern causality proposed in Zhang et al. (2025)\n<doi:10.1080/13658816.2025.2581207>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0636,
    "stars": 0
  },
  {
    "id": 17569,
    "package_name": "mistyR",
    "title": "Multiview Intercellular SpaTial modeling framework",
    "description": "mistyR is an implementation of the Multiview Intercellular\nSpaTialmodeling framework (MISTy). MISTy is an explainable\nmachine learning framework for knowledge extraction and\nanalysis of single-cell, highly multiplexed, spatially resolved\ndata. MISTy facilitates an in-depth understanding of marker\ninteractions by profiling the intra- and intercellular\nrelationships. MISTy is a flexible framework able to process a\ncustom number of views. Each of these views can describe a\ndifferent spatial context, i.e., define a relationship among\nthe observed expressions of the markers, such as intracellular\nregulation or paracrine regulation, but also, the views can\nalso capture cell-type specific relationships, capture\nrelations between functional footprints or focus on relations\nbetween different anatomical regions. Each MISTy view is\nconsidered as a potential source of variability in the measured\nmarker expressions. Each MISTy view is then analyzed for its\ncontribution to the total expression of each marker and is\nexplained in terms of the interactions with other measurements\nthat led to the observed contribution.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0435,
    "stars": 0
  },
  {
    "id": 24090,
    "package_name": "survHE",
    "title": "Survival Analysis in Health Economic Evaluation",
    "description": "Contains a suite of functions for survival analysis in\nhealth economics. These can be used to run survival models\nunder a frequentist (based on maximum likelihood) or a Bayesian\napproach (both based on Integrated Nested Laplace Approximation\nor Hamiltonian Monte Carlo). To run the Bayesian models, the\nuser needs to install additional modules (packages), i.e.\n'survHEinla' and 'survHEhmc'. These can be installed from\n<https://giabaio.r-universe.dev/> using\n'install.packages(\"survHEhmc\", repos =\nc(\"https://giabaio.r-universe.dev\",\n\"https://cloud.r-project.org\"))' and\n'install.packages(\"survHEinla\", repos =\nc(\"https://giabaio.r-universe.dev\",\n\"https://cloud.r-project.org\"))' respectively. 'survHEinla' is\nbased on the package INLA, which is available for download at\n<https://inla.r-inla-download.org/R/stable/>. The user can\nspecify a set of parametric models using a common notation and\nselect the preferred mode of inference. The results can also be\npost-processed to produce probabilistic sensitivity analysis\nand can be used to export the output to an Excel file (e.g. for\na Markov model, as often done by modellers and practitioners).\n<doi:10.18637/jss.v095.i14>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 8.0435,
    "stars": 0
  },
  {
    "id": 14932,
    "package_name": "hdnom",
    "title": "Benchmarking and Visualization Toolkit for Penalized Cox Models",
    "description": "Creates nomogram visualizations for penalized Cox\nregression models, with the support of reproducible survival\nmodel building, validation, calibration, and comparison for\nhigh-dimensional data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.9806,
    "stars": 0
  },
  {
    "id": 14412,
    "package_name": "glmSparseNet",
    "title": "Network Centrality Metrics for Elastic-Net Regularized Models",
    "description": "glmSparseNet is an R-package that generalizes sparse\nregression models when the features (e.g. genes) have a graph\nstructure (e.g. protein-protein interactions), by including\nnetwork-based regularizers.  glmSparseNet uses the glmnet\nR-package, by including centrality measures of the network as\npenalty weights in the regularization. The current version\nimplements regularization based on node degree, i.e. the\nstrength and/or number of its associated edges, either by\npromoting hubs in the solution or orphan genes in the solution.\nAll the glmnet distribution families are supported, namely\n\"gaussian\", \"poisson\", \"binomial\", \"multinomial\", \"cox\", and\n\"mgaussian\".",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.9629,
    "stars": 0
  },
  {
    "id": 2618,
    "package_name": "GENIE3",
    "title": "GEne Network Inference with Ensemble of trees",
    "description": "This package implements the GENIE3 algorithm for inferring\ngene regulatory networks from expression data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.9315,
    "stars": 0
  },
  {
    "id": 13237,
    "package_name": "fishpond",
    "title": "Fishpond: downstream methods and tools for expression data",
    "description": "Fishpond contains methods for differential transcript and\ngene expression analysis of RNA-seq data using inferential\nreplicates for uncertainty of abundance quantification, as\ngenerated by Gibbs sampling or bootstrap sampling. Also the\npackage contains a number of utilities for working with Salmon\nand Alevin quantification files.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.9218,
    "stars": 0
  },
  {
    "id": 71,
    "package_name": "AIPW",
    "title": "Augmented Inverse Probability Weighting",
    "description": "The 'AIPW' package implements the augmented inverse\nprobability weighting, a doubly robust estimator, for average\ncausal effect estimation with user-defined stacked machine\nlearning algorithms. To cite the 'AIPW' package, please use:\n\"Yongqi Zhong, Edward H. Kennedy, Lisa M. Bodnar, Ashley I.\nNaimi (2021). AIPW: An R Package for Augmented Inverse\nProbability Weighted Estimation of Average Causal Effects.\nAmerican Journal of Epidemiology. <doi:10.1093/aje/kwab207>\".\nVisit: <https://yqzhong7.github.io/AIPW/> for more information.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8833,
    "stars": 0
  },
  {
    "id": 17338,
    "package_name": "methylSig",
    "title": "MethylSig: Differential Methylation Testing for WGBS and RRBS\nData",
    "description": "MethylSig is a package for testing for differentially\nmethylated cytosines (DMCs) or regions (DMRs) in whole-genome\nbisulfite sequencing (WGBS) or reduced representation bisulfite\nsequencing (RRBS) experiments.  MethylSig uses a beta binomial\nmodel to test for significant differences between groups of\nsamples. Several options exist for either site-specific or\nsliding window tests, and variance estimation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8764,
    "stars": 0
  },
  {
    "id": 19548,
    "package_name": "personalized",
    "title": "Estimation and Validation Methods for Subgroup Identification\nand Personalized Medicine",
    "description": "Provides functions for fitting and validation of models\nfor subgroup identification and personalized medicine /\nprecision medicine under the general subgroup identification\nframework of Chen et al. (2017) <doi:10.1111/biom.12676>. This\npackage is intended for use for both randomized controlled\ntrials and observational studies and is described in detail in\nHuling and Yu (2021) <doi:10.18637/jss.v098.i05>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8672,
    "stars": 0
  },
  {
    "id": 23103,
    "package_name": "simts",
    "title": "Time Series Analysis Tools",
    "description": "A system contains easy-to-use tools as a support for time\nseries analysis courses. In particular, it incorporates a\ntechnique called Generalized Method of Wavelet Moments (GMWM)\nas well as its robust implementation for fast and robust\nparameter estimation of time series models which is described,\nfor example, in Guerrier et al. (2013) <doi:\n10.1080/01621459.2013.799920>. More details can also be found\nin the paper linked to via the URL below.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8641,
    "stars": 0
  },
  {
    "id": 24597,
    "package_name": "tidyAML",
    "title": "Automatic Machine Learning with 'tidymodels'",
    "description": "The goal of this package will be to provide a simple\ninterface for automatic machine learning that fits the\n'tidymodels' framework. The intention is to work for regression\nand classification problems with a simple verb framework.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8407,
    "stars": 0
  },
  {
    "id": 9076,
    "package_name": "bayesian",
    "title": "Bindings for Bayesian TidyModels",
    "description": "Fit Bayesian models using 'brms'/'Stan' with\n'parsnip'/'tidymodels' via 'bayesian'\n<doi:10.5281/zenodo.4426836>. 'tidymodels' is a collection of\npackages for machine learning; see Kuhn and Wickham (2020)\n<https://www.tidymodels.org>). The technical details of 'brms'\nand 'Stan' are described in Bürkner (2017)\n<doi:10.18637/jss.v080.i01>, Bürkner (2018)\n<doi:10.32614/RJ-2018-017>, and Carpenter et al. (2017)\n<doi:10.18637/jss.v076.i01>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.8213,
    "stars": 0
  },
  {
    "id": 7756,
    "package_name": "TrialEmulation",
    "title": "Causal Analysis of Observational Time-to-Event Data",
    "description": "Implements target trial emulation methods to apply\nrandomized clinical trial design and analysis in an\nobservational setting. Using marginal structural models, it can\nestimate intention-to-treat and per-protocol effects in\nemulated trials using electronic health records. A description\nand application of the method can be found in Danaei et al\n(2013) <doi:10.1177/0962280211403603>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7991,
    "stars": 0
  },
  {
    "id": 7591,
    "package_name": "TSrepr",
    "title": "Time Series Representations",
    "description": "Methods for representations (i.e. dimensionality\nreduction, preprocessing, feature extraction) of time series to\nhelp more accurate and effective time series data mining.\nNon-data adaptive, data adaptive, model-based and data dictated\n(clipped) representation methods are implemented. Also various\nnormalisation methods (min-max, z-score, Box-Cox, Yeo-Johnson),\nand forecasting accuracy measures are implemented.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7655,
    "stars": 0
  },
  {
    "id": 12313,
    "package_name": "edge",
    "title": "Extraction of Differential Gene Expression",
    "description": "The edge package implements methods for carrying out\ndifferential expression analyses of genome-wide gene expression\nstudies. Significance testing using the optimal discovery\nprocedure and generalized likelihood ratio tests (equivalent to\nF-tests and t-tests) are implemented for general study designs.\nSpecial functions are available to facilitate the analysis of\ncommon study designs, including time course experiments. Other\npackages such as sva and qvalue are integrated in edge to\nprovide a wide range of tools for gene expression analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.754,
    "stars": 0
  },
  {
    "id": 19358,
    "package_name": "pathwayPCA",
    "title": "Integrative Pathway Analysis with Modern PCA Methodology and\nGene Selection",
    "description": "pathwayPCA is an integrative analysis tool that implements\nthe principal component analysis (PCA) based pathway analysis\napproaches described in Chen et al. (2008), Chen et al. (2010),\nand Chen (2011). pathwayPCA allows users to: (1) Test pathway\nassociation with binary, continuous, or survival phenotypes.\n(2) Extract relevant genes in the pathways using the SuperPCA\nand AES-PCA approaches. (3) Compute principal components (PCs)\nbased on the selected genes. These estimated latent variables\nrepresent pathway activities for individual subjects, which can\nthen be used to perform integrative pathway analysis, such as\nmulti-omics analysis. (4) Extract relevant genes that drive\npathway significance as well as data corresponding to these\nrelevant genes for additional in-depth analysis. (5) Perform\nanalyses with enhanced computational efficiency with parallel\ncomputing and enhanced data safety with S4-class data objects.\n(6) Analyze studies with complex experimental designs, with\nmultiple covariates, and with interaction effects, e.g.,\ntesting whether pathway association with clinical phenotype is\ndifferent between male and female subjects. Citations: Chen et\nal. (2008) <https://doi.org/10.1093/bioinformatics/btn458>;\nChen et al. (2010) <https://doi.org/10.1002/gepi.20532>; and\nChen (2011) <https://doi.org/10.2202/1544-6115.1697>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.754,
    "stars": 0
  },
  {
    "id": 5124,
    "package_name": "POMA",
    "title": "Tools for Omics Data Analysis",
    "description": "The POMA package offers a comprehensive toolkit designed\nfor omics data analysis, streamlining the process from initial\nvisualization to final statistical analysis. Its primary goal\nis to simplify and unify the various steps involved in omics\ndata processing, making it more accessible and manageable\nwithin a single, intuitive R package. Emphasizing on\nreproducibility and user-friendliness, POMA leverages the\nstandardized SummarizedExperiment class from Bioconductor,\nensuring seamless integration and compatibility with a wide\narray of Bioconductor tools. This approach guarantees maximum\nflexibility and replicability, making POMA an essential asset\nfor researchers handling omics datasets. See\nhttps://github.com/pcastellanoescuder/POMAShiny. Paper:\nCastellano-Escuder et al. (2021)\n<doi:10.1371/journal.pcbi.1009148> for more details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7167,
    "stars": 0
  },
  {
    "id": 16571,
    "package_name": "lmomco",
    "title": "L-Moments, Censored L-Moments, Trimmed L-Moments, L-Comoments,\nand Many Distributions",
    "description": "Extensive functions for Lmoments (LMs) and\nprobability-weighted moments (PWMs), distribution parameter\nestimation, LMs for distributions, LM ratio diagrams,\nmultivariate Lcomoments, and asymmetric (asy) trimmed LMs\n(TLMs). Maximum likelihood and maximum product spacings\nestimation are available. Right-tail and left-tail LM censoring\nby threshold or indicator variable are available. LMs of\nresidual (resid) and reversed (rev) residual life are\nimplemented along with 13 quantile operators for reliability\nanalyses. Exact analytical bootstrap estimates of order\nstatistics, LMs, and LM var-covars are available. Harri-Coble\nTau34-squared Normality Test is available. Distributions with\nL, TL, and added (+) support for right-tail censoring (RC)\nencompass: Asy Exponential (Exp) Power [L], Asy Triangular [L],\nCauchy [TL], Eta-Mu [L], Exp. [L], Gamma [L], Generalized (Gen)\nExp Poisson [L], Gen Extreme Value [L], Gen Lambda [L, TL], Gen\nLogistic [L], Gen Normal [L], Gen Pareto [L+RC, TL],\nGovindarajulu [L], Gumbel [L], Kappa [L], Kappa-Mu [L],\nKumaraswamy [L], Laplace [L], Linear Mean Residual Quantile\nFunction [L], Normal [L], 3p log-Normal [L], Pearson Type III\n[L], Polynomial Density-Quantile 3 and 4 [L], Rayleigh [L],\nRev-Gumbel [L+RC], Rice [L], Singh Maddala [L], Slash [TL], 3p\nStudent t [L], Truncated Exponential [L], Wakeby [L], and\nWeibull [L].",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.7155,
    "stars": 0
  },
  {
    "id": 24833,
    "package_name": "tmle3",
    "title": "The Extensible TMLE Framework",
    "description": "A general framework supporting the implementation of\ntargeted maximum likelihood estimators (TMLEs) of a diverse\nrange of statistical target parameters through a unified\ninterface. The goal is that the exposed framework be as general\nas the mathematical framework upon which it draws.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6492,
    "stars": 0
  },
  {
    "id": 21558,
    "package_name": "ridge",
    "title": "Ridge Regression with Automatic Selection of the Penalty\nParameter",
    "description": "Linear and logistic ridge regression functions.\nAdditionally includes special functions for genome-wide\nsingle-nucleotide polymorphism (SNP) data. More details can be\nfound in <doi: 10.1002/gepi.21750> and <doi:\n10.1186/1471-2105-12-372>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6453,
    "stars": 0
  },
  {
    "id": 11759,
    "package_name": "dirichletprocess",
    "title": "Build Dirichlet Process Objects for Bayesian Modelling",
    "description": "Perform nonparametric Bayesian analysis using Dirichlet\nprocesses without the need to program the inference algorithms.\nUtilise included pre-built models or specify custom models and\nallow the 'dirichletprocess' package to handle the Markov chain\nMonte Carlo sampling. Our Dirichlet process objects can act as\nbuilding blocks for a variety of statistical models including\nand not limited to: density estimation, clustering and prior\ndistributions in hierarchical models. See Teh, Y. W. (2011)\n<https://www.stats.ox.ac.uk/~teh/research/npbayes/Teh2010a.pdf>,\namong many other sources.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6394,
    "stars": 0
  },
  {
    "id": 17170,
    "package_name": "medflex",
    "title": "Flexible Mediation Analysis Using Natural Effect Models",
    "description": "Run flexible mediation analyses using natural effect\nmodels as described in Lange, Vansteelandt and Bekaert (2012)\n<DOI:10.1093/aje/kwr525>, Vansteelandt, Bekaert and Lange\n(2012) <DOI:10.1515/2161-962X.1014> and Loeys, Moerkerke, De\nSmet, Buysse, Steen and Vansteelandt (2013)\n<DOI:10.1080/00273171.2013.832132>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.625,
    "stars": 0
  },
  {
    "id": 16783,
    "package_name": "mFilter",
    "title": "Miscellaneous Time Series Filters",
    "description": "The mFilter package implements several time series filters\nuseful for smoothing and extracting trend and cyclical\ncomponents of a time series. The routines are commonly used in\neconomics and finance, however they should also be interest to\nother areas. Currently, Christiano-Fitzgerald, Baxter-King,\nHodrick-Prescott, Butterworth, and trigonometric regression\nfilters are included in the package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6244,
    "stars": 0
  },
  {
    "id": 22414,
    "package_name": "scde",
    "title": "Single Cell Differential Expression",
    "description": "The scde package implements a set of statistical methods\nfor analyzing single-cell RNA-seq data. scde fits individual\nerror models for single-cell RNA-seq measurements. These models\ncan then be used for assessment of differential expression\nbetween groups of cells, as well as other types of analysis.\nThe scde package also contains the pagoda framework which\napplies pathway and gene set overdispersion analysis to\nidentify and characterize putative cell subpopulations based on\ntranscriptional signatures. The overall approach to the\ndifferential expression analysis is detailed in the following\npublication: \"Bayesian approach to single-cell differential\nexpression analysis\" (Kharchenko PV, Silberstein L, Scadden DT,\nNature Methods, doi: 10.1038/nmeth.2967). The overall approach\nto subpopulation identification and characterization is\ndetailed in the following pre-print: \"Characterizing\ntranscriptional heterogeneity through pathway and gene set\noverdispersion analysis\" (Fan J, Salathia N, Liu R, Kaeser G,\nYung Y, Herman J, Kaper F, Fan JB, Zhang K, Chun J, and\nKharchenko PV, Nature Methods, doi:10.1038/nmeth.3734).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.6116,
    "stars": 0
  },
  {
    "id": 24037,
    "package_name": "sugrrants",
    "title": "Supporting Graphs for Analysing Time Series",
    "description": "Provides 'ggplot2' graphics for analysing time series\ndata. It aims to fit into the 'tidyverse' and grammar of\ngraphics framework for handling temporal data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.486,
    "stars": 0
  },
  {
    "id": 21569,
    "package_name": "rigr",
    "title": "Regression, Inference, and General Data Analysis Tools in R",
    "description": "A set of tools to streamline data analysis. Learning both\nR and introductory statistics at the same time can be\nchallenging, and so we created 'rigr' to facilitate common data\nanalysis tasks and enable learners to focus on statistical\nconcepts. We provide easy-to-use interfaces for descriptive\nstatistics, one- and two-sample inference, and regression\nanalyses. 'rigr' output includes key information while omitting\nunnecessary details that can be confusing to beginners.\nHeteroscedasticity-robust (\"sandwich\") standard errors are\nreturned by default, and multiple partial F-tests and tests for\ncontrasts are easy to specify. A single regression function can\nfit both linear and generalized linear models, allowing\nstudents to more easily make connections between different\nclasses of models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4502,
    "stars": 0
  },
  {
    "id": 24432,
    "package_name": "tempdisagg",
    "title": "Methods for Temporal Disaggregation and Interpolation of Time\nSeries",
    "description": "Temporal disaggregation methods are used to disaggregate\nand interpolate a low frequency time series to a higher\nfrequency series, where either the sum, the mean, the first or\nthe last value of the resulting high frequency series is\nconsistent with the low frequency series. Temporal\ndisaggregation can be performed with or without one or more\nhigh frequency indicator series. Contains the methods of\nChow-Lin, Santos-Silva-Cardoso, Fernandez, Litterman, Denton\nand Denton-Cholette, summarized in Sax and Steiner (2013)\n<doi:10.32614/RJ-2013-028>. Supports most R time series\nclasses.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4461,
    "stars": 0
  },
  {
    "id": 21832,
    "package_name": "ropls",
    "title": "PCA, PLS(-DA) and OPLS(-DA) for multivariate analysis and\nfeature selection of omics data",
    "description": "Latent variable modeling with Principal Component Analysis\n(PCA) and Partial Least Squares (PLS) are powerful methods for\nvisualization, regression, classification, and feature\nselection of omics data where the number of variables exceeds\nthe number of samples and with multicollinearity among\nvariables. Orthogonal Partial Least Squares (OPLS) enables to\nseparately model the variation correlated (predictive) to the\nfactor of interest and the uncorrelated (orthogonal) variation.\nWhile performing similarly to PLS, OPLS facilitates\ninterpretation. Successful applications of these chemometrics\ntechniques include spectroscopic data such as Raman\nspectroscopy, nuclear magnetic resonance (NMR), mass\nspectrometry (MS) in metabolomics and proteomics, but also\ntranscriptomics data. In addition to scores, loadings and\nweights plots, the package provides metrics and graphics to\ndetermine the optimal number of components (e.g. with the R2\nand Q2 coefficients), check the validity of the model by\npermutation testing, detect outliers, and perform feature\nselection (e.g. with Variable Importance in Projection or\nregression coefficients). The package can be accessed via a\nuser interface on the Workflow4Metabolomics.org online resource\nfor computational metabolomics (built upon the Galaxy\nenvironment).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4448,
    "stars": 0
  },
  {
    "id": 16381,
    "package_name": "lemur",
    "title": "Latent Embedding Multivariate Regression",
    "description": "Fit a latent embedding multivariate regression (LEMUR)\nmodel to multi-condition single-cell data. The model provides a\nparametric description of single-cell data measured with\ntreatment vs. control or more complex experimental designs. The\nparametric model is used to (1) align conditions, (2) predict\nlog fold changes between conditions for all cells, and (3)\nidentify cell neighborhoods with consistent log fold changes.\nFor those neighborhoods, a pseudobulked differential expression\ntest is conducted to assess which genes are significantly\nchanged.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4368,
    "stars": 0
  },
  {
    "id": 9041,
    "package_name": "baySeq",
    "title": "Empirical Bayesian analysis of patterns of differential\nexpression in count data",
    "description": "This package identifies differential expression in\nhigh-throughput 'count' data, such as that derived from\nnext-generation sequencing machines, calculating estimated\nposterior likelihoods of differential expression (or more\ncomplex hypotheses) via empirical Bayesian methods.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4329,
    "stars": 0
  },
  {
    "id": 536,
    "package_name": "BVAR",
    "title": "Hierarchical Bayesian Vector Autoregression",
    "description": "Estimation of hierarchical Bayesian vector autoregressive\nmodels following Kuschnig & Vashold (2021)\n<doi:10.18637/jss.v100.i14>. Implements hierarchical prior\nselection for conjugate priors in the fashion of Giannone,\nLenza & Primiceri (2015) <doi:10.1162/REST_a_00483>. Functions\nto compute and identify impulse responses, calculate forecasts,\nforecast error variance decompositions and scenarios are\navailable. Several methods to print, plot and summarise results\nfacilitate analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.431,
    "stars": 0
  },
  {
    "id": 17997,
    "package_name": "msqrob2",
    "title": "Robust statistical inference for quantitative LC-MS proteomics",
    "description": "msqrob2 provides a robust linear mixed model framework for\nassessing differential abundance in MS-based Quantitative\nproteomics experiments. Our workflows can start from raw\npeptide intensities or summarised protein expression values.\nThe model parameter estimates can be stabilized by ridge\nregression, empirical Bayes variance estimation and robust\nM-estimation. msqrob2's hurde workflow can handle missing data\nwithout having to rely on hard-to-verify imputation\nassumptions, and, outcompetes state-of-the-art methods with and\nwithout imputation for both high and low missingness. It builds\non QFeature infrastructure for quantitative mass spectrometry\ndata to store the model results together with the raw data and\npreprocessed data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4289,
    "stars": 0
  },
  {
    "id": 13862,
    "package_name": "gemma.R",
    "title": "A wrapper for Gemma's Restful API to access curated gene\nexpression data and differential expression analyses",
    "description": "Low- and high-level wrappers for Gemma's RESTful API. They\nenable access to curated expression and differential expression\ndata from over 10,000 published studies. Gemma is a web site,\ndatabase and a set of tools for the meta-analysis, re-use and\nsharing of genomics data, currently primarily targeted at the\nanalysis of gene expression profiles.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.4078,
    "stars": 0
  },
  {
    "id": 12380,
    "package_name": "eisaR",
    "title": "Exon-Intron Split Analysis (EISA) in R",
    "description": "Exon-intron split analysis (EISA) uses ordinary RNA-seq\ndata to measure changes in mature RNA and pre-mRNA reads across\ndifferent experimental conditions to quantify transcriptional\nand post-transcriptional regulation of gene expression. For\ndetails see Gaidatzis et al., Nat Biotechnol 2015. doi:\n10.1038/nbt.3269. eisaR implements the major steps of EISA in\nR.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3888,
    "stars": 0
  },
  {
    "id": 21450,
    "package_name": "revdbayes",
    "title": "Ratio-of-Uniforms Sampling for Bayesian Extreme Value Analysis",
    "description": "Provides functions for the Bayesian analysis of extreme\nvalue models.  The 'rust' package\n<https://cran.r-project.org/package=rust> is used to simulate a\nrandom sample from the required posterior distribution. The\nfunctionality of 'revdbayes' is similar to the 'evdbayes'\npackage <https://cran.r-project.org/package=evdbayes>, which\nuses Markov Chain Monte Carlo ('MCMC') methods for posterior\nsimulation.  In addition, there are functions for making\ninferences about the extremal index, using the models for\nthreshold inter-exceedance times of Suveges and Davison (2010)\n<doi:10.1214/09-AOAS292> and Holesovsky and Fusek (2020)\n<doi:10.1007/s10687-020-00374-3>. Also provided are d,p,q,r\nfunctions for the Generalised Extreme Value ('GEV') and\nGeneralised Pareto ('GP') distributions that deal appropriately\nwith cases where the shape parameter is very close to zero.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3853,
    "stars": 0
  },
  {
    "id": 9908,
    "package_name": "casebase",
    "title": "Fitting Flexible Smooth-in-Time Hazards and Risk Functions via\nLogistic and Multinomial Regression",
    "description": "Fit flexible and fully parametric hazard regression models\nto survival data with single event type or multiple competing\ncauses via logistic and multinomial regression. Our formulation\nallows for arbitrary functional forms of time and its\ninteractions with other predictors for time-dependent hazards\nand hazard ratios. From the fitted hazard model, we provide\nfunctions to readily calculate and plot cumulative incidence\nand survival curves for a given covariate profile. This\napproach accommodates any log-linear hazard function of\nprognostic time, treatment, and covariates, and readily allows\nfor non-proportionality. We also provide a plot method for\nvisualizing incidence density via population time plots. Based\non the case-base sampling approach of Hanley and Miettinen\n(2009) <DOI:10.2202/1557-4679.1125>, Saarela and Arjas (2015)\n<DOI:10.1111/sjos.12125>, and Saarela (2015)\n<DOI:10.1007/s10985-015-9352-x>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3798,
    "stars": 0
  },
  {
    "id": 153,
    "package_name": "ASSIGN",
    "title": "Adaptive Signature Selection and InteGratioN (ASSIGN)",
    "description": "ASSIGN is a computational tool to evaluate the pathway\nderegulation/activation status in individual patient samples.\nASSIGN employs a flexible Bayesian factor analysis approach\nthat adapts predetermined pathway signatures derived either\nfrom knowledge-based literature or from perturbation\nexperiments to the cell-/tissue-specific pathway signatures.\nThe deregulation/activation level of each context-specific\npathway is quantified to a score, which represents the extent\nto which a patient sample encompasses the pathway\nderegulation/activation signature.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3758,
    "stars": 0
  },
  {
    "id": 8519,
    "package_name": "amerifluxr",
    "title": "Interface to 'AmeriFlux' Data Services",
    "description": "Programmatic interface to the 'AmeriFlux' database\n(<https://ameriflux.lbl.gov/>). Provide query, download, and\ndata summary tools.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3659,
    "stars": 0
  },
  {
    "id": 20362,
    "package_name": "psfmi",
    "title": "Prediction Model Pooling, Selection and Performance Evaluation\nAcross Multiply Imputed Datasets",
    "description": "Pooling, backward and forward selection of linear,\nlogistic and Cox regression models in multiply imputed\ndatasets. Backward and forward selection can be done from the\npooled model using Rubin's Rules (RR), the D1, D2, D3, D4 and\nthe median p-values method. This is also possible for Mixed\nmodels. The models can contain continuous, dichotomous,\ncategorical and restricted cubic spline predictors and\ninteraction terms between all these type of predictors. The\nstability of the models can be evaluated using (cluster)\nbootstrapping. The package further contains functions to pool\nmodel performance measures as ROC/AUC, Reclassification,\nR-squared, scaled Brier score, H&L test and calibration plots\nfor logistic regression models. Internal validation can be done\nacross multiply imputed datasets with cross-validation or\nbootstrapping. The adjusted intercept after shrinkage of pooled\nregression coefficients can be obtained. Backward and forward\nselection as part of internal validation is possible. A\nfunction to externally validate logistic prediction models in\nmultiple imputed datasets is available and a function to\ncompare models. For Cox models a strata variable can be\nincluded. Eekhout (2017) <doi:10.1186/s12874-017-0404-7>. Wiel\n(2009) <doi:10.1093/biostatistics/kxp011>. Marshall (2009)\n<doi:10.1186/1471-2288-9-57>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3274,
    "stars": 0
  },
  {
    "id": 24561,
    "package_name": "theft",
    "title": "Tools for Handling Extraction of Features from Time Series",
    "description": "Consolidates and calculates different sets of time-series\nfeatures from multiple 'R' and 'Python' packages including\n'Rcatch22' Henderson, T. (2021) <doi:10.5281/zenodo.5546815>,\n'feasts' O'Hara-Wild, M., Hyndman, R., and Wang, E. (2021)\n<https://CRAN.R-project.org/package=feasts>, 'tsfeatures'\nHyndman, R., Kang, Y., Montero-Manso, P., Talagala, T., Wang,\nE., Yang, Y., and O'Hara-Wild, M. (2020)\n<https://CRAN.R-project.org/package=tsfeatures>, 'tsfresh'\nChrist, M., Braun, N., Neuffer, J., and Kempa-Liehr A.W. (2018)\n<doi:10.1016/j.neucom.2018.03.067>, 'TSFEL' Barandas, M., et\nal. (2020) <doi:10.1016/j.softx.2020.100456>, and 'Kats'\nFacebook Infrastructure Data Science (2021)\n<https://facebookresearch.github.io/Kats/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3257,
    "stars": 0
  },
  {
    "id": 25139,
    "package_name": "tsmp",
    "title": "Time Series with Matrix Profile",
    "description": "A toolkit implementing the Matrix Profile concept that was\ncreated by CS-UCR\n<http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.3088,
    "stars": 0
  },
  {
    "id": 13006,
    "package_name": "fastcpd",
    "title": "Fast Change Point Detection via Sequential Gradient Descent",
    "description": "Implements fast change point detection algorithm based on\nthe paper \"Sequential Gradient Descent and Quasi-Newton's\nMethod for Change-Point Analysis\" by Xianyang Zhang, Trisha\nDawn <https://proceedings.mlr.press/v206/zhang23b.html>. The\nalgorithm is based on dynamic programming with pruning and\nsequential gradient descent. It is able to detect change points\na magnitude faster than the vanilla Pruned Exact Linear\nTime(PELT). The package includes examples of linear regression,\nlogistic regression, Poisson regression, penalized linear\nregression data, and whole lot more examples with custom cost\nfunction in case the user wants to use their own cost function.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.2095,
    "stars": 0
  },
  {
    "id": 24323,
    "package_name": "tactile",
    "title": "New and Extended Plots, Methods, and Panel Functions for\n'lattice'",
    "description": "Extensions to 'lattice', providing new high-level\nfunctions, methods for existing functions, panel functions, and\na theme.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.2067,
    "stars": 0
  },
  {
    "id": 17155,
    "package_name": "measr",
    "title": "Bayesian Psychometric Measurement Using 'Stan'",
    "description": "Estimate diagnostic classification models (also called\ncognitive diagnostic models) with 'Stan'. Diagnostic\nclassification models are confirmatory latent class models, as\ndescribed by Rupp et al. (2010, ISBN: 978-1-60623-527-0).\nAutomatically generate 'Stan' code for the general loglinear\ncognitive diagnostic diagnostic model proposed by Henson et al.\n(2009) <doi:10.1007/s11336-008-9089-5> and other subtypes that\nintroduce additional model constraints. Using the generated\n'Stan' code, estimate the model evaluate the model's\nperformance using model fit indices, information criteria, and\nreliability metrics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1998,
    "stars": 0
  },
  {
    "id": 24642,
    "package_name": "tidyfit",
    "title": "Regularized Linear Modeling with Tidy Data",
    "description": "An extension to the 'R' tidy data environment for\nautomated machine learning. The package allows fitting and\ncross validation of linear regression and classification\nalgorithms on grouped data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1953,
    "stars": 0
  },
  {
    "id": 17333,
    "package_name": "methylGSA",
    "title": "Gene Set Analysis Using the Outcome of Differential Methylation",
    "description": "The main functions for methylGSA are methylglm and\nmethylRRA. methylGSA implements logistic regression adjusting\nnumber of probes as a covariate. methylRRA adjusts multiple\np-values of each gene by Robust Rank Aggregation. For more\ndetailed help information, please see the vignette.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1553,
    "stars": 0
  },
  {
    "id": 24873,
    "package_name": "topconfects",
    "title": "Top Confident Effect Sizes",
    "description": "Rank results by confident effect sizes, while maintaining\nFalse Discovery Rate and False Coverage-statement Rate control.\nTopconfects is an alternative presentation of TREAT results\nwith improved usability, eliminating p-values and instead\nproviding confidence bounds. The main application is\ndifferential gene expression analysis, providing genes ranked\nin order of confident log2 fold change, but it can be applied\nto any collection of effect sizes with associated standard\nerrors.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1361,
    "stars": 0
  },
  {
    "id": 14849,
    "package_name": "haldensify",
    "title": "Highly Adaptive Lasso Conditional Density Estimation",
    "description": "An algorithm for flexible conditional density estimation\nbased on application of pooled hazard regression to an\nartificial repeated measures dataset constructed by\ndiscretizing the support of the outcome variable. To facilitate\nflexible estimation of the conditional density, the highly\nadaptive lasso, a non-parametric regression function shown to\nestimate cadlag (RCLL) functions at a suitably fast convergence\nrate, is used. The use of pooled hazards regression for\nconditional density estimation as implemented here was first\ndescribed for by Díaz and van der Laan (2011)\n<doi:10.2202/1557-4679.1356>. Building on the conditional\ndensity estimation utilities, non-parametric inverse\nprobability weighted (IPW) estimators of the causal effects of\nadditive modified treatment policies are implemented, using\nconditional density estimation to estimate the generalized\npropensity score. Non-parametric IPW estimators based on this\ncan be coupled with undersmoothing of the generalized\npropensity score estimator to attain the semi-parametric\nefficiency bound (per Hejazi, Díaz, and van der Laan\n<doi:10.48550/arXiv.2205.05777>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1286,
    "stars": 0
  },
  {
    "id": 9484,
    "package_name": "blorr",
    "title": "Tools for Developing Binary Logistic Regression Models",
    "description": "Tools designed to make it easier for beginner and\nintermediate users to build and validate binary logistic\nregression models. Includes bivariate analysis, comprehensive\nregression output, model fit statistics, variable selection\nprocedures, model validation techniques and a 'shiny' app for\ninteractive model building.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.1273,
    "stars": 0
  },
  {
    "id": 3484,
    "package_name": "JMbayes",
    "title": "Joint Modeling of Longitudinal and Time-to-Event Data under a\nBayesian Approach",
    "description": "Shared parameter models for the joint modeling of\nlongitudinal and time-to-event data using MCMC; Dimitris\nRizopoulos (2016) <doi:10.18637/jss.v072.i07>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0898,
    "stars": 0
  },
  {
    "id": 22293,
    "package_name": "satuRn",
    "title": "Scalable Analysis of Differential Transcript Usage for Bulk and\nSingle-Cell RNA-sequencing Applications",
    "description": "satuRn provides a higly performant and scalable framework\nfor performing differential transcript usage analyses. The\npackage consists of three main functions. The first function,\nfitDTU, fits quasi-binomial generalized linear models that\nmodel transcript usage in different groups of interest. The\nsecond function, testDTU, tests for differential usage of\ntranscripts between groups of interest. Finally, plotDTU\nvisualizes the usage profiles of transcripts in groups of\ninterest.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0796,
    "stars": 0
  },
  {
    "id": 16403,
    "package_name": "lfa",
    "title": "Logistic Factor Analysis for Categorical Data",
    "description": "Logistic Factor Analysis is a method for a PCA analogue on\nBinomial data via estimation of latent structure in the natural\nparameter.  The main method estimates genetic population\nstructure from genotype data.  There are also methods for\nestimating individual-specific allele frequencies using the\npopulation structure.  Lastly, a structured Hardy-Weinberg\nequilibrium (HWE) test is developed, which quantifies the\ngoodness of fit of the genotype data to the estimated\npopulation structure, via the estimated individual-specific\nallele frequencies (all of which generalizes traditional HWE\ntests).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0542,
    "stars": 0
  },
  {
    "id": 11089,
    "package_name": "crumblr",
    "title": "Count ratio uncertainty modeling base linear regression",
    "description": "Crumblr enables analysis of count ratio data using\nprecision weighted linear (mixed) models.  It uses an\nasymptotic normal approximation of the variance following the\ncentered log ration transform (CLR) that is widely used in\ncompositional data analysis.  Crumblr provides a fast, flexible\nalternative to GLMs and GLMM's while retaining high power and\ncontrolling the false positive rate.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0465,
    "stars": 0
  },
  {
    "id": 12837,
    "package_name": "exuber",
    "title": "Econometric Analysis of Explosive Time Series",
    "description": "Testing for and dating periods of explosive dynamics\n(exuberance) in time series using the univariate and panel\nrecursive unit root tests proposed by Phillips et al. (2015)\n<doi:10.1111/iere.12132> and Pavlidis et al. (2016)\n<doi:10.1007/s11146-015-9531-2>.The recursive least-squares\nalgorithm utilizes the matrix inversion lemma to avoid matrix\ninversion which results in significant speed improvements.\nSimulation of a variety of periodically-collapsing bubble\nprocesses. Details can be found in Vasilopoulos et al. (2022)\n<doi:10.18637/jss.v103.i10>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0367,
    "stars": 0
  },
  {
    "id": 15927,
    "package_name": "joineR",
    "title": "Joint Modelling of Repeated Measurements and Time-to-Event Data",
    "description": "Analysis of repeated measurements and time-to-event data\nvia random effects joint models. Fits the joint models proposed\nby Henderson and colleagues <doi:10.1093/biostatistics/1.4.465>\n(single event time) and by Williamson and colleagues (2008)\n<doi:10.1002/sim.3451> (competing risks events time) to a\nsingle continuous repeated measure. The time-to-event data is\nmodelled using a (cause-specific) Cox proportional hazards\nregression model with time-varying covariates. The longitudinal\noutcome is modelled using a linear mixed effects model. The\nassociation is captured by a latent Gaussian process. The model\nis estimated using am Expectation Maximization algorithm. Some\nplotting functions and the variogram are also included. This\nproject is funded by the Medical Research Council (Grant\nnumbers G0400615 and MR/M013227/1).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 7.0023,
    "stars": 0
  },
  {
    "id": 6381,
    "package_name": "ResidualMatrix",
    "title": "Creating a DelayedMatrix of Regression Residuals",
    "description": "Provides delayed computation of a matrix of residuals\nafter fitting a linear model to each column of an input matrix.\nAlso supports partial computation of residuals where selected\nfactors are to be preserved in the output matrix. Implements a\nnumber of efficient methods for operating on the delayed matrix\nof residuals, most notably matrix multiplication and\ncalculation of row/column sums or means.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.999,
    "stars": 0
  },
  {
    "id": 23181,
    "package_name": "skpr",
    "title": "Design of Experiments Suite: Generate and Evaluate Optimal\nDesigns",
    "description": "Generates and evaluates D, I, A, Alias, E, T, and G\noptimal designs. Supports generation and evaluation of blocked\nand split/split-split/.../N-split plot designs. Includes\nparametric and Monte Carlo power evaluation functions, and\nsupports calculating power for censored responses. Provides a\nframework to evaluate power using functions provided in other\npackages or written by the user. Includes a Shiny graphical\nuser interface that displays the underlying code used to create\nand evaluate the design to improve ease-of-use and make\nanalyses more reproducible. For details, see Morgan-Wall et al.\n(2021) <doi:10.18637/jss.v099.i01>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9864,
    "stars": 0
  },
  {
    "id": 8407,
    "package_name": "aion",
    "title": "Archaeological Time Series",
    "description": "A toolkit for archaeological time series and time\nintervals. This package provides a system of classes and\nmethods to represent and work with archaeological time series\nand time intervals. Dates are represented as \"rata die\" and can\nbe converted to (virtually) any calendar defined by Reingold\nand Dershowitz (2018) <doi:10.1017/9781107415058>. This\npackages offers a simple API that can be used by other\nspecialized packages.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9779,
    "stars": 0
  },
  {
    "id": 21292,
    "package_name": "regmedint",
    "title": "Regression-Based Causal Mediation Analysis with Interaction and\nEffect Modification Terms",
    "description": "This is an extension of the regression-based causal\nmediation analysis first proposed by Valeri and VanderWeele\n(2013) <doi:10.1037/a0031034> and Valeri and VanderWeele (2015)\n<doi:10.1097/EDE.0000000000000253>). It supports including\neffect measure modification by covariates(treatment-covariate\nand mediator-covariate product terms in mediator and outcome\nregression models) as proposed by Li et al (2023)\n<doi:10.1097/EDE.0000000000001643>. It also accommodates the\noriginal 'SAS' macro and 'PROC CAUSALMED' procedure in 'SAS'\nwhen there is no effect measure modification. Linear and\nlogistic models are supported for the mediator model. Linear,\nlogistic, loglinear, Poisson, negative binomial, Cox, and\naccelerated failure time (exponential and Weibull) models are\nsupported for the outcome model.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9713,
    "stars": 0
  },
  {
    "id": 12023,
    "package_name": "drtmle",
    "title": "Doubly-Robust Nonparametric Estimation and Inference",
    "description": "Targeted minimum loss-based estimators of counterfactual\nmeans and causal effects that are doubly-robust with respect\nboth to consistency and asymptotic normality (Benkeser et al\n(2017), <doi:10.1093/biomet/asx053>; MJ van der Laan (2014),\n<doi:10.1515/ijb-2012-0038>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9227,
    "stars": 0
  },
  {
    "id": 21013,
    "package_name": "rbacon",
    "title": "Age-Depth Modelling using Bayesian Statistics",
    "description": "An approach to age-depth modelling that uses Bayesian\nstatistics to reconstruct accumulation histories for deposits,\nthrough combining radiocarbon and other dates with prior\ninformation on accumulation rates and their variability. See\nBlaauw & Christen (2011).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.9194,
    "stars": 0
  },
  {
    "id": 9760,
    "package_name": "bvartools",
    "title": "Bayesian Inference of Vector Autoregressive and Error Correction\nModels",
    "description": "Assists in the set-up of algorithms for Bayesian inference\nof vector autoregressive (VAR) and error correction (VEC)\nmodels. Functions for posterior simulation, forecasting,\nimpulse response analysis and forecast error variance\ndecomposition are largely based on the introductory texts of\nChan, Koop, Poirier and Tobias (2019, ISBN: 9781108437493),\nKoop and Korobilis (2010) <doi:10.1561/0800000013> and\nLuetkepohl (2006, ISBN: 9783540262398).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8987,
    "stars": 0
  },
  {
    "id": 16576,
    "package_name": "lmtp",
    "title": "Non-Parametric Causal Effects of Feasible Interventions Based on\nModified Treatment Policies",
    "description": "Non-parametric estimators for casual effects based on\nlongitudinal modified treatment policies as described in Diaz,\nWilliams, Hoffman, and Schenck\n<doi:10.1080/01621459.2021.1955691>, traditional point\ntreatment, and traditional longitudinal effects. Continuous,\nbinary, categorical treatments, and multivariate treatments are\nallowed as well are censored outcomes. The treatment mechanism\nis estimated via a density ratio classification procedure\nirrespective of treatment variable type. For both continuous\nand binary outcomes, additive treatment effects can be\ncalculated and relative risks and odds ratios may be calculated\nfor binary outcomes. Supports survival outcomes with competing\nrisks (Diaz, Hoffman, and Hejazi;\n<doi:10.1007/s10985-023-09606-7>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8976,
    "stars": 0
  },
  {
    "id": 14654,
    "package_name": "gratis",
    "title": "Generating Time Series with Diverse and Controllable\nCharacteristics",
    "description": "Generates synthetic time series based on various\nunivariate time series models including MAR and ARIMA\nprocesses. Kang, Y., Hyndman, R.J., Li, F.(2020)\n<doi:10.1002/sam.11461>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8526,
    "stars": 0
  },
  {
    "id": 6159,
    "package_name": "Rbeast",
    "title": "Bayesian Change-Point Detection and Time Series Decomposition",
    "description": "Interpretation of time series data is affected by model\nchoices. Different models can give different or even\ncontradicting estimates of patterns, trends, and mechanisms for\nthe same data--a limitation alleviated by the Bayesian\nestimator of abrupt change,seasonality, and trend (BEAST) of\nthis package. BEAST seeks to improve time series decomposition\nby forgoing the \"single-best-model\" concept and embracing all\ncompeting models into the inference via a Bayesian model\naveraging scheme. It is a flexible tool to uncover abrupt\nchanges (i.e., change-points), cyclic variations (e.g.,\nseasonality), and nonlinear trends in time-series observations.\nBEAST not just tells when changes occur but also quantifies how\nlikely the detected changes are true. It detects not just\npiecewise linear trends but also arbitrary nonlinear trends.\nBEAST is applicable to real-valued time series data of all\nkinds, be it for remote sensing, economics, climate sciences,\necology, and hydrology. Example applications include its use to\nidentify regime shifts in ecological data, map forest\ndisturbance and land degradation from satellite imagery, detect\nmarket trends in economic data, pinpoint anomaly and extreme\nevents in climate data, and unravel system dynamics in\nbiological data. Details on BEAST are reported in Zhao et al.\n(2019) <doi:10.1016/j.rse.2019.04.034>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.8444,
    "stars": 0
  },
  {
    "id": 632,
    "package_name": "BayesTools",
    "title": "Tools for Bayesian Analyses",
    "description": "Provides tools for conducting Bayesian analyses and\nBayesian model averaging (Kass and Raftery, 1995,\n<doi:10.1080/01621459.1995.10476572>, Hoeting et al., 1999,\n<doi:10.1214/ss/1009212519>). The package contains functions\nfor creating a wide range of prior distribution objects, mixing\nposterior samples from 'JAGS' and 'Stan' models, plotting\nposterior distributions, and etc... The tools for working with\nprior distribution span from visualization, generating 'JAGS'\nand 'bridgesampling' syntax to basic functions such as rng,\nquantile, and distribution functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7835,
    "stars": 0
  },
  {
    "id": 6739,
    "package_name": "SIAMCAT",
    "title": "Statistical Inference of Associations between Microbial\nCommunities And host phenoTypes",
    "description": "Pipeline for Statistical Inference of Associations between\nMicrobial Communities And host phenoTypes (SIAMCAT). A primary\ngoal of analyzing microbiome data is to determine changes in\ncommunity composition that are associated with environmental\nfactors. In particular, linking human microbiome composition to\nhost phenotypes such as diseases has become an area of intense\nresearch. For this, robust statistical modeling and biomarker\nextraction toolkits are crucially needed. SIAMCAT provides a\nfull pipeline supporting data preprocessing, statistical\nassociation testing, statistical modeling (LASSO logistic\nregression) including tools for evaluation and interpretation\nof these models (such as cross validation, parameter selection,\nROC analysis and diagnostic model plots).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7764,
    "stars": 0
  },
  {
    "id": 26128,
    "package_name": "zenith",
    "title": "Gene set analysis following differential expression using linear\n(mixed) modeling with dream",
    "description": "Zenith performs gene set analysis on the result of\ndifferential expression using linear (mixed) modeling with\ndream by considering the correlation between gene expression\ntraits.  This package implements the camera method from the\nlimma package proposed by Wu and Smyth (2012).  Zenith is a\nsimple extension of camera to be compatible with linear mixed\nmodels implemented in variancePartition::dream().",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7705,
    "stars": 0
  },
  {
    "id": 6172,
    "package_name": "Rcatch22",
    "title": "Calculation of 22 CAnonical Time-Series CHaracteristics",
    "description": "Calculate 22 summary statistics coded in C on time-series\nvectors to enable pattern detection, classification, and\nregression applications in the feature space as proposed by\nLubba et al. (2019) <doi:10.1007/s10618-019-00647-x>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7554,
    "stars": 0
  },
  {
    "id": 3638,
    "package_name": "LEA",
    "title": "LEA: an R package for Landscape and Ecological Association\nStudies",
    "description": "LEA is an R package dedicated to population genomics,\nlandscape genomics and genotype-environment association tests.\nLEA can run analyses of population structure and genome-wide\ntests for local adaptation, and also performs imputation of\nmissing genotypes. The package includes statistical methods for\nestimating ancestry coefficients from large genotypic matrices\nand for evaluating the number of ancestral populations (snmf).\nIt performs statistical tests using latent factor mixed models\nfor identifying genetic polymorphisms that exhibit association\nwith environmental gradients or phenotypic traits (lfmm2). In\naddition, LEA computes values of genetic offset statistics\nbased on new or predicted environments (genetic.gap,\ngenetic.offset). LEA is mainly based on optimized programs that\ncan scale with the dimensions of large data sets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7033,
    "stars": 0
  },
  {
    "id": 22343,
    "package_name": "scLANE",
    "title": "Model Gene Expression Dynamics with Spline-Based NB GLMs, GEEs,\n& GLMMs",
    "description": "Our scLANE model uses truncated power basis spline models\nto build flexible, interpretable models of single cell gene\nexpression over pseudotime or latent time. The modeling\narchitectures currently supported are Negative-binomial GLMs,\nGEEs, & GLMMs. Downstream analysis functionalities include\nmodel comparison, dynamic gene clustering, smoothed counts\ngeneration, gene set enrichment testing, & visualization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.7004,
    "stars": 0
  },
  {
    "id": 24588,
    "package_name": "thurstonianIRT",
    "title": "Thurstonian IRT Models",
    "description": "Fit Thurstonian Item Response Theory (IRT) models in R.\nThis package supports fitting Thurstonian IRT models and its\nextensions using 'Stan', 'lavaan', or 'Mplus' for the model\nestimation. Functionality for extracting results, making\npredictions, and simulating data is provided as well.\nReferences: Brown & Maydeu-Olivares (2011)\n<doi:10.1177/0013164410375112>; Bürkner et al. (2019)\n<doi:10.1177/0013164419832063>.",
    "version": "0.12.6",
    "maintainer": "Paul-Christian Bürkner <paul.buerkner@gmail.com>",
    "url": "https://github.com/paul-buerkner/thurstonianIRT",
    "exports": [
      ["cor_matrix"],
      ["empty_block"],
      ["fit_TIRT_lavaan"],
      ["fit_TIRT_mplus"],
      ["fit_TIRT_stan"],
      ["gof"],
      ["make_lavaan_code"],
      ["make_mplus_code"],
      ["make_sem_data"],
      ["make_stan_data"],
      ["make_TIRT_data"],
      ["set_block"],
      ["set_blocks_from_df"],
      ["sim_TIRT_data"]
    ],
    "topics": [
      ["cpp"]
    ],
    "score": 6.6866,
    "stars": 36
  },
  {
    "id": 20236,
    "package_name": "proDA",
    "title": "Differential Abundance Analysis of Label-Free Mass Spectrometry\nData",
    "description": "Account for missing values in label-free mass spectrometry\ndata without imputation. The package implements a probabilistic\ndropout model that ensures that the information from observed\nand missing values are properly combined. It adds empirical\nBayesian priors to increase power to detect differentially\nabundant proteins.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6847,
    "stars": 0
  },
  {
    "id": 21016,
    "package_name": "rbcb",
    "title": "R Interface to Brazilian Central Bank Web Services",
    "description": "The Brazilian Central Bank API delivers many datasets\nwhich regard economic activity, regional economy, international\neconomy, public finances, credit indicators and many more. For\nmore information please see <http://dadosabertos.bcb.gov.br/>.\nThese datasets can be accessed through 'rbcb' functions and can\nbe obtained in different data structures common to R ('tibble',\n'data.frame', 'xts', ...).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6591,
    "stars": 0
  },
  {
    "id": 22928,
    "package_name": "shinymrp",
    "title": "Interface for Multilevel Regression and Poststratification",
    "description": "Dual interfaces, graphical and programmatic, designed for\nintuitive applications of Multilevel Regression and\nPoststratification (MRP). Users can apply the method to a\nvariety of datasets, from electronic health records to sample\nsurvey data, through an end-to-end Bayesian data analysis\nworkflow. The package provides robust tools for data cleaning,\nexploratory analysis, flexible model building, and insightful\nresult visualization. For more details, see Si et al. (2020)\n<https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2020002/article/00003-eng.pdf?st=iF1_Fbrh>\nand Si (2025) <doi:10.1214/24-STS932>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6584,
    "stars": 0
  },
  {
    "id": 5886,
    "package_name": "RMediation",
    "title": "Mediation Analysis Confidence Intervals",
    "description": "Computes confidence intervals for nonlinear functions of\nmodel parameters (e.g., product of k coefficients) in\nsingle-level and multilevel structural equation models. Methods\ninclude the distribution of the product, Monte Carlo\nsimulation, and bootstrap methods. It also performs the\nModel-Based Constrained Optimization (MBCO) procedure for\nhypothesis testing of indirect effects. References: Tofighi,\nD., and MacKinnon, D. P. (2011). RMediation: An R package for\nmediation analysis confidence intervals. Behavior Research\nMethods, 43, 692-700. <doi:10.3758/s13428-011-0076-x>; Tofighi,\nD., and Kelley, K. (2020). Improved inference in mediation\nanalysis: Introducing the model-based constrained optimization\nprocedure. Psychological Methods, 25(4), 496-515.\n<doi:10.1037/met0000259>; Tofighi, D. (2020). Bootstrap\nModel-Based Constrained Optimization Tests of Indirect Effects.\nFrontiers in Psychology, 10, 2989.\n<doi:10.3389/fpsyg.2019.02989>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6535,
    "stars": 0
  },
  {
    "id": 7751,
    "package_name": "Trendy",
    "title": "Breakpoint analysis of time-course expression data",
    "description": "Trendy implements segmented (or breakpoint) regression\nmodels to estimate breakpoints which represent changes in\nexpression for each feature/gene in high throughput data with\nordered conditions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6232,
    "stars": 0
  },
  {
    "id": 11883,
    "package_name": "dmrseq",
    "title": "Detection and inference of differentially methylated regions\nfrom Whole Genome Bisulfite Sequencing",
    "description": "This package implements an approach for scanning the\ngenome to detect and perform accurate inference on\ndifferentially methylated regions from Whole Genome Bisulfite\nSequencing data. The method is based on comparing detected\nregions to a pooled null distribution, that can be implemented\neven when as few as two samples per population are available.\nRegion-level statistics are obtained by fitting a generalized\nleast squares (GLS) regression model with a nested\nautoregressive correlated error structure for the effect of\ninterest on transformed methylation proportions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.6232,
    "stars": 0
  },
  {
    "id": 1317,
    "package_name": "CoGAPS",
    "title": "Coordinated Gene Activity in Pattern Sets",
    "description": "Coordinated Gene Activity in Pattern Sets (CoGAPS)\nimplements a Bayesian MCMC matrix factorization algorithm,\nGAPS, and links it to gene set statistic methods to infer\nbiological process activity.  It can be used to perform sparse\nmatrix factorization on any data, and when this data represents\nbiomolecules, to do gene set analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5944,
    "stars": 0
  },
  {
    "id": 24709,
    "package_name": "tidytreatment",
    "title": "Tidy Methods for Bayesian Treatment Effect Models",
    "description": "Functions for extracting tidy data from Bayesian treatment\neffect models, in particular BART, but extensions are possible.\nFunctionality includes extracting tidy posterior summaries as\nin 'tidybayes' <https://github.com/mjskay/tidybayes>,\nestimating (average) treatment effects, common support\ncalculations, and plotting useful summaries of these.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5453,
    "stars": 0
  },
  {
    "id": 15904,
    "package_name": "jfa",
    "title": "Statistical Methods for Auditing",
    "description": "Provides statistical methods for auditing as implemented\nin JASP for Audit (Derks et al., 2021\n<doi:10.21105/joss.02733>). First, the package makes it easy\nfor an auditor to plan a statistical sample, select the sample\nfrom the population, and evaluate the misstatement in the\nsample compliant with international auditing standards. Second,\nthe package provides statistical methods for auditing data,\nincluding tests of digit distributions and repeated values.\nFinally, the package includes methods for auditing algorithms\non the aspect of fairness and bias. Next to classical\nstatistical methodology, the package implements Bayesian\nequivalents of these methods whose statistical underpinnings\nare described in Derks et al. (2021) <doi:10.1111/ijau.12240>,\nDerks et al. (2024) <doi:10.2308/AJPT-2021-086>, Derks et al.\n(2022) <doi:10.31234/osf.io/8nf3e> Derks et al. (2024)\n<doi:10.31234/osf.io/tgq5z>, and Derks et al. (2025)\n<doi:10.31234/osf.io/b8tu2>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5138,
    "stars": 0
  },
  {
    "id": 25634,
    "package_name": "vizdraws",
    "title": "Visualize Draws from the Prior and Posterior Distributions",
    "description": "Interactive visualization for Bayesian prior and posterior\ndistributions. This package facilitates an animated transition\nbetween prior and posterior distributions. Additionally, it\nsplits the distribution into bars based on the provided\n'breaks,' displaying the probability for each region. If no\n'breaks' are provided, it defaults to zero.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.5105,
    "stars": 0
  },
  {
    "id": 9712,
    "package_name": "bssm",
    "title": "Bayesian Inference of Non-Linear and Non-Gaussian State Space\nModels",
    "description": "Efficient methods for Bayesian inference of state space\nmodels via Markov chain Monte Carlo (MCMC) based on parallel\nimportance sampling type weighted estimators (Vihola, Helske,\nand Franks, 2020, <doi:10.1111/sjos.12492>), particle MCMC, and\nits delayed acceptance version. Gaussian, Poisson, binomial,\nnegative binomial, and Gamma observation densities and basic\nstochastic volatility models with linear-Gaussian state\ndynamics, as well as general non-linear Gaussian models and\ndiscretised diffusion models are supported. See Helske and\nVihola (2021, <doi:10.32614/RJ-2021-103>) for details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.494,
    "stars": 0
  },
  {
    "id": 14814,
    "package_name": "gwasurvivr",
    "title": "gwasurvivr: an R package for genome wide survival analysis",
    "description": "gwasurvivr is a package to perform survival analysis using\nCox proportional hazard models on imputed genetic data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4776,
    "stars": 0
  },
  {
    "id": 23441,
    "package_name": "sparklyr.flint",
    "title": "Sparklyr Extension for 'Flint'",
    "description": "This sparklyr extension makes 'Flint' time series library\nfunctionalities (<https://github.com/twosigma/flint>) easily\naccessible through R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4648,
    "stars": 0
  },
  {
    "id": 25718,
    "package_name": "walker",
    "title": "Bayesian Generalized Linear Models with Time-Varying\nCoefficients",
    "description": "Efficient Bayesian generalized linear models with\ntime-varying coefficients as in Helske (2022,\n<doi:10.1016/j.softx.2022.101016>). Gaussian, Poisson, and\nbinomial observations are supported. The Markov chain Monte\nCarlo (MCMC) computations are done using Hamiltonian Monte\nCarlo provided by Stan, using a state space representation of\nthe model in order to marginalise over the coefficients for\nefficient sampling. For non-Gaussian models, the package uses\nthe importance sampling type estimators based on approximate\nmarginal MCMC as in Vihola, Helske, Franks (2020,\n<doi:10.1111/sjos.12492>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4594,
    "stars": 0
  },
  {
    "id": 1129,
    "package_name": "CalibraCurve",
    "title": "Calibration curves for targeted proteomics, lipidomics and\nmetabolomics data",
    "description": "CalibraCurve is a computational tool designed to generate\ncalibration curves for targeted mass spectrometry-based\nquantitative data. It is applicable to various omics\ndisciplines, including proteomics, lipidomics, and\nmetabolomics. The package also offers functionalities for data\nand calibration curve visualization and concentration\nprediction from new datasets based on the established curves.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4548,
    "stars": 0
  },
  {
    "id": 24782,
    "package_name": "tinysnapshot",
    "title": "Snapshots for Unit Tests using the 'tinytest' Framework",
    "description": "Snapshots for unit tests using the 'tinytest' framework\nfor R. Includes expectations to test base R and 'ggplot2' plots\nas well as console output from print().",
    "version": "0.2.0.2",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://github.com/vincentarelbundock/tinysnapshot",
    "exports": [
      ["expect_equivalent_images"],
      ["expect_snapshot_plot"],
      ["expect_snapshot_print"]
    ],
    "topics": [],
    "score": 6.4533,
    "stars": 18
  },
  {
    "id": 16039,
    "package_name": "kebabs",
    "title": "Kernel-Based Analysis of Biological Sequences",
    "description": "The package provides functionality for kernel-based\nanalysis of DNA, RNA, and amino acid sequences via SVM-based\nmethods. As core functionality, kebabs implements following\nsequence kernels: spectrum kernel, mismatch kernel, gappy pair\nkernel, and motif kernel. Apart from an efficient\nimplementation of standard position-independent functionality,\nthe kernels are extended in a novel way to take the position of\npatterns into account for the similarity measure. Because of\nthe flexibility of the kernel formulation, other kernels like\nthe weighted degree kernel or the shifted weighted degree\nkernel with constant weighting of positions are included as\nspecial cases. An annotation-specific variant of the kernels\nuses annotation information placed along the sequence together\nwith the patterns in the sequence. The package allows for the\ngeneration of a kernel matrix or an explicit feature\nrepresentation in dense or sparse format for all available\nkernels which can be used with methods implemented in other R\npackages. With focus on SVM-based methods, kebabs provides a\nframework which simplifies the usage of existing SVM\nimplementations in kernlab, e1071, and LiblineaR. Binary and\nmulti-class classification as well as regression tasks can be\nused in a unified way without having to deal with the different\nfunctions, parameters, and formats of the selected SVM. As\nsupport for choosing hyperparameters, the package provides\ncross validation - including grouped cross validation, grid\nsearch and model selection functions. For easier biological\ninterpretation of the results, the package computes feature\nweights for all SVMs and prediction profiles which show the\ncontribution of individual sequence positions to the prediction\nresult and indicate the relevance of sequence sections for the\nlearning result and the underlying biological functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4136,
    "stars": 0
  },
  {
    "id": 8539,
    "package_name": "anansi",
    "title": "Annotation-Based Analysis of Specific Interactions",
    "description": "Studies including both microbiome and metabolomics data\nare becoming more common. Often, it would be helpful to\nintegrate both datasets in order to see if they corroborate\neach others patterns. All vs all association is imprecise and\nlikely to yield spurious associations. This package takes a\nknowledge-based approach to constrain association search space,\nonly considering metabolite-function pairs that have been\nrecorded in a pathway database. This package also provides a\nframework to assess differential association.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.4065,
    "stars": 0
  },
  {
    "id": 21617,
    "package_name": "rjd3toolkit",
    "title": "Utility Functions Around 'JDemetra+ 3.0'",
    "description": "R Interface to 'JDemetra+ 3.x'\n(<https://github.com/jdemetra>) time series analysis software.\nIt provides functions allowing to model time series (create\noutlier regressors, user-defined calendar regressors, UCARIMA\nmodels...), to test the presence of trading days or seasonal\neffects and also to set specifications in pre-adjustment and\nbenchmarking when using 'rjd3x13' or 'rjd3tramoseats'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.315,
    "stars": 0
  },
  {
    "id": 6441,
    "package_name": "Rlibeemd",
    "title": "Ensemble Empirical Mode Decomposition (EEMD) and Its Complete\nVariant (CEEMDAN)",
    "description": "An R interface for libeemd (Luukko, Helske, Räsänen, 2016)\n<doi:10.1007/s00180-015-0603-9>, a C library of highly\nefficient parallelizable functions for performing the ensemble\nempirical mode decomposition (EEMD), its complete variant\n(CEEMDAN), the regular empirical mode decomposition (EMD), and\nbivariate EMD (BEMD). Due to the possible portability issues\nCRAN version no longer supports OpenMP, but you can install\nOpenMP-supported version from GitHub:\n<https://github.com/helske/Rlibeemd/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.3125,
    "stars": 0
  },
  {
    "id": 11662,
    "package_name": "dfms",
    "title": "Dynamic Factor Models",
    "description": "Efficient estimation of Dynamic Factor Models using the\nExpectation Maximization (EM) algorithm or Two-Step (2S)\nestimation, supporting datasets with missing data. Factors are\nassumed to follow a stationary VAR process of order p. The\nestimation options follow advances in the econometric\nliterature: either running the Kalman Filter and Smoother once\nwith initial values from PCA - 2S estimation as in Doz,\nGiannone and Reichlin (2011)\n<doi:10.1016/j.jeconom.2011.02.012> - or via iterated Kalman\nFiltering and Smoothing until EM convergence - following Doz,\nGiannone and Reichlin (2012) <doi:10.1162/REST_a_00225> - or\nusing the adapted EM algorithm of Banbura and Modugno (2014)\n<doi:10.1002/jae.2306>, allowing arbitrary patterns of missing\ndata. The implementation makes heavy use of the 'Armadillo'\n'C++' library and the 'collapse' package, providing for\nparticularly speedy estimation. A comprehensive set of methods\nsupports interpretation and visualization of the model as well\nas forecasting. Information criteria to choose the number of\nfactors are also provided - following Bai and Ng (2002)\n<doi:10.1111/1468-0262.00273>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.3096,
    "stars": 0
  },
  {
    "id": 22345,
    "package_name": "scMET",
    "title": "Bayesian modelling of cell-to-cell DNA methylation heterogeneity",
    "description": "High-throughput single-cell measurements of DNA methylomes\ncan quantify methylation heterogeneity and uncover its role in\ngene regulation. However, technical limitations and sparse\ncoverage can preclude this task. scMET is a hierarchical\nBayesian model which overcomes sparsity, sharing information\nacross cells and genomic features to robustly quantify genuine\nbiological heterogeneity. scMET can identify highly variable\nfeatures that drive epigenetic heterogeneity, and perform\ndifferential methylation and variability analyses. We\nillustrate how scMET facilitates the characterization of\nepigenetically distinct cell populations and how it enables the\nformulation of novel hypotheses on the epigenetic regulation of\ngene expression.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.3045,
    "stars": 0
  },
  {
    "id": 17308,
    "package_name": "metaseqR2",
    "title": "An R package for the analysis and result reporting of RNA-Seq\ndata by combining multiple statistical algorithms",
    "description": "Provides an interface to several normalization and\nstatistical testing packages for RNA-Seq gene expression data.\nAdditionally, it creates several diagnostic plots, performs\nmeta-analysis by combinining the results of several statistical\ntests and reports the results in an interactive way.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.301,
    "stars": 0
  },
  {
    "id": 7117,
    "package_name": "SimNPH",
    "title": "Simulate Non-Proportional Hazards",
    "description": "A toolkit for simulation studies concerning time-to-event\nendpoints with non-proportional hazards. 'SimNPH' encompasses\nfunctions for simulating time-to-event data in various\nscenarios, simulating different trial designs like\nfixed-followup, event-driven, and group sequential designs. The\npackage provides functions to calculate the true values of\ncommon summary statistics for the implemented scenarios and\noffers common analysis methods for time-to-event data. Helper\nfunctions for running simulations with the 'SimDesign' package\nand for aggregating and presenting the results are also\nincluded. Results of the conducted simulation study are\navailable in the paper: \"A Comparison of Statistical Methods\nfor Time-To-Event Analyses in Randomized Controlled Trials\nUnder Non-Proportional Hazards\", Klinglmüller et al. (2025)\n<doi:10.1002/sim.70019>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2967,
    "stars": 0
  },
  {
    "id": 11471,
    "package_name": "dearseq",
    "title": "Differential Expression Analysis for RNA-seq data through a\nrobust variance component test",
    "description": "Differential Expression Analysis RNA-seq data with\nvariance component score test accounting for data\nheteroscedasticity through precision weights. Perform both\ngene-wise and gene set analyses, and can deal with repeated or\nlongitudinal data. Methods are detailed in: i) Agniel D &\nHejblum BP (2017) Variance component score test for time-course\ngene set analysis of longitudinal RNA-seq data, Biostatistics,\n18(4):589-604 ; and ii) Gauthier M, Agniel D, Thiébaut R &\nHejblum BP (2020) dearseq: a variance component score test for\nRNA-Seq differential analysis that effectively controls the\nfalse discovery rate, NAR Genomics and Bioinformatics,\n2(4):lqaa093.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2723,
    "stars": 0
  },
  {
    "id": 13352,
    "package_name": "flowPloidy",
    "title": "Analyze flow cytometer data to determine sample ploidy",
    "description": "Determine sample ploidy via flow cytometry histogram\nanalysis. Reads Flow Cytometry Standard (FCS) files via the\nflowCore bioconductor package, and provides functions for\ndetermining the DNA ploidy of samples based on internal\nstandards.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2553,
    "stars": 0
  },
  {
    "id": 25192,
    "package_name": "tvReg",
    "title": "Time-Varying Coefficient for Single and Multi-Equation\nRegressions",
    "description": "Fitting time-varying coefficient models for single and\nmulti-equation regressions, using kernel smoothing techniques.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2402,
    "stars": 0
  },
  {
    "id": 9309,
    "package_name": "bimets",
    "title": "Time Series and Econometric Modeling",
    "description": "Time series analysis, (dis)aggregation and manipulation,\ne.g. time series extension, merge, projection, lag, lead,\ndelta, moving and cumulative average and product, selection by\nindex, date and year-period, conversion to daily, monthly,\nquarterly, (semi)annually. Simultaneous equation models\ndefinition, estimation, simulation and forecasting with\ncoefficient restrictions, error autocorrelation, exogenization,\nadd-factors, impact and interim multipliers analysis,\nconditional equation evaluation, rational expectations,\nendogenous targeting and model renormalization, structural\nstability, stochastic simulation and forecast, optimal control,\nby A. Luciani (2022) <doi:10.13140/RG.2.2.31160.83202>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.2041,
    "stars": 0
  },
  {
    "id": 5416,
    "package_name": "PortfolioTesteR",
    "title": "Test Investment Strategies with English-Like Code",
    "description": "Design, backtest, and analyze portfolio strategies using\nsimple, English-like function chains. Includes technical\nindicators, flexible stock selection, portfolio construction\nmethods (equal weighting, signal weighting, inverse volatility,\nhierarchical risk parity), and a compact backtesting engine for\nportfolio returns, drawdowns, and summary metrics.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1864,
    "stars": 0
  },
  {
    "id": 14749,
    "package_name": "gseries",
    "title": "Improve the Coherence of Your Time Series Data",
    "description": "'R' version of 'G-Series', Statistics Canada's generalized\nsystem devoted to the benchmarking and reconciliation of time\nseries data. The methods used in 'G-Series' essentially come\nfrom Dagum, E. B., and P. Cholette (2006)\n<doi:10.1007/0-387-35439-5>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1584,
    "stars": 0
  },
  {
    "id": 19011,
    "package_name": "optweight",
    "title": "Optimization-Based Stable Balancing Weights",
    "description": "Use optimization to estimate weights that balance\ncovariates for binary, multi-category, continuous, and\nmultivariate treatments in the spirit of Zubizarreta (2015)\n<doi:10.1080/01621459.2015.1023805>. The degree of balance can\nbe specified for each covariate. In addition, sampling weights\ncan be estimated that allow a sample to generalize to a\npopulation specified with given target moments of covariates,\nas in matching-adjusted indirect comparison (MAIC).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.142,
    "stars": 0
  },
  {
    "id": 10785,
    "package_name": "contsurvplot",
    "title": "Visualize the Effect of a Continuous Variable on a Time-to-Event\nOutcome",
    "description": "Graphically display the (causal) effect of a continuous\nvariable on a time-to-event outcome using multiple different\ntypes of plots based on g-computation. Those functions include,\namong others, survival area plots, survival contour plots,\nsurvival quantile plots and 3D surface plots. Due to the use of\ng-computation, all plot allow confounder-adjustment naturally.\nFor details, see Robin Denz, Nina Timmesfeld (2023)\n<doi:10.1097/EDE.0000000000001630>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1351,
    "stars": 0
  },
  {
    "id": 22541,
    "package_name": "season",
    "title": "Seasonal Analysis of Health Data",
    "description": "Routines for the seasonal analysis of health data,\nincluding regression models, time-stratified case-crossover,\nplotting functions and residual checks, see Barnett and Dobson\n(2010) ISBN 978-3-642-10748-1. Thanks to Yuming Guo for\nchecking the case-crossover code.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1338,
    "stars": 0
  },
  {
    "id": 6712,
    "package_name": "SEQTaRget",
    "title": "Sequential Trial Emulation",
    "description": "Implementation of sequential trial emulation for the\nanalysis of observational databases. The 'SEQTaRget' software\naccommodates time-varying treatments and confounders, as well\nas binary and failure time outcomes. 'SEQTaRget' allows to\ncompare both static and dynamic strategies, can be used to\nestimate observational analogs of intention-to-treat and\nper-protocol effects, and can adjust for potential selection\nbias induced by losses-to-follow-up. (Paper to come).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1303,
    "stars": 0
  },
  {
    "id": 884,
    "package_name": "CBNplot",
    "title": "plot bayesian network inferred from gene expression data based\non enrichment analysis results",
    "description": "This package provides the visualization of bayesian\nnetwork inferred from gene expression data. The networks are\nbased on enrichment analysis results inferred from packages\nincluding clusterProfiler and ReactomePA. The networks between\npathways and genes inside the pathways can be inferred and\nvisualized.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1206,
    "stars": 0
  },
  {
    "id": 20046,
    "package_name": "posteriordb",
    "title": "R Functionality for PosteriorDB",
    "description": "R functionality of easy handling of the posteriordb\nposteriors.",
    "version": "0.3.5",
    "maintainer": "Mans Magnusson <mans.magnusson@gmail.com>",
    "url": "",
    "exports": [
      ["alias_names"],
      ["as.data_info"],
      ["as.model_code"],
      ["as.model_info"],
      ["as.pdb_data"],
      ["as.pdb_data_info"],
      ["as.pdb_model_code"],
      ["as.pdb_model_info"],
      ["as.pdb_posterior"],
      ["as.pdb_reference_posterior_draws"],
      ["as.pdb_reference_posterior_info"],
      ["as.posterior"],
      ["as.reference_posterior_draws"],
      ["as.reference_posterior_info"],
      ["as.stan_code"],
      ["assert_checked_reference_posterior_draws"],
      ["assert_checked_summary_statistics_draws"],
      ["assert_reference_posterior_draws"],
      ["bibliography"],
      ["check_pdb"],
      ["check_pdb_posterior"],
      ["check_reference_posterior_draws"],
      ["check_summary_statistics_draws"],
      ["compute_reference_posterior_draws"],
      ["compute_reference_posterior_summary_statistic"],
      ["compute_summary_statistic"],
      ["data_file_path"],
      ["data_info"],
      ["data_names"],
      ["filter_posteriors"],
      ["framework"],
      ["framework<-"],
      ["get_data"],
      ["github_pat"],
      ["info"],
      ["info<-"],
      ["model_code"],
      ["model_code_file_path"],
      ["model_info"],
      ["model_names"],
      ["pdb"],
      ["pdb_bibliography"],
      ["pdb_config"],
      ["pdb_data"],
      ["pdb_data_info"],
      ["pdb_default"],
      ["pdb_github"],
      ["pdb_local"],
      ["pdb_model_code"],
      ["pdb_model_info"],
      ["pdb_posterior"],
      ["pdb_reference_posterior_draws"],
      ["pdb_reference_posterior_draws_info"],
      ["pdb_reference_posterior_info"],
      ["pdb_reference_posterior_summary_statistic"],
      ["pdb_reference_posterior_summary_statistics"],
      ["pdb_stan_code"],
      ["pdb_version"],
      ["posterior"],
      ["posterior_name"],
      ["posterior_names"],
      ["posteriors_tbl_df"],
      ["reference_posterior_draws"],
      ["reference_posterior_draws_file_path"],
      ["reference_posterior_draws_info"],
      ["reference_posterior_info"],
      ["reference_posterior_names"],
      ["reference_posterior_summary_statistic"],
      ["reference_posterior_summary_statistics"],
      ["remove_pdb"],
      ["rename_pdb"],
      ["stan_code"],
      ["stan_code_file_path"],
      ["stan_data"],
      ["stan_data_file_path"],
      ["thin_draws.pdb_reference_posterior_draws"],
      ["write_pdb"]
    ],
    "topics": [],
    "score": 6.1193,
    "stars": 9
  },
  {
    "id": 22722,
    "package_name": "sevenC",
    "title": "Computational Chromosome Conformation Capture by Correlation of\nChIP-seq at CTCF motifs",
    "description": "Chromatin looping is an essential feature of eukaryotic\ngenomes and can bring regulatory sequences, such as enhancers\nor transcription factor binding sites, in the close physical\nproximity of regulated target genes. Here, we provide sevenC,\nan R package that uses protein binding signals from ChIP-seq\nand sequence motif information to predict chromatin looping\nevents. Cross-linking of proteins that bind close to loop\nanchors result in ChIP-seq signals at both anchor loci. These\nsignals are used at CTCF motif pairs together with their\ndistance and orientation to each other to predict whether they\ninteract or not. The resulting chromatin loops might be used to\nassociate enhancers or transcription factor binding sites\n(e.g., ChIP-seq peaks) to regulated target genes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1139,
    "stars": 0
  },
  {
    "id": 19511,
    "package_name": "pense",
    "title": "Penalized Elastic Net S/MM-Estimator of Regression",
    "description": "Robust penalized (adaptive) elastic net S and M estimators\nfor linear regression. The methods are proposed in Cohen Freue,\nG. V., Kepplinger, D., Salibián-Barrera, M., and Smucler, E.\n(2019) <https://projecteuclid.org/euclid.aoas/1574910036>. The\npackage implements the extensions and algorithms described in\nKepplinger, D. (2020) <doi:10.14288/1.0392915>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1072,
    "stars": 0
  },
  {
    "id": 18633,
    "package_name": "normr",
    "title": "Normalization and difference calling in ChIP-seq data",
    "description": "Robust normalization and difference calling procedures for\nChIP-seq and alike data. Read counts are modeled jointly as a\nbinomial mixture model with a user-specified number of\ncomponents. A fitted background estimate accounts for the\neffect of enrichment in certain regions and, therefore,\nrepresents an appropriate null hypothesis. This robust\nbackground is used to identify significantly enriched or\ndepleted regions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.1021,
    "stars": 0
  },
  {
    "id": 22662,
    "package_name": "sentometrics",
    "title": "An Integrated Framework for Textual Sentiment Time Series\nAggregation and Prediction",
    "description": "Optimized prediction based on textual sentiment,\naccounting for the intrinsic challenge that sentiment can be\ncomputed and pooled across texts and time in various ways. See\nArdia et al. (2021) <doi:10.18637/jss.v099.i02>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0952,
    "stars": 0
  },
  {
    "id": 24975,
    "package_name": "transformGamPoi",
    "title": "Variance Stabilizing Transformation for Gamma-Poisson Models",
    "description": "Variance-stabilizing transformations help with the\nanalysis of heteroskedastic data (i.e., data where the variance\nis not constant, like count data). This package provide two\ntypes of variance stabilizing transformations: (1) methods\nbased on the delta method (e.g., 'acosh', 'log(x+1)'), (2)\nmodel residual based (Pearson and randomized quantile\nresiduals).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0906,
    "stars": 0
  },
  {
    "id": 22270,
    "package_name": "sarima",
    "title": "Simulation and Prediction with Seasonal ARIMA Models",
    "description": "Functions, classes and methods for time series modelling\nwith ARIMA and related models. The aim of the package is to\nprovide consistent interface for the user. For example, a\nsingle function autocorrelations() computes various kinds of\ntheoretical and sample autocorrelations. This is work in\nprogress, see the documentation and vignettes for the current\nfunctionality.  Function sarima() fits extended multiplicative\nseasonal ARIMA models with trends, exogenous variables and\narbitrary roots on the unit circle, which can be fixed or\nestimated (for the algebraic basis for this see\n<arXiv:2208.05055>, a paper on the methodology is being\nprepared).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0827,
    "stars": 0
  },
  {
    "id": 11494,
    "package_name": "deconvR",
    "title": "Simulation and Deconvolution of Omic Profiles",
    "description": "This package provides a collection of functions designed\nfor analyzing deconvolution of the bulk sample(s) using an\natlas of reference omic signature profiles and a user-selected\nmodel. Users are given the option to create or extend a\nreference atlas and,also simulate the desired size of the bulk\nsignature profile of the reference cell types.The package\nincludes the cell-type-specific methylation atlas and, Illumina\nEpic B5 probe ids that can be used in deconvolution.\nAdditionally,we included BSmeth2Probe, to make mapping WGBS\ndata to their probe IDs easier.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0792,
    "stars": 0
  },
  {
    "id": 5169,
    "package_name": "PROPS",
    "title": "PRObabilistic Pathway Score (PROPS)",
    "description": "This package calculates probabilistic pathway scores using\ngene expression data. Gene expression values are aggregated\ninto pathway-based scores using Bayesian network\nrepresentations of biological pathways.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0645,
    "stars": 0
  },
  {
    "id": 13773,
    "package_name": "gasmodel",
    "title": "Generalized Autoregressive Score Models",
    "description": "Estimation, forecasting, and simulation of generalized\nautoregressive score (GAS) models of Creal, Koopman, and Lucas\n(2013) <doi:10.1002/jae.1279> and Harvey (2013)\n<doi:10.1017/cbo9781139540933>. Model specification allows for\nvarious data types and distributions, different\nparametrizations, exogenous variables, joint and separate\nmodeling of exogenous variables and dynamics, higher score and\nautoregressive orders, custom and unconditional initial values\nof time-varying parameters, fixed and bounded values of\ncoefficients, and missing values. Model estimation is performed\nby the maximum likelihood method.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0512,
    "stars": 0
  },
  {
    "id": 13860,
    "package_name": "gemini",
    "title": "GEMINI: Variational inference approach to infer genetic\ninteractions from pairwise CRISPR screens",
    "description": "GEMINI uses log-fold changes to model sample-dependent and\nindependent effects, and uses a variational Bayes approach to\ninfer these effects. The inferred effects are used to score and\nidentify genetic interactions, such as lethality and recovery.\nMore details can be found in Zamanighomi et al. 2019 (in\npress).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0492,
    "stars": 0
  },
  {
    "id": 368,
    "package_name": "BANDITS",
    "title": "BANDITS: Bayesian ANalysis of DIfferenTial Splicing",
    "description": "BANDITS is a Bayesian hierarchical model for detecting\ndifferential splicing of genes and transcripts, via\ndifferential transcript usage (DTU), between two or more\nconditions. The method uses a Bayesian hierarchical framework,\nwhich allows for sample specific proportions in a\nDirichlet-Multinomial model, and samples the allocation of\nfragments to the transcripts. Parameters are inferred via\nMarkov chain Monte Carlo (MCMC) techniques and a DTU test is\nperformed via a multivariate Wald test on the posterior\ndensities for the average relative abundance of transcripts.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0459,
    "stars": 0
  },
  {
    "id": 9090,
    "package_name": "bayesplay",
    "title": "The Bayes Factor Playground",
    "description": "A lightweight modelling syntax for defining likelihoods\nand priors and for computing Bayes factors for simple one\nparameter models. It includes functionality for computing and\nplotting priors, likelihoods, and model predictions. Additional\nfunctionality is included for computing and plotting\nposteriors.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0334,
    "stars": 0
  },
  {
    "id": 1876,
    "package_name": "Dino",
    "title": "Normalization of Single-Cell mRNA Sequencing Data",
    "description": "Dino normalizes single-cell, mRNA sequencing data to\ncorrect for technical variation, particularly sequencing depth,\nprior to downstream analysis. The approach produces a matrix of\ncorrected expression for which the dependency between\nsequencing depth and the full distribution of normalized\nexpression; many existing methods aim to remove only the\ndependency between sequencing depth and the mean of the\nnormalized expression. This is particuarly useful in the\ncontext of highly sparse datasets such as those produced by 10X\ngenomics and other uninque molecular identifier (UMI) based\nmicrofluidics protocols for which the depth-dependent\nproportion of zeros in the raw expression data can otherwise\npresent a challenge.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0224,
    "stars": 0
  },
  {
    "id": 24221,
    "package_name": "switchde",
    "title": "Switch-like differential expression across single-cell\ntrajectories",
    "description": "Inference and detection of switch-like differential\nexpression across single-cell RNA-seq trajectories.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0212,
    "stars": 0
  },
  {
    "id": 8864,
    "package_name": "autonomics",
    "title": "Unified Statistical Modeling of Omics Data",
    "description": "This package unifies access to Statistal Modeling of Omics\nData. Across linear modeling engines (lm, lme, lmer, limma, and\nwilcoxon). Across coding systems (treatment, difference,\ndeviation, etc). Across model formulae (with/without intercept,\nrandom effect, interaction or nesting). Across omics platforms\n(microarray, rnaseq, msproteomics, affinity proteomics,\nmetabolomics). Across projection methods (pca, pls, sma, lda,\nspls, opls). Across clustering methods (hclust, pam, cmeans).\nAcross survival methods (coxph, survdiff, coin). It provides a\nfast enrichment analysis implementation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6.0086,
    "stars": 0
  },
  {
    "id": 4334,
    "package_name": "MetaboDynamics",
    "title": "Bayesian analysis of longitudinal metabolomics data",
    "description": "MetaboDynamics is an R-package that provides a framework\nof probabilistic models to analyze longitudinal metabolomics\ndata. It enables robust estimation of mean concentrations\ndespite varying spread between timepoints and reports\ndifferences between timepoints as well as metabolite specific\ndynamics profiles that can be used for identifying \"dynamics\nclusters\" of metabolites of similar dynamics. Provides\nprobabilistic over-representation analysis of KEGG functional\nmodules and pathways as well as comparison between clusters of\ndifferent experimental conditions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 6,
    "stars": 0
  },
  {
    "id": 7927,
    "package_name": "VedicDateTime",
    "title": "Vedic Calendar System",
    "description": "Provides platform for Vedic calendar system having several\nfunctionalities to facilitate conversion between Gregorian and\nVedic calendar systems, and helpful in examining its impact in\nthe time series analysis domain.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9823,
    "stars": 0
  },
  {
    "id": 11762,
    "package_name": "disaggR",
    "title": "Two-Steps Benchmarks for Time Series Disaggregation",
    "description": "The twoStepsBenchmark() and threeRuleSmooth() functions\nallow you to disaggregate a low-frequency time series with\nhigher frequency time series, using the French National\nAccounts methodology. The aggregated sum of the resulting time\nseries is strictly equal to the low-frequency time series\nwithin the benchmarking window. Typically, the low-frequency\ntime series is an annual one, unknown for the last year, and\nthe high frequency one is either quarterly or monthly. See\n\"Methodology of quarterly national accounts\", Insee Méthodes\nN°126, by Insee (2012, ISBN:978-2-11-068613-8,\n<https://www.insee.fr/en/information/2579410>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9809,
    "stars": 0
  },
  {
    "id": 2067,
    "package_name": "ENmix",
    "title": "Quality control and analysis tools for Illumina DNA methylation\nBeadChip",
    "description": "Tools for quanlity control, analysis and visulization of\nIllumina DNA methylation array data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9795,
    "stars": 0
  },
  {
    "id": 10956,
    "package_name": "coxphw",
    "title": "Weighted Estimation in Cox Regression",
    "description": "Implements weighted estimation in Cox regression as\nproposed by Schemper, Wakounig and Heinze (Statistics in\nMedicine, 2009, <doi:10.1002/sim.3623>) and as described in\nDunkler, Ploner, Schemper and Heinze (Journal of Statistical\nSoftware, 2018, <doi:10.18637/jss.v084.i02>). Weighted Cox\nregression provides unbiased average hazard ratio estimates\nalso in case of non-proportional hazards. Approximated\ngeneralized concordance probability an effect size measure for\nclear-cut decisions can be obtained. The package provides\noptions to estimate time-dependent effects conveniently by\nincluding interactions of covariates with arbitrary functions\nof time, with or without making use of the weighting option.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9759,
    "stars": 0
  },
  {
    "id": 18435,
    "package_name": "neverhpfilter",
    "title": "An Alternative to the Hodrick-Prescott Filter",
    "description": "In the working paper titled \"Why You Should Never Use the\nHodrick-Prescott Filter\", James D. Hamilton proposes a new\nalternative to economic time series filtering. The\nneverhpfilter package provides functions and data for\nreproducing his work. Hamilton (2017) <doi:10.3386/w23429>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.975,
    "stars": 0
  },
  {
    "id": 1295,
    "package_name": "ClustIRR",
    "title": "Clustering of immune receptor repertoires",
    "description": "ClustIRR analyzes repertoires of B- and T-cell receptors.\nIt starts by identifying communities of immune receptors with\nsimilar specificities, based on the sequences of their\ncomplementarity-determining regions (CDRs). Next, it employs a\nBayesian probabilistic models to quantify differential\ncommunity occupancy (DCO) between repertoires, allowing the\nidentification of expanding or contracting communities in\nresponse to e.g. infection or cancer treatment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9542,
    "stars": 0
  },
  {
    "id": 17617,
    "package_name": "mixvlmc",
    "title": "Variable Length Markov Chains with Covariates",
    "description": "Estimates Variable Length Markov Chains (VLMC) models and\nVLMC with covariates models from discrete sequences. Supports\nmodel selection via information criteria and simulation of new\nsequences from an estimated model. See Bühlmann, P. and Wyner,\nA. J. (1999) <doi:10.1214/aos/1018031204> for VLMC and Zanin\nZambom, A., Kim, S. and Lopes Garcia, N. (2022)\n<doi:10.1111/jtsa.12615> for VLMC with covariates.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9542,
    "stars": 0
  },
  {
    "id": 16469,
    "package_name": "limpa",
    "title": "Quantification and Differential Analysis of Proteomics Data",
    "description": "Quantification and differential analysis of\nmass-spectrometry proteomics data, with probabilistic recovery\nof information from missing values. Estimates the detection\nprobability curve (DPC), which relates the probability of\nsuccessful detection to the underlying expression level of each\npeptide, and uses it to incorporate peptide missing values into\nprotein quantification and into subsequent differential\nexpression analyses. The package produces objects suitable for\ndownstream analysis in limma. The package accepts peptide-level\ndata with missing values and produces complete protein\nquantifications without missing values. The uncertainty\nintroduced by missing value imputation is propagated through to\nthe limma analyses using variance modeling and precision\nweights. The package name \"limpa\" is an acronym for \"Linear\nModels for Proteomics Data\".",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9329,
    "stars": 0
  },
  {
    "id": 9062,
    "package_name": "bayesSSM",
    "title": "Bayesian Methods for State Space Models",
    "description": "Implements methods for Bayesian analysis of State Space\nModels. Includes implementations of the Particle Marginal\nMetropolis-Hastings algorithm described in Andrieu et al.\n(2010) <doi:10.1111/j.1467-9868.2009.00736.x> and automatic\ntuning inspired by Pitt et al. (2012)\n<doi:10.1016/j.jeconom.2012.06.004> and J. Dahlin and T. B.\nSchön (2019) <doi:10.18637/jss.v088.c02>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.9058,
    "stars": 0
  },
  {
    "id": 8003,
    "package_name": "Wats",
    "title": "Wrap Around Time Series Graphics",
    "description": "Wrap-around Time Series (WATS) plots for interrupted time\nseries designs with seasonal patterns. Longitudinal\ntrajectories are shown in both Cartesian and polar coordinates.\nIn many scenarios, a WATS plot more clearly shows the existence\nand effect size of of an intervention. This package accompanies\n\"Graphical Data Analysis on the Circle: Wrap-Around Time Series\nPlots for (Interrupted) Time Series Designs\" by Rodgers,\nBeasley, & Schuelke (2014) <doi:10.1080/00273171.2014.946589>;\nsee 'citation(\"Wats\")' for details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8963,
    "stars": 0
  },
  {
    "id": 1500,
    "package_name": "CytoGLMM",
    "title": "Conditional Differential Analysis for Flow and Mass Cytometry\nExperiments",
    "description": "The CytoGLMM R package implements two multiple regression\nstrategies: A bootstrapped generalized linear model (GLM) and a\ngeneralized linear mixed model (GLMM). Most current data\nanalysis tools compare expressions across many computationally\ndiscovered cell types. CytoGLMM focuses on just one cell type.\nOur narrower field of application allows us to define a more\nspecific statistical model with easier to control statistical\nguarantees. As a result, CytoGLMM finds differential proteins\nin flow and mass cytometry data while reducing biases arising\nfrom marker correlations and safeguarding against false\ndiscoveries induced by patient heterogeneity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8573,
    "stars": 0
  },
  {
    "id": 11940,
    "package_name": "dosearch",
    "title": "Causal Effect Identification from Multiple Incomplete Data\nSources",
    "description": "Identification of causal effects from arbitrary\nobservational and experimental probability distributions via\ndo-calculus and standard probability manipulations using a\nsearch-based algorithm by Tikka, Hyttinen and Karvanen (2021)\n<doi:10.18637/jss.v099.i05>. Allows for the presence of\nmechanisms related to selection bias (Bareinboim and Tian,\n2015) <doi:10.1609/aaai.v29i1.9679>, transportability\n(Bareinboim and Pearl, 2014)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf>, missing data\n(Mohan, Pearl, and Tian, 2013)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r410.pdf>) and arbitrary\ncombinations of these. Also supports identification in the\npresence of context-specific independence (CSI) relations\nthrough labeled directed acyclic graphs (LDAG). For details on\nCSIs see (Corander et al., 2019)\n<doi:10.1016/j.apal.2019.04.004>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8573,
    "stars": 0
  },
  {
    "id": 24274,
    "package_name": "tEDM",
    "title": "Temporal Empirical Dynamic Modeling",
    "description": "Inferring causation from time series data through\nempirical dynamic modeling (EDM), with methods such as\nconvergent cross mapping from Sugihara et al. (2012)\n<doi:10.1126/science.1227079>, partial cross mapping as\noutlined in Leng et al. (2020)\n<doi:10.1038/s41467-020-16238-0>, and cross mapping cardinality\nas described in Tao et al. (2023)\n<doi:10.1016/j.fmre.2023.01.007>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8543,
    "stars": 0
  },
  {
    "id": 23554,
    "package_name": "speckle",
    "title": "Statistical methods for analysing single cell RNA-seq data",
    "description": "The speckle package contains functions for the analysis of\nsingle cell RNA-seq data. The speckle package currently\ncontains functions to analyse differences in cell type\nproportions. There are also functions to estimate the\nparameters of the Beta distribution based on a given counts\nmatrix, and a function to normalise a counts matrix to the\nmedian library size. There are plotting functions to visualise\ncell type proportions and the mean-variance relationship in\ncell type proportions and counts. As our research into\nspecialised analyses of single cell data continues we\nanticipate that the package will be updated with new functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8445,
    "stars": 0
  },
  {
    "id": 25979,
    "package_name": "wv",
    "title": "Wavelet Variance",
    "description": "Provides a series of tools to compute and plot quantities\nrelated to classical and robust wavelet variance for time\nseries and regular lattices. More details can be found, for\nexample, in Serroukh, A., Walden, A.T., & Percival, D.B. (2000)\n<doi:10.2307/2669537> and Guerrier, S. & Molinari, R. (2016)\n<doi:10.48550/arXiv.1607.05858>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8411,
    "stars": 0
  },
  {
    "id": 11559,
    "package_name": "demuxmix",
    "title": "Demultiplexing oligo-barcoded scRNA-seq data using regression\nmixture models",
    "description": "A package for demultiplexing single-cell sequencing\nexperiments of pooled cells labeled with barcode\noligonucleotides. The package implements methods to fit\nregression mixture models for a probabilistic classification of\ncells, including multiplet detection. Demultiplexing error\nrates can be estimated, and methods for quality control are\nprovided.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8388,
    "stars": 0
  },
  {
    "id": 8978,
    "package_name": "bang",
    "title": "Bayesian Analysis, No Gibbs",
    "description": "Provides functions for the Bayesian analysis of some\nsimple commonly-used models, without using Markov Chain Monte\nCarlo (MCMC) methods such as Gibbs sampling.  The 'rust'\npackage <https://cran.r-project.org/package=rust> is used to\nsimulate a random sample from the required posterior\ndistribution, using the generalized ratio-of-uniforms method.\nSee Wakefield, Gelfand and Smith (1991)\n<DOI:10.1007/BF01889987> for details. At the moment three\nconjugate hierarchical models are available: beta-binomial,\ngamma-Poisson and a 1-way analysis of variance (ANOVA).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.836,
    "stars": 0
  },
  {
    "id": 9960,
    "package_name": "causaleffect",
    "title": "Deriving Expressions of Joint Interventional Distributions and\nTransport Formulas in Causal Models",
    "description": "Functions for identification and transportation of causal\neffects. Provides a conditional causal effect identification\nalgorithm (IDC) by Shpitser, I. and Pearl, J. (2006)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r329-uai.pdf>, an\nalgorithm for transportability from multiple domains with\nlimited experiments by Bareinboim, E. and Pearl, J. (2014)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r443.pdf>, and a selection\nbias recovery algorithm by Bareinboim, E. and Tian, J. (2015)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r445.pdf>. All of the\npreviously mentioned algorithms are based on a causal effect\nidentification algorithm by Tian , J. (2002)\n<http://ftp.cs.ucla.edu/pub/stat_ser/r309.pdf>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8111,
    "stars": 0
  },
  {
    "id": 20987,
    "package_name": "rater",
    "title": "Statistical Models of Repeated Categorical Rating Data",
    "description": "Fit statistical models based on the Dawid-Skene model -\nDawid and Skene (1979) <doi:10.2307/2346806> - to repeated\ncategorical rating data.  Full Bayesian inference for these\nmodels is supported through the Stan modelling language.\n'rater' also allows the user to extract and plot key parameters\nof these models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.8102,
    "stars": 0
  },
  {
    "id": 15530,
    "package_name": "imputeFin",
    "title": "Imputation of Financial Time Series with Missing Values and/or\nOutliers",
    "description": "Missing values often occur in financial data due to a\nvariety of reasons (errors in the collection process or in the\nprocessing stage, lack of asset liquidity, lack of reporting of\nfunds, etc.). However, most data analysis methods expect\ncomplete data and cannot be employed with missing values. One\nconvenient way to deal with this issue without having to\nredesign the data analysis method is to impute the missing\nvalues. This package provides an efficient way to impute the\nmissing values based on modeling the time series with a random\nwalk or an autoregressive (AR) model, convenient to model\nlog-prices and log-volumes in financial data. In the current\nversion, the imputation is univariate-based (so no asset\ncorrelation is used). In addition, outliers can be detected and\nremoved. The package is based on the paper: J. Liu, S. Kumar,\nand D. P. Palomar (2019). Parameter Estimation of Heavy-Tailed\nAR Model With Missing Data Via Stochastic EM. IEEE Trans. on\nSignal Processing, vol. 67, no. 8, pp. 2159-2172.\n<doi:10.1109/TSP.2019.2899816>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7959,
    "stars": 0
  },
  {
    "id": 491,
    "package_name": "BPRMeth",
    "title": "Model higher-order methylation profiles",
    "description": "The BPRMeth package is a probabilistic method to quantify\nexplicit features of methylation profiles, in a way that would\nmake it easier to formally use such profiles in downstream\nmodelling efforts, such as predicting gene expression levels or\nclustering genomic regions or cells according to their\nmethylation profiles.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7694,
    "stars": 0
  },
  {
    "id": 24745,
    "package_name": "timeOmics",
    "title": "Time-Course Multi-Omics data integration",
    "description": "timeOmics is a generic data-driven framework to integrate\nmulti-Omics longitudinal data measured on the same biological\nsamples and select key temporal features with strong\nassociations within the same sample group. The main steps of\ntimeOmics are: 1. Plaform and time-specific normalization and\nfiltering steps; 2. Modelling each biological into one time\nexpression profile; 3. Clustering features with the same\nexpression profile over time; 4. Post-hoc validation step.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7574,
    "stars": 0
  },
  {
    "id": 18061,
    "package_name": "multibias",
    "title": "Multiple Bias Analysis in Causal Inference",
    "description": "Quantify the causal effect of a binary exposure on a\nbinary outcome with adjustment for multiple biases. The\nfunctions can simultaneously adjust for any combination of\nuncontrolled confounding, exposure/outcome misclassification,\nand selection bias. The underlying method generalizes the\nconcept of combining inverse probability of selection weighting\nwith predictive value weighting. Simultaneous multi-bias\nanalysis can be used to enhance the validity and transparency\nof real-world evidence obtained from observational,\nlongitudinal studies. Based on the work from Paul Brendel,\nAracelis Torres, and Onyebuchi Arah (2023)\n<doi:10.1093/ije/dyad001>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7404,
    "stars": 0
  },
  {
    "id": 8812,
    "package_name": "atsar",
    "title": "Stan Routines For Univariate And Multivariate Time Series",
    "description": "Bundles univariate and multivariate STAN scripts for FISH\n507 class.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7266,
    "stars": 0
  },
  {
    "id": 8956,
    "package_name": "baker",
    "title": "\"Nested Partially Latent Class Models\"",
    "description": "Provides functions to specify, fit and visualize nested\npartially-latent class models ( Wu, Deloria-Knoll, Hammitt, and\nZeger (2016) <doi:10.1111/rssc.12101>; Wu, Deloria-Knoll, and\nZeger (2017) <doi:10.1093/biostatistics/kxw037>; Wu and Chen\n(2021) <doi:10.1002/sim.8804>) for inference of population\ndisease etiology and individual diagnosis. In the motivating\nPneumonia Etiology Research for Child Health (PERCH) study,\nbecause both quantities of interest sum to one hundred percent,\nthe PERCH scientists frequently refer to them as population\netiology pie and individual etiology pie, hence the name of the\npackage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.7226,
    "stars": 0
  },
  {
    "id": 8974,
    "package_name": "bandle",
    "title": "An R package for the Bayesian analysis of differential\nsubcellular localisation experiments",
    "description": "The Bandle package enables the analysis and visualisation\nof differential localisation experiments using\nmass-spectrometry data. Experimental methods supported include\ndynamic LOPIT-DC, hyperLOPIT, Dynamic Organellar Maps, Dynamic\nPCP. It provides Bioconductor infrastructure to analyse these\ndata.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6812,
    "stars": 0
  },
  {
    "id": 23543,
    "package_name": "spduration",
    "title": "Split-Population Duration (Cure) Regression",
    "description": "An implementation of split-population duration regression\nmodels. Unlike regular duration models, split-population\nduration models are mixture models that accommodate the\npresence of a sub-population that is not at risk for failure,\ne.g. cancer patients who have been cured by treatment. This\npackage implements Weibull and Loglogistic forms for the\nduration component, and focuses on data with time-varying\ncovariates. These models were originally formulated in Boag\n(1949) and Berkson and Gage (1952), and extended in Schmidt and\nWitte (1989).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6812,
    "stars": 0
  },
  {
    "id": 7900,
    "package_name": "VLTimeCausality",
    "title": "Variable-Lag Time Series Causality Inference Framework",
    "description": "A framework to infer causality on a pair of time series of\nreal numbers based on variable-lag Granger causality and\ntransfer entropy. Typically, Granger causality and transfer\nentropy have an assumption of a fixed and constant time delay\nbetween the cause and effect. However, for a non-stationary\ntime series, this assumption is not true. For example,\nconsidering two time series of velocity of person A and person\nB where B follows A. At some time, B stops tying his shoes,\nthen running to catch up A. The fixed-lag assumption is not\ntrue in this case. We propose a framework that allows\nvariable-lags between cause and effect in Granger causality and\ntransfer entropy to allow them to deal with variable-lag\nnon-stationary time series. Please see Chainarong\nAmornbunchornvej, Elena Zheleva, and Tanya Berger-Wolf (2021)\n<doi:10.1145/3441452> when referring to this package in\npublications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6309,
    "stars": 0
  },
  {
    "id": 22544,
    "package_name": "seasonalview",
    "title": "Graphical User Interface for Seasonal Adjustment",
    "description": "A graphical user interface to the 'seasonal' package and\n'X-13ARIMA-SEATS', the U.S. Census Bureau's seasonal adjustment\nsoftware.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6302,
    "stars": 0
  },
  {
    "id": 12122,
    "package_name": "dynamac",
    "title": "Dynamic Simulation and Testing for Single-Equation ARDL Models",
    "description": "While autoregressive distributed lag (ARDL) models allow\nfor extremely flexible dynamics, interpreting substantive\nsignificance of complex lag structures remains difficult. This\npackage is designed to assist users in dynamically simulating\nand plotting the results of various ARDL models. It also\ncontains post-estimation diagnostics, including a test for\ncointegration when estimating the error-correction variant of\nthe autoregressive distributed lag model (Pesaran, Shin, and\nSmith 2001 <doi:10.1002/jae.616>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.6123,
    "stars": 0
  },
  {
    "id": 1516,
    "package_name": "DAGassist",
    "title": "Test Robustness with Directed Acyclic Graphs",
    "description": "Provides robustness checks to align estimands with the\nidentification that they require. Given a 'dagitty' object and\na model specification, 'DAGassist' classifies variables by\ncausal roles, flags problematic controls, and generates a\nreport comparing the original model with minimal and canonical\nadjustment sets. Exports publication-grade reports in 'LaTeX',\n'Word', 'Excel', 'dotwhisker', or plain text/'markdown'.\n'DAGassist' is built on 'dagitty', an 'R' package that uses the\n'DAGitty' web tool (<https://dagitty.net/>) for creating and\nanalyzing DAGs. Methods draw on Pearl (2009)\n<doi:10.1017/CBO9780511803161> and Textor et al. (2016)\n<doi:10.1093/ije/dyw341>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.574,
    "stars": 0
  },
  {
    "id": 12799,
    "package_name": "explainer",
    "title": "Machine Learning Model Explainer",
    "description": "It enables detailed interpretation of complex\nclassification and regression models through Shapley analysis\nincluding data-driven characterization of subgroups of\nindividuals. Furthermore, it facilitates multi-measure model\nevaluation, model fairness, and decision curve analysis.\nAdditionally, it offers enhanced visualizations with\ninteractive elements.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5688,
    "stars": 0
  },
  {
    "id": 3154,
    "package_name": "HiLDA",
    "title": "Conducting statistical inference on comparing the mutational\nexposures of mutational signatures by using hierarchical latent\nDirichlet allocation",
    "description": "A package built under the Bayesian framework of applying\nhierarchical latent Dirichlet allocation. It statistically\ntests whether the mutational exposures of mutational signatures\n(Shiraishi-model signatures) are different between two groups.\nThe package also provides inference and visualization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5563,
    "stars": 0
  },
  {
    "id": 5819,
    "package_name": "RIVER",
    "title": "R package for RIVER (RNA-Informed Variant Effect on Regulation)",
    "description": "An implementation of a probabilistic modeling framework\nthat jointly analyzes personal genome and transcriptome data to\nestimate the probability that a variant has regulatory impact\nin that individual. It is based on a generative model that\nassumes that genomic annotations, such as the location of a\nvariant with respect to regulatory elements, determine the\nprior probability that variant is a functional regulatory\nvariant, which is an unobserved variable. The functional\nregulatory variant status then influences whether nearby genes\nare likely to display outlier levels of gene expression in that\nperson. See the RIVER website for more information,\ndocumentation and examples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5563,
    "stars": 0
  },
  {
    "id": 18980,
    "package_name": "optic",
    "title": "Simulation Tool for Causal Inference Using Longitudinal Data",
    "description": "Provides a simulation framework for evaluation of causal\ninference methods using real panel data. See Griffin et al.\n(2021) <doi:10.1007/s10742-022-00284-w> and Griffin et al.\n(2022) <doi:10.1186/s12874-021-01471-y> for a description of\nour methods.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5563,
    "stars": 0
  },
  {
    "id": 813,
    "package_name": "BranchGLM",
    "title": "Efficient Best Subset Selection for GLMs via Branch and Bound\nAlgorithms",
    "description": "Performs efficient and scalable glm best subset selection\nusing a novel implementation of a branch and bound algorithm.\nTo speed up the model fitting process, a range of optimization\nmethods are implemented in 'RcppArmadillo'. Parallel\ncomputation is available using 'OpenMP'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5563,
    "stars": 0
  },
  {
    "id": 23163,
    "package_name": "skellam",
    "title": "Densities and Sampling for the Skellam Distribution",
    "description": "Functions for the Skellam distribution, including: density\n(pmf), cdf, quantiles and regression.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.538,
    "stars": 0
  },
  {
    "id": 3978,
    "package_name": "MFDFA",
    "title": "MultiFractal Detrended Fluctuation Analysis",
    "description": "Contains the MultiFractal Detrended Fluctuation Analysis\n(MFDFA), MultiFractal Detrended Cross-Correlation Analysis\n(MFXDFA), and the Multiscale Multifractal Analysis (MMA). The\nMFDFA() function proposed in this package was used in Laib et\nal. (<doi:10.1016/j.chaos.2018.02.024> and\n<doi:10.1063/1.5022737>). See references for more information.\nInterested users can find a parallel version of the MFDFA()\nfunction on GitHub.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5205,
    "stars": 0
  },
  {
    "id": 9567,
    "package_name": "bootUR",
    "title": "Bootstrap Unit Root Tests",
    "description": "Set of functions to perform various bootstrap unit root\ntests for both individual time series (including augmented\nDickey-Fuller test and union tests), multiple time series and\npanel data; see Smeekes and Wilms (2023)\n<doi:10.18637/jss.v106.i12>, Palm, Smeekes and Urbain (2008)\n<doi:10.1111/j.1467-9892.2007.00565.x>, Palm, Smeekes and\nUrbain (2011) <doi:10.1016/j.jeconom.2010.11.010>, Moon and\nPerron (2012) <doi:10.1016/j.jeconom.2012.01.008>, Smeekes and\nTaylor (2012) <doi:10.1017/S0266466611000387> and Smeekes\n(2015) <doi:10.1111/jtsa.12110> for key references.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5051,
    "stars": 0
  },
  {
    "id": 17331,
    "package_name": "methyLImp2",
    "title": "Missing value estimation of DNA methylation data",
    "description": "This package allows to estimate missing values in DNA\nmethylation data. methyLImp method is based on linear\nregression since methylation levels show a high degree of\ninter-sample correlation. Implementation is parallelised over\nchromosomes since probes on different chromosomes are usually\nindependent. Mini-batch approach to reduce the runtime in case\nof large number of samples is available.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.5051,
    "stars": 0
  },
  {
    "id": 4345,
    "package_name": "MethReg",
    "title": "Assessing the regulatory potential of DNA methylation regions or\nsites on gene transcription",
    "description": "Epigenome-wide association studies (EWAS) detects a large\nnumber of DNA methylation differences, often hundreds of\ndifferentially methylated regions and thousands of CpGs, that\nare significantly associated with a disease, many are located\nin non-coding regions. Therefore, there is a critical need to\nbetter understand the functional impact of these CpG\nmethylations and to further prioritize the significant changes.\nMethReg is an R package for integrative modeling of DNA\nmethylation, target gene expression and transcription factor\nbinding sites data, to systematically identify and rank\nfunctional CpG methylations. MethReg evaluates, prioritizes and\nannotates CpG sites with high regulatory potential using\nmatched methylation and gene expression data, along with\nexternal TF-target interaction databases based on manually\ncuration, ChIP-seq experiments or gene regulatory network\nanalysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4983,
    "stars": 0
  },
  {
    "id": 9087,
    "package_name": "bayesmsm",
    "title": "Fitting Bayesian Marginal Structural Models for Longitudinal\nObservational Data",
    "description": "Implements Bayesian marginal structural models for causal\neffect estimation with time-varying treatment and confounding.\nIt includes an extension to handle informative right censoring.\nThe Bayesian importance sampling weights are estimated using\nJAGS. See Saarela (2015) <doi:10.1111/biom.12269> for\nmethodological details.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4942,
    "stars": 0
  },
  {
    "id": 2613,
    "package_name": "GEM",
    "title": "GEM: fast association study for the interplay of Gene,\nEnvironment and Methylation",
    "description": "Tools for analyzing EWAS, methQTL and GxE genome widely.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4914,
    "stars": 0
  },
  {
    "id": 9078,
    "package_name": "bayesianVARs",
    "title": "MCMC Estimation of Bayesian Vectorautoregressions",
    "description": "Efficient Markov Chain Monte Carlo (MCMC) algorithms for\nthe fully Bayesian estimation of vectorautoregressions (VARs)\nfeaturing stochastic volatility (SV). Implements\nstate-of-the-art shrinkage priors following Gruber & Kastner\n(2025) <doi:10.1016/j.ijforecast.2025.02.001>. Efficient\nequation-per-equation estimation following Kastner & Huber\n(2020) <doi:10.1002/for.2680> and Carrerio et al. (2021)\n<doi:10.1016/j.jeconom.2021.11.010>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4771,
    "stars": 0
  },
  {
    "id": 7818,
    "package_name": "USgas",
    "title": "The Demand for Natural Gas in the US",
    "description": "Provides an overview of the demand for natural gas in the\nUS by state and country level. Data source: US Energy\nInformation Administration <https://www.eia.gov/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4594,
    "stars": 0
  },
  {
    "id": 22709,
    "package_name": "serrsBayes",
    "title": "Bayesian Modelling of Raman Spectroscopy",
    "description": "Sequential Monte Carlo (SMC) algorithms for fitting a\ngeneralised additive mixed model (GAMM) to surface-enhanced\nresonance Raman spectroscopy (SERRS), using the method of\nMoores et al. (2016) <arXiv:1604.07299>. Multivariate\nobservations of SERRS are highly collinear and lend themselves\nto a reduced-rank representation. The GAMM separates the SERRS\nsignal into three components: a sequence of Lorentzian,\nGaussian, or pseudo-Voigt peaks; a smoothly-varying baseline;\nand additive white noise. The parameters of each component of\nthe model are estimated iteratively using SMC. The posterior\ndistributions of the parameters given the observed spectra are\nrepresented as a population of weighted particles.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4594,
    "stars": 0
  },
  {
    "id": 23871,
    "package_name": "stdmod",
    "title": "Standardized Moderation Effect and Its Confidence Interval",
    "description": "Functions for computing a standardized moderation effect\nin moderated regression and forming its confidence interval by\nnonparametric bootstrapping as proposed in Cheung, Cheung, Lau,\nHui, and Vong (2022) <doi:10.1037/hea0001188>. Also includes\nsimple-to-use functions for computing conditional effects\n(unstandardized or standardized) and plotting moderation\neffects.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4502,
    "stars": 0
  },
  {
    "id": 5623,
    "package_name": "R3CPET",
    "title": "3CPET: Finding Co-factor Complexes in Chia-PET experiment using\na Hierarchical Dirichlet Process",
    "description": "The package provides a method to infer the set of proteins\nthat are more probably to work together to maintain chormatin\ninteraction given a ChIA-PET experiment results.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4472,
    "stars": 0
  },
  {
    "id": 16470,
    "package_name": "limpca",
    "title": "An R package for the linear modeling of high-dimensional\ndesigned data based on ASCA/APCA family of methods",
    "description": "This package has for objectives to provide a method to\nmake Linear Models for high-dimensional designed data. limpca\napplies a GLM (General Linear Model) version of ASCA and APCA\nto analyse multivariate sample profiles generated by an\nexperimental design. ASCA/APCA provide powerful visualization\ntools for multivariate structures in the space of each effect\nof the statistical model linked to the experimental design and\ncontrarily to MANOVA, it can deal with mutlivariate datasets\nhaving more variables than observations. This method can handle\nunbalanced design.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4314,
    "stars": 0
  },
  {
    "id": 24836,
    "package_name": "tmle3shift",
    "title": "Targeted Learning of the Causal Effects of Stochastic\nInterventions",
    "description": "Targeted maximum likelihood estimation (TMLE) of\npopulation-level causal effects under stochastic treatment\nregimes and related nonparametric variable importance analyses.\nTools are provided for TML estimation of the counterfactual\nmean under a stochastic intervention characterized as a\nmodified treatment policy, such as treatment policies that\nshift the natural value of the exposure. The causal parameter\nand estimation were described in Díaz and van der Laan (2013)\n<doi:10.1111/j.1541-0420.2011.01685.x> and an improved\nestimation approach was given by Díaz and van der Laan (2018)\n<doi:10.1007/978-3-319-65304-4_14>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.4151,
    "stars": 0
  },
  {
    "id": 11492,
    "package_name": "decontX",
    "title": "Decontamination of single cell genomics data",
    "description": "This package contains implementation of DecontX (Yang et\nal. 2020), a decontamination algorithm for single-cell RNA-seq,\nand DecontPro (Yin et al. 2023), a decontamination algorithm\nfor single cell protein expression data. DecontX is a novel\nBayesian method to computationally estimate and remove RNA\ncontamination in individual cells without empty droplet\ninformation. DecontPro is a Bayesian method that estimates the\nlevel of contamination from ambient and background sources in\nCITE-seq ADT dataset and decontaminate the dataset.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.415,
    "stars": 0
  },
  {
    "id": 3378,
    "package_name": "IgGeneUsage",
    "title": "Differential gene usage in immune repertoires",
    "description": "Detection of biases in the usage of immunoglobulin (Ig)\ngenes is an important task in immune repertoire profiling.\nIgGeneUsage detects aberrant Ig gene usage between biological\nconditions using a probabilistic model which is analyzed\ncomputationally by Bayes inference. With this IgGeneUsage also\navoids some common problems related to the current practice of\nnull-hypothesis significance testing.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3802,
    "stars": 0
  },
  {
    "id": 9509,
    "package_name": "bmstdr",
    "title": "Bayesian Modeling of Spatio-Temporal Data with R",
    "description": "Fits, validates and compares a number of Bayesian models\nfor spatial and space time point referenced and areal unit\ndata. Model fitting is done using several packages: 'rstan',\n'INLA', 'spBayes', 'spTimer', 'spTDyn', 'CARBayes' and\n'CARBayesST'. Model comparison is performed using the DIC and\nWAIC, and K-fold cross-validation where the user is free to\nselect their own subset of data rows for validation. Sahu\n(2022) <doi:10.1201/9780429318443> describes the methods in\ndetail.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3802,
    "stars": 0
  },
  {
    "id": 18094,
    "package_name": "multimedia",
    "title": "Multimodal Mediation Analysis",
    "description": "Multimodal mediation analysis is an emerging problem in\nmicrobiome data analysis. Multimedia make advanced mediation\nanalysis techniques easy to use, ensuring that all statistical\ncomponents are transparent and adaptable to specific problem\ncontexts. The package provides a uniform interface to direct\nand indirect effect estimation, synthetic null hypothesis\ntesting, bootstrap confidence interval construction, and\nsensitivity analysis. More details are available in Jiang et\nal. (2024) \"multimedia: Multimodal Mediation Analysis of\nMicrobiome Data\" <doi:10.1101/2024.03.27.587024>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3802,
    "stars": 0
  },
  {
    "id": 3312,
    "package_name": "INTACT",
    "title": "Integrate TWAS and Colocalization Analysis for Gene Set\nEnrichment Analysis",
    "description": "This package integrates colocalization probabilities from\ncolocalization analysis with transcriptome-wide association\nstudy (TWAS) scan summary statistics to implicate genes that\nmay be biologically relevant to a complex trait. The\nprobabilistic framework implemented in this package constrains\nthe TWAS scan z-score-based likelihood using a gene-level\ncolocalization probability. Given gene set annotations, this\npackage can estimate gene set enrichment using posterior\nprobabilities from the TWAS-colocalization integration step.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3766,
    "stars": 0
  },
  {
    "id": 10402,
    "package_name": "clustcurv",
    "title": "Determining Groups in Multiples Curves",
    "description": "A method for determining groups in multiple curves with an\nautomatic selection of their number based on k-means or\nk-medians algorithms. The selection of the optimal number is\nprovided by bootstrap methods or other approaches with lower\ncomputational cost. The methodology can be applied both in\nregression and survival framework. Implemented methods are:\nGrouping multiple survival curves described by Villanueva et\nal. (2018) <doi:10.1002/sim.8016>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3692,
    "stars": 0
  },
  {
    "id": 21444,
    "package_name": "retrofit",
    "title": "RETROFIT: Reference-free deconvolution of cell mixtures in\nspatial transcriptomics",
    "description": "RETROFIT is a Bayesian non-negative matrix factorization\nframework to decompose cell type mixtures in ST data without\nusing external single-cell expression references. RETROFIT\noutperforms existing reference-based methods in estimating cell\ntype proportions and reconstructing gene expressions in\nsimulations with varying spot size and sample heterogeneity,\nirrespective of the quality or availability of the single-cell\nreference. RETROFIT recapitulates known cell-type localization\npatterns in a Slide-seq dataset of mouse cerebellum without\nusing any single-cell data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3579,
    "stars": 0
  },
  {
    "id": 22359,
    "package_name": "scQTLtools",
    "title": "scQTLtools: an R/Bioconductor package for comprehensive\nidentification and visualization of single-cell eQTLs",
    "description": "scQTLtools is a comprehensive R/Bioconductor package that\nfacilitates end-to-end single-cell eQTL analysis, from\npreprocessing to visualization",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3522,
    "stars": 0
  },
  {
    "id": 4706,
    "package_name": "NewWave",
    "title": "Negative binomial model for scRNA-seq",
    "description": "A model designed for dimensionality reduction and batch\neffect removal for scRNA-seq data. It is designed to be\nmassively parallelizable using shared objects that prevent\nmemory duplication, and it can be used with different\nmini-batch approaches in order to reduce time consumption. It\nassumes a negative binomial distribution for the data with a\ndispersion parameter that can be both commonwise across gene\nboth genewise.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3502,
    "stars": 0
  },
  {
    "id": 14451,
    "package_name": "globalSeq",
    "title": "Global Test for Counts",
    "description": "The method may be conceptualised as a test of overall\nsignificance in regression analysis, where the response\nvariable is overdispersed and the number of explanatory\nvariables exceeds the sample size. Useful for testing for\nassociation between RNA-Seq and high-dimensional data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3222,
    "stars": 0
  },
  {
    "id": 16405,
    "package_name": "lfc",
    "title": "Log Fold Change Distribution Tools for Working with Ratios of\nCounts",
    "description": "Ratios of count data such as obtained from RNA-seq are\nmodelled using Bayesian statistics to derive posteriors for\neffects sizes. This approach is described in Erhard & Zimmer\n(2015) <doi:10.1093/nar/gkv696> and Erhard (2018)\n<doi:10.1093/bioinformatics/bty471>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3222,
    "stars": 0
  },
  {
    "id": 2941,
    "package_name": "GlobalAncova",
    "title": "Global test for groups of variables via model comparisons",
    "description": "The association between a variable of interest (e.g. two\ngroups) and the global pattern of a group of variables (e.g. a\ngene set) is tested via a global F-test. We give the following\narguments in support of the GlobalAncova approach: After\nappropriate normalisation, gene-expression-data appear rather\nsymmetrical and outliers are no real problem, so least squares\nshould be rather robust. ANCOVA with interaction yields\nsaturated data modelling e.g. different means per group and\ngene. Covariate adjustment can help to correct for possible\nselection bias. Variance homogeneity and uncorrelated residuals\ncannot be expected. Application of ordinary least squares gives\nunbiased, but no longer optimal estimates\n(Gauss-Markov-Aitken). Therefore, using the classical F-test is\ninappropriate, due to correlation. The test statistic however\nmirrors deviations from the null hypothesis. In combination\nwith a permutation approach, empirical significance levels can\nbe approximated. Alternatively, an approximation yields\nasymptotic p-values. The framework is generalized to groups of\ncategorical variables or even mixed data by a likelihood ratio\napproach. Closed and hierarchical testing procedures are\nsupported. This work was supported by the NGFN grant 01 GR\n0459, BMBF, Germany and BMBF grant 01ZX1309B, Germany.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3195,
    "stars": 0
  },
  {
    "id": 11385,
    "package_name": "dateutils",
    "title": "Date Utils",
    "description": "Utilities for mixed frequency data. In particular, use to\naggregate and normalize tabular mixed frequency data, index\ndates to end of period, and seasonally adjust tabular data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.3096,
    "stars": 0
  },
  {
    "id": 4313,
    "package_name": "MetNet",
    "title": "Inferring metabolic networks from untargeted high-resolution\nmass spectrometry data",
    "description": "MetNet contains functionality to infer metabolic network\ntopologies from quantitative data and high-resolution\nmass/charge information. Using statistical models (including\ncorrelation, mutual information, regression and Bayes\nstatistics) and quantitative data (intensity values of\nfeatures) adjacency matrices are inferred that can be combined\nto a consensus matrix. Mass differences calculated between\nmass/charge values of features will be matched against a data\nframe of supplied mass/charge differences referring to\ntransformations of enzymatic activities. In a third step, the\ntwo levels of information are combined to form a adjacency\nmatrix inferred from both quantitative and structure\ninformation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.301,
    "stars": 0
  },
  {
    "id": 9400,
    "package_name": "biotmle",
    "title": "Targeted Learning with Moderated Statistics for Biomarker\nDiscovery",
    "description": "Tools for differential expression biomarker discovery\nbased on microarray and next-generation sequencing data that\nleverage efficient semiparametric estimators of the average\ntreatment effect for variable importance analysis. Estimation\nand inference of the (marginal) average treatment effects of\npotential biomarkers are computed by targeted minimum\nloss-based estimation, with joint, stable inference constructed\nacross all biomarkers using a generalization of moderated\nstatistics for use with the estimated efficient influence\nfunction. The procedure accommodates the use of ensemble\nmachine learning for the estimation of nuisance functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.301,
    "stars": 0
  },
  {
    "id": 15938,
    "package_name": "jointVIP",
    "title": "Prioritize Variables with Joint Variable Importance Plot in\nObservational Study Design",
    "description": "In the observational study design stage,\nmatching/weighting methods are conducted. However, when many\nbackground variables are present, the decision as to which\nvariables to prioritize for matching/weighting is not trivial.\nThus, the joint treatment-outcome variable importance plots are\ncreated to guide variable selection. The joint variable\nimportance plots enhance variable comparisons via unadjusted\nbias curves derived under the omitted variable bias framework.\nThe plots translate variable importance into recommended values\nfor tuning parameters in existing methods. Post-matching and/or\nweighting plots can also be used to visualize and assess the\nquality of the observational study design. The method\nmotivation and derivation is presented in \"Prioritizing\nVariables for Observational Study Design using the Joint\nVariable Importance Plot\" by Liao et al. (2024)\n<doi:10.1080/00031305.2024.2303419>. See the package paper by\nLiao and Pimentel (2023) <arxiv:2302.10367> for a beginner\nfriendly user introduction.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2923,
    "stars": 0
  },
  {
    "id": 7122,
    "package_name": "SimSurvNMarker",
    "title": "Simulate Survival Time and Markers",
    "description": "Provides functions to simulate from joint survival and\nmarker models. The user can specific all basis functions of\ntime, random or deterministic covariates, random or\ndeterministic left-truncation and right-censoring times, and\nmodel parameters.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2867,
    "stars": 0
  },
  {
    "id": 15932,
    "package_name": "jointCalib",
    "title": "A Joint Calibration of Totals and Quantiles",
    "description": "A small package containing functions to perform a joint\ncalibration of totals and quantiles. The calibration for totals\nis based on Deville and Särndal (1992)\n<doi:10.1080/01621459.1992.10475217>, the calibration for\nquantiles is based on Harms and Duchesne (2006)\n<https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X20060019255>.\nThe package uses standard calibration via the 'survey',\n'sampling' or 'laeken' packages. In addition, entropy balancing\nvia the 'ebal' package and empirical likelihood based on codes\nfrom Wu (2005)\n<https://www150.statcan.gc.ca/n1/pub/12-001-x/2005002/article/9051-eng.pdf>\ncan be used. See the paper by Beręsewicz and Szymkowiak (2023)\nfor details <arXiv:2308.13281>. The package also includes\nfunctions to reweight the control group to the treatment\nreference distribution and to balance the covariate\ndistribution using the covariate balancing propensity score via\nthe 'CBPS' package for binary treatment observational studies.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2789,
    "stars": 0
  },
  {
    "id": 10184,
    "package_name": "chipenrich",
    "title": "Gene Set Enrichment For ChIP-seq Peak Data",
    "description": "ChIP-Enrich and Poly-Enrich perform gene set enrichment\ntesting using peaks called from a ChIP-seq experiment. The\nmethod empirically corrects for confounding factors such as the\nlength of genes, and the mappability of the sequence\nsurrounding genes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2695,
    "stars": 0
  },
  {
    "id": 9176,
    "package_name": "beer",
    "title": "Bayesian Enrichment Estimation in R",
    "description": "BEER implements a Bayesian model for analyzing\nphage-immunoprecipitation sequencing (PhIP-seq) data. Given a\nPhIPData object, BEER returns posterior probabilities of\nenriched antibody responses, point estimates for the relative\nfold-change in comparison to negative control samples, and\nmore. Additionally, BEER provides a convenient implementation\nfor using edgeR to identify enriched antibody responses.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2589,
    "stars": 0
  },
  {
    "id": 883,
    "package_name": "CBN2Path",
    "title": "CBN2Path: an R/Bioconductor package for the analysis of cancer\nprogression pathways using Conjunctive Bayesian Networks",
    "description": "CBN2Path package provides a unifying interface to\nfacilitate CBN-based quantification, analysis and visualization\nof cancer progression pathways.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2553,
    "stars": 0
  },
  {
    "id": 10047,
    "package_name": "cellmig",
    "title": "Uncertainty-aware quantitative analysis of high-throughput live\ncell migration data",
    "description": "High-throughput cell imaging facilitates the analysis of\ncell migration across many wells treated under different\nbiological conditions. These workflows generate considerable\ntechnical noise and biological variability, and therefore\ntechnical and biological replicates are necessary, leading to\nlarge, hierarchically structured datasets, i.e., cells are\nnested within technical replicates that are nested within\nbiological replicates. Current statistical analyses of such\ndata usually ignore the hierarchical structure of the data and\nfail to explicitly quantify uncertainty arising from technical\nor biological variability. To address this gap, we present\ncellmig, an R package implementing Bayesian hierarchical models\nfor migration analysis. cellmig quantifies condition- specific\nvelocity changes (e.g., drug effects) while modeling nested\ndata structures and technical artifacts. It further enables\nsynthetic data generation for experimental design optimization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2553,
    "stars": 0
  },
  {
    "id": 23795,
    "package_name": "stan4bart",
    "title": "Bayesian Additive Regression Trees with Stan-Sampled Parametric\nExtensions",
    "description": "Fits semiparametric linear and multilevel models with\nnon-parametric additive Bayesian additive regression tree\n(BART; Chipman, George, and McCulloch (2010)\n<doi:10.1214/09-AOAS285>) components and Stan (Stan Development\nTeam (2021) <https://mc-stan.org/>) sampled parametric ones.\nMultilevel models can be expressed using 'lme4' syntax (Bates,\nMaechler, Bolker, and Walker (2015)\n<doi:10.18637/jss.v067.i01>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2284,
    "stars": 0
  },
  {
    "id": 11800,
    "package_name": "dispositionEffect",
    "title": "Analysis of Disposition Effect on Financial Portfolios",
    "description": "Evaluate the presence of disposition effect and others\nirrational investor's behaviors based solely on investor's\ntransactions and financial market data. Experimental data can\nalso be used to perform the analysis. Four different\nmethodologies are implemented to account for the different\nnature of human behaviors on financial markets. Novel analyses\nsuch as portfolio driven and time series disposition effect are\nalso allowed.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.2041,
    "stars": 0
  },
  {
    "id": 21615,
    "package_name": "rjd3filters",
    "title": "Trend-Cycle Extraction with Linear Filters based on JDemetra+\nv3.x",
    "description": "This package provides functions to build and apply\nsymmetric and asymmetric moving averages (= linear filters) for\ntrend-cycle extraction.  In particular, it implements several\nmodern approaches for real-time estimates from the viewpoint of\nrevisions and time delay in detecting turning points.  It\nincludes the local polynomial approach of Proietti and Luati\n(2008), the Reproducing Kernel Hilbert Space (RKHS) of Dagum\nand Bianconcini (2008) and the Fidelity-Smoothness-Timeliness\napproach of Grun-Rehomme, Guggemos, and Ladiray (2018).  It is\nbased on Java libraries developped in 'JDemetra+'\n(<https://github.com/jdemetra>), time series analysis software.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1998,
    "stars": 0
  },
  {
    "id": 9781,
    "package_name": "cIRT",
    "title": "Choice Item Response Theory",
    "description": "Jointly model the accuracy of cognitive responses and item\nchoices within a Bayesian hierarchical framework as described\nby Culpepper and Balamuta (2015)\n<doi:10.1007/s11336-015-9484-7>. In addition, the package\ncontains the datasets used within the analysis of the paper.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1931,
    "stars": 0
  },
  {
    "id": 24562,
    "package_name": "theftdlc",
    "title": "Analyse and Interpret Time Series Features",
    "description": "Provides a suite of functions for analysing, interpreting,\nand visualising time-series features calculated from different\nfeature sets from the 'theft' package. Implements statistical\nlearning methodologies described in Henderson, T., Bryant, A.,\nand Fulcher, B. (2023) <doi:10.48550/arXiv.2303.17809>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1761,
    "stars": 0
  },
  {
    "id": 6871,
    "package_name": "SPONGE",
    "title": "Sparse Partial Correlations On Gene Expression",
    "description": "This package provides methods to efficiently detect\ncompetitive endogeneous RNA interactions between two genes.\nSuch interactions are mediated by one or several miRNAs such\nthat both gene and miRNA expression data for a larger number of\nsamples is needed as input. The SPONGE package now also\nincludes spongEffects: ceRNA modules offer patient-specific\ninsights into the miRNA regulatory landscape.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1673,
    "stars": 0
  },
  {
    "id": 9047,
    "package_name": "bayesDP",
    "title": "Implementation of the Bayesian Discount Prior Approach for\nClinical Trials",
    "description": "Functions for data augmentation using the Bayesian\ndiscount prior method for single arm and two-arm clinical\ntrials, as described in Haddad et al. (2017)\n<doi:10.1080/10543406.2017.1300907>. The discount power prior\nmethodology was developed in collaboration with the The Medical\nDevice Innovation Consortium (MDIC) Computer Modeling &\nSimulation Working Group.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1584,
    "stars": 0
  },
  {
    "id": 12348,
    "package_name": "effectplots",
    "title": "Effect Plots",
    "description": "High-performance implementation of various effect plots\nuseful for regression and probabilistic classification tasks.\nThe package includes partial dependence plots (Friedman, 2021,\n<doi:10.1214/aos/1013203451>), accumulated local effect plots\nand M-plots (both from Apley and Zhu, 2016,\n<doi:10.1111/rssb.12377>), as well as plots that describe the\nstatistical associations between model response and features.\nIt supports visualizations with either 'ggplot2' or 'plotly',\nand is compatible with most models, including 'Tidymodels',\nmodels wrapped in 'DALEX' explainers, or models with case\nweights.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1547,
    "stars": 0
  },
  {
    "id": 23040,
    "package_name": "simdata",
    "title": "Generate Simulated Datasets",
    "description": "Generate simulated datasets from an initial underlying\ndistribution and apply transformations to obtain realistic\ndata. Implements the 'NORTA' (Normal-to-anything) approach from\nCario and Nelson (1997) and other data generating mechanisms.\nSimple network visualization tools are provided to facilitate\ncommunicating the simulation setup.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1351,
    "stars": 0
  },
  {
    "id": 3457,
    "package_name": "IsoBayes",
    "title": "IsoBayes: Single Isoform protein inference Method via Bayesian\nAnalyses",
    "description": "IsoBayes is a Bayesian method to perform inference on\nsingle protein isoforms. Our approach infers the\npresence/absence of protein isoforms, and also estimates their\nabundance; additionally, it provides a measure of the\nuncertainty of these estimates, via: i) the posterior\nprobability that a protein isoform is present in the sample;\nii) a posterior credible interval of its abundance. IsoBayes\ninputs liquid cromatography mass spectrometry (MS) data, and\ncan work with both PSM counts, and intensities. When available,\ntrascript isoform abundances (i.e., TPMs) are also\nincorporated: TPMs are used to formulate an informative prior\nfor the respective protein isoform relative abundance. We\nfurther identify isoforms where the relative abundance of\nproteins and transcripts significantly differ. We use a\ntwo-layer latent variable approach to model two sources of\nuncertainty typical of MS data: i) peptides may be erroneously\ndetected (even when absent); ii) many peptides are compatible\nwith multiple protein isoforms. In the first layer, we sample\nthe presence/absence of each peptide based on its estimated\nprobability of being mistakenly detected, also known as PEP\n(i.e., posterior error probability). In the second layer, for\npeptides that were estimated as being present, we allocate\ntheir abundance across the protein isoforms they map to. These\ntwo steps allow us to recover the presence and abundance of\neach protein isoform.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1206,
    "stars": 0
  },
  {
    "id": 25137,
    "package_name": "tsmarch",
    "title": "Multivariate ARCH Models",
    "description": "Feasible Multivariate Generalized Autoregressive\nConditional Heteroscedasticity (GARCH) models including Dynamic\nConditional Correlation (DCC), Copula GARCH and Generalized\nOrthogonal GARCH with Generalized Hyperbolic distribution. A\nreview of some of these models can be found in Boudt, Galanos,\nPayseur and Zivot (2019) <doi:10.1016/bs.host.2019.01.001>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.1206,
    "stars": 0
  },
  {
    "id": 25243,
    "package_name": "txshift",
    "title": "Efficient Estimation of the Causal Effects of Stochastic\nInterventions",
    "description": "Efficient estimation of the population-level causal\neffects of stochastic interventions on a continuous-valued\nexposure. Both one-step and targeted minimum loss estimators\nare implemented for the counterfactual mean value of an outcome\nof interest under an additive modified treatment policy, a\nstochastic intervention that may depend on the natural value of\nthe exposure. To accommodate settings with outcome-dependent\ntwo-phase sampling, procedures incorporating inverse\nprobability of censoring weighting are provided to facilitate\nthe construction of inefficient and efficient one-step and\ntargeted minimum loss estimators.  The causal parameter and its\nestimation were first described by Díaz and van der Laan (2013)\n<doi:10.1111/j.1541-0420.2011.01685.x>, while the multiply\nrobust estimation procedure and its application to data from\ntwo-phase sampling designs is detailed in NS Hejazi, MJ van der\nLaan, HE Janes, PB Gilbert, and DC Benkeser (2020)\n<doi:10.1111/biom.13375>. The software package implementation\nis described in NS Hejazi and DC Benkeser (2020)\n<doi:10.21105/joss.02447>. Estimation of nuisance parameters\nmay be enhanced through the Super Learner ensemble model in\n'sl3', available for download from GitHub using\n'remotes::install_github(\"tlverse/sl3\")'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0917,
    "stars": 0
  },
  {
    "id": 21620,
    "package_name": "rjd3x13",
    "title": "Seasonal Adjustment with X-13 in 'JDemetra+ 3.x'",
    "description": "R Interface to 'JDemetra+ 3.x'\n(<https://github.com/jdemetra>) time series analysis software.\nIt offers full acces to options and outputs of X-13, including\nRegARIMA modelling (automatic ARIMA model with outlier\ndetection and trading days adjustment) and X-11 decomposition.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0846,
    "stars": 0
  },
  {
    "id": 631,
    "package_name": "BayesSurvive",
    "title": "Bayesian Survival Models for High-Dimensional Data",
    "description": "An implementation of Bayesian survival models with\ngraph-structured selection priors for sparse identification of\nomics features predictive of survival (Madjar et al., 2021\n<doi:10.1186/s12859-021-04483-z>) and its extension to use a\nfixed graph via a Markov Random Field (MRF) prior for capturing\nknown structure of omics features, e.g. disease-specific\npathways from the Kyoto Encyclopedia of Genes and Genomes\ndatabase (Hermansen et al., 2025\n<doi:10.48550/arXiv.2503.13078>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0792,
    "stars": 0
  },
  {
    "id": 11809,
    "package_name": "distantia",
    "title": "Advanced Toolset for Efficient Time Series Dissimilarity\nAnalysis",
    "description": "Fast C++ implementation of Dynamic Time Warping for time\nseries dissimilarity analysis, with applications in\nenvironmental monitoring and sensor data analysis, climate\nscience, signal processing and pattern recognition, and\nfinancial data analysis. Built upon the ideas presented in\nBenito and Birks (2020) <doi:10.1111/ecog.04895>, provides\ntools for analyzing time series of varying lengths and\nstructures, including irregular multivariate time series. Key\nfeatures include individual variable contribution analysis,\nrestricted permutation tests for statistical significance, and\nimputation of missing data via GAMs. Additionally, the package\nprovides an ample set of tools to prepare and manage time\nseries data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.043,
    "stars": 0
  },
  {
    "id": 1873,
    "package_name": "DifferentialRegulation",
    "title": "Differentially regulated genes from scRNA-seq data",
    "description": "DifferentialRegulation is a method for detecting\ndifferentially regulated genes between two groups of samples\n(e.g., healthy vs. disease, or treated vs. untreated samples),\nby targeting differences in the balance of spliced and\nunspliced mRNA abundances, obtained from single-cell\nRNA-sequencing (scRNA-seq) data. From a mathematical point of\nview, DifferentialRegulation accounts for the sample-to-sample\nvariability, and embeds multiple samples in a Bayesian\nhierarchical model. Furthermore, our method also deals with two\nmajor sources of mapping uncertainty: i) 'ambiguous' reads,\ncompatible with both spliced and unspliced versions of a gene,\nand ii) reads mapping to multiple genes. In particular,\nambiguous reads are treated separately from spliced and\nunsplced reads, while reads that are compatible with multiple\ngenes are allocated to the gene of origin. Parameters are\ninferred via Markov chain Monte Carlo (MCMC) techniques\n(Metropolis-within-Gibbs).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0414,
    "stars": 0
  },
  {
    "id": 22115,
    "package_name": "rwavelet",
    "title": "Wavelet Analysis",
    "description": "Perform wavelet analysis (orthogonal,translation\ninvariant, tensorial, 1-2-3d transforms, thresholding, block\nthresholding, linear,...) with applications to data\ncompression, denoising/regression or clustering. The core of\nthe code is a port of 'MATLAB' Wavelab toolbox written by D.\nDonoho, A. Maleki and M. Shahram\n(<https://statweb.stanford.edu/~wavelab/>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5.0107,
    "stars": 0
  },
  {
    "id": 3094,
    "package_name": "HTLR",
    "title": "Bayesian Logistic Regression with Heavy-Tailed Priors",
    "description": "Efficient Bayesian multinomial logistic regression based\non heavy-tailed (hyper-LASSO, non-convex) priors. The posterior\nof coefficients and hyper-parameters is sampled with restricted\nGibbs sampling for leveraging the high-dimensionality and\nHamiltonian Monte Carlo for handling the high-correlation among\ncoefficients. A detailed description of the method: Li and Yao\n(2018), Journal of Statistical Computation and Simulation,\n88:14, 2827-2851, <doi:10.48550/arXiv.1405.3319>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5,
    "stars": 0
  },
  {
    "id": 4098,
    "package_name": "MOSClip",
    "title": "Multi Omics Survival Clip",
    "description": "Topological pathway analysis tool able to integrate\nmulti-omics data. It finds survival-associated modules or\nsignificant modules for two-class analysis. This tool have two\nmain methods: pathway tests and module tests. The latter method\nallows the user to dig inside the pathways itself.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5,
    "stars": 0
  },
  {
    "id": 6508,
    "package_name": "RolDE",
    "title": "RolDE: Robust longitudinal Differential Expression",
    "description": "RolDE detects longitudinal differential expression between\ntwo conditions in noisy high-troughput data. Suitable even for\ndata with a moderate amount of missing values.RolDE is a\ncomposite method, consisting of three independent modules with\ndifferent approaches to detecting longitudinal differential\nexpression. The combination of these diverse modules allows\nRolDE to robustly detect varying differences in longitudinal\ntrends and expression levels in diverse data types and\nexperimental settings.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5,
    "stars": 0
  },
  {
    "id": 14893,
    "package_name": "hbamr",
    "title": "Hierarchical Bayesian Aldrich-McKelvey Scaling via 'Stan'",
    "description": "Perform hierarchical Bayesian Aldrich-McKelvey scaling\nusing Hamiltonian Monte Carlo via 'Stan'. Aldrich-McKelvey\n('AM') scaling is a method for estimating the ideological\npositions of survey respondents and political actors on a\ncommon scale using positional survey data. The hierarchical\nversions of the Bayesian 'AM' model included in this package\noutperform other versions both in terms of yielding meaningful\nposterior distributions for respondent positions and in terms\nof recovering true respondent positions in simulations. The\npackage contains functions for preparing data, fitting models,\nextracting estimates, plotting key results, and comparing\nmodels using cross-validation. The original version of the\ndefault model is described in Bølstad (2024)\n<doi:10.1017/pan.2023.18>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 5,
    "stars": 0
  },
  {
    "id": 13393,
    "package_name": "fmrs",
    "title": "Variable Selection in Finite Mixture of AFT Regression and FMR\nModels",
    "description": "The package obtains parameter estimation, i.e., maximum\nlikelihood estimators (MLE), via the Expectation-Maximization\n(EM) algorithm for the Finite Mixture of Regression (FMR)\nmodels with Normal distribution, and MLE for the Finite Mixture\nof Accelerated Failure Time Regression (FMAFTR) subject to\nright censoring with Log-Normal and Weibull distributions via\nthe EM algorithm and the Newton-Raphson algorithm (for Weibull\ndistribution). More importantly, the package obtains the\nmaximum penalized likelihood (MPLE) for both FMR and FMAFTR\nmodels (collectively called FMRs). A component-wise tuning\nparameter selection based on a component-wise BIC is\nimplemented in the package. Furthermore, this package provides\nRidge Regression and Elastic Net.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9956,
    "stars": 0
  },
  {
    "id": 7318,
    "package_name": "StatescopeR",
    "title": "StatescopeR framework for discovery of cell states from cell\ntype-specific gene expression profiles inferred from bulk mRNA\nprofiles",
    "description": "StatescopeR is an R wrapper around Statescope, a\ncomputational framework designed to discover cell states from\ncell type-specific gene expression profiles inferred from bulk\nRNA profiles.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9777,
    "stars": 0
  },
  {
    "id": 8938,
    "package_name": "bacon",
    "title": "Controlling bias and inflation in association studies using the\nempirical null distribution",
    "description": "Bacon can be used to remove inflation and bias often\nobserved in epigenome- and transcriptome-wide association\nstudies. To this end bacon constructs an empirical null\ndistribution using a Gibbs Sampling algorithm by fitting a\nthree-component normal mixture on z-scores.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9675,
    "stars": 0
  },
  {
    "id": 2950,
    "package_name": "GofCens",
    "title": "Goodness-of-Fit Methods for Right-Censored Data",
    "description": "Graphical tools and goodness-of-fit tests for\nright-censored data: 1. Kolmogorov-Smirnov, Cramér-von Mises,\nand Anderson-Darling tests, which use the empirical\ndistribution function for complete data and are extended for\nright-censored data. 2. Generalized chi-squared-type test,\nwhich is based on the squared differences between observed and\nexpected counts using random cells with right-censored data. 3.\nA series of graphical tools such as probability or cumulative\nhazard plots to guide the decision about the most suitable\nparametric model for the data. These functions share several\nfeatures as they can handle both complete and right-censored\ndata, and they provide parameter estimates for the\ndistributions under study.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 3366,
    "package_name": "IVAS",
    "title": "Identification of genetic Variants affecting Alternative\nSplicing",
    "description": "Identification of genetic variants affecting alternative\nsplicing.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 12002,
    "package_name": "dreamer",
    "title": "Dose Response Models for Bayesian Model Averaging",
    "description": "Fits dose-response models utilizing a Bayesian model\naveraging approach as outlined in Gould (2019)\n<doi:10.1002/bimj.201700211> for both continuous and binary\nresponses. Longitudinal dose-response modeling is also\nsupported in a Bayesian model averaging framework as outlined\nin Payne, Ray, and Thomann (2024)\n<doi:10.1080/10543406.2023.2292214>. Functions for plotting and\ncalculating various posterior quantities (e.g. posterior mean,\nquantiles, probability of minimum efficacious dose, etc.) are\nalso implemented.  Copyright Eli Lilly and Company (2019).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 18060,
    "package_name": "multiWGCNA",
    "title": "multiWGCNA",
    "description": "An R package for deeping mining gene co-expression\nnetworks in multi-trait expression data. Provides functions for\nanalyzing, comparing, and visualizing WGCNA networks across\nconditions. multiWGCNA was designed to handle the common case\nwhere there are multiple biologically meaningful sample traits,\nsuch as disease vs wildtype across development or anatomical\nregion.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 24898,
    "package_name": "tornado",
    "title": "Plots for Model Sensitivity and Variable Importance",
    "description": "Draws tornado plots for model sensitivity to univariate\nchanges.  Implements methods for many modeling methods\nincluding linear models, generalized linear models, survival\nregression models, and arbitrary machine learning models in the\ncaret package.  Also draws variable importance plots.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9542,
    "stars": 0
  },
  {
    "id": 18038,
    "package_name": "multe",
    "title": "Multiple Treatment Effects Regression",
    "description": "Implements contamination bias diagnostics and alternative\nestimators for regressions with multiple treatments. The\nimplementation is based on Goldsmith-Pinkham, Hull, and Kolesár\n(2024) <doi:10.48550/arXiv.2106.05024>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9294,
    "stars": 0
  },
  {
    "id": 4302,
    "package_name": "Melissa",
    "title": "Bayesian clustering and imputationa of single cell methylomes",
    "description": "Melissa is a Baysian probabilistic model for jointly\nclustering and imputing single cell methylomes. This is done by\ntaking into account local correlations via a Generalised Linear\nModel approach and global similarities using a mixture\nmodelling approach.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9031,
    "stars": 0
  },
  {
    "id": 6978,
    "package_name": "SanityR",
    "title": "R/Bioconductor interface to the Sanity model gene expression\nanalysis",
    "description": "a Bayesian normalization procedure derived from first\nprinciples. Sanity estimates expression values and associated\nerror bars directly from raw unique molecular identifier (UMI)\ncounts without any tunable parameters.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.9031,
    "stars": 0
  },
  {
    "id": 85,
    "package_name": "AMARETTO",
    "title": "Regulatory Network Inference and Driver Gene Evaluation using\nIntegrative Multi-Omics Analysis and Penalized Regression",
    "description": "Integrating an increasing number of available multi-omics\ncancer data remains one of the main challenges to improve our\nunderstanding of cancer. One of the main challenges is using\nmulti-omics data for identifying novel cancer driver genes. We\nhave developed an algorithm, called AMARETTO, that integrates\ncopy number, DNA methylation and gene expression data to\nidentify a set of driver genes by analyzing cancer samples and\nconnects them to clusters of co-expressed genes, which we\ndefine as modules. We applied AMARETTO in a pancancer setting\nto identify cancer driver genes and their modules on multiple\ncancer sites. AMARETTO captures modules enriched in\nangiogenesis, cell cycle and EMT, and modules that accurately\npredict survival and molecular subtypes. This allows AMARETTO\nto identify novel cancer driver genes directing canonical\ncancer pathways.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8751,
    "stars": 0
  },
  {
    "id": 8878,
    "package_name": "avar",
    "title": "Allan Variance",
    "description": "Implements the allan variance and allan variance linear\nregression estimator for latent time series models. More\ndetails about the method can be found, for example, in\nGuerrier, S., Molinari, R., & Stebler, Y. (2016)\n<doi:10.1109/LSP.2016.2541867>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8751,
    "stars": 0
  },
  {
    "id": 9469,
    "package_name": "blocklength",
    "title": "Select an Optimal Block-Length to Bootstrap Dependent Data\n(Block Bootstrap)",
    "description": "A set of functions to select the optimal block-length for\na dependent bootstrap (block-bootstrap). Includes the Hall,\nHorowitz, and Jing (1995) <doi:10.1093/biomet/82.3.561>\nsubsampling-based cross-validation method, the Politis and\nWhite (2004) <doi:10.1081/ETC-120028836> Spectral Density\nPlug-in method, including the Patton, Politis, and White (2009)\n<doi:10.1080/07474930802459016> correction, and the Lahiri,\nFurukawa, and Lee (2007) <doi:10.1016/j.stamet.2006.08.002>\nnonparametric plug-in method, with a corresponding set of S3\nplot methods.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8751,
    "stars": 0
  },
  {
    "id": 16659,
    "package_name": "longevity",
    "title": "Statistical Methods for the Analysis of Excess Lifetimes",
    "description": "A collection of parametric and nonparametric methods for\nthe analysis of survival data. Parametric families implemented\ninclude Gompertz-Makeham, exponential and generalized Pareto\nmodels and extended models. The package includes an\nimplementation of the nonparametric maximum likelihood\nestimator for arbitrary truncation and censoring pattern based\non Turnbull (1976) <doi:10.1111/j.2517-6161.1976.tb01597.x>,\nalong with graphical goodness-of-fit diagnostics. Parametric\nmodels for positive random variables and peaks over threshold\nmodels based on extreme value theory are described in Rootzén\nand Zholud (2017) <doi:10.1007/s10687-017-0305-5>; Belzile et\nal. (2021) <doi:10.1098/rsos.202097> and Belzile et al. (2022)\n<doi:10.1146/annurev-statistics-040120-025426>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8751,
    "stars": 0
  },
  {
    "id": 23261,
    "package_name": "smoothHR",
    "title": "Smooth Hazard Ratio Curves Taking a Reference Value",
    "description": "Provides flexible hazard ratio curves allowing non-linear\nrelationships between continuous predictors and survival. To\nbetter understand the effects that each continuous covariate\nhas on the outcome, results are expressed in terms of hazard\nratio curves, taking a specific covariate value as reference.\nConfidence bands for these curves are also derived.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8692,
    "stars": 0
  },
  {
    "id": 11143,
    "package_name": "ctmle",
    "title": "Collaborative Targeted Maximum Likelihood Estimation",
    "description": "Implements the general template for collaborative targeted\nmaximum likelihood estimation. It also provides several\ncommonly used C-TMLE instantiation, like the vanilla/scalable\nvariable-selection C-TMLE (Ju et al. (2017)\n<doi:10.1177/0962280217729845>) and the glmnet-C-TMLE algorithm\n(Ju et al. (2017) <arXiv:1706.10029>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8451,
    "stars": 0
  },
  {
    "id": 858,
    "package_name": "CAMeRa",
    "title": "CAMeRa (Cross Ancestral Mendelian Randomisation)",
    "description": "CAMERA estimates joint causal effect in multiple\nancestries and detects pleiotropy via the zero relevance model.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8426,
    "stars": 0
  },
  {
    "id": 15442,
    "package_name": "ife",
    "title": "Autodiff for Influence Function Based Estimates",
    "description": "Implements an S7 class for estimates based on influence\nfunctions, with forward mode automatic differentiation defined\nfor standard arithmetic operations.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8403,
    "stars": 0
  },
  {
    "id": 10996,
    "package_name": "cragg",
    "title": "Tests for Weak Instruments in R",
    "description": "Implements Cragg-Donald (1993)\n<doi:10.1017/S0266466600007519> and Stock and Yogo (2005)\n<doi:10.1017/CBO9780511614491.006> tests for weak instruments\nin R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8228,
    "stars": 0
  },
  {
    "id": 7543,
    "package_name": "TPmsm",
    "title": "Estimation of Transition Probabilities in Multistate Models",
    "description": "Estimation of transition probabilities for the\nillness-death model and or the three-state progressive model.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8195,
    "stars": 0
  },
  {
    "id": 15690,
    "package_name": "intsurv",
    "title": "Integrative Survival Modeling",
    "description": "Contains implementations of the integrative Cox model with\nuncertain event times proposed by Wang, et al. (2020)\n<doi:10.1214/19-AOAS1287>, the regularized Cox cure rate model\nwith uncertain event status proposed by Wang, et al. (2023)\n<doi:10.1007/s12561-023-09374-w>, and other survival analysis\nroutines including the Cox cure rate model proposed by Kuk and\nChen (1992) <doi:10.1093/biomet/79.3.531> via an EM algorithm\nproposed by Sy and Taylor (2000)\n<doi:10.1111/j.0006-341X.2000.00227.x>, the regularized Cox\ncure rate model with elastic net penalty following Masud et al.\n(2018) <doi:10.1177/0962280216677748>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.8116,
    "stars": 0
  },
  {
    "id": 15742,
    "package_name": "iprior",
    "title": "Regression Modelling using I-Priors",
    "description": "Provides methods to perform and analyse I-prior regression\nmodels. Estimation is done either via direct optimisation of\nthe log-likelihood or an EM algorithm.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7974,
    "stars": 0
  },
  {
    "id": 8416,
    "package_name": "airpart",
    "title": "Differential cell-type-specific allelic imbalance",
    "description": "Airpart identifies sets of genes displaying differential\ncell-type-specific allelic imbalance across cell types or\nstates, utilizing single-cell allelic counts. It makes use of a\ngeneralized fused lasso with binomial observations of allelic\ncounts to partition cell types by their allelic imbalance.\nAlternatively, a nonparametric method for partitioning cell\ntypes is offered. The package includes a number of\nvisualizations and quality control functions for examining\nsingle cell allelic imbalance datasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 13672,
    "package_name": "gCrisprTools",
    "title": "Suite of Functions for Pooled Crispr Screen QC and Analysis",
    "description": "Set of tools for evaluating pooled high-throughput\nscreening experiments, typically employing CRISPR/Cas9 or shRNA\nexpression cassettes. Contains methods for interrogating\nlibrary and cassette behavior within an experiment, identifying\ndifferentially abundant cassettes, aggregating signals to\nidentify candidate targets for empirical validation, hypothesis\ntesting, and comprehensive reporting. Version 2.0 extends these\napplications to include a variety of tools for contextualizing\nand integrating signals across many experiments, incorporates\nextended signal enrichment methodologies via the \"sparrow\"\npackage, and streamlines many formal requirements to aid in\ninterpretablity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 21564,
    "package_name": "rifi",
    "title": "'rifi' analyses data from rifampicin time series created by\nmicroarray or RNAseq",
    "description": "'rifi' analyses data from rifampicin time series created\nby microarray or RNAseq. 'rifi' is a transcriptome data\nanalysis tool for the holistic identification of transcription\nand decay associated processes. The decay constants and the\ndelay of the onset of decay is fitted for each probe/bin.\nSubsequently, probes/bins of equal properties are combined into\nsegments by dynamic programming, independent of a existing\ngenome annotation. This allows to detect transcript segments of\ndifferent stability or transcriptional events within one\nannotated gene. In addition to the classic decay\nconstant/half-life analysis, 'rifi' detects processing sites,\ntranscription pausing sites, internal transcription start sites\nin operons, sites of partial transcription termination in\noperons, identifies areas of likely transcriptional\ninterference by the collision mechanism and gives an estimate\nof the transcription velocity. All data are integrated to give\nan estimate of continous transcriptional units, i.e. operons.\nComprehensive output tables and visualizations of the full\ngenome result and the individual fits for all probes/bins are\nproduced.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 21709,
    "package_name": "rnaEditr",
    "title": "Statistical analysis of RNA editing sites and hyper-editing\nregions",
    "description": "RNAeditr analyzes site-specific RNA editing events, as\nwell as hyper-editing regions. The editing frequencies can be\ntested against binary, continuous or survival outcomes.\nMultiple covariate variables as well as interaction effects can\nalso be incorporated in the statistical models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 24060,
    "package_name": "supersigs",
    "title": "Supervised mutational signatures",
    "description": "Generate SuperSigs (supervised mutational signatures) from\nsingle nucleotide variants in the cancer genome. Functions\nincluded in the package allow the user to learn supervised\nmutational signatures from their data and apply them to new\ndata. The methodology is based on the one described in Afsari\n(2021, ELife).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7782,
    "stars": 0
  },
  {
    "id": 16290,
    "package_name": "lax",
    "title": "Loglikelihood Adjustment for Extreme Value Models",
    "description": "Performs adjusted inferences based on model objects\nfitted, using maximum likelihood estimation, by the extreme\nvalue analysis packages 'eva'\n<https://cran.r-project.org/package=eva>, 'evd'\n<https://cran.r-project.org/package=evd>, 'evir'\n<https://cran.r-project.org/package=evir>, 'extRemes'\n<https://cran.r-project.org/package=extRemes>, 'fExtremes'\n<https://cran.r-project.org/package=fExtremes>, 'ismev'\n<https://cran.r-project.org/package=ismev>, 'mev'\n<https://cran.r-project.org/package=mev>, 'POT'\n<https://cran.r-project.org/package=POT> and 'texmex'\n<https://cran.r-project.org/package=texmex>. Adjusted standard\nerrors and an adjusted loglikelihood are provided, using the\n'chandwich' package\n<https://cran.r-project.org/package=chandwich> and the\nobject-oriented features of the 'sandwich' package\n<https://cran.r-project.org/package=sandwich>. The adjustment\nis based on a robust sandwich estimator of the parameter\ncovariance matrix, based on the methodology in Chandler and\nBate (2007) <doi:10.1093/biomet/asm015>. This can be used for\ncluster correlated data when interest lies in the parameters of\nthe marginal distributions, or for performing inferences that\nare robust to certain types of model misspecification.\nUnivariate extreme value models, including regression models,\nare supported.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7672,
    "stars": 0
  },
  {
    "id": 4181,
    "package_name": "MSstatsResponse",
    "title": "Statistical Methods for Chemoproteomics Dose-Response Analysis",
    "description": "Tools for detecting drug-protein interactions and\nestimating IC50 values from chemoproteomics data. Implements\nsemi-parametric isotonic regression, bootstrapping, and curve\nfitting to evaluate compound effects on protein abundance.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7404,
    "stars": 0
  },
  {
    "id": 17458,
    "package_name": "midasml",
    "title": "Estimation and Prediction Methods for High-Dimensional Mixed\nFrequency Time Series Data",
    "description": "The 'midasml' package implements estimation and prediction\nmethods for high-dimensional mixed-frequency (MIDAS)\ntime-series and panel data regression models. The regularized\nMIDAS models are estimated using orthogonal (e.g. Legendre)\npolynomials and sparse-group LASSO (sg-LASSO) estimator. For\nmore information on the 'midasml' approach see Babii, Ghysels,\nand Striaukas (2021, JBES forthcoming)\n<doi:10.1080/07350015.2021.1899933>. The package is equipped\nwith the fast implementation of the sg-LASSO estimator by means\nof proximal block coordinate descent. High-dimensional mixed\nfrequency time-series data can also be easily manipulated with\nfunctions provided in the package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7226,
    "stars": 0
  },
  {
    "id": 9099,
    "package_name": "bayesvl",
    "title": "Visually Learning the Graphical Structure of Bayesian Networks\nand Performing MCMC with 'Stan'",
    "description": "Provides users with its associated functions for\npedagogical purposes in visually learning Bayesian networks and\nMarkov chain Monte Carlo (MCMC) computations. It enables users\nto: a) Create and examine the (starting) graphical structure of\nBayesian networks; b) Create random Bayesian networks using a\ndataset with customized constraints; c) Generate Stan code for\nstructures of Bayesian networks for sampling the data and\nlearning parameters; d) Plot the network graphs; e) Perform\nMarkov chain Monte Carlo computations and produce graphs for\nposteriors checks. The package refers to one reference item,\nwhich describes the methods and algorithms: Vuong, Quan-Hoang\nand La, Viet-Phuong (2019) <doi:10.31219/osf.io/w5dx6> The\n'bayesvl' R package. Open Science Framework (May 18).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7202,
    "stars": 0
  },
  {
    "id": 10236,
    "package_name": "ciflyr",
    "title": "Reachability-Based Primitives for Graphical Causal Inference",
    "description": "Provides a framework for specifying and running flexible\nlinear-time reachability-based algorithms for graphical causal\ninference. Rule tables are used to encode and customize the\nreachability algorithm to typical causal and probabilistic\nreasoning tasks such as finding d-connected nodes or more\nadvanced applications. For more information, see Wienöbst,\nWeichwald and Henckel (2025) <doi:10.48550/arXiv.2506.15758>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.716,
    "stars": 0
  },
  {
    "id": 21618,
    "package_name": "rjd3tramoseats",
    "title": "Seasonal Adjustment with TRAMO-SEATS in 'JDemetra+ 3.x'",
    "description": "R Interface to 'JDemetra+ 3.x'\n(<https://github.com/jdemetra>) time series analysis software.\nIt offers full acces to options and outputs of TRAMO-SEATS\n(Time series Regression with ARIMA noise, Missing values and\nOutliers - Signal Extraction in ARIMA Time Series), including\nTRAMO modelling (automatic ARIMA model with outlier detection\nand trading days adjustment).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7147,
    "stars": 0
  },
  {
    "id": 20431,
    "package_name": "puma",
    "title": "Propagating Uncertainty in Microarray Analysis(including\nAffymetrix tranditional 3' arrays and exon arrays and Human\nTranscriptome Array 2.0)",
    "description": "Most analyses of Affymetrix GeneChip data (including\ntranditional 3' arrays and exon arrays and Human Transcriptome\nArray 2.0) are based on point estimates of expression levels\nand ignore the uncertainty of such estimates. By propagating\nuncertainty to downstream analyses we can improve results from\nmicroarray analyses. For the first time, the puma package makes\na suite of uncertainty propagation methods available to a\ngeneral audience. In additon to calculte gene expression from\nAffymetrix 3' arrays, puma also provides methods to process\nexon arrays and produces gene and isoform expression for\nalternative splicing study. puma also offers improvements in\nterms of scope and speed of execution over previously available\nuncertainty propagation methods. Included are summarisation,\ndifferential expression detection, clustering and PCA methods,\ntogether with useful plotting functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.7076,
    "stars": 0
  },
  {
    "id": 1512,
    "package_name": "DA",
    "title": "Discriminant Analysis for Evolutionary Inference",
    "description": "Discriminant Analysis (DA) for evolutionary inference\n(Qin, X. et al, 2020, <doi:10.22541/au.159256808.83862168>),\nespecially for population genetic structure and community\nstructure inference. This package incorporates the commonly\nused linear and non-linear, local and global supervised\nlearning approaches (discriminant analysis), including Linear\nDiscriminant Analysis of Kernel Principal Components (LDAKPC),\nLocal (Fisher) Linear Discriminant Analysis (LFDA), Local\n(Fisher) Discriminant Analysis of Kernel Principal Components\n(LFDAKPC) and Kernel Local (Fisher) Discriminant Analysis\n(KLFDA). These discriminant analyses can be used to do\necological and evolutionary inference, including demography\ninference, species identification, and population/community\nstructure inference.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 2736,
    "package_name": "GPTCM",
    "title": "Generalized Promotion Time Cure Model with Bayesian Shrinkage\nPriors",
    "description": "Generalized promotion time cure model (GPTCM) via Bayesian\nhierarchical modeling for multiscale data integration (Zhao et\nal. (2025) <doi:10.48550/arXiv.2509.01001>). The Bayesian\nGPTCMs are applicable for both low- and high-dimensional data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 8243,
    "package_name": "adaptDiag",
    "title": "Bayesian Adaptive Designs for Diagnostic Trials",
    "description": "Simulate clinical trials for diagnostic test devices and\nevaluate the operating characteristics under an adaptive design\nwith futility assessment determined via the posterior\npredictive probabilities.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 10747,
    "package_name": "consensus",
    "title": "Cross-platform consensus analysis of genomic measurements via\ninterlaboratory testing method",
    "description": "An implementation of the American Society for Testing and\nMaterials (ASTM) Standard E691 for interlaboratory testing\nprocedures, designed for cross-platform genomic measurements.\nGiven three (3) or more genomic platforms or laboratory\nprotocols, this package provides interlaboratory testing\nprocedures giving per-locus comparisons for sensitivity and\nprecision between platforms.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 25824,
    "package_name": "weitrix",
    "title": "Tools for matrices with precision weights, test and explore\nweighted or sparse data",
    "description": "Data type and tools for working with matrices having\nprecision weights and missing data. This package provides a\ncommon representation and tools that can be used with many\ntypes of high-throughput data. The meaning of the weights is\ncompatible with usage in the base R function \"lm\" and the\npackage \"limma\". Calibrate weights to account for known\npredictors of precision. Find rows with excess variability.\nPerform differential testing and find rows with the largest\nconfident differences. Find PCA-like components of variation\neven with many missing values, rotated so that individual\ncomponents may be meaningfully interpreted. DelayedArray\nmatrices and BiocParallel are supported.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.699,
    "stars": 0
  },
  {
    "id": 25154,
    "package_name": "tsviz",
    "title": "Easy and Interactive Time Series Visualization",
    "description": "An 'RStudio' add-in to visualize time series. Time series\nare searched in the global environment as data.frame objects\nwith a column of type date and a column of type numeric.\nInteractive charts are produced using 'plotly' package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6946,
    "stars": 0
  },
  {
    "id": 20189,
    "package_name": "prettyglm",
    "title": "Pretty Summaries of Generalized Linear Model Coefficients",
    "description": "One of the main advantages of using Generalised Linear\nModels is their interpretability.  The goal of 'prettyglm' is\nto provide a set of functions which easily create beautiful\ncoefficient summaries which can readily be shared and\nexplained. 'prettyglm' helps users create coefficient summaries\nwhich include categorical base levels, variable importance and\ntype III p.values. 'prettyglm' also creates beautiful\nrelativity plots for categorical, continuous and splined\ncoefficients.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6946,
    "stars": 0
  },
  {
    "id": 25107,
    "package_name": "tscompdata",
    "title": "Time series data from various forecasting competitions",
    "description": "Time series data from the following forecasting\ncompetitions are provided: M, M3, NN3, NN5, NNGC1, Tourism, and\nGEFCom2012.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6866,
    "stars": 0
  },
  {
    "id": 22324,
    "package_name": "scDDboost",
    "title": "A compositional model to assess expression changes from\nsingle-cell rna-seq data",
    "description": "scDDboost is an R package to analyze changes in the\ndistribution of single-cell expression data between two\nexperimental conditions. Compared to other methods that assess\ndifferential expression, scDDboost benefits uniquely from\ninformation conveyed by the clustering of cells into cellular\nsubtypes. Through a novel empirical Bayesian formulation it\ncalculates gene-specific posterior probabilities that the\nmarginal expression distribution is the same (or different)\nbetween the two conditions. The implementation in scDDboost\ntreats gene-level expression data within each condition as a\nmixture of negative binomial distributions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6767,
    "stars": 0
  },
  {
    "id": 19621,
    "package_name": "phenopath",
    "title": "Genomic trajectories with heterogeneous genetic and\nenvironmental backgrounds",
    "description": "PhenoPath infers genomic trajectories (pseudotimes) in the\npresence of heterogeneous genetic and environmental backgrounds\nand tests for interactions between them.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6628,
    "stars": 0
  },
  {
    "id": 535,
    "package_name": "BUSseq",
    "title": "Batch Effect Correction with Unknow Subtypes for scRNA-seq data",
    "description": "BUSseq R package fits an interpretable Bayesian\nhierarchical model---the Batch Effects Correction with Unknown\nSubtypes for scRNA seq Data (BUSseq)---to correct batch effects\nin the presence of unknown cell types. BUSseq is able to\nsimultaneously correct batch effects, clusters cell types, and\ntakes care of the count data nature, the overdispersion, the\ndropout events, and the cell-specific sequencing depth of\nscRNA-seq data. After correcting the batch effects with BUSseq,\nthe corrected value can be used for downstream analysis as if\nall cells were sequenced in a single batch. BUSseq can\nintegrate read count matrices obtained from different scRNA-seq\nplatforms and allow cell types to be measured in some but not\nall of the batches as long as the experimental design fulfills\nthe conditions listed in our manuscript.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 3145,
    "package_name": "HiCPotts",
    "title": "HiCPotts: Hierarchical Modeling to Identify and Correct Genomic\nBiases in Hi-C",
    "description": "The HiCPotts package provides a comprehensive Bayesian\nframework for analyzing Hi-C interaction data, integrating both\nspatial and genomic biases within a probabilistic modeling\nframework. At its core, HiCPotts leverages the Potts model (Wu,\n1982)—a well-established graphical model—to capture and\nquantify spatial dependencies across interaction loci arranged\non a genomic lattice. By treating each interaction as a\nspatially correlated random variable, the Potts model enables\nrobust segmentation of the genomic landscape into meaningful\ncomponents, such as noise, true signals, and false signals. To\nmodel the influence of various genomic biases, HiCPotts employs\na regression-based approach incorporating multiple covariates:\nGenomic distance (D): The distance between interacting loci,\nrecognized as a fundamental driver of contact frequency.\nGC-content (GC): The local GC composition around the\ninteracting loci, which can influence chromatin structure and\ninteraction patterns. Transposable elements (TEs): The presence\nand abundance of repetitive elements that may shape contact\nprobability through chromatin organization. Accessibility score\n(Acc): A measure of chromatin openness, informing how\naccessible certain genomic regions are to interaction. By\nembedding these covariates into a hierarchical mixture model,\nHiCPotts characterizes each interaction’s probability of\nbelonging to one of several latent components. The model\nparameters, including regression coefficients, zero-inflation\nparameters (for ZIP/ZINB distributions), and dispersion terms\n(for NB/ZINB distributions), are inferred via a MCMC sampler.\nThis algorithm draws samples from the joint posterior\ndistribution, allowing for flexible posterior inference on\nmodel parameters and hidden states. From these posterior\nsamples, HiCPotts computes posterior means of regression\nparameters and other quantities of interest. These posterior\nestimates are then used to calculate the posterior\nprobabilities that assign each interaction to a specific\ncomponent. The resulting classification sheds light on the\nunderlying structure: distinguishing genuine high-confidence\ninteractions (signal) from background noise and potential false\nsignals, while simultaneously quantifying the impact of genomic\nbiases on observed interaction frequencies. In summary,\nHiCPotts seamlessly integrates spatial modeling, bias\ncorrection, and probabilistic classification into a unified\nBayesian inference framework. It provides rich posterior\nsummaries and interpretable, model-based assignments of\ninteraction states, enabling researchers to better understand\nthe interplay between genomic organization, biases, and spatial\ncorrelation in Hi-C data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 4691,
    "package_name": "NetworkChange",
    "title": "Bayesian Package for Network Changepoint Analysis",
    "description": "Network changepoint analysis for undirected network data.\nThe package implements a hidden Markov network change point\nmodel (Park and Sohn (2020)). Functions for break number\ndetection using the approximate marginal likelihood and WAIC\nare also provided.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 8789,
    "package_name": "asuri",
    "title": "Analysis of SUrvival and RIsk prediction in patients based on\ngene signatures",
    "description": "The ASURI (Analysis of SUrvival and patients RIsk\nprediction based on gene signatures) package discovers marker\ngenes that are related to risk prediction capabilities and to a\nclinical variable of interest. It uses two main steps,\nincluding subsampling glmnet and unicox. The package implements\nrobust functions to discover survival markers related to a\nclinical phenotype and to predict a risk score, allowing to\nstudy the patient's risk based on the gene signatures. Several\nplots are provided to visualise the relevance of the genes, the\nrisk score, and patient stratification, as well as a robust\nversion of the Kaplan-Meier curves.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 9216,
    "package_name": "betaselectr",
    "title": "Betas-Select in Structural Equation Models and Linear Models",
    "description": "It computes betas-select, coefficients after\nstandardization in structural equation models and regression\nmodels, standardizing only selected variables. Supports models\nwith moderation, with product terms formed after\nstandardization. It also offers confidence intervals that\naccount for standardization, including bootstrap confidence\nintervals as proposed by Cheung et al. (2022)\n<doi:10.1037/hea0001188>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 10099,
    "package_name": "cfid",
    "title": "Identification of Counterfactual Queries in Causal Models",
    "description": "Facilitates the identification of counterfactual queries\nin structural causal models via the ID* and IDC* algorithms by\nShpitser, I. and Pearl, J. (2007, 2008)\n<doi:10.48550/arXiv.1206.5294>,\n<https://jmlr.org/papers/v9/shpitser08a.html>. Provides a\nsimple interface for defining causal diagrams and\ncounterfactual conjunctions. Construction of parallel worlds\ngraphs and counterfactual graphs is carried out automatically\nbased on the counterfactual query and the causal diagram. See\nTikka, S. (2023) <doi:10.32614/RJ-2023-053> for a tutorial of\nthe package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6532,
    "stars": 0
  },
  {
    "id": 23930,
    "package_name": "stocks",
    "title": "Stock Market Analysis",
    "description": "Functions for analyzing and visualizing stock market data.\nMain features are loading and aligning historical data,\ncalculating performance metrics for individual funds or\nportfolios (e.g. annualized growth, maximum drawdown,\nSharpe/Sortino ratio), and creating graphs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6325,
    "stars": 0
  },
  {
    "id": 2761,
    "package_name": "GRaNIE",
    "title": "GRaNIE: Reconstruction cell type specific gene regulatory\nnetworks including enhancers using single-cell or bulk\nchromatin accessibility and RNA-seq data",
    "description": "Genetic variants associated with diseases often affect\nnon-coding regions, thus likely having a regulatory role. To\nunderstand the effects of genetic variants in these regulatory\nregions, identifying genes that are modulated by specific\nregulatory elements (REs) is crucial. The effect of gene\nregulatory elements, such as enhancers, is often cell-type\nspecific, likely because the combinations of transcription\nfactors (TFs) that are regulating a given enhancer have\ncell-type specific activity. This TF activity can be quantified\nwith existing tools such as diffTF and captures differences in\nbinding of a TF in open chromatin regions. Collectively, this\nforms a gene regulatory network (GRN) with cell-type and\ndata-specific TF-RE and RE-gene links. Here, we reconstruct\nsuch a GRN using single-cell or bulk RNAseq and open chromatin\n(e.g., using ATACseq or ChIPseq for open chromatin marks) and\noptionally (Capture) Hi-C data. Our network contains different\ntypes of links, connecting TFs to regulatory elements, the\nlatter of which is connected to genes in the vicinity or within\nthe same chromatin domain (TAD). We use a statistical framework\nto assign empirical FDRs and weights to all links using a\npermutation-based approach.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6232,
    "stars": 0
  },
  {
    "id": 12935,
    "package_name": "fairadapt",
    "title": "Fair Data Adaptation with Quantile Preservation",
    "description": "An implementation of the fair data adaptation with\nquantile preservation described in Plecko & Meinshausen (2019)\n<arXiv:1911.06685>. The adaptation procedure uses the specified\ncausal graph to pre-process the given training and testing data\nin such a way to remove the bias caused by the protected\nattribute. The procedure uses tree ensembles for quantile\nregression.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6232,
    "stars": 0
  },
  {
    "id": 17179,
    "package_name": "medoutcon",
    "title": "Efficient Natural and Interventional Causal Mediation Analysis",
    "description": "Efficient estimators of interventional (in)direct effects\nin the presence of mediator-outcome confounding affected by\nexposure. The effects estimated allow for the impact of the\nexposure on the outcome through a direct path to be\ndisentangled from that through mediators, even in the presence\nof intermediate confounders that complicate such a\nrelationship. Currently supported are non-parametric efficient\none-step and targeted minimum loss estimators based on the\nformulation of Díaz, Hejazi, Rudolph, and van der Laan (2020)\n<doi:10.1093/biomet/asaa085>. Support for efficient estimation\nof the natural (in)direct effects is also provided, appropriate\nfor settings in which intermediate confounders are absent. The\npackage also supports estimation of these effects when the\nmediators are measured using outcome-dependent two-phase\nsampling designs (e.g., case-cohort).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6232,
    "stars": 0
  },
  {
    "id": 2703,
    "package_name": "GNET2",
    "title": "Constructing gene regulatory networks from expression data\nthrough functional module inference",
    "description": "Cluster genes to functional groups with E-M process.\nIteratively perform TF assigning and Gene assigning, until the\nassignment of genes did not change, or max number of iterations\nis reached.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 3521,
    "package_name": "KBoost",
    "title": "Inference of gene regulatory networks from gene expression data",
    "description": "Reconstructing gene regulatory networks and transcription\nfactor activity is crucial to understand biological processes\nand holds potential for developing personalized treatment. Yet,\nit is still an open problem as state-of-art algorithm are often\nnot able to handle large amounts of data. Furthermore, many of\nthe present methods predict numerous false positives and are\nunable to integrate other sources of information such as\npreviously known interactions. Here we introduce KBoost, an\nalgorithm that uses kernel PCA regression, boosting and\nBayesian model averaging for fast and accurate reconstruction\nof gene regulatory networks. KBoost can also use a prior\nnetwork built on previously known transcription factor targets.\nWe have benchmarked KBoost using three different datasets\nagainst other high performing algorithms. The results show that\nour method compares favourably to other methods across\ndatasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 5831,
    "package_name": "RJMCMCNucleosomes",
    "title": "Bayesian hierarchical model for genome-wide nucleosome\npositioning with high-throughput short-read data (MNase-Seq)",
    "description": "This package does nucleosome positioning using informative\nMultinomial-Dirichlet prior in a t-mixture with reversible jump\nestimation of nucleosome positions for genome-wide profiling.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 8352,
    "package_name": "affylmGUI",
    "title": "GUI for limma Package with Affymetrix Microarrays",
    "description": "A Graphical User Interface (GUI) for analysis of\nAffymetrix microarray gene expression data using the affy and\nlimma packages.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 8986,
    "package_name": "barbieQ",
    "title": "Analyze Barcode Data from Clonal Tracking Experiments",
    "description": "The barbieQ package provides a series of robust\nstatistical tools for analysing barcode count data generated\nfrom cell clonal tracking (i.e., lineage tracing) experiments.\nIn these experiments, an initial cell and its offspring\ncollectively form a clone (i.e., lineage). A unique barcode\nsequence, incorporated into the DNA of the inital cell, is\ninherited within the clone. This one-to-one mapping of barcodes\nto clones enables clonal tracking of their behaviors. By\ncounting barcodes, researchers can quantify the population\nabundance of individual clones under specific experimental\nperturbations. barbieQ supports barcode count data\npreprocessing, statistical testing, and visualization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 16465,
    "package_name": "limmaGUI",
    "title": "GUI for limma Package With Two Color Microarrays",
    "description": "A Graphical User Interface for differential expression\nanalysis of two-color microarray data using the limma package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 23405,
    "package_name": "spINAR",
    "title": "(Semi)Parametric Estimation and Bootstrapping of INAR Models",
    "description": "Semiparametric and parametric estimation of INAR models\nincluding a finite sample refinement (Faymonville et al. (2022)\n<doi:10.1007/s10260-022-00655-0>) for the semiparametric\nsetting introduced in Drost et al. (2009)\n<doi:10.1111/j.1467-9868.2008.00687.x>, different procedures to\nbootstrap INAR data (Jentsch, C. and Weiß, C.H. (2017)\n<doi:10.3150/18-BEJ1057>) and flexible simulation of INAR data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 24131,
    "package_name": "survival.svb",
    "title": "Fit High-Dimensional Proportional Hazards Models",
    "description": "Implementation of methodology designed to perform: (i)\nvariable selection, (ii) effect estimation, and (iii)\nuncertainty quantification, for high-dimensional survival data.\nOur method uses a spike-and-slab prior with Laplace slab and\nDirac spike and approximates the corresponding posterior using\nvariational inference, a popular method in machine learning for\nscalable conditional inference. Although approximate, the\nvariational posterior provides excellent point estimates and\ngood control of the false discovery rate. For more information\nsee Komodromos et al. (2021) <arXiv:2112.10270>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 25747,
    "package_name": "wavClusteR",
    "title": "Sensitive and highly resolved identification of RNA-protein\ninteraction sites in PAR-CLIP data",
    "description": "The package provides an integrated pipeline for the\nanalysis of PAR-CLIP data. PAR-CLIP-induced transitions are\nfirst discriminated from sequencing errors, SNPs and additional\nnon-experimental sources by a non- parametric mixture model.\nThe protein binding sites (clusters) are then resolved at high\nresolution and cluster statistics are estimated using a\nrigorous Bayesian framework. Post-processing of the results,\ndata export for UCSC genome browser visualization and motif\nsearch analysis are provided. In addition, the package allows\nto integrate RNA-Seq data to estimate the False Discovery Rate\nof cluster detection. Key functions support parallel multicore\ncomputing. Note: while wavClusteR was designed for PAR-CLIP\ndata analysis, it can be applied to the analysis of other NGS\ndata obtained from experimental procedures that induce\nnucleotide substitutions (e.g. BisSeq).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.6021,
    "stars": 0
  },
  {
    "id": 1811,
    "package_name": "DeepLearningCausal",
    "title": "Causal Inference with Super Learner and Deep Neural Networks",
    "description": "Functions for deep learning estimation of Conditional\nAverage Treatment Effects (CATEs) from meta-learner models and\nPopulation Average Treatment Effects on the Treated (PATT) in\nsettings with treatment noncompliance using reticulate,\nTensorFlow and Keras3. Functions in the package also implements\nthe conformal prediction framework that enables computation and\nillustration of conformal prediction (CP) intervals for\nestimated individual treatment effects (ITEs) from meta-learner\nmodels. Additional functions in the package permit users to\nestimate the meta-learner CATEs and the PATT in settings with\ntreatment noncompliance using weighted ensemble learning via\nthe super learner approach and R neural networks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5911,
    "stars": 0
  },
  {
    "id": 14627,
    "package_name": "granulator",
    "title": "Rapid benchmarking of methods for *in silico* deconvolution of\nbulk RNA-seq data",
    "description": "granulator is an R package for the cell type deconvolution\nof heterogeneous tissues based on bulk RNA-seq data or single\ncell RNA-seq expression profiles. The package provides a\nunified testing interface to rapidly run and benchmark multiple\nstate-of-the-art deconvolution methods. Data for the\ndeconvolution of peripheral blood mononuclear cells (PBMCs)\ninto individual immune cell types is provided as well.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5911,
    "stars": 0
  },
  {
    "id": 14628,
    "package_name": "graper",
    "title": "Adaptive penalization in high-dimensional regression and\nclassification with external covariates using variational Bayes",
    "description": "This package enables regression and classification on\nhigh-dimensional data with different relative strengths of\npenalization for different feature groups, such as different\nassays or omic types. The optimal relative strengths are chosen\nadaptively. Optimisation is performed using a variational Bayes\napproach.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5911,
    "stars": 0
  },
  {
    "id": 627,
    "package_name": "BayesSampling",
    "title": "Bayes Linear Estimators for Finite Population",
    "description": "Allows the user to apply the Bayes Linear approach to\nfinite population with the Simple Random Sampling - BLE_SRS() -\nand the Stratified Simple Random Sampling design - BLE_SSRS() -\n(both without replacement), to the Ratio estimator (using\nauxiliary information) - BLE_Ratio() - and to categorical data\n- BLE_Categorical(). The Bayes linear estimation approach is\napplied to a general linear regression model for finite\npopulation prediction in BLE_Reg() and it is also possible to\nachieve the design based estimators using vague prior\ndistributions. Based on Gonçalves, K.C.M, Moura, F.A.S and\nMigon, H.S.(2014)\n<https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X201400111886>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5563,
    "stars": 0
  },
  {
    "id": 1098,
    "package_name": "CTSV",
    "title": "Identification of cell-type-specific spatially variable genes\naccounting for excess zeros",
    "description": "The R package CTSV implements the CTSV approach developed\nby Jinge Yu and Xiangyu Luo that detects cell-type-specific\nspatially variable genes accounting for excess zeros. CTSV\ndirectly models sparse raw count data through a zero-inflated\nnegative binomial regression model, incorporates cell-type\nproportions, and performs hypothesis testing based on R package\npscl. The package outputs p-values and q-values for genes in\neach cell type, and CTSV is scalable to datasets with tens of\nthousands of genes measured on hundreds of spots. CTSV can be\ninstalled in Windows, Linux, and Mac OS.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5563,
    "stars": 0
  },
  {
    "id": 16526,
    "package_name": "lite",
    "title": "Likelihood-Based Inference for Time Series Extremes",
    "description": "Performs likelihood-based inference for stationary time\nseries extremes.  The general approach follows Fawcett and\nWalshaw (2012) <doi:10.1002/env.2133>.  Marginal extreme value\ninferences are adjusted for cluster dependence in the data\nusing the methodology in Chandler and Bate (2007)\n<doi:10.1093/biomet/asm015>, producing an adjusted\nlog-likelihood for the model parameters.  A log-likelihood for\nthe extremal index is produced using the K-gaps model of\nSuveges and Davison (2010) <doi:10.1214/09-AOAS292>. These\nlog-likelihoods are combined to make inferences about extreme\nvalues. Both maximum likelihood and Bayesian approaches are\navailable.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5563,
    "stars": 0
  },
  {
    "id": 13418,
    "package_name": "foqat",
    "title": "Field Observation Quick Analysis Toolkit",
    "description": "Tools for quickly processing and analyzing field\nobservation data and air quality data. This tools contain\nfunctions that facilitate analysis in atmospheric chemistry\n(especially in ozone pollution). Some functions of time series\nare also applicable to other fields. For detail please view\nhomepage<https://github.com/tianshu129/foqat>. Scientific\nReference: 1. The Hydroxyl Radical (OH) Reactivity: Roger\nAtkinson and Janet Arey (2003) <doi:10.1021/cr0206420>. 2.\nOzone Formation Potential (OFP):\n<https://ww2.arb.ca.gov/sites/default/files/classic/regact/2009/mir2009/mir10.pdf>,\nZhang et al.(2021) <doi:10.5194/acp-21-11053-2021>. 3. Aerosol\nFormation Potential (AFP): Wenjing Wu et al. (2016)\n<doi:10.1016/j.jes.2016.03.025>. 4. TUV model:\n<https://www2.acom.ucar.edu/modeling/tropospheric-ultraviolet-and-visible-tuv-radiation-model>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5441,
    "stars": 0
  },
  {
    "id": 14536,
    "package_name": "goldilocks",
    "title": "Goldilocks Adaptive Trial Designs for Time-to-Event Endpoints",
    "description": "Implements the Goldilocks adaptive trial design for a time\nto event outcome using a piecewise exponential model and\nconjugate Gamma prior distributions. The method closely follows\nthe article by Broglio and colleagues\n<doi:10.1080/10543406.2014.888569>, which allows users to\nexplore the operating characteristics of different trial\ndesigns.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5441,
    "stars": 0
  },
  {
    "id": 23167,
    "package_name": "sketching",
    "title": "Sketching of Data via Random Subspace Embeddings",
    "description": "Construct sketches of data via random subspace embeddings.\nFor more details, see the following papers. Lee, S. and Ng, S.\n(2022). \"Least Squares Estimation Using Sketched Data with\nHeteroskedastic Errors,\" Proceedings of the 39th International\nConference on Machine Learning (ICML22), 162:12498-12520. Lee,\nS. and Ng, S. (2020). \"An Econometric Perspective on\nAlgorithmic Subsampling,\" Annual Review of Economics, 12(1):\n45–80.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5441,
    "stars": 0
  },
  {
    "id": 17052,
    "package_name": "mbQTL",
    "title": "mbQTL: A package for SNP-Taxa mGWAS analysis",
    "description": "mbQTL is a statistical R package for simultaneous\n16srRNA,16srDNA (microbial) and variant, SNP, SNV (host)\nrelationship, correlation, regression studies. We apply linear,\nlogistic and correlation based statistics to identify the\nrelationships of taxa, genus, species and variant, SNP, SNV in\nthe infected host. We produce various statistical significance\nmeasures such as P values, FDR, BC and probability estimation\nto show significance of these relationships. Further we provide\nvarious visualization function for ease and clarification of\nthe results of these analysis. The package is compatible with\ndataframe, MRexperiment and text formats.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.5315,
    "stars": 0
  },
  {
    "id": 3514,
    "package_name": "JointFPM",
    "title": "A Parametric Model for Estimating the Mean Number of Events",
    "description": "Implementation of a parametric joint model for modelling\nrecurrent and competing event processes using generalised\nsurvival models as described in Entrop et al., (2025)\n<doi:10.1002/bimj.70038>. The joint model can subsequently be\nused to predict the mean number of events in the presence of\ncompeting risks at different time points. Comparisons of the\nmean number of event functions, e.g. the differences in mean\nnumber of events between two exposure groups, are also\navailable.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4886,
    "stars": 0
  },
  {
    "id": 5848,
    "package_name": "RLassoCox",
    "title": "A reweighted Lasso-Cox by integrating gene interaction\ninformation",
    "description": "RLassoCox is a package that implements the RLasso-Cox\nmodel proposed by Wei Liu. The RLasso-Cox model integrates gene\ninteraction information into the Lasso-Cox model for accurate\nsurvival prediction and survival biomarker discovery. It is\nbased on the hypothesis that topologically important genes in\nthe gene interaction network tend to have stable expression\nchanges. The RLasso-Cox model uses random walk to evaluate the\ntopological weight of genes, and then highlights topologically\nimportant genes to improve the generalization ability of the\nLasso-Cox model. The RLasso-Cox model has the advantage of\nidentifying small gene sets with high prognostic performance on\nindependent datasets, which may play an important role in\nidentifying robust survival biomarkers for various cancer\ntypes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 12020,
    "package_name": "drord",
    "title": "Doubly-Robust Estimators for Ordinal Outcomes",
    "description": "Efficient covariate-adjusted estimators of quantities that\nare useful for establishing the effects of treatments on\nordinal outcomes (Benkeser, Diaz, Luedtke 2020\n<doi:10.1111/biom.13377>)",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 16782,
    "package_name": "mFLICA",
    "title": "Leadership-Inference Framework for Multivariate Time Series",
    "description": "A leadership-inference framework for multivariate time\nseries. The framework for multiple-faction-leadership inference\nfrom coordinated activities or 'mFLICA' uses a notion of a\nleader as an individual who initiates collective patterns that\neveryone in a group follows. Given a set of time series of\nindividual activities, our goal is to identify periods of\ncoordinated activity, find factions of coordination if more\nthan one exist, as well as identify leaders of each faction.\nFor each time step, the framework infers following relations\nbetween individual time series, then identifying a leader of\neach faction whom many individuals follow but it follows no\none. A faction is defined as a group of individuals that\neveryone follows the same leader. 'mFLICA' reports following\nrelations, leaders of factions, and members of each faction for\neach time step. Please see Chainarong Amornbunchornvej and\nTanya Berger-Wolf (2018) <doi:10.1137/1.9781611975321.62> for\nmethodology and Chainarong Amornbunchornvej (2021)\n<doi:10.1016/j.softx.2021.100781> for software when referring\nto this package in publications.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 20349,
    "package_name": "psbcSpeedUp",
    "title": "Penalized Semiparametric Bayesian Cox Models",
    "description": "Algorithms to speed up the Bayesian Lasso Cox model (Lee\net al., Int J Biostat, 2011 <doi:10.2202/1557-4679.1301>) and\nthe Bayesian Lasso Cox with mandatory variables (Zucknick et\nal. Biometrical J, 2015 <doi:10.1002/bimj.201400160>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 22283,
    "package_name": "saseR",
    "title": "Scalable Aberrant Splicing and Expression Retrieval",
    "description": "saseR is a highly performant and fast framework for\naberrant expression and splicing analyses. The main functions\nare: \\itemize{ \\item \\code{\\link{BamtoAspliCounts}} - Process\nBAM files to ASpli counts \\item \\code{\\link{convertASpli}} -\nGet gene, bin or junction counts from ASpli\nSummarizedExperiment \\item \\code{\\link{calculateOffsets}} -\nCreate an offsets assays for aberrant expression or splicing\nanalysis \\item \\code{\\link{saseRfindEncodingDim}} - Estimate\nthe optimal number of latent factors to include when estimating\nthe mean expression \\item \\code{\\link{saseRfit}} - Parameter\nestimation of the negative binomial distribution and compute\np-values for aberrant expression and splicing } For information\nupon how to use these functions, check out our vignette at\n\\url{https://github.com/statOmics/saseR/blob/main/vignettes/Vignette.Rmd}\nand the saseR paper: Segers, A. et al. (2023). Juggling offsets\nunlocks RNA-seq tools for fast scalable differential usage,\naberrant splicing and expression analyses. bioRxiv.\n\\url{https://doi.org/10.1101/2023.06.29.547014}.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 23697,
    "package_name": "squallms",
    "title": "Speedy quality assurance via lasso labeling for LC-MS data",
    "description": "squallms is a Bioconductor R package that implements a\n\"semi-labeled\" approach to untargeted mass spectrometry data.\nIt pulls in raw data from mass-spec files to calculate several\nmetrics that are then used to label MS features in bulk as high\nor low quality. These metrics of peak quality are then passed\nto a simple logistic model that produces a fully-labeled\ndataset suitable for downstream analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 24109,
    "package_name": "survdnn",
    "title": "Deep Neural Networks for Survival Analysis with R 'torch'",
    "description": "Provides deep learning models for right-censored survival\ndata using the 'torch' backend. Supports multiple loss\nfunctions, including Cox partial likelihood, L2-penalized Cox,\ntime-dependent Cox, and accelerated failure time (AFT) loss.\nOffers a formula-based interface, built-in support for\ncross-validation, hyperparameter tuning, survival curve\nplotting, and evaluation metrics such as the C-index, Brier\nscore, and integrated Brier score. For methodological details,\nsee Kvamme et al. (2019)\n<https://www.jmlr.org/papers/v20/18-424.html>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4771,
    "stars": 0
  },
  {
    "id": 8704,
    "package_name": "arima2",
    "title": "Likelihood Based Inference for ARIMA Modeling",
    "description": "Estimating and analyzing auto regressive integrated moving\naverage (ARIMA) models. The primary function in this package is\narima(), which fits an ARIMA model to univariate time series\ndata using a random restart algorithm. This approach frequently\nleads to models that have model likelihood greater than or\nequal to that of the likelihood obtained by fitting the same\nmodel using the arima() function from the 'stats' package. This\npackage enables proper optimization of model likelihoods, which\nis a necessary condition for performing likelihood ratio tests.\nThis package relies heavily on the source code of the arima()\nfunction of the 'stats' package. For more information, please\nsee Jesse Wheeler and Edward L. Ionides (2025)\n<doi:10.1371/journal.pcbi.1012032>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4683,
    "stars": 0
  },
  {
    "id": 12178,
    "package_name": "easier",
    "title": "Estimate Systems Immune Response from RNA-seq data",
    "description": "This package provides a workflow for the use of EaSIeR\ntool, developed to assess patients' likelihood to respond to\nICB therapies providing just the patients' RNA-seq data as\ninput. We integrate RNA-seq data with different types of prior\nknowledge to extract quantitative descriptors of the tumor\nmicroenvironment from several points of view, including\ncomposition of the immune repertoire, and activity of intra-\nand extra-cellular communications. Then, we use multi-task\nmachine learning trained in TCGA data to identify how these\ndescriptors can simultaneously predict several state-of-the-art\nhallmarks of anti-cancer immune response. In this way we derive\ncancer-specific models and identify cancer-specific systems\nbiomarkers of immune response. These biomarkers have been\nexperimentally validated in the literature and the performance\nof EaSIeR predictions has been validated using independent\ndatasets form four different cancer types with patients treated\nwith anti-PD1 or anti-PDL1 therapy.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4624,
    "stars": 0
  },
  {
    "id": 12952,
    "package_name": "familiar",
    "title": "End-to-End Automated Machine Learning and Model Evaluation",
    "description": "Single unified interface for end-to-end modelling of\nregression, categorical and time-to-event (survival) outcomes.\nModels created using familiar are self-containing, and their\nuse does not require additional information such as baseline\nsurvival, feature clustering, or feature transformation and\nnormalisation parameters. Model performance, calibration, risk\ngroup stratification, (permutation) variable importance,\nindividual conditional expectation, partial dependence, and\nmore, are assessed automatically as part of the evaluation\nprocess and exported in tabular format and plotted, and may\nalso be computed manually using export and plot functions.\nWhere possible, metrics and values obtained during the\nevaluation process come with confidence intervals.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4548,
    "stars": 0
  },
  {
    "id": 1691,
    "package_name": "DPI",
    "title": "The Directed Prediction Index for Causal Direction Inference\nfrom Observational Data",
    "description": "The Directed Prediction Index ('DPI') is a quasi-causal\ninference (causal discovery) method for observational data\ndesigned to quantify the relative endogeneity (relative\ndependence) of outcome (Y) versus predictor (X) variables in\nregression models. By comparing the proportion of variance\nexplained (R-squared) between the Y-as-outcome model and the\nX-as-outcome model while controlling for a sufficient number of\npossible confounders, it can suggest a plausible (admissible)\ndirection of influence from a less endogenous variable (X) to a\nmore endogenous variable (Y). Methodological details are\nprovided at <https://psychbruce.github.io/DPI/>. This package\nalso includes functions for data simulation and network\nanalysis (correlation, partial correlation, and Bayesian\nnetworks).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4472,
    "stars": 0
  },
  {
    "id": 23753,
    "package_name": "ssr",
    "title": "Semi-Supervised Regression Methods",
    "description": "An implementation of semi-supervised regression methods\nincluding self-learning and co-training by committee based on\nHady, M. F. A., Schwenker, F., & Palm, G. (2009)\n<doi:10.1007/978-3-642-04274-4_13>. Users can define which set\nof regressors to use as base models from the 'caret' package,\nother packages, or custom functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4472,
    "stars": 0
  },
  {
    "id": 24992,
    "package_name": "transx",
    "title": "Transform Univariate Time Series",
    "description": "Univariate time series operations that follow an\nopinionated design. The main principle of 'transx' is to keep\nthe number of observations the same. Operations that reduce\nthis number have to fill the observations gap.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.415,
    "stars": 0
  },
  {
    "id": 8576,
    "package_name": "anota2seq",
    "title": "Generally applicable transcriptome-wide analysis of\ntranslational efficiency using anota2seq",
    "description": "anota2seq provides analysis of translational efficiency\nand differential expression analysis for polysome-profiling and\nribosome-profiling studies (two or more sample classes)\nquantified by RNA sequencing or DNA-microarray.\nPolysome-profiling and ribosome-profiling typically generate\ndata for two RNA sources; translated mRNA and total mRNA.\nAnalysis of differential expression is used to estimate changes\nwithin each RNA source (i.e. translated mRNA or total mRNA).\nAnalysis of translational efficiency aims to identify changes\nin translation efficiency leading to altered protein levels\nthat are independent of total mRNA levels (i.e. changes in\ntranslated mRNA that are independent of levels of total mRNA)\nor buffering, a mechanism regulating translational efficiency\nso that protein levels remain constant despite fluctuating\ntotal mRNA levels (i.e. changes in total mRNA that are\nindependent of levels of translated mRNA). anota2seq applies\nanalysis of partial variance and the random variance model to\nfulfill these tasks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4082,
    "stars": 0
  },
  {
    "id": 1616,
    "package_name": "DHARMa.helpers",
    "title": "Helper functions to check models not (yet) directly supported by\nDHARMa",
    "description": "Helper functions to check models not (yet) directly\nsupported by the DHARMa package, such as Bayesian models fitted\nwith brms, or exponential random graph models (ERGMs).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.4014,
    "stars": 0
  },
  {
    "id": 172,
    "package_name": "AWAggregator",
    "title": "Attribute-Weighted Aggregation",
    "description": "This package implements an attribute-weighted aggregation\nalgorithm which leverages peptide-spectrum match (PSM)\nattributes to provide a more accurate estimate of protein\nabundance compared to conventional aggregation methods. This\nalgorithm employs pre-trained random forest models to predict\nthe quantitative inaccuracy of PSMs based on their attributes.\nPSMs are then aggregated to the protein level using a weighted\naverage, taking the predicted inaccuracy into account.\nAdditionally, the package allows users to construct their own\ntraining sets that are more relevant to their specific\nexperimental conditions if desired.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3979,
    "stars": 0
  },
  {
    "id": 9966,
    "package_name": "causens",
    "title": "Perform Causal Sensitivity Analyses Using Various Statistical\nMethods",
    "description": "While data from randomized experiments remain the gold\nstandard for causal inference, estimation of causal estimands\nfrom observational data is possible through various confounding\nadjustment methods. However, the challenge of unmeasured\nconfounding remains a concern in causal inference, where\nfailure to account for unmeasured confounders can lead to\nbiased estimates of causal estimands. Sensitivity analysis\nwithin the framework of causal inference can help adjust for\npossible unmeasured confounding. In `causens`, three main\nmethods are implemented: adjustment via sensitivity functions\n(Brumback, Hernán, Haneuse, and Robins (2004)\n<doi:10.1002/sim.1657> and Li, Shen, Wu, and Li (2011)\n<doi:10.1093/aje/kwr096>), Bayesian parametric modelling and\nMonte Carlo approaches (McCandless, Lawrence C and Gustafson,\nPaul (2017) <doi:10.1002/sim.7298>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3979,
    "stars": 0
  },
  {
    "id": 11412,
    "package_name": "dbnlearn",
    "title": "Dynamic Bayesian Network Structure Learning, Parameter Learning\nand Forecasting",
    "description": "It allows to learn the structure of univariate time\nseries, learning parameters and forecasting. Implements a model\nof Dynamic Bayesian Networks with temporal windows, with\ncollections of linear regressors for Gaussian nodes, based on\nthe introductory texts of Korb and Nicholson (2010)\n<doi:10.1201/b10391> and Nagarajan, Scutari and Lèbre (2013)\n<doi:10.1007/978-1-4614-6446-4>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3927,
    "stars": 0
  },
  {
    "id": 362,
    "package_name": "BAGS",
    "title": "A Bayesian Approach for Geneset Selection",
    "description": "R package providing functions to perform geneset\nsignificance analysis over simple cross-sectional data between\n2 and 5 phenotypes of interest.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3909,
    "stars": 0
  },
  {
    "id": 6288,
    "package_name": "Rdatasets",
    "title": "Access Datasets from the Rdatasets Archive",
    "description": "Download and access datasets from the Rdatasets archive\n(<https://vincentarelbundock.github.io/Rdatasets/>). The\npackage provides functions to search, download, and view\ndocumentation for thousands of datasets from various R\npackages, available in both CSV and Parquet formats for\nefficient access.",
    "version": "0.0.1.1",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://vincentarelbundock.github.io/Rdatasetspkg/,\nhttps://vincentarelbundock.github.io/Rdatasets/",
    "exports": [
      ["rddata"],
      ["rddocs"],
      ["rdindex"],
      ["rdsearch"]
    ],
    "topics": [],
    "score": 4.371,
    "stars": 2
  },
  {
    "id": 1832,
    "package_name": "DepInfeR",
    "title": "Inferring tumor-specific cancer dependencies through integrating\nex-vivo drug response assays and drug-protein profiling",
    "description": "DepInfeR integrates two experimentally accessible input\ndata matrices: the drug sensitivity profiles of cancer cell\nlines or primary tumors ex-vivo (X), and the drug affinities of\na set of proteins (Y), to infer a matrix of molecular protein\ndependencies of the cancers (ß). DepInfeR deconvolutes the\nprotein inhibition effect on the viability phenotype by using\nregularized multivariate linear regression. It assigns a\n“dependence coefficient” to each protein and each sample, and\ntherefore could be used to gain a causal and accurate\nunderstanding of functional consequences of genomic aberrations\nin a heterogeneous disease, as well as to guide the choice of\npharmacological intervention for a specific cancer type,\nsub-type, or an individual patient. For more information,\nplease read out preprint on bioRxiv:\nhttps://doi.org/10.1101/2022.01.11.475864.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3617,
    "stars": 0
  },
  {
    "id": 20039,
    "package_name": "posologyr",
    "title": "Individual Dose Optimization using Population Pharmacokinetics",
    "description": "Personalize drug regimens using individual pharmacokinetic\n(PK) and pharmacokinetic-pharmacodynamic (PK-PD) profiles. By\ncombining therapeutic drug monitoring (TDM) data with a\npopulation model, 'posologyr' offers accurate posterior\nestimates and helps compute optimal individualized dosing\nregimens. The empirical Bayes estimates are computed following\nthe method described by Kang et al. (2012)\n<doi:10.4196/kjpp.2012.16.2.97>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3522,
    "stars": 0
  },
  {
    "id": 19076,
    "package_name": "oscar",
    "title": "Optimal Subset Cardinality Regression (OSCAR) Models Using the\nL0-Pseudonorm",
    "description": "Optimal Subset Cardinality Regression (OSCAR) models offer\nregularized linear regression using the L0-pseudonorm,\nconventionally known as the number of non-zero coefficients.\nThe package estimates an optimal subset of features using the\nL0-penalization via cross-validation, bootstrapping and visual\ndiagnostics. Effective Fortran implementations are offered\nalong the package for finding optima for the DC-decomposition,\nwhich is used for transforming the discrete L0-regularized\noptimization problem into a continuous non-convex optimization\ntask. These optimization modules include DBDC ('Double Bundle\nmethod for nonsmooth DC optimization' as described in Joki et\nal. (2018) <doi:10.1137/16M1115733>) and LMBM ('Limited Memory\nBundle Method for large-scale nonsmooth optimization' as in\nHaarala et al. (2004) <doi:10.1080/10556780410001689225>). The\nOSCAR models are comprehensively exemplified in Halkola et al.\n(2023) <doi:10.1371/journal.pcbi.1010333>). Multiple regression\nmodel families are supported: Cox, logistic, and Gaussian.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3424,
    "stars": 0
  },
  {
    "id": 17531,
    "package_name": "mirTarRnaSeq",
    "title": "mirTarRnaSeq",
    "description": "mirTarRnaSeq R package can be used for interactive mRNA\nmiRNA sequencing statistical analysis. This package utilizes\nexpression or differential expression mRNA and miRNA sequencing\nresults and performs interactive correlation and various GLMs\n(Regular GLM, Multivariate GLM, and Interaction GLMs ) analysis\nbetween mRNA and miRNA expriments. These experiments can be\ntime point experiments, and or condition expriments.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3222,
    "stars": 0
  },
  {
    "id": 24835,
    "package_name": "tmle3mopttx",
    "title": "Targeted Maximum Likelihood Estimation of the Mean under Optimal\nIndividualized Treatment",
    "description": "This package estimates the optimal individualized\ntreatment rule for the categorical treatment using Super\nLearner (sl3). In order to avoid nested cross-validation, it\nuses split-specific estimates of Q and g to estimate the rule\nas described by Coyle et al. In addition, it provides the\nTargeted Maximum Likelihood estimates of the mean performance\nusing CV-TMLE under such estimated rules. This is an adapter\npackage for use with the tmle3 framework and the tlverse\nsoftware ecosystem for Targeted Learning.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.3153,
    "stars": 0
  },
  {
    "id": 1280,
    "package_name": "Clomial",
    "title": "Infers clonal composition of a tumor",
    "description": "Clomial fits binomial distributions to counts obtained\nfrom Next Gen Sequencing data of multiple samples of the same\ntumor. The trained parameters can be interpreted to infer the\nclonal structure of the tumor.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 2663,
    "package_name": "GIGSEA",
    "title": "Genotype Imputed Gene Set Enrichment Analysis",
    "description": "We presented the Genotype-imputed Gene Set Enrichment\nAnalysis (GIGSEA), a novel method that uses\nGWAS-and-eQTL-imputed trait-associated differential gene\nexpression to interrogate gene set enrichment for the\ntrait-associated SNPs. By incorporating eQTL from large gene\nexpression studies, e.g. GTEx, GIGSEA appropriately addresses\nsuch challenges for SNP enrichment as gene size, gene boundary,\nSNP distal regulation, and multiple-marker regulation. The\nweighted linear regression model, taking as weights both\nimputation accuracy and model completeness, was used to perform\nthe enrichment test, properly adjusting the bias due to\nredundancy in different gene sets. The permutation test,\nfurthermore, is used to evaluate the significance of\nenrichment, whose efficiency can be largely elevated by\nexpressing the computational intensive part in terms of large\nmatrix operation. We have shown the appropriate type I error\nrates for GIGSEA (<5%), and the preliminary results also\ndemonstrate its good performance to uncover the real signal.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 3851,
    "package_name": "MAGAR",
    "title": "MAGAR: R-package to compute methylation Quantitative Trait Loci\n(methQTL) from DNA methylation and genotyping data",
    "description": "\"Methylation-Aware Genotype Association in R\" (MAGAR)\ncomputes methQTL from DNA methylation and genotyping data from\nmatched samples. MAGAR uses a linear modeling stragety to call\nCpGs/SNPs that are methQTLs. MAGAR accounts for the local\ncorrelation structure of CpGs.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 4782,
    "package_name": "OPWeight",
    "title": "Optimal p-value weighting with independent information",
    "description": "This package perform weighted-pvalue based multiple\nhypothesis test and provides corresponding information such as\nranking probability, weight, significant tests, etc . To\nconduct this testing procedure, the testing method apply a\nprobabilistic relationship between the test rank and the\ncorresponding test effect size.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 5294,
    "package_name": "PepSetTest",
    "title": "Peptide Set Test",
    "description": "Peptide Set Test (PepSetTest) is a peptide-centric\nstrategy to infer differentially expressed proteins in LC-MS/MS\nproteomics data. This test detects coordinated changes in the\nexpression of peptides originating from the same protein and\ncompares these changes against the rest of the peptidome.\nCompared to traditional aggregation-based approaches, the\npeptide set test demonstrates improved statistical power, yet\ncontrolling the Type I error rate correctly in most cases. This\ntest can be valuable for discovering novel biomarkers and\nprioritizing drug targets, especially when the direct\napplication of statistical analysis to protein data fails to\nprovide substantial insights.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 18854,
    "package_name": "omicRexposome",
    "title": "Exposome and omic data associatin and integration analysis",
    "description": "omicRexposome systematizes the association evaluation\nbetween exposures and omic data, taking advantage of\nMultiDataSet for coordinated data management, rexposome for\nexposome data definition and limma for association testing.\nAlso to perform data integration mixing exposome and omic data\nusing multi co-inherent analysis (omicade4) and multi-canonical\ncorrelation analysis (PMA).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 20009,
    "package_name": "popbayes",
    "title": "Bayesian Model to Estimate Population Trends from Counts Series",
    "description": "Infers the trends of one or several animal populations\nover time from series of counts. It does so by accounting for\ncount precision (provided or inferred based on expert\nknowledge, e.g. guesstimates), smoothing the population rate of\nincrease over time, and accounting for the maximum demographic\npotential of species. Inference is carried out in a Bayesian\nframework. This work is part of the FRB-CESAB working group\nAfroBioDrivers\n<https://www.fondationbiodiversite.fr/en/the-frb-in-action/programs-and-projects/le-cesab/afrobiodrivers/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 20306,
    "package_name": "protGear",
    "title": "Protein Micro Array Data Management and Interactive\nVisualization",
    "description": "A generic three-step pre-processing package for protein\nmicroarray data. This package contains different data\npre-processing procedures to allow comparison of their\nperformance.These steps are background correction, the\ncoefficient of variation (CV) based filtering, batch correction\nand normalization.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 21340,
    "package_name": "remiod",
    "title": "Reference-Based Multiple Imputation for Ordinal/Binary Response",
    "description": "Reference-based multiple imputation of ordinal and binary\nresponses under Bayesian framework, as described in Wang and\nLiu (2022) <arXiv:2203.02771>. Methods for\nmissing-not-at-random include Jump-to-Reference (J2R), Copy\nReference (CR), and Delta Adjustment which can generate tipping\npoint analysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 21911,
    "package_name": "rqt",
    "title": "rqt: utilities for gene-level meta-analysis",
    "description": "Despite the recent advances of modern GWAS methods, it\nstill remains an important problem of addressing calculation an\neffect size and corresponding p-value for the whole gene rather\nthan for single variant. The R- package rqt offers gene-level\nGWAS meta-analysis. For more information, see: \"Gene-set\nassociation tests for next-generation sequencing data\" by Lee\net al (2016), Bioinformatics, 32(17), i611-i619,\n<doi:10.1093/bioinformatics/btw429>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 24915,
    "package_name": "tpSVG",
    "title": "Thin plate models to detect spatially variable genes",
    "description": "The goal of `tpSVG` is to detect and visualize spatial\nvariation in the gene expression for spatially resolved\ntranscriptomics data analysis. Specifically, `tpSVG` introduces\na family of count-based models, with generalizable parametric\nassumptions such as Poisson distribution or negative binomial\ndistribution. In addition, comparing to currently available\ncount-based model for spatially resolved data analysis, the\n`tpSVG` models improves computational time, and hence greatly\nimproves the applicability of count-based models in SRT data\nanalysis.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.301,
    "stars": 0
  },
  {
    "id": 12141,
    "package_name": "dynsurv",
    "title": "Dynamic Models for Survival Data",
    "description": "Time-varying coefficient models for interval censored and\nright censored survival data including 1) Bayesian Cox model\nwith time-independent, time-varying or dynamic coefficients for\nright censored and interval censored data studied by Sinha et\nal. (1999) <doi:10.1111/j.0006-341X.1999.00585.x> and Wang et\nal. (2013) <doi:10.1007/s10985-013-9246-8>, 2) Spline based\ntime-varying coefficient Cox model for right censored data\nproposed by Perperoglou et al. (2006)\n<doi:10.1016/j.cmpb.2005.11.006>, and 3) Transformation model\nwith time-varying coefficients for right censored data using\nestimating equations proposed by Peng and Huang (2007)\n<doi:10.1093/biomet/asm058>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2967,
    "stars": 0
  },
  {
    "id": 12883,
    "package_name": "fRLR",
    "title": "Fit Repeated Linear Regressions",
    "description": "When fitting a set of linear regressions which have some\nsame variables, we can separate the matrix and reduce the\ncomputation cost. This package aims to fit a set of repeated\nlinear regressions faster. More details can be found in this\nblog Lijun Wang (2017)\n<https://stats.hohoweiya.xyz/regression/2017/09/26/An-R-Package-Fit-Repeated-Linear-Regressions/>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.29,
    "stars": 0
  },
  {
    "id": 19831,
    "package_name": "plotBart",
    "title": "Diagnostic and Plotting Functions to Supplement 'bartCause'",
    "description": "Functions to assist in diagnostics and plotting during the\ncausal inference modeling process. Supplements the 'bartCause'\npackage.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2788,
    "stars": 0
  },
  {
    "id": 15157,
    "package_name": "hrqglas",
    "title": "Group Variable Selection for Quantile and Robust Mean Regression",
    "description": "A program that conducts group variable selection for\nquantile and robust mean regression (Sherwood and Li, 2022).\nThe group lasso penalty (Yuan and Lin, 2006) is used for\ngroup-wise variable selection. Both of the quantile and mean\nregression models are based on the Huber loss. Specifically,\nwith the tuning parameter in the Huber loss approaching to 0,\nthe quantile check function can be approximated by the Huber\nloss for the median and the tilted version of Huber loss at\nother quantiles. Such approximation provides computational\nefficiency and stability, and has also been shown to be\nstatistical consistent.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2583,
    "stars": 0
  },
  {
    "id": 16238,
    "package_name": "landmaRk",
    "title": "Time-to-Event Landmark Analysis using an Array of Longitudinal\nand Survival Sub-Models",
    "description": "Provides a comprehensive framework for conducting\ntime-to-event landmarking analysis using a variety of\nlongitudinal and survival sub-models. It aims to facilitate the\nanalysis of time-to-event data with time-dependent covariates.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2553,
    "stars": 0
  },
  {
    "id": 17234,
    "package_name": "metaCCA",
    "title": "Summary Statistics-Based Multivariate Meta-Analysis of\nGenome-Wide Association Studies Using Canonical Correlation\nAnalysis",
    "description": "metaCCA performs multivariate analysis of a single or\nmultiple GWAS based on univariate regression coefficients. It\nallows multivariate representation of both phenotype and\ngenotype. metaCCA extends the statistical technique of\ncanonical correlation analysis to the setting where original\nindividual-level records are not available, and employs a\ncovariance shrinkage algorithm to achieve robustness.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2553,
    "stars": 0
  },
  {
    "id": 17110,
    "package_name": "mcompanion",
    "title": "Objects and Methods for Multi-Companion Matrices",
    "description": "Provides a class for multi-companion matrices with methods\nfor arithmetic and factorization.  A method for generation of\nmulti-companion matrices with prespecified spectral properties\nis provided, as well as some utilities for periodically\ncorrelated and multivariate time series models. See Boshnakov\n(2002) <doi:10.1016/S0024-3795(01)00475-X> and Boshnakov &\nIqelan (2009) <doi:10.1111/j.1467-9892.2009.00617.x>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.233,
    "stars": 0
  },
  {
    "id": 2753,
    "package_name": "GRENITS",
    "title": "Gene Regulatory Network Inference Using Time Series",
    "description": "The package offers four network inference statistical\nmodels using Dynamic Bayesian Networks and Gibbs Variable\nSelection: a linear interaction model, two linear interaction\nmodels with added experimental noise (Gaussian and Student\ndistributed) for the case where replicates are available and a\nnon-linear interaction model.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.2041,
    "stars": 0
  },
  {
    "id": 167,
    "package_name": "ATbounds",
    "title": "Bounding Treatment Effects by Limited Information Pooling",
    "description": "Estimation and inference methods for bounding average\ntreatment effects (on the treated) that are valid under an\nunconfoundedness assumption. The bounds are designed to be\nrobust in challenging situations, for example, when the\nconditioning variables take on a large number of different\nvalues in the observed sample, or when the overlap condition is\nviolated. This robustness is achieved by only using limited\n\"pooling\" of information across observations. For more details,\nsee the paper by Lee and Weidner (2021), \"Bounding Treatment\nEffects by Pooling Limited Information across Observations,\"\n<arXiv:2111.05243>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 2889,
    "package_name": "GenomicOZone",
    "title": "Delineate outstanding genomic zones of differential gene\nactivity",
    "description": "The package clusters gene activity along chromosome into\nzones, detects differential zones as outstanding, and\nvisualizes maps of outstanding zones across the genome. It\nenables characterization of effects on multiple genes within\nadaptive genomic neighborhoods, which could arise from genome\nreorganization, structural variation, or epigenome alteration.\nIt guarantees cluster optimality, linear runtime to sample\nsize, and reproducibility. One can apply it on genome-wide\nactivity measurements such as copy number, transcriptomic,\nproteomic, and methylation data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 3269,
    "package_name": "IFAA",
    "title": "Robust Inference for Absolute Abundance in Microbiome Analysis",
    "description": "This package offers a robust approach to make inference on\nthe association of covariates with the absolute abundance (AA)\nof microbiome in an ecosystem. It can be also directly applied\nto relative abundance (RA) data to make inference on AA because\nthe ratio of two RA is equal to the ratio of their AA. This\nalgorithm can estimate and test the associations of interest\nwhile adjusting for potential confounders. The estimates of\nthis method have easy interpretation like a typical regression\nanalysis. High-dimensional covariates are handled with\nregularization and it is implemented by parallel computing.\nFalse discovery rate is automatically controlled by this\napproach. Zeros do not need to be imputed by a positive value\nfor the analysis. The IFAA package also offers the 'MZILN'\nfunction for estimating and testing associations of abundance\nratios with covariates.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 4160,
    "package_name": "MSTest",
    "title": "Hypothesis Testing for Markov Switching Models",
    "description": "Implementation of hypothesis testing procedures described\nin Hansen (1992) <doi:10.1002/jae.3950070506>, Carrasco, Hu, &\nPloberger (2014) <doi:10.3982/ECTA8609>, Dufour & Luger (2017)\n<doi:10.1080/07474938.2017.1307548>, and Rodriguez Rondon &\nDufour (2024)\n<https://grodriguezrondon.com/files/RodriguezRondon_Dufour_2025_MonteCarlo_LikelihoodRatioTest_MarkovSwitchingModels_20251014.pdf>\nthat can be used to identify the number of regimes in Markov\nswitching models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 6839,
    "package_name": "SOMNiBUS",
    "title": "Smooth modeling of bisulfite sequencing",
    "description": "This package aims to analyse count-based methylation data\non predefined genomic regions, such as those obtained by\ntargeted sequencing, and thus to identify differentially\nmethylated regions (DMRs) that are associated with phenotypes\nor traits. The method is built a rich flexible model that\nallows for the effects, on the methylation levels, of multiple\ncovariates to vary smoothly along genomic regions. At the same\ntime, this method also allows for sequencing errors and can\nadjust for variability in cell type mixture.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 8538,
    "package_name": "ananke",
    "title": "Quantitative Chronology in Archaeology",
    "description": "Simple radiocarbon calibration and chronological analysis.\nThis package allows the calibration of radiocarbon ages and\nmodern carbon fraction values using multiple calibration\ncurves. It allows the calculation of highest density region\nintervals and credible intervals. The package also provides\ntools for visualising results and estimating statistical\nsummaries.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 14590,
    "package_name": "gpls",
    "title": "Classification using generalized partial least squares",
    "description": "Classification using generalized partial least squares for\ntwo-group and multi-group (more than 2 group) classification.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 19436,
    "package_name": "pcts",
    "title": "Periodically Correlated and Periodically Integrated Time Series",
    "description": "Classes and methods for modelling and simulation of\nperiodically correlated (PC) and periodically integrated time\nseries.  Compute theoretical periodic autocovariances and\nrelated properties of PC autoregressive moving average models.\nSome original methods including Boshnakov & Iqelan (2009)\n<doi:10.1111/j.1467-9892.2009.00617.x>, Boshnakov (1996)\n<doi:10.1111/j.1467-9892.1996.tb00281.x>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 19509,
    "package_name": "pengls",
    "title": "Fit Penalised Generalised Least Squares models",
    "description": "Combine generalised least squares methodology from the\nnlme package for dealing with autocorrelation with penalised\nleast squares methods from the glmnet package to deal with high\ndimensionality. This pengls packages glues them together\nthrough an iterative loop. The resulting method is applicable\nto high dimensional datasets that exhibit autocorrelation, such\nas spatial or temporal data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 21619,
    "package_name": "rjd3workspace",
    "title": "Interface to 'JDemetra+ 3.x' time series analysis software",
    "description": "R Interface to 'JDemetra+ 3.x'\n(<https://github.com/jdemetra>).  It offers several functions\nto manipulate 'JDemetra+' workspaces, which can be read by the\nsoftware and can store several seasonal adjusted series along\nwith user-defined calendars or regression variables.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1761,
    "stars": 0
  },
  {
    "id": 23340,
    "package_name": "softbib",
    "title": "Software Bibliographies for R Projects",
    "description": "Detect libraries used in a project and automatically\ncreate software bibliographies in 'PDF', 'Word', 'Rmarkdown',\nand 'BibTeX' formats.",
    "version": "0.0.2",
    "maintainer": "Vincent Arel-Bundock <vincent.arel-bundock@umontreal.ca>",
    "url": "https://github.com/vincentarelbundock/softbib",
    "exports": [
      ["softbib"]
    ],
    "topics": [],
    "score": 4.1761,
    "stars": 30
  },
  {
    "id": 531,
    "package_name": "BUMHMM",
    "title": "Computational pipeline for computing probability of modification\nfrom structure probing experiment data",
    "description": "This is a probabilistic modelling pipeline for computing\nper- nucleotide posterior probabilities of modification from\nthe data collected in structure probing experiments. The model\nsupports multiple experimental replicates and empirically\ncorrects coverage- and sequence-dependent biases. The model\nutilises the measure of a \"drop-off rate\" for each nucleotide,\nwhich is compared between replicates through a log-ratio (LDR).\nThe LDRs between control replicates define a null distribution\nof variability in drop-off rate observed by chance and LDRs\nbetween treatment and control replicates gets compared to this\ndistribution. Resulting empirical p-values (probability of\nbeing \"drawn\" from the null distribution) are used as\nobservations in a Hidden Markov Model with a Beta-Uniform\nMixture model used as an emission model. The resulting\nposterior probabilities indicate the probability of a\nnucleotide of having being modified in a structure probing\nexperiment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1461,
    "stars": 0
  },
  {
    "id": 19903,
    "package_name": "pmm",
    "title": "Parallel Mixed Model",
    "description": "The Parallel Mixed Model (PMM) approach is suitable for\nhit selection and cross-comparison of RNAi screens generated in\nexperiments that are performed in parallel under several\nconditions. For example, we could think of the measurements or\nreadouts from cells under RNAi knock-down, which are infected\nwith several pathogens or which are grown from different cell\nlines.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1461,
    "stars": 0
  },
  {
    "id": 9050,
    "package_name": "bayesGARCH",
    "title": "Bayesian Estimation of the GARCH(1,1) Model with Student-t\nInnovations",
    "description": "Provides the bayesGARCH() function which performs the\nBayesian estimation of the GARCH(1,1) model with Student's t\ninnovations as described in Ardia (2008)\n<doi:10.1007/978-3-540-78657-3>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1335,
    "stars": 0
  },
  {
    "id": 25202,
    "package_name": "tvvarss",
    "title": "Time Varying Vector Autoregressive State Space Models",
    "description": "The tvvarss package uses Stan (mc-stan.org) to fit\nmulti-site multivariate autoregressive (aka vector\nautoregressive) state space models with a time varying\ninteraction matrix.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.1206,
    "stars": 0
  },
  {
    "id": 926,
    "package_name": "CFAssay",
    "title": "Statistical analysis for the Colony Formation Assay",
    "description": "The package provides functions for calculation of\nlinear-quadratic cell survival curves and for ANOVA of\nexperimental 2-way designs along with the colony formation\nassay.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0792,
    "stars": 0
  },
  {
    "id": 8981,
    "package_name": "banocc",
    "title": "Bayesian ANalysis Of Compositional Covariance",
    "description": "BAnOCC is a package designed for compositional data, where\neach sample sums to one. It infers the approximate covariance\nof the unconstrained data using a Bayesian model coded with\n`rstan`. It provides as output the `stanfit` object as well as\nposterior median and credible interval estimates for each\ncorrelation element.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0792,
    "stars": 0
  },
  {
    "id": 17025,
    "package_name": "matrixprofiler",
    "title": "Matrix Profile for R",
    "description": "This is the core functions needed by the 'tsmp' package.\nThe low level and carefully checked mathematical functions are\nhere. These are implementations of the Matrix Profile concept\nthat was created by CS-UCR\n<http://www.cs.ucr.edu/~eamonn/MatrixProfile.html>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0792,
    "stars": 0
  },
  {
    "id": 11364,
    "package_name": "dataseries",
    "title": "Switzerland's Data Series in One Place",
    "description": "Download and import time series from\n<http://www.dataseries.org>, a comprehensive and up-to-date\ncollection of open data from Switzerland.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0682,
    "stars": 0
  },
  {
    "id": 13516,
    "package_name": "frailtySurv",
    "title": "General Semiparametric Shared Frailty Model",
    "description": "Simulates and fits semiparametric shared frailty models\nunder a wide range of frailty distributions using a consistent\nand asymptotically-normal estimator. Currently supports: gamma,\npower variance function, log-normal, and inverse Gaussian\nfrailty models.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0569,
    "stars": 0
  },
  {
    "id": 25999,
    "package_name": "xcore",
    "title": "xcore expression regulators inference",
    "description": "xcore is an R package for transcription factor activity\nmodeling based on known molecular signatures and user's gene\nexpression data. Accompanying xcoredata package provides a\ncollection of molecular signatures, constructed from publicly\navailable ChiP-seq experiments. xcore use ridge regression to\nmodel changes in expression as a linear combination of\nmolecular signatures and find their unknown activities.\nObtained, estimates can be further tested for significance to\nselect molecular signatures with the highest predicted effect\non the observed expression changes.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0414,
    "stars": 0
  },
  {
    "id": 3289,
    "package_name": "IMAS",
    "title": "Integrative analysis of Multi-omics data for Alternative\nSplicing",
    "description": "Integrative analysis of Multi-omics data for Alternative\nsplicing.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0212,
    "stars": 0
  },
  {
    "id": 23626,
    "package_name": "splineTimeR",
    "title": "Time-course differential gene expression data analysis using\nspline regression models followed by gene association network\nreconstruction",
    "description": "This package provides functions for differential gene\nexpression analysis of gene expression time-course data.\nNatural cubic spline regression models are used. Identified\ngenes may further be used for pathway enrichment analysis\nand/or the reconstruction of time dependent gene regulatory\nassociation networks.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4.0086,
    "stars": 0
  },
  {
    "id": 213,
    "package_name": "AffiXcan",
    "title": "A Functional Approach To Impute Genetically Regulated Expression",
    "description": "Impute a GReX (Genetically Regulated Expression) for a set\nof genes in a sample of individuals, using a method based on\nthe Total Binding Affinity (TBA). Statistical models to impute\nGReX can be trained with a training dataset where the real\ntotal expression values are known.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 425,
    "package_name": "BG2",
    "title": "Performs Bayesian GWAS analysis for non-Gaussian data using BG2",
    "description": "This package is built to perform GWAS analysis for\nnon-Gaussian data using BG2. The BG2 method uses penalized\nquasi-likelihood along with nonlocal priors in a two step\nmanner to identify SNPs in GWAS analysis. The research related\nto this package was supported in part by National Science\nFoundation awards DMS 1853549 and DMS 2054173.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 533,
    "package_name": "BUScorrect",
    "title": "Batch Effects Correction with Unknown Subtypes",
    "description": "High-throughput experimental data are accumulating\nexponentially in public databases. However, mining valid\nscientific discoveries from these abundant resources is\nhampered by technical artifacts and inherent biological\nheterogeneity. The former are usually termed \"batch effects,\"\nand the latter is often modelled by \"subtypes.\" The R package\nBUScorrect fits a Bayesian hierarchical model, the\nBatch-effects-correction-with-Unknown-Subtypes model (BUS), to\ncorrect batch effects in the presence of unknown subtypes. BUS\nis capable of (a) correcting batch effects explicitly, (b)\ngrouping samples that share similar characteristics into\nsubtypes, (c) identifying features that distinguish subtypes,\nand (d) enjoying a linear-order computation complexity.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 544,
    "package_name": "BaalChIP",
    "title": "BaalChIP: Bayesian analysis of allele-specific transcription\nfactor binding in cancer genomes",
    "description": "The package offers functions to process multiple ChIP-seq\nBAM files and detect allele-specific events. Computes allele\ncounts at individual variants (SNPs/SNVs), implements extensive\nQC steps to remove problematic variants, and utilizes a\nbayesian framework to identify statistically significant\nallele- specific events. BaalChIP is able to account for copy\nnumber differences between the two alleles, a known\nphenotypical feature of cancer samples.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 1499,
    "package_name": "CytoDx",
    "title": "Robust prediction of clinical outcomes using cytometry data\nwithout cell gating",
    "description": "This package provides functions that predict clinical\noutcomes using single cell data (such as flow cytometry data,\nRNA single cell sequencing data) without the requirement of\ncell gating or clustering.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 1810,
    "package_name": "DeductiveR",
    "title": "Deductive Rational Method",
    "description": "Apply the Deductive Rational Method to a monthly series of\nflow or precipitation data to fill in missing data. The method\nis as described in: Campos, D.F., (1984, ISBN:9686194444).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 1928,
    "package_name": "Doscheda",
    "title": "A DownStream Chemo-Proteomics Analysis Pipeline",
    "description": "Doscheda focuses on quantitative chemoproteomics used to\ndetermine protein interaction profiles of small molecules from\nwhole cell or tissue lysates using Mass Spectrometry data. The\npackage provides a shiny application to run the pipeline,\nseveral visualisations and a downloadable report of an\nexperiment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 1979,
    "package_name": "EBcoexpress",
    "title": "EBcoexpress for Differential Co-Expression Analysis",
    "description": "An Empirical Bayesian Approach to Differential\nCo-Expression Analysis at the Gene-Pair Level",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 2579,
    "package_name": "GAprediction",
    "title": "Prediction of gestational age with Illumina HumanMethylation450\ndata",
    "description": "[GAprediction] predicts gestational age using Illumina\nHumanMethylation450 CpG data.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 2698,
    "package_name": "GMRP",
    "title": "GWAS-based Mendelian Randomization and Path Analyses",
    "description": "Perform Mendelian randomization analysis of multiple SNPs\nto determine risk factors causing disease of study and to\nexclude confounding variabels and perform path analysis to\nconstruct path of risk factors to the disease.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 2944,
    "package_name": "GmicR",
    "title": "Combines WGCNA and xCell readouts with bayesian network\nlearrning to generate a Gene-Module Immune-Cell network (GMIC)",
    "description": "This package uses bayesian network learning to detect\nrelationships between Gene Modules detected by WGCNA and immune\ncell signatures defined by xCell. It is a hypothesis generating\ntool.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 3794,
    "package_name": "LinkHD",
    "title": "LinkHD: a versatile framework to explore and integrate\nheterogeneous data",
    "description": "Here we present Link-HD, an approach to integrate\nheterogeneous datasets, as a generalization of STATIS-ACT\n(“Structuration des Tableaux A Trois Indices de la\nStatistique–Analyse Conjointe de Tableaux”), a family of\nmethods to join and compare information from multiple\nsubspaces. However, STATIS-ACT has some drawbacks since it only\nallows continuous data and it is unable to establish\nrelationships between samples and features. In order to tackle\nthese constraints, we incorporate multiple distance options and\na linear regression based Biplot model in order to stablish\nrelationships between observations and variable and perform\nvariable selection.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 5894,
    "package_name": "RNAdecay",
    "title": "Maximum Likelihood Decay Modeling of RNA Degradation Data",
    "description": "RNA degradation is monitored through measurement of RNA\nabundance after inhibiting RNA synthesis. This package has\nfunctions and example scripts to facilitate (1) data\nnormalization, (2) data modeling using constant decay rate or\ntime-dependent decay rate models, (3) the evaluation of\ntreatment or genotype effects, and (4) plotting of the data and\nmodels. Data Normalization: functions and scripts make easy the\nnormalization to the initial (T0) RNA abundance, as well as a\nmethod to correct for artificial inflation of Reads per Million\n(RPM) abundance in global assessments as the total size of the\nRNA pool decreases. Modeling: Normalized data is then modeled\nusing maximum likelihood to fit parameters. For making\ntreatment or genotype comparisons (up to four), the modeling\nstep models all possible treatment effects on each gene by\nrepeating the modeling with constraints on the model parameters\n(i.e., the decay rate of treatments A and B are modeled once\nwith them being equal and again allowing them to both vary\nindependently). Model Selection: The AICc value is calculated\nfor each model, and the model with the lowest AICc is chosen.\nModeling results of selected models are then compiled into a\nsingle data frame. Graphical Plotting: functions are provided\nto easily visualize decay data model, or half-life\ndistributions using ggplot2 package functions.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 9993,
    "package_name": "ccfindR",
    "title": "Cancer Clone Finder",
    "description": "A collection of tools for cancer genomic data clustering\nanalyses, including those for single cell RNA-seq. Cell\nclustering and feature gene selection analysis employ Bayesian\n(and maximum likelihood) non-negative matrix factorization\n(NMF) algorithm. Input data set consists of RNA count matrix,\ngene, and cell bar code annotations.  Analysis outputs are\nfactor matrices for multiple ranks and marginal likelihood\nvalues for each rank. The package includes utilities for\ndownstream analyses, including meta-gene identification,\nvisualization, and construction of rank-based trees for\nclusters.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 11150,
    "package_name": "ctsGE",
    "title": "Clustering of Time Series Gene Expression data",
    "description": "Methodology for supervised clustering of potentially many\npredictor variables, such as genes etc., in time series\ndatasets Provides functions that help the user assigning genes\nto predefined set of model profiles.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 13194,
    "package_name": "findIPs",
    "title": "Influential Points Detection for Feature Rankings",
    "description": "Feature rankings can be distorted by a single case in the\ncontext of high-dimensional data. The cases exerts abnormal\ninfluence on feature rankings are called influential points\n(IPs). The package aims at detecting IPs based on case deletion\nand quantifies their effects by measuring the rank changes\n(DOI:10.48550/arXiv.2303.10516). The package applies a novel\nrank comparing measure using the adaptive weights that stress\nthe top-ranked important features and adjust the weights to\nranking properties.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 15047,
    "package_name": "hierinf",
    "title": "Hierarchical Inference",
    "description": "Tools to perform hierarchical inference for one or\nmultiple studies / data sets based on high-dimensional\nmultivariate (generalised) linear models. A possible\napplication is to perform hierarchical inference for GWA\nstudies to find significant groups or single SNPs (if the\nsignal is strong) in a data-driven and automated procedure. The\nmethod is based on an efficient hierarchical multiple testing\ncorrection and controls the FWER. The functions can easily be\nrun in parallel.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 15206,
    "package_name": "hummingbird",
    "title": "Bayesian Hidden Markov Model for the detection of differentially\nmethylated regions",
    "description": "A package for detecting differential methylation. It\nexploits a Bayesian hidden Markov model that incorporates\nlocation dependence among genomic loci, unlike most existing\nmethods that assume independence among observations. Bayesian\npriors are applied to permit information sharing across an\nentire chromosome for improved power of detection. The direct\noutput of our software package is the best sequence of\nmethylation states, eliminating the use of a subjective, and\nmost of the time an arbitrary, threshold of p-value for\ndetermining significance. At last, our methodology does not\nrequire replication in either or both of the two comparison\ngroups.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 18856,
    "package_name": "omicplotR",
    "title": "Visual Exploration of Omic Datasets Using a Shiny App",
    "description": "A Shiny app for visual exploration of omic datasets as\ncompositions, and differential abundance analysis using ALDEx2.\nUseful for exploring RNA-seq, meta-RNA-seq, 16s rRNA gene\nsequencing with visualizations such as principal component\nanalysis biplots (coloured using metadata for visualizing each\nvariable), dendrograms and stacked bar plots, and effect plots\n(ALDEx2). Input is a table of counts and metadata file (if\nmetadata exists), with options to filter data by count or by\nmetadata to remove low counts, or to visualize select samples\naccording to selected metadata.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 19202,
    "package_name": "pairkat",
    "title": "PaIRKAT",
    "description": "PaIRKAT is model framework for assessing statistical\nrelationships between networks of metabolites (pathways) and an\noutcome of interest (phenotype). PaIRKAT queries the KEGG\ndatabase to determine interactions between metabolites from\nwhich network connectivity is constructed. This model framework\nimproves testing power on high dimensional data by including\ngraph topography in the kernel machine regression setting.\nStudies on high dimensional data can struggle to include the\ncomplex relationships between variables. The semi-parametric\nkernel machine regression model is a powerful tool for\ncapturing these types of relationships. They provide a\nframework for testing for relationships between outcomes of\ninterest and high dimensional data such as metabolomic,\ngenomic, or proteomic pathways. PaIRKAT uses known biological\nconnections between high dimensional variables by representing\nthem as edges of ‘graphs’ or ‘networks.’ It is common for nodes\n(e.g. metabolites) to be disconnected from all others within\nthe graph, which leads to meaningful decreases in testing power\nwhether or not the graph information is included. We include a\ngraph regularization or ‘smoothing’ approach for managing this\nissue.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 19795,
    "package_name": "plasmut",
    "title": "Stratifying mutations observed in cell-free DNA and white blood\ncells as germline, hematopoietic, or somatic",
    "description": "A Bayesian method for quantifying the liklihood that a\ngiven plasma mutation arises from clonal hematopoesis or the\nunderlying tumor. It requires sequencing data of the mutation\nin plasma and white blood cells with the number of distinct and\nmutant reads in both tissues. We implement a Monte Carlo\nimportance sampling method to assess the likelihood that a\nmutation arises from the tumor relative to non-tumor origin.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 21512,
    "package_name": "rgnoisefilt",
    "title": "Elimination of Noisy Samples in Regression Datasets using Noise\nFilters",
    "description": "Traditional noise filtering methods aim at removing noisy\nsamples from a classification dataset. This package adapts\nclassic and recent filtering techniques for use in regression\nproblems, and it also incorporates methods specifically\ndesigned for regression data. In order to do this, it uses\napproaches proposed in the specialized literature, such as\nMartin et al. (2021) [<doi:10.1109/ACCESS.2021.3123151>] and\nArnaiz-Gonzalez et al. (2016)\n[<doi:10.1016/j.eswa.2015.12.046>]. Thus, the goal of the\nimplemented noise filters is to eliminate samples with noise in\nregression datasets.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 21977,
    "package_name": "rsmatch",
    "title": "Matching Methods for Time-Varying Observational Studies",
    "description": "Implements popular methods for matching in time-varying\nobservational studies. Matching is difficult in this scenario\nbecause participants can be treated at different times which\nmay have an influence on the outcomes. The core methods\ninclude: \"Balanced Risk Set Matching\" from Li, Propert, and\nRosenbaum (2011) <doi:10.1198/016214501753208573> and\n\"Propensity Score Matching with Time-Dependent Covariates\" from\nLu (2005) <doi:10.1111/j.1541-0420.2005.00356.x>. Some\nfunctions use the 'Gurobi' optimization back-end to improve the\noptimization problem speed; the 'gurobi' R package and\nassociated software can be downloaded from\n<https://www.gurobi.com> after obtaining a license.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 22895,
    "package_name": "shinybrms",
    "title": "Graphical User Interface ('shiny' App) for 'brms'",
    "description": "A graphical user interface (GUI) for fitting Bayesian\nregression models using the package 'brms' which in turn relies\non 'Stan' (<https://mc-stan.org/>). The 'shinybrms' GUI is a\n'shiny' app.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 23609,
    "package_name": "spillR",
    "title": "Spillover Compensation in Mass Cytometry Data",
    "description": "Channel interference in mass cytometry can cause spillover\nand may result in miscounting of protein markers. We develop a\nnonparametric finite mixture model and use the mixture\ncomponents to estimate the probability of spillover. We\nimplement our method using expectation-maximization to fit the\nmixture model.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 4,
    "stars": 0
  },
  {
    "id": 15359,
    "package_name": "ibis.iSDM",
    "title": "Modelling framework for integrated biodiversity distribution\nscenarios",
    "description": "Integrated framework of modelling the distribution of\nspecies and ecosystems in a suitability framing. This package\nallows the estimation of integrated species distribution models\n(iSDM) based on several sources of evidence and provided\npresence-only and presence-absence datasets. It makes heavy use\nof point-process models for estimating habitat suitability and\nallows to include spatial latent effects and priors in the\nestimation. To do so 'ibis.iSDM' supports a number of engines\nfor Bayesian and more non-parametric machine learning\nestimation. Further, the 'ibis.iSDM' is specifically customized\nto support spatial-temporal projections of habitat suitability\ninto the future.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9542,
    "stars": 0
  },
  {
    "id": 3342,
    "package_name": "IRon",
    "title": "Solving Imbalanced Regression Tasks",
    "description": "Imbalanced domain learning has almost exclusively focused\non solving classification tasks, where the objective is to\npredict cases labelled with a rare class accurately. Such a\nwell-defined approach for regression tasks lacked due to two\nmain factors. First, standard regression tasks assume that each\nvalue is equally important to the user. Second, standard\nevaluation metrics focus on assessing the performance of the\nmodel on the most common cases. This package contains methods\nto tackle imbalanced domain learning problems in regression\ntasks, where the objective is to predict extreme (rare) values.\nThe methods contained in this package are: 1) an automatic and\nnon-parametric method to obtain such relevance functions; 2)\nvisualisation tools; 3) suite of evaluation measures for\noptimisation/validation processes; 4) the squared-error\nrelevance area measure, an evaluation metric tailored for\nimbalanced regression tasks. More information can be found in\nRibeiro and Moniz (2020) <doi:10.1007/s10994-020-05900-9>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9509,
    "stars": 0
  },
  {
    "id": 3070,
    "package_name": "HMMpa",
    "title": "Analysing Accelerometer Data Using Hidden Markov Models",
    "description": "Analysing time-series accelerometer data to quantify\nlength and intensity of physical activity using hidden Markov\nmodels. It also contains the traditional cut-off point method.\nWitowski V, Foraita R, Pitsiladis Y, Pigeot I, Wirsik N (2014).\n<doi:10.1371/journal.pone.0114089>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9243,
    "stars": 0
  },
  {
    "id": 1469,
    "package_name": "CptNonPar",
    "title": "Nonparametric Change Point Detection for Multivariate Time\nSeries",
    "description": "Implements the nonparametric moving sum procedure for\ndetecting changes in the joint characteristic function\n(NP-MOJO) for multiple change point detection in multivariate\ntime series. See McGonigle, E. T., Cho, H. (2025)\n<doi:10.1093/biomet/asaf024> for description of the NP-MOJO\nmethodology.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9031,
    "stars": 0
  },
  {
    "id": 8989,
    "package_name": "bark",
    "title": "Bayesian Additive Regression Kernels",
    "description": "Bayesian Additive Regression Kernels (BARK) provides an\nimplementation for non-parametric function estimation using\nLevy Random Field priors for functions that may be represented\nas a sum of additive multivariate kernels.  Kernels are located\nat every data point as in Support Vector Machines, however,\ncoefficients may be heavily shrunk to zero under the Cauchy\nprocess prior, or even, set to zero.  The number of active\nfeatures is controlled by priors on precision parameters within\nthe kernels, permitting feature selection. For more details see\nOuyang, Z (2008) \"Bayesian Additive Regression Kernels\", Duke\nUniversity. PhD dissertation, Chapter 3 and Wolpert, R. L,\nClyde, M.A, and Tu, C. (2011) \"Stochastic Expansions with\nContinuous Dictionaries Levy Adaptive Regression Kernels,\nAnnals of Statistics Vol (39) pages 1916-1962\n<doi:10.1214/11-AOS889>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.9031,
    "stars": 0
  },
  {
    "id": 11744,
    "package_name": "dina",
    "title": "Bayesian Estimation of DINA Model",
    "description": "Estimate the Deterministic Input, Noisy \"And\" Gate (DINA)\ncognitive diagnostic model parameters using the Gibbs sampler\ndescribed by Culpepper (2015) <doi:10.3102/1076998615595403>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8751,
    "stars": 0
  },
  {
    "id": 16973,
    "package_name": "marssTMB",
    "title": "Fast fitting of MARSS models with TMB",
    "description": "Companion to the MARSS package. Fast fitting of MARSS\nmodels with TMB. See the MARSS documentation. All the model\nsyntax and features are the same as for the MARSS package.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8751,
    "stars": 0
  },
  {
    "id": 15167,
    "package_name": "hsstan",
    "title": "Hierarchical Shrinkage Stan Models for Biomarker Selection",
    "description": "Linear and logistic regression models penalized with\nhierarchical shrinkage priors for selection of biomarkers (or\nmore general variable selection), which can be fitted using\nStan (Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>). It\nimplements the horseshoe and regularized horseshoe priors\n(Piironen and Vehtari (2017) <doi:10.1214/17-EJS1337SI>), as\nwell as the projection predictive selection approach to recover\na sparse set of predictive biomarkers (Piironen, Paasiniemi and\nVehtari (2020) <doi:10.1214/20-EJS1711>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8663,
    "stars": 0
  },
  {
    "id": 17357,
    "package_name": "mfa",
    "title": "Bayesian hierarchical mixture of factor analyzers for modelling\ngenomic bifurcations",
    "description": "MFA models genomic bifurcations using a Bayesian\nhierarchical mixture of factor analysers.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.8451,
    "stars": 0
  },
  {
    "id": 375,
    "package_name": "BASiCStan",
    "title": "Stan implementation of BASiCS",
    "description": "Provides an interface to infer the parameters of BASiCS\nusing the variational inference (ADVI), Markov chain Monte\nCarlo (NUTS), and maximum a posteriori (BFGS) inference engines\nin the Stan programming language. BASiCS is a Bayesian\nhierarchical model that uses an adaptive Metropolis within\nGibbs sampling scheme. Alternative inference methods provided\nby Stan may be preferable in some situations, for example for\nparticularly large data or posterior distributions with\ndifficult geometries.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 7904,
    "package_name": "VPdtw",
    "title": "Variable Penalty Dynamic Time Warping",
    "description": "Variable Penalty Dynamic Time Warping (VPdtw) for aligning\nchromatographic signals. With an appropriate penalty this\nmethod performs good alignment of chromatographic data without\ndeforming the peaks (Clifford, D., Stone, G., Montoliu, I.,\nRezzi S., Martin F., Guy P., Bruce S., and Kochhar S.(2009)\n<doi:10.1021/ac802041e>; Clifford, D. and Stone, G. (2012)\n<doi:10.18637/jss.v047.i08>).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 23277,
    "package_name": "smovie",
    "title": "Some Movies to Illustrate Concepts in Statistics",
    "description": "Provides movies to help students to understand statistical\nconcepts.  The 'rpanel' package\n<https://cran.r-project.org/package=rpanel> is used to create\ninteractive plots that move to illustrate key statistical ideas\nand methods.  There are movies to: visualise probability\ndistributions (including user-supplied ones); illustrate\nsampling distributions of the sample mean (central limit\ntheorem), the median, the sample maximum (extremal types\ntheorem) and (the Fisher transformation of the) product moment\ncorrelation coefficient; examine the influence of an individual\nobservation in simple linear regression; illustrate key\nconcepts in statistical hypothesis testing. Also provided are\ndpqr functions for the distribution of the Fisher\ntransformation of the correlation coefficient under sampling\nfrom a bivariate normal distribution.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7782,
    "stars": 0
  },
  {
    "id": 17182,
    "package_name": "medshift",
    "title": "Causal mediation analysis for stochastic interventions",
    "description": "Estimators of a parameter arising in the decomposition of\nthe population intervention (in)direct effect of stochastic\ninterventions in causal mediation analysis, including efficient\none-step, targeted minimum loss (TML), re-weighting (IPW), and\nsubstitution estimators. The parameter estimated constitutes a\npart of each of the population intervention (in)direct effects.\nThese estimators may be used in assessing population\nintervention (in)direct effects under stochastic treatment\nregimes, including incremental propensity score interventions\nand modified treatment policies. The methodology was first\ndiscussed by I Díaz and NS Hejazi (2020)\n<doi:10.1111/rssb.12362>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.7404,
    "stars": 0
  },
  {
    "id": 3644,
    "package_name": "LFDREmpiricalBayes",
    "title": "Estimating Local False Discovery Rates Using Empirical Bayes\nMethods",
    "description": "New empirical Bayes methods aiming at analyzing the\nassociation of single nucleotide polymorphisms (SNPs) to some\nparticular disease are implemented in this package. The package\nuses local false discovery rate (LFDR) estimates of SNPs within\na sample population defined as a \"reference class\" and\ndiscovers if SNPs are associated with the corresponding\ndisease. Although SNPs are used throughout this document, other\nbiological data such as protein data and other gene data can be\nused. Karimnezhad, Ali and Bickel, D. R. (2016)\n<http://hdl.handle.net/10393/34889>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 3937,
    "package_name": "MCTrend",
    "title": "Monte Carlo Trend Analysis",
    "description": "Application of a test to rule out that trends detected in\nhydrological time series are explained exclusively by the\nrandomness of the climate. Based on: Ricchetti, (2018)\n<https://repositorio.uchile.cl/handle/2250/168487>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 16886,
    "package_name": "manymome.table",
    "title": "Publication-Ready Tables for 'manymome' Results",
    "description": "Converts results from the 'manymome' package, presented in\nCheung and Cheung (2023) <doi:10.3758/s13428-023-02224-z>, to\npublication-ready tables.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 17238,
    "package_name": "metaEnsembleR",
    "title": "Intuitive Package for Meta-Ensemble Learning (Classification,\nRegression) that is Fully-Automated",
    "description": "This package significantly lowers the barrier for the\npractitioners to apply heterogeneous ensemble learning\ntechniques in an amateur fashion to their everyday predictive\nproblems.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.699,
    "stars": 0
  },
  {
    "id": 13744,
    "package_name": "gamlssx",
    "title": "Generalized Additive Extreme Value Models for Location, Scale\nand Shape",
    "description": "Fits generalized additive models for the location, scale\nand shape parameters of a generalized extreme value response\ndistribution. The methodology is based on Rigby, R.A. and\nStasinopoulos, D.M. (2005),\n<doi:10.1111/j.1467-9876.2005.00510.x> and implemented using\nfunctions from the 'gamlss' package\n<doi:10.32614/CRAN.package.gamlss>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6532,
    "stars": 0
  },
  {
    "id": 15558,
    "package_name": "independenceWeights",
    "title": "Estimates Weights for Confounding Control for Continuous-Valued\nExposures",
    "description": "Estimates weights to make a continuous-valued exposure\nstatistically independent of a vector of pre-treatment\ncovariates using the method proposed in Huling, Greifer, and\nChen (2021) <arXiv:2107.07086>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6532,
    "stars": 0
  },
  {
    "id": 25111,
    "package_name": "tsdb",
    "title": "Terribly-Simple Data Base for Time Series",
    "description": "A terribly-simple data base for numeric time series,\nwritten purely in R, so no external database-software is\nneeded. Series are stored in plain-text files (the\nmost-portable and enduring file type) in CSV format. Timestamps\nare encoded using R's native numeric representation for\n'Date'/'POSIXct', which makes them fast to parse, but keeps\nthem accessible with other software. The package provides tools\nfor saving and updating series in this standardised format, for\nretrieving and joining data, for summarising files and\ndirectories, and for coercing series from and to other data\ntypes (such as 'zoo' series).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6232,
    "stars": 0
  },
  {
    "id": 1654,
    "package_name": "DMCFB",
    "title": "Differentially Methylated Cytosines via a Bayesian Functional\nApproach",
    "description": "DMCFB is a pipeline for identifying differentially\nmethylated cytosines using a Bayesian functional regression\nmodel in bisulfite sequencing data. By using a functional\nregression data model, it tries to capture position-specific,\ngroup-specific and other covariates-specific methylation\npatterns as well as spatial correlation patterns and unknown\nunderlying models of methylation data. It is robust and\nflexible with respect to the true underlying models and\ninclusion of any covariates, and the missing values are imputed\nusing spatial correlation between positions and samples. A\nBayesian approach is adopted for estimation and inference in\nthe proposed method.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 2803,
    "package_name": "GWAS.BAYES",
    "title": "Bayesian analysis of Gaussian GWAS data",
    "description": "This package is built to perform GWAS analysis using\nBayesian techniques. Currently, GWAS.BAYES has functionality\nfor the implementation of BICOSS (Williams, J., Ferreira, M.\nA., and Ji, T. (2022). BICOSS: Bayesian iterative conditional\nstochastic search for GWAS. BMC Bioinformatics), BGWAS\n(Williams, J., Xu, S., Ferreira, M. A.. (2023) \"BGWAS: Bayesian\nvariable selection in linear mixed models with nonlocal priors\nfor genome-wide association studies.\" BMC Bioinformatics), and\nGINA. All methods currently are for the analysis of Gaussian\nphenotypes The research related to this package was supported\nin part by National Science Foundation awards DMS 1853549, DMS\n1853556, and DMS 2054173.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.6021,
    "stars": 0
  },
  {
    "id": 6475,
    "package_name": "RoBSA",
    "title": "Robust Bayesian Survival Analysis",
    "description": "A framework for estimating ensembles of parametric\nsurvival models with different parametric families. The RoBSA\nframework uses Bayesian model-averaging to combine the\ncompeting parametric survival models into a model ensemble,\nweights the posterior parameter distributions based on\nposterior model probabilities and uses Bayes factors to test\nfor the presence or absence of the individual predictors or\npreference for a parametric family (Bartoš, Aust & Haaf, 2022,\n<doi:10.1186/s12874-022-01676-9>). The user can define a wide\nrange of informative priors for all parameters of interest. The\npackage provides convenient functions for summary,\nvisualizations, fit diagnostics, and prior distribution\ncalibration.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.5441,
    "stars": 0
  },
  {
    "id": 3697,
    "package_name": "LRcell",
    "title": "Differential cell type change analysis using Logistic/linear\nRegression",
    "description": "The goal of LRcell is to identify specific sub-cell types\nthat drives the changes observed in a bulk RNA-seq differential\ngene expression experiment. To achieve this, LRcell utilizes\nsets of cell marker genes acquired from single-cell\nRNA-sequencing (scRNA-seq) as indicators for various cell types\nin the tissue of interest. Next, for each cell type, using its\nmarker genes as indicators, we apply Logistic Regression on the\ncomplete set of genes with differential expression p-values to\ncalculate a cell-type significance p-value. Finally, these\np-values are compared to predict which one(s) are likely to be\nresponsible for the differential gene expression pattern\nobserved in the bulk RNA-seq experiments. LRcell is inspired by\nthe LRpath[@sartor2009lrpath] algorithm developed by Sartor et\nal., originally designed for pathway/gene set enrichment\nanalysis. LRcell contains three major components: LRcell\nanalysis, plot generation and marker gene selection. All\nmodules in this package are written in R. This package also\nprovides marker genes in the Prefrontal Cortex (pFC) human\nbrain region, human PBMC and nine mouse brain regions (Frontal\nCortex, Cerebellum, Globus Pallidus, Hippocampus,\nEntopeduncular, Posterior Cortex, Striatum, Substantia Nigra\nand Thalamus).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 14499,
    "package_name": "gmvjoint",
    "title": "Joint Models of Survival and Multivariate Longitudinal Data",
    "description": "Fit joint models of survival and multivariate longitudinal\ndata. The longitudinal data is specified by generalised linear\nmixed models. The joint models are fit via maximum likelihood\nusing an approximate expectation maximisation algorithm.\nBernhardt (2015) <doi:10.1016/j.csda.2014.11.011>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 19496,
    "package_name": "penAFT",
    "title": "Fit the Semiparameteric Accelerated Failure Time Model with\nElastic Net and Sparse Group Lasso Penalties",
    "description": "The semiparametric accelerated failure time (AFT) model is\nan attractive alternative to the Cox proportional hazards\nmodel. This package provides a suite of functions for fitting\none popular rank-based estimator of the semiparametric AFT\nmodel, the regularized Gehan estimator. Specifically, we\nprovide functions for cross-validation, prediction, coefficient\nextraction, and visualizing both trace plots and\ncross-validation curves. For further details, please see Suder,\nP. M. and Molstad, A. J., (2022) Scalable algorithms for\nsemiparametric accelerated failure time models in high\ndimensions, Statistics in Medicine <doi:10.1002/sim.9264>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 23644,
    "package_name": "splusTimeSeries",
    "title": "Time Series from 'S-PLUS'",
    "description": "A collection of classes and methods for working with\nindexed rectangular data. The index values can be calendar\n(timeSeries class) or numeric (signalSeries class). Methods are\nincluded for aggregation, alignment, merging, and summaries.\nThe code was originally available in 'S-PLUS'.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.4771,
    "stars": 0
  },
  {
    "id": 24727,
    "package_name": "tigre",
    "title": "Transcription factor Inference through Gaussian process\nReconstruction of Expression",
    "description": "The tigre package implements our methodology of Gaussian\nprocess differential equation models for analysis of gene\nexpression time series from single input motif networks. The\npackage can be used for inferring unobserved transcription\nfactor (TF) protein concentrations from expression measurements\nof known target genes, or for ranking candidate targets of a\nTF.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.3802,
    "stars": 0
  },
  {
    "id": 4881,
    "package_name": "OpenStats",
    "title": "A Robust and Scalable Software Package for Reproducible Analysis\nof High-Throughput genotype-phenotype association",
    "description": "Package contains several methods for statistical analysis\nof genotype to phenotype association in high-throughput\nscreening pipelines.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.3424,
    "stars": 0
  },
  {
    "id": 5565,
    "package_name": "QregBB",
    "title": "Block Bootstrap Methods for Quantile Regression in Time Series",
    "description": "Implements moving-blocks bootstrap and extended\ntapered-blocks bootstrap, as well as smooth versions of each,\nfor quantile regression in time series. This package\naccompanies the paper: Gregory, K. B., Lahiri, S. N., &\nNordman, D. J. (2018). A smooth block bootstrap for quantile\nregression with time series. The Annals of Statistics, 46(3),\n1138-1166.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.3222,
    "stars": 0
  },
  {
    "id": 1169,
    "package_name": "CeTF",
    "title": "Coexpression for Transcription Factors using Regulatory Impact\nFactors and Partial Correlation and Information Theory analysis",
    "description": "This package provides the necessary functions for\nperforming the Partial Correlation coefficient with Information\nTheory (PCIT) (Reverter and Chan 2008) and Regulatory Impact\nFactors (RIF) (Reverter et al. 2010) algorithm. The PCIT\nalgorithm identifies meaningful correlations to define edges in\na weighted network and can be applied to any correlation-based\nnetwork including but not limited to gene co-expression\nnetworks, while the RIF algorithm identify critical\nTranscription Factors (TF) from gene expression data. These two\nalgorithms when combined provide a very relevant layer of\ninformation for gene expression studies (Microarray, RNA-seq\nand single-cell RNA-seq data).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 9824,
    "package_name": "calm",
    "title": "Covariate Assisted Large-scale Multiple testing",
    "description": "Statistical methods for multiple testing with covariate\ninformation. Traditional multiple testing methods only consider\na list of test statistics, such as p-values. Our methods\nincorporate the auxiliary information, such as the lengths of\ngene coding regions or the minor allele frequencies of SNPs, to\nimprove power.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 23470,
    "package_name": "sparsenetgls",
    "title": "Using Gaussian graphical structue learning estimation in\ngeneralized least squared regression for multivariate normal\nregression",
    "description": "The package provides methods of combining the graph\nstructure learning and generalized least squares regression to\nimprove the regression estimation. The main function\nsparsenetgls() provides solutions for multivariate regression\nwith Gaussian distributed dependant variables and explanatory\nvariables utlizing multiple well-known graph structure learning\napproaches to estimating the precision matrix, and uses a\npenalized variance covariance matrix with a distance tuning\nparameter of the graph structure in deriving the sandwich\nestimators in generalized least squares (gls) regression. This\npackage also provides functions for assessing a Gaussian\ngraphical model which uses the penalized approach. It uses\nReceiver Operative Characteristics curve as a visualization\ntool in the assessment.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 24464,
    "package_name": "ternarynet",
    "title": "Ternary Network Estimation",
    "description": "Gene-regulatory network (GRN) modeling seeks to infer\ndependencies between genes and thereby provide insight into the\nregulatory relationships that exist within a cell. This package\nprovides a computational Bayesian approach to GRN estimation\nfrom perturbation experiments using a ternary network model, in\nwhich gene expression is discretized into one of 3 states: up,\nunchanged, or down). The ternarynet package includes a parallel\nimplementation of the replica exchange Monte Carlo algorithm\nfor fitting network models, using MPI.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.301,
    "stars": 0
  },
  {
    "id": 7748,
    "package_name": "TrendLSW",
    "title": "Wavelet Methods for Analysing Locally Stationary Time Series",
    "description": "Fitting models for, and simulation of, trend locally\nstationary wavelet (TLSW) time series models, which take\naccount of time-varying trend and dependence structure in a\nunivariate time series. The TLSW model, and its estimation, is\ndescribed in McGonigle, Killick and Nunes (2022a)\n<doi:10.1111/jtsa.12643>, (2022b) <doi:10.1214/22-EJS2044>. New\nusers will likely want to start with the TLSW function.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.1761,
    "stars": 0
  },
  {
    "id": 13286,
    "package_name": "flatr",
    "title": "Transforms Contingency Tables to Data Frames, and Analyses Them",
    "description": "Contingency Tables are a pain to work with when you want\nto run regressions. This package takes them, flattens them into\na long data frame, so you can more easily analyse them! As\nwell, you can calculate other related statistics. All of this\nis done so in a 'tidy' manner, so it should tie in nicely with\n'tidyverse' series of packages.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.1761,
    "stars": 0
  },
  {
    "id": 17506,
    "package_name": "miniPCH",
    "title": "Survival Distributions with Piece-Wise Constant Hazards",
    "description": "Density, distribution function, ... hazard function,\ncumulative hazard function, survival function for survival\ndistributions with piece-wise constant hazards and multiple\nstates and methods to plot and summarise those distributions. A\nderivation of the used algorithms can be found in my masters\nthesis <doi:10.25365/thesis.76098>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.1761,
    "stars": 0
  },
  {
    "id": 14472,
    "package_name": "glueformula",
    "title": "String interpolation to build regression formulas",
    "description": "Has the main function gl that is a variant of glue. It is\nuseful for building regression formulas given vectors of\nvariable names.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3.0792,
    "stars": 0
  },
  {
    "id": 678,
    "package_name": "BiCausality",
    "title": "Binary Causality Inference Framework",
    "description": "A framework to infer causality on binary data using\ntechniques in frequent pattern mining and estimation\nstatistics. Given a set of individual vectors S={x} where x(i)\nis a realization value of binary variable i, the framework\ninfers empirical causal relations of binary variables i,j from\nS in a form of causal graph G=(V,E) where V is a set of nodes\nrepresenting binary variables and there is an edge from i to j\nin E if the variable i causes j. The framework determines\ndependency among variables as well as analyzing confounding\nfactors before deciding whether i causes j.  The publication of\nthis package is at Chainarong Amornbunchornvej, Navaporn\nSurasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong\n(2023) <doi:10.1016/j.heliyon.2023.e15947>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3,
    "stars": 0
  },
  {
    "id": 3875,
    "package_name": "MAR1",
    "title": "Multivariate Autoregressive Modeling for Analysis of Community\nTime-Series Data",
    "description": "The MAR1 package provides basic tools for preparing\necological community time-series data for MAR modeling,\nbuilding MAR-1 models via model selection and bootstrapping,\nand visualizing and exporting model results. It is intended to\nmake MAR analysis sensu Ives et al. (2003) Analysis of\ncommunity stability and ecological interactions from\ntime-series data) a more accessible tool for anyone studying\ncommunity dynamics.  The user need not necessarily be familiar\nwith time-series modeling or command-based statistics programs\nsuch as R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3,
    "stars": 0
  },
  {
    "id": 5704,
    "package_name": "RChest",
    "title": "Locating Distributional Changes in Highly Dependent Time Series",
    "description": "Provides algorithms to locate multiple distributional\nchange-points in piecewise stationary time series. The\nalgorithms are provably consistent, even in the presence of\nlong-range dependencies. Knowledge of the number of\nchange-points is not required. The code is written in Go and\ninterfaced with R.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3,
    "stars": 0
  },
  {
    "id": 25481,
    "package_name": "varlasso",
    "title": "Vector Autoregressive State Space Models With Shrinkage",
    "description": "The varlasso package uses Stan (mc-stan.org) to fit VAR\nstate space models with optional shrinkage priors on B matrix\nelements (autoregression coefficients).",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 3,
    "stars": 0
  },
  {
    "id": 24834,
    "package_name": "tmle3mediate",
    "title": "Targeted Learning for Causal Mediation Analysis",
    "description": "Targeted maximum likelihood (TML) estimation of\npopulation-level causal effects in mediation analysis. The\ncausal effects are defined by joint static or stochastic\ninterventions applied to the exposure and the mediator.\nTargeted doubly robust estimators are provided for the\nclassical natural direct and indirect effects, as well as the\nmore recently developed population intervention direct and\nindirect effects.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.9294,
    "stars": 0
  },
  {
    "id": 492,
    "package_name": "BPrinStratTTE",
    "title": "Causal Effects in Principal Strata Defined by Antidrug\nAntibodies",
    "description": "Bayesian models to estimate causal effects of biological\ntreatments on time-to-event endpoints in clinical trials with\nprincipal strata defined by the occurrence of antidrug\nantibodies. The methodology is based on Frangakis and Rubin\n(2002) <doi:10.1111/j.0006-341x.2002.00021.x> and Imbens and\nRubin (1997) <doi:10.1214/aos/1034276631>, and here adapted to\na specific time-to-event setting.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.699,
    "stars": 0
  },
  {
    "id": 17579,
    "package_name": "mixAR",
    "title": "Mixture Autoregressive Models",
    "description": "Model time series using mixture autoregressive (MAR)\nmodels.  Implemented are frequentist (EM) and Bayesian methods\nfor estimation, prediction and model evaluation. See Wong and\nLi (2002) <doi:10.1111/1467-9868.00222>, Boshnakov (2009)\n<doi:10.1016/j.spl.2009.04.009>), and the extensive references\nin the documentation.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.699,
    "stars": 0
  },
  {
    "id": 18179,
    "package_name": "mvdlm",
    "title": "Multivariate Dynamic Linear Modelling With Stan",
    "description": "Fits multivariate dynamic linear models in a Bayesian\nframework using Stan.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.699,
    "stars": 0
  },
  {
    "id": 24091,
    "package_name": "survHEhmc",
    "title": "Survival Analysis in Health Economic Evaluation using\nHamiltonian Monte Carlo",
    "description": "A module to complement the backbone structure of the\npackage 'survHE' and expand its functionality to run survival\nmodels under a Bayesian approach (based on Hamiltonian Monte\nCarlo). <doi:10.18637/jss.v095.i14>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.699,
    "stars": 0
  },
  {
    "id": 24092,
    "package_name": "survHEinla",
    "title": "Survival Analysis in Health Economic Evaluation using INLA",
    "description": "A module to complement the backbone structure of the\npackage survHE and expand its functionality to run survival\nmodels under a Bayesian approach (based on Integrated Nested\nLaplace Approximation; the underlying 'INLA' package is\navailable for download at\n<https://inla.r-inla-download.org/R/stable/>).\n<doi:10.18637/jss.v095.i14>.",
    "version": "",
    "maintainer": "",
    "url": "",
    "exports": [],
    "topics": [],
    "score": 2.4771,
    "stars": 0
  },
  {
    "id": 12,
    "package_name": "ABSurvTDC",
    "title": "Survival Analysis using Time Dependent Covariate for Animal\nBreeding",
    "description": "Survival analysis is employed to model the time it takes for events to occur. Survival model examines the relationship between survival and one or more predictors, usually termed covariates in the survival-analysis literature. To this end, Cox-proportional (Cox-PH) hazard rate model introduced in a seminal paper by Cox (1972) <doi:10.1111/j.2517-6161.1972.tb00899.x>, is a broadly applicable and the most widely used method of survival analysis. This package can be used to estimate the effect of fixed and time-dependent covariates and also to compute the survival probabilities of the lactation of dairy animal. This package has been developed using algorithm of Klein and  Moeschberger (2003) <doi:10.1007/b97377>.",
    "version": "0.1.0",
    "maintainer": "Dr. Himadri Ghosh <hghosh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 27,
    "package_name": "ACV",
    "title": "Optimal Out-of-Sample Forecast Evaluation and Testing under\nStationarity",
    "description": "Package 'ACV' (short for Affine Cross-Validation) offers an improved time-series cross-validation loss estimator which utilizes both in-sample and out-of-sample forecasting performance via a carefully constructed affine weighting scheme. Under the assumption of stationarity, the estimator is the best linear unbiased estimator of the out-of-sample loss. Besides that, the package also offers improved versions of Diebold-Mariano and Ibragimov-Muller tests of equal predictive ability which deliver more power relative to their conventional counterparts. For more information, see the accompanying article Stanek (2021) <doi:10.2139/ssrn.3996166>.",
    "version": "1.0.2",
    "maintainer": "Filip Stanek <stanek.fi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 38,
    "package_name": "ADLP",
    "title": "Accident and Development Period Adjusted Linear Pools for\nActuarial Stochastic Reserving",
    "description": "Loss reserving generally focuses on identifying a single model that can generate superior predictive performance. However, different loss reserving models specialise in capturing different aspects of loss data.\n    This is recognised in practice in the sense that results from different models are often considered, and sometimes combined. For instance, actuaries may take a weighted average of the prediction outcomes from\n    various loss reserving models, often based on subjective assessments.\n        This package allows for the use of a systematic framework to objectively combine (i.e. ensemble) multiple stochastic loss reserving models such that the strengths offered by different models can be utilised effectively. Our\n    framework is developed in Avanzi et al. (2023). Firstly, our criteria model combination considers the full distributional properties of the ensemble and not just the central\n    estimate - which is of particular importance in the reserving context. Secondly, our framework is that it is tailored for the features inherent to reserving data. These include, for instance, accident, development,\n    calendar, and claim maturity effects. Crucially, the relative importance and scarcity of data across accident periods renders the problem distinct from the traditional ensemble techniques in statistical learning.\n        Our framework is illustrated with a complex synthetic dataset. In the results, the optimised ensemble outperforms both (i) traditional model selection strategies, and (ii) an equally weighted ensemble. In\n    particular, the improvement occurs not only with central estimates but also relevant quantiles, such as the 75th percentile of reserves (typically of interest to both insurers and regulators).\n    Reference: Avanzi B, Li Y, Wong B, Xian A (2023) \"Ensemble distributional forecasting for insurance loss reserving\" <doi:10.48550/arXiv.2206.08541>.",
    "version": "0.1.0",
    "maintainer": "Yanfeng Li <yanfeng.li@student.unsw.edu.au>",
    "url": "https://github.com/agi-lab/ADLP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 41,
    "package_name": "ADPF",
    "title": "Use Least Squares Polynomial Regression and Statistical Testing\nto Improve Savitzky-Golay",
    "description": "This function takes a vector or matrix of data and smooths\n    the data with an improved Savitzky Golay transform. The Savitzky-Golay\n    method for data smoothing and differentiation calculates convolution\n    weights using Gram polynomials that exactly reproduce the results of\n    least-squares polynomial regression. Use of the Savitzky-Golay\n    method requires specification of both filter length and\n    polynomial degree to calculate convolution weights. For maximum\n    smoothing of statistical noise in data, polynomials with\n    low degrees are desirable, while a high polynomial degree\n    is necessary for accurate reproduction of peaks in the data.\n    Extension of the least-squares regression formalism with\n    statistical testing of additional terms of polynomial degree\n    to a heuristically chosen minimum for each data window leads\n    to an adaptive-degree polynomial filter (ADPF). Based on noise\n    reduction for data that consist of pure noise and on signal\n    reproduction for data that is purely signal, ADPF performed\n    nearly as well as the optimally chosen fixed-degree\n    Savitzky-Golay filter and outperformed sub-optimally chosen\n    Savitzky-Golay filters. For synthetic data consisting of noise\n    and signal, ADPF outperformed both optimally chosen and\n    sub-optimally chosen fixed-degree Savitzky-Golay filters. See Barak, P. (1995) <doi:10.1021/ac00113a006> for more information.",
    "version": "0.0.1",
    "maintainer": "Samuel Kruse <samdkruse@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 43,
    "package_name": "ADSIHT",
    "title": "Adaptive Double Sparse Iterative Hard Thresholding",
    "description": "Solving high-dimensional double sparse linear regression via an iterative hard thresholding algorithm. Furthermore, the method is extended to jointly estimate multiple graphical models.  For more details, please see <https://www.jmlr.org/papers/v25/23-0653.html> and <doi:10.48550/arXiv.2503.18722>.",
    "version": "0.2.1",
    "maintainer": "Yanhang Zhang <zhangyh98@tsinghua.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 44,
    "package_name": "ADTSA",
    "title": "Time Series Analysis",
    "description": "Analyzes autocorrelation and partial autocorrelation using surrogate methods and \n  bootstrapping, and computes the acceleration constants for the vectorized moving block \n  bootstrap provided by this package. It generates percentile, bias-corrected, and accelerated \n  intervals and estimates partial autocorrelations using Durbin-Levinson. This package \n  calculates the autocorrelation power spectrum, computes cross-correlations between two \n  time series, computes bandwidth for any time series, and performs autocorrelation frequency \n  analysis. It also calculates the periodicity of a time series.",
    "version": "1.0.1",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 45,
    "package_name": "ADVICE",
    "title": "Automatic Direct Variable Selection via Interrupted Coefficient\nEstimation",
    "description": "Accurate point and interval estimation methods for multiple linear regression coefficients, under classical normal and independent error assumptions, taking into account variable selection.   ",
    "version": "1.0",
    "maintainer": "L. Tazik <ladan.tazik@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 47,
    "package_name": "AEDForecasting",
    "title": "Change Point Analysis in ARIMA Forecasting",
    "description": "Package to incorporate change point analysis in ARIMA forecasting.",
    "version": "0.20.0",
    "maintainer": "Nhat Cuong Pham <acmetal74@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 48,
    "package_name": "AEP",
    "title": "Statistical Modelling for Asymmetric Exponential Power\nDistribution",
    "description": "Developed for Computing the probability density function, cumulative \n             distribution function, random generation, estimating the parameters of asymmetric\n             exponential power distribution, and robust regression analysis with error\n             term that follows asymmetric exponential power distribution. The asymmetric\n             exponential power distribution studied here is a special case of that introduced\n             by Dongming and Zinde-Walsh (2009) <doi:10.1016/j.jeconom.2008.09.038>.",
    "version": "0.1.4",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 49,
    "package_name": "AER",
    "title": "Applied Econometrics with R",
    "description": "Functions, data sets, examples, demos, and vignettes for the book\n             Christian Kleiber and Achim Zeileis (2008),\n\t     Applied Econometrics with R, Springer-Verlag, New York.\n\t     ISBN 978-0-387-77316-2. <doi:10.1007/978-0-387-77318-6>\n             (See the vignette \"AER\" for a package overview.)",
    "version": "1.2-15",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 53,
    "package_name": "AFR",
    "title": "Toolkit for Regression Analysis of Kazakhstan Banking Sector\nData",
    "description": "Tool is created for regression, prediction and forecast analysis of macroeconomic and credit data.\n  The package includes functions from existing R packages adapted for banking sector of Kazakhstan.\n  The purpose of the package is to optimize statistical functions for easier interpretation for bank analysts and non-statisticians.",
    "version": "0.3.7",
    "maintainer": "Sultan Zhaparov <saldau.sultan@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 66,
    "package_name": "AHSurv",
    "title": "Flexible Parametric Accelerated Hazards Models",
    "description": "Flexible parametric Accelerated Hazards (AH) regression models in overall and relative survival frameworks with 13 distinct Baseline Distributions. The AH Model can also be applied to lifetime data with crossed survival curves. Any user-defined parametric distribution can be fitted, given at least an R function defining the cumulative hazard and hazard rate functions. See Chen and Wang (2000) <doi:10.1080/01621459.2000.10474236>, and Lee (2015) <doi:10.1007/s10985-015-9349-5> for more details.",
    "version": "0.1.0",
    "maintainer": "Abdisalam Hassan Muse <abdisalam.h.muse@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 67,
    "package_name": "AICcmodavg",
    "title": "Model Selection and Multimodel Inference Based on (Q)AIC(c)",
    "description": "Functions to implement model selection and multimodel inference based on Akaike's information criterion (AIC) and the second-order AIC (AICc), as well as their quasi-likelihood counterparts (QAIC, QAICc) from various model object classes.  The package implements classic model averaging for a given parameter of interest or predicted values, as well as a shrinkage version of model averaging parameter estimates or effect sizes.  The package includes diagnostics and goodness-of-fit statistics for certain model types including those of 'unmarkedFit' classes estimating demographic parameters after accounting for imperfect detection probabilities.  Some functions also allow the creation of model selection tables for Bayesian models of the 'bugs', 'rjags', and 'jagsUI' classes.  Functions also implement model selection using BIC.  Objects following model selection and multimodel inference can be formatted to LaTeX using 'xtable' methods included in the package.",
    "version": "2.3-4",
    "maintainer": "Marc J. Mazerolle <marc.mazerolle@sbf.ulaval.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 75,
    "package_name": "ALDqr",
    "title": "Quantile Regression Using Asymmetric Laplace Distribution",
    "description": "EM algorithm for estimation of parameters and other methods in a quantile regression. ",
    "version": "1.0",
    "maintainer": "Luis Benites Sanchez <lbenitesanchez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 81,
    "package_name": "ALTopt",
    "title": "Optimal Experimental Designs for Accelerated Life Testing",
    "description": "Creates the optimal (D, U and I) designs for the accelerated life\n    testing with right censoring or interval censoring. It uses generalized \n    linear model (GLM) approach to derive the asymptotic variance-covariance \n    matrix of regression coefficients. The failure time distribution is assumed \n    to follow Weibull distribution with a known shape parameter and log-linear \n    link functions are used to model the relationship between failure time \n    parameters and stress variables. The acceleration model may have multiple \n    stress factors, although most ALTs involve only two or less stress factors. \n    ALTopt package also provides several plotting functions including contour plot,\n    Fraction of Use Space (FUS) plot and Variance Dispersion graphs of Use Space\n    (VDUS) plot. For more details, see Seo and Pan (2015) <doi:10.32614/RJ-2015-029>.",
    "version": "0.1.2",
    "maintainer": "Kangwon Seo <seoka@missouri.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 83,
    "package_name": "ALassoSurvIC",
    "title": "Adaptive Lasso for the Cox Regression with Interval Censored and\nPossibly Left Truncated Data",
    "description": "Penalized variable selection tools for the Cox \n    proportional hazards model with interval censored and possibly \n    left truncated data. It performs variable selection via \n    penalized nonparametric maximum likelihood estimation with an \n    adaptive lasso penalty. The optimal thresholding parameter can be \n    searched by the package based on the profile Bayesian information \n    criterion (BIC). The asymptotic validity of the methodology is \n    established in Li et al. (2019 <doi:10.1177/0962280219856238>). \n    The unpenalized nonparametric maximum likelihood estimation for \n    interval censored and possibly left truncated data is also \n    available.",
    "version": "0.1.1",
    "maintainer": "Daewoo Pak <heavyrain.pak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 112,
    "package_name": "APCtools",
    "title": "Routines for Descriptive and Model-Based APC Analysis",
    "description": "Age-Period-Cohort (APC) analyses are used to differentiate relevant drivers for long-term developments.\n    The 'APCtools' package offers visualization techniques and general routines to simplify the workflow of an APC analysis.\n    Sophisticated functions are available both for descriptive and regression model-based analyses.\n    For the former, we use density (or ridgeline) matrices and (hexagonally binned) heatmaps as innovative visualization\n    techniques building on the concept of Lexis diagrams.\n    Model-based analyses build on the separation of the temporal dimensions based on generalized additive models,\n    where a tensor product interaction surface (usually between age and period) is utilized\n    to represent the third dimension (usually cohort) on its diagonal.\n    Such tensor product surfaces can also be estimated while accounting for\n    further covariates in the regression model.\n    See Weigert et al. (2021) <doi:10.1177/1354816620987198> for methodological details.",
    "version": "1.0.8",
    "maintainer": "Alexander Bauer <baueralexander@posteo.de>",
    "url": "https://bauer-alex.github.io/APCtools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 113,
    "package_name": "APFr",
    "title": "Multiple Testing Approach using Average Power Function (APF) and\nBayes FDR Robust Estimation",
    "description": "Implements a multiple testing approach to the\n    choice of a threshold gamma on the p-values using the\n    Average Power Function (APF) and Bayes False Discovery\n    Rate (FDR) robust estimation. Function apf_fdr() \n    estimates both quantities from either raw data or\n    p-values. Function apf_plot() produces smooth graphs \n    and tables of the relevant results. Details of the methods\n    can be found in Quatto P, Margaritella N, et al. (2019) \n    <doi:10.1177/0962280219844288>.",
    "version": "1.0.2",
    "maintainer": "Nicolò Margaritella <N.Margaritella@sms.ed.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 117,
    "package_name": "APRScenario",
    "title": "Structural Scenario Analysis for Bayesian Structural Vector\nAutoregression Models",
    "description": "Implements the scenario analysis proposed by Antolin-Diaz, \n    Petrella and Rubio-Ramirez (2021) \n    \"Structural scenario analysis with SVARs\" <doi:10.1016/j.jmoneco.2020.06.001>.",
    "version": "0.0.3.1",
    "maintainer": "Giovanni Lombardo <giannilmbd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 120,
    "package_name": "APtools",
    "title": "Average Positive Predictive Values (AP) for Binary Outcomes and\nCensored Event Times",
    "description": "We provide tools to estimate two prediction accuracy metrics,\n    the average positive predictive values (AP) as well as the well-known AUC\n    (the area under the receiver operator characteristic curve) for risk scores. \n    The outcome of interest is either binary or censored event time.\n    Note that for censored event time, our functions' estimates, the AP and the\n    AUC, are time-dependent for pre-specified time interval(s). A function that\n    compares the APs of two risk scores/markers is also included. Optional\n    outputs include positive predictive values and true positive fractions at\n    the specified marker cut-off values, and a plot of the time-dependent AP\n    versus time (available for event time data).",
    "version": "6.8.8",
    "maintainer": "Hengrui Cai <hengruicai@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 127,
    "package_name": "ARCensReg",
    "title": "Fitting Univariate Censored Linear Regression Model with\nAutoregressive Errors",
    "description": "It fits a univariate left, right, or interval censored linear regression model\n    with autoregressive errors, considering the normal or the Student-t distribution for the \n    innovations. It provides estimates and standard errors of the parameters, predicts \n    future observations, and supports missing values on the dependent variable.\n    References used for this package:\n    Schumacher, F. L., Lachos, V. H., & Dey, D. K. (2017). Censored regression models with \n    autoregressive errors: A likelihood-based perspective. Canadian Journal of Statistics,\n    45(4), 375-392 <doi:10.1002/cjs.11338>.\n    Schumacher, F. L., Lachos, V. H., Vilca-Labra, F. E., & Castro, L. M. (2018). Influence \n    diagnostics for censored regression models with autoregressive errors. Australian & New \n    Zealand Journal of Statistics, 60(2), 209-229 <doi:10.1111/anzs.12229>.\n    Valeriano, K. A., Schumacher, F. L., Galarza, C. E., & Matos, L. A. (2024). Censored \n    autoregressive regression models with Student‐t innovations. Canadian Journal of Statistics, \n    52(3), 804-828 <doi:10.1002/cjs.11804>.",
    "version": "3.0.2",
    "maintainer": "Fernanda L. Schumacher <fernandalschumacher@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 129,
    "package_name": "ARDL",
    "title": "ARDL, ECM and Bounds-Test for Cointegration",
    "description": "Creates complex autoregressive distributed lag (ARDL) models and \n    constructs the underlying unrestricted and restricted error correction \n    model (ECM) automatically, just by providing the order. It also performs\n    the bounds-test for cointegration as described in Pesaran et al. (2001) \n    <doi:10.1002/jae.616> and provides the multipliers and the cointegrating\n    equation. The validity and the accuracy of this package have been verified \n    by successfully replicating the results of Pesaran et al. (2001) in \n    Natsiopoulos and Tzeremes (2022) <doi:10.1002/jae.2919>.",
    "version": "0.2.4",
    "maintainer": "Kleanthis Natsiopoulos <klnatsio@gmail.com>",
    "url": "https://github.com/Natsiopoulos/ARDL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 130,
    "package_name": "ARGOS",
    "title": "Automatic Regression for Governing Equations (ARGOS)",
    "description": "Comprehensive set of tools for performing system identification of both linear and nonlinear dynamical systems directly from data. The Automatic Regression for Governing Equations (ARGOS) simplifies the complex task of constructing mathematical models of dynamical systems from observed input and output data, supporting various types of systems, including those described by ordinary differential equations. It employs optimal numerical derivatives for enhanced accuracy and employs formal variable selection techniques to help identify the most relevant variables, thereby enabling the development of predictive models for system behavior analysis.",
    "version": "0.1.1",
    "maintainer": "Kevin Egan <kevin.egan@durham.ac.uk>",
    "url": "<https://github.com/kevinegan31/ARGOS-Package>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 151,
    "package_name": "ASSA",
    "title": "Applied Singular Spectrum Analysis (ASSA)",
    "description": "Functions to model and decompose time series into principal components using singular spectrum analysis (de Carvalho and Rua (2017) <doi:10.1016/j.ijforecast.2015.09.004>; de Carvalho et al (2012) <doi:10.1016/j.econlet.2011.09.007>).",
    "version": "2.0",
    "maintainer": "Miguel de Carvalho <miguel.decarvalho@ed.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 171,
    "package_name": "AVGAS",
    "title": "A Variable Selection using Genetic Algorithms",
    "description": "We provide a stage-wise selection method using genetic algorithm which can perform fast interaction selection in high-dimensional linear regression models with two-way interaction effects under strong, weak, or no heredity condition. Ye, C.,and Yang,Y. (2019) <doi:10.1109/TIT.2019.2913417>.",
    "version": "0.1.0",
    "maintainer": "Leiyue Li <lli289.git@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 195,
    "package_name": "AdMit",
    "title": "Adaptive Mixture of Student-t Distributions",
    "description": "Provides functions to perform the fitting of an adaptive mixture\n    of Student-t distributions to a target density through its kernel function as described in\n    Ardia et al. (2009) <doi:10.18637/jss.v029.i03>. The\n    mixture approximation can then be used as the importance density in importance\n    sampling or as the candidate density in the Metropolis-Hastings algorithm to\n    obtain quantities of interest for the target density itself. ",
    "version": "2.1.9",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "url": "https://github.com/ArdiaD/AdMit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 208,
    "package_name": "AdverseEvents",
    "title": "'shiny' Application for Adverse Event Analysis of 'OnCore' Data",
    "description": "An application for analysis of Adverse Events, as described in Chen, et al., (2023) <doi:10.3390/cancers15092521>.\n             The required data for the application includes demographics, follow up, adverse event, drug administration and optional tumor measurement data.\n             The app can produce swimmers plots of adverse events, Kaplan-Meier plots and Cox Proportional Hazards model results\n             for the association of adverse event biomarkers and overall survival and progression free survival.\n             The adverse event biomarkers include occurrence of grade 3, low grade (1-2), and treatment related adverse events.\n             Plots and tables of results are downloadable.",
    "version": "0.0.4",
    "maintainer": "Z Thompson <zachary.thompson@moffitt.org>",
    "url": "https://github.com/dungtsa/AdverseEvents",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 209,
    "package_name": "AeRobiology",
    "title": "A Computational Tool for Aerobiological Data",
    "description": "Different tools for managing databases of airborne particles, elaborating the main calculations and visualization of results. In a first step, data are checked using tools for quality control and all missing gaps are completed. Then, the main parameters of the pollen season are calculated and represented graphically. Multiple graphical tools are available: pollen calendars, phenological plots, time series, tendencies, interactive plots, abundance plots...",
    "version": "2.0.1",
    "maintainer": "\"Jose Oteros\" <OterosJose@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 221,
    "package_name": "AgroR",
    "title": "Experimental Statistics and Graphics for Agricultural Sciences",
    "description": "Performs the analysis of completely randomized experimental designs (CRD), randomized blocks (RBD) and Latin square (LSD), experiments in double and triple factorial scheme (in CRD and RBD), experiments in subdivided plot scheme (in CRD and RBD), subdivided and joint analysis of experiments in CRD and RBD, linear regression analysis, test for two samples. The package performs analysis of variance, ANOVA assumptions and multiple comparison test of means or regression, according to Pimentel-Gomes (2009, ISBN: 978-85-7133-055-9), nonparametric test (Conover, 1999, ISBN: 0471160687), test for two samples, joint analysis of experiments according to Ferreira (2018, ISBN: 978-85-7269-566-4) and generalized linear model (glm) for binomial and Poisson family in CRD and RBD (Carvalho, FJ (2019), <doi:10.14393/ufu.te.2019.1244>). It can also be used to obtain descriptive measures and graphics, in addition to correlations and creative graphics used in agricultural sciences (Agronomy, Zootechnics, Food Science and related areas). Shimizu, G. D., Marubayashi, R. Y. P., Goncalves, L. S. A. (2025) <doi:10.4025/actasciagron.v47i1.73889>.",
    "version": "1.3.7",
    "maintainer": "Gabriel Danilo Shimizu <gabrield.shimizu@gmail.com>",
    "url": "https://agronomiar.github.io/AgroR_package/index.html,\nhttps://fisher.uel.br/AgroR_shiny,\nhttps://fisher.uel.br/AgroR_shiny.pt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 230,
    "package_name": "Ake",
    "title": "Associated Kernel Estimations",
    "description": "Continuous and discrete (count or categorical) estimation of density, probability mass function (p.m.f.) and regression functions are performed using associated kernels. The cross-validation technique and the local Bayesian procedure are also implemented for bandwidth selection.",
    "version": "1.0.2",
    "maintainer": "W. E. Wansouwé <ericwansouwe@gmail.com>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 237,
    "package_name": "AlleleShift",
    "title": "Predict and Visualize Population-Level Changes in Allele\nFrequencies in Response to Climate Change",
    "description": "Methods (<doi:10.7717/peerj.11534>) are provided of calibrating and predicting shifts in allele frequencies through redundancy analysis ('vegan::rda()') and generalized additive models ('mgcv::gam()'). Visualization functions for predicted changes in allele frequencies include 'shift.dot.ggplot()', 'shift.pie.ggplot()', 'shift.moon.ggplot()', 'shift.waffle.ggplot()' and 'shift.surf.ggplot()' that are made with input data sets that are prepared by helper functions for each visualization method. Examples in the documentation show how to prepare animated climate change graphics through a time series with the 'gganimate' package. Function 'amova.rda()' shows how Analysis of Molecular Variance can be directly conducted with the results from redundancy analysis.",
    "version": "1.1-3",
    "maintainer": "Roeland Kindt <RoelandCEKindt@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 247,
    "package_name": "Amelia",
    "title": "A Program for Missing Data",
    "description": "A tool that \"multiply imputes\" missing data in a single cross-section\n  (such as a survey), from a time series (like variables collected for\n  each year in a country), or from a time-series-cross-sectional data\n  set (such as collected by years for each of several countries).\n  Amelia II implements our bootstrapping-based algorithm that gives\n  essentially the same answers as the standard IP or EMis approaches,\n  is usually considerably faster than existing approaches and can\n  handle many more variables.  Unlike Amelia I and other statistically\n  rigorous imputation software, it virtually never crashes (but please\n  let us know if you find to the contrary!).  The program also\n  generalizes existing approaches by allowing for trends in time series\n  across observations within a cross-sectional unit, as well as priors\n  that allow experts to incorporate beliefs they have about the values\n  of missing cells in their data.  Amelia II also includes useful\n  diagnostics of the fit of multiple imputation models.  The program\n  works from the R command line or via a graphical user interface that\n  does not require users to know R.",
    "version": "1.8.3",
    "maintainer": "Matthew Blackwell <mblackwell@gmail.com>",
    "url": "https://gking.harvard.edu/amelia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 251,
    "package_name": "AmoudSurv",
    "title": "Tractable Parametric Odds-Based Regression Models",
    "description": "Fits tractable fully parametric odds-based regression models for survival data, including proportional odds (PO), accelerated failure time (AFT), accelerated odds (AO), and General Odds (GO) models in overall survival frameworks. Given at least an R function specifying the survivor, hazard rate and cumulative distribution functions, any user-defined parametric distribution can be fitted. We applied and evaluated a minimum of seventeen (17) various baseline distributions that can handle different failure rate shapes for each of the four different proposed odds-based regression models. For more information see Bennet et al., (1983) <doi:10.1002/sim.4780020223>, and Muse et al., (2022) <doi:10.1016/j.aej.2022.01.033>.",
    "version": "0.1.0",
    "maintainer": "Abdisalam Hassan Muse <abdisalam.h.muse@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 262,
    "package_name": "AnaCoDa",
    "title": "Analysis of Codon Data under Stationarity using a Bayesian\nFramework",
    "description": "Is a collection of models to analyze genome scale codon\n        data using a Bayesian framework. Provides visualization\n        routines and checkpointing for model fittings. Currently\n        published models to analyze gene data for selection on codon\n        usage based on Ribosome Overhead Cost (ROC) are: ROC (Gilchrist\n        et al. (2015) <doi:10.1093/gbe/evv087>), and ROC with phi\n        (Wallace & Drummond (2013) <doi:10.1093/molbev/mst051>). In\n        addition 'AnaCoDa' contains three currently unpublished models.\n        The FONSE (First order approximation On NonSense Error) model\n        analyzes gene data for selection on codon usage against of\n        nonsense error rates. The PA (PAusing time) and PANSE (PAusing\n        time + NonSense Error) models use ribosome footprinting data to\n        analyze estimate ribosome pausing times with and without\n        nonsense error rate from ribosome footprinting data.",
    "version": "0.1.4.4",
    "maintainer": "Cedric Landerer <cedric.landerer@gmail.com>",
    "url": "https://github.com/clandere/AnaCoDa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 268,
    "package_name": "AncReg",
    "title": "Ancestor Regression",
    "description": "Causal discovery in linear structural equation models (Schultheiss, and Bühlmann (2023) <doi:10.1093/biomet/asad008>) and vector autoregressive models (Schultheiss, Ulmer, and Bühlmann (2025) <doi:10.1515/jci-2024-0011>) with explicit error control for false discovery, at least asymptotically.",
    "version": "1.0.1",
    "maintainer": "Markus Ulmer <markus.ulmer@stat.math.ethz.ch>",
    "url": "http://www.markus-ulmer.ch/AncReg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 270,
    "package_name": "AnchorRegression",
    "title": "Perform AnchorRegression",
    "description": "Performs AnchorRegression proposed by Rothenhäusler et al. 2020. \n    The code is adapted from the original paper repository. (<https://github.com/rothenhaeusler/anchor-regression>)\n    The code was developed independently from the authors of the paper. ",
    "version": "0.1.3",
    "maintainer": "Simon Zimmermann <zimmersi@hu-berlin.de>",
    "url": "https://github.com/simzim96/AnchorRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 283,
    "package_name": "AnomalyScore",
    "title": "Anomaly Scoring for Multivariate Time Series",
    "description": "Compute an anomaly score for multivariate time series based on the k-nearest neighbors algorithm. Different computations of distances between time series are provided.  ",
    "version": "0.1",
    "maintainer": "Guillermo Granados <guillermo.granadosgarcia@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 291,
    "package_name": "AovBay",
    "title": "Classic, Nonparametric and Bayesian One-Way Analysis of Variance\nPanel",
    "description": "It covers various approaches to analysis of variance, provides an assumption testing section in order to provide a decision diagram that allows selecting the most appropriate technique. It provides the classical analysis of variance, the nonparametric equivalent of Kruskal Wallis, and the Bayesian approach. These results are shown in an interactive shiny panel, which allows modifying the arguments of the tests, contains interactive graphics and presents automatic conclusions depending on the tests in order to contribute to the interpretation of these analyzes. 'AovBay' uses 'Stan' and 'FactorBayes' for Bayesian analysis and 'Highcharts' for interactive charts.",
    "version": "0.1.0",
    "maintainer": "Mauricio Rojas-Campuzano <maujroja@espol.edu.ec>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 300,
    "package_name": "ArArRedux",
    "title": "Rigorous Data Reduction and Error Propagation of Ar40 / Ar39\nData",
    "description": "Processes noble gas mass spectrometer data to determine the isotopic composition of argon (comprised of Ar36, Ar37, Ar38, Ar39 and Ar40) released from neutron-irradiated potassium-bearing minerals. Then uses these compositions to calculate precise and accurate geochronological ages for multiple samples as well as the covariances between them. Error propagation is done in matrix form, which jointly treats all samples and all isotopes simultaneously at every step of the data reduction process. Includes methods for regression of the time-resolved mass spectrometer signals to t=0 ('time zero') for both single- and multi-collector instruments, blank correction, mass fractionation correction, detector intercalibration, decay corrections, interference corrections, interpolation of the irradiation parameter between neutron fluence monitors, and (weighted mean) age calculation. All operations are performed on the logs of the ratios between the different argon isotopes so as to properly treat them as 'compositional data', sensu Aitchison [1986, The Statistics of Compositional Data, Chapman and Hall].",
    "version": "1.0",
    "maintainer": "Pieter Vermeesch <p.vermeesch@ucl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 302,
    "package_name": "ArDec",
    "title": "Time Series Autoregressive-Based Decomposition",
    "description": "Autoregressive-based decomposition of a time series based on the approach in West (1997). Particular cases include the extraction of trend and seasonal components.",
    "version": "2.1-1",
    "maintainer": "Susana Barbosa <sabarbosa@fc.ul.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 303,
    "package_name": "ArchaeoPhases",
    "title": "Post-Processing of Markov Chain Monte Carlo Simulations for\nChronological Modelling",
    "description": "Statistical analysis of archaeological dates and groups of\n    dates. This package allows to post-process Markov Chain Monte Carlo\n    (MCMC) simulations from 'ChronoModel' <https://chronomodel.com/>,\n    'Oxcal' <https://c14.arch.ox.ac.uk/oxcal.html> or 'BCal'\n    <https://bcal.shef.ac.uk/>. It provides functions for the study of\n    rhythms of the long term from the posterior distribution of a series\n    of dates (tempo and activity plot). It also allows the estimation and\n    visualization of time ranges from the posterior distribution of groups\n    of dates (e.g. duration, transition and hiatus between successive\n    phases) as described in Philippe and Vibet (2020)\n    <doi:10.18637/jss.v093.c01>.",
    "version": "2.1.0",
    "maintainer": "Anne Philippe <anne.philippe@univ-nantes.fr>",
    "url": "https://ArchaeoStat.github.io/ArchaeoPhases/,\nhttps://github.com/ArchaeoStat/ArchaeoPhases",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 322,
    "package_name": "AteMeVs",
    "title": "Average Treatment Effects with Measurement Error and Variable\nSelection for Confounders",
    "description": "A recent method proposed by Yi and Chen (2023) <doi:10.1177/09622802221146308> is used to estimate the average treatment effects using noisy data containing both measurement error and spurious variables. The package 'AteMeVs' contains a set of functions that provide a step-by-step estimation procedure, including the correction of the measurement error effects, variable selection for building the model used to estimate the propensity scores, and estimation of the average treatment effects. The functions contain multiple options for users to implement, including different ways to correct for the measurement error effects, distinct choices of penalty functions to do variable selection, and various regression models to characterize propensity scores.",
    "version": "0.1.0",
    "maintainer": "Li-Pang Chen <lchen723@nccu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 329,
    "package_name": "AutoAds",
    "title": "Advertisement Metrics Calculation",
    "description": "Calculations of the most common metrics of automated advertisement and plotting of them with trend and forecast. Calculations and description of metrics is taken from different RTB platforms support documentation. Plotting and forecasting is based on packages 'forecast', described in Rob J Hyndman and George Athanasopoulos (2021) \"Forecasting: Principles and Practice\" <https://otexts.com/fpp3/> and Rob J Hyndman et al \"Documentation for 'forecast'\" (2003) <https://pkg.robjhyndman.com/forecast/>, and 'ggplot2', described in Hadley Wickham et al \"Documentation for 'ggplot2'\" (2015) <https://ggplot2.tidyverse.org/>, and Hadley Wickham, Danielle Navarro, and Thomas Lin Pedersen (2015) \"ggplot2: Elegant Graphics for Data Analysis\" <https://ggplot2-book.org/>.",
    "version": "0.1.0",
    "maintainer": "Ivan Nemtsev <nemtsev.v@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 334,
    "package_name": "AutoStepwiseGLM",
    "title": "Builds Stepwise GLMs via Train and Test Approach",
    "description": "Randomly splits data into testing and training sets. Then, uses stepwise selection to fit numerous multiple regression models on the training data, and tests them on the test data. Returned for each model are plots comparing model Akaike Information Criterion (AIC), Pearson correlation coefficient (r) between the predicted and actual values, Mean Absolute Error (MAE), and R-Squared among the models. Each model is ranked relative to the other models by the model evaluation metrics (i.e., AIC, r, MAE, and R-Squared) and the model with the best mean ranking among the model evaluation metrics is returned. Model evaluation metric weights for AIC, r, MAE, and R-Squared are taken in as arguments as aic_wt, r_wt, mae_wt, and r_squ_wt, respectively. They are equally weighted as default but may be adjusted relative to each other if the user prefers one or more metrics to the others, Field, A. (2013, ISBN:978-1-4462-4918-5).",
    "version": "0.2.0",
    "maintainer": "Aaron England <aaron.england24@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 337,
    "package_name": "AutoWeatherIndices",
    "title": "Calculating Weather Indices",
    "description": "Weather indices are formed from weather variables in this package. The users can input any number of weather variables recorded over any number of weeks. This package has no restriction on the number of weeks and weather variables to be taken as input.The details of the method can be seen (i)'Joint effects of weather variables on rice yields' by  R. Agrawal, R. C. Jain  and M. P. Jha in Mausam, vol. 34, pp. 189-194, 1983,<doi:10.54302/mausam.v34i2.2392>,(ii)'Improved weather indices based Bayesian regression model for forecasting crop yield' by  M. Yeasin, K. N. Singh, A. Lama and B. Gurung in Mausam, vol. 72, pp.879-886, 2021,<doi:10.54302/mausam.v72i4.670>.",
    "version": "0.1.0",
    "maintainer": "Achal Lama <achal.lama@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 358,
    "package_name": "BACCO",
    "title": "Bayesian Analysis of Computer Code Output (BACCO)",
    "description": "The BACCO bundle of packages is replaced by the BACCO\n package, which provides a vignette that illustrates the constituent\n packages (emulator, approximator, calibrator) in use.",
    "version": "2.1-0",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 359,
    "package_name": "BACCT",
    "title": "Bayesian Augmented Control for Clinical Trials",
    "description": "Implements the Bayesian Augmented Control (BAC, a.k.a. Bayesian historical data borrowing) method under clinical trial setting by calling 'Just Another Gibbs Sampler' ('JAGS') software. In addition, the 'BACCT' package evaluates user-specified decision rules by computing the type-I error/power, or probability of correct go/no-go decision at interim look. The evaluation can be presented numerically or graphically. Users need to have 'JAGS' 4.0.0 or newer installed due to a compatibility issue with 'rjags' package. Currently, the package implements the BAC method for binary outcome only. Support for continuous and survival endpoints will be added in future releases. We would like to thank AbbVie's Statistical Innovation group and Clinical Statistics group for their support in developing the 'BACCT' package.",
    "version": "1.0",
    "maintainer": "Hongtao Zhang <hongtao.zhang@abbvie.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 360,
    "package_name": "BACprior",
    "title": "Choice of Omega in the BAC Algorithm",
    "description": "The Bayesian Adjustment for Confounding (BAC) algorithm (Wang et al., 2012)\n can be used to estimate the causal effect of a continuous exposure on a continuous outcome.\n This package provides an approximate sensitivity analysis of BAC with regards to the\n hyperparameter omega. BACprior also provides functions to guide the user in their choice\n of an appropriate omega value. The method is based on Lefebvre, Atherton and Talbot (2014).",
    "version": "2.1.2",
    "maintainer": "Denis Talbot <denis.talbot@fmed.ulaval.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 363,
    "package_name": "BAGofT",
    "title": "A Binary Regression Adaptive Goodness-of-Fit Test (BAGofT)",
    "description": "The BAGofT assesses the goodness-of-fit of binary classifiers. Details can be found in Zhang, Ding and Yang (2021) <arXiv:1911.03063v2>.",
    "version": "1.0.0",
    "maintainer": "Jiawei Zhang <zhan4362@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 365,
    "package_name": "BAMBI",
    "title": "Bivariate Angular Mixture Models",
    "description": "Fit (using Bayesian methods) and simulate mixtures of univariate and bivariate angular distributions. Chakraborty and Wong (2021) <doi:10.18637/jss.v099.i11>.",
    "version": "2.3.6",
    "maintainer": "Saptarshi Chakraborty <chakra.saptarshi@gmail.com>",
    "url": "https://doi.org/10.18637/jss.v099.i11",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 367,
    "package_name": "BANAM",
    "title": "Bayesian Analysis of the Network Autocorrelation Model",
    "description": "The network autocorrelation model (NAM) can be used for studying the degree of social influence \n    regarding an outcome variable based on one or more known networks. \n    The degree of social influence is quantified via the network autocorrelation parameters. In case of a single\n    network, the Bayesian methods of Dittrich, Leenders, and Mulder\n    (2017) <DOI:10.1016/j.socnet.2016.09.002> and Dittrich, Leenders, and Mulder (2019)\n    <DOI:10.1177/0049124117729712> are implemented using a normal, flat, or independence  \n    Jeffreys prior for the network autocorrelation. In the case of multiple \n    networks, the Bayesian methods of Dittrich, Leenders, and Mulder (2020) \n    <DOI:10.1177/0081175020913899> are implemented using a multivariate normal prior for \n    the network autocorrelation parameters. Flat priors are implemented \n    for estimating the coefficients. For Bayesian testing of equality and order-constrained \n    hypotheses, the default Bayes factor of Gu, Mulder, and Hoijtink, (2018) \n    <DOI:10.1111/bmsp.12110> is used with the posterior mean and posterior covariance \n    matrix of the NAM parameters based on flat priors as input.",
    "version": "0.2.2",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 371,
    "package_name": "BART",
    "title": "Bayesian Additive Regression Trees",
    "description": "Bayesian Additive Regression Trees (BART) provide flexible nonparametric modeling of covariates for continuous, binary, categorical and time-to-event outcomes.  For more information see Sparapani, Spanbauer and McCulloch <doi:10.18637/jss.v097.i01>.",
    "version": "2.9.9",
    "maintainer": "Rodney Sparapani <rsparapa@mcw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 373,
    "package_name": "BASS",
    "title": "Bayesian Adaptive Spline Surfaces",
    "description": "Bayesian fitting and sensitivity analysis methods for adaptive\n    spline surfaces described in <doi:10.18637/jss.v094.i08>. Built to handle continuous and categorical inputs as well as\n    functional or scalar output. An extension of the methodology in Denison, Mallick\n    and Smith (1998) <doi:10.1023/A:1008824606259>.",
    "version": "1.3.1",
    "maintainer": "Devin Francom <devinfrancom@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 379,
    "package_name": "BATSS",
    "title": "Bayesian Adaptive Trial Simulator Software (BATSS) for\nGeneralised Linear Models",
    "description": "Defines operating characteristics of Bayesian Adaptive Trials considering a generalised linear model response via Monte Carlo simulations of Bayesian GLM fitted via integrated Laplace approximations (INLA). ",
    "version": "1.1.1",
    "maintainer": "Dominique-Laurent Couturier <dominique.couturier@mrc-bsu.cam.ac.uk>",
    "url": "https://batss-stable.github.io/BATSS/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 380,
    "package_name": "BAYSTAR",
    "title": "On Bayesian Analysis of Threshold Autoregressive Models",
    "description": "Fit two-regime threshold autoregressive (TAR) models by Markov chain Monte Carlo methods. ",
    "version": "0.2-10",
    "maintainer": "Edward M.H. Lin <ed.mhlin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 384,
    "package_name": "BBEST",
    "title": "Bayesian Estimation of Incoherent Neutron Scattering Backgrounds",
    "description": "We implemented a Bayesian-statistics approach for \n        subtraction of incoherent scattering from neutron total-scattering data. \n        In this approach, the estimated background signal associated with \n        incoherent scattering maximizes the posterior probability, which combines \n        the likelihood of this signal in reciprocal and real spaces with the prior \n        that favors smooth lines. The description of the corresponding approach \n        could be found at Gagin and Levin (2014) <DOI:10.1107/S1600576714023796>.",
    "version": "0.1-8",
    "maintainer": "Anton Gagin <av.gagin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 387,
    "package_name": "BCA1SG",
    "title": "Block Coordinate Ascent with One-Step Generalized Rosen\nAlgorithm",
    "description": "Implementing the Block Coordinate Ascent with One-Step Generalized Rosen (BCA1SG) algorithm on the semiparametric models for panel count data, interval-censored survival data, and degradation data. A comprehensive description of the BCA1SG algorithm can be found in Wang et al. (2020) <https://github.com/yudongstat/BCA1SG/blob/master/BCA1SG.pdf>. For details of the semiparametric models for panel count data, interval-censored survival data, and degradation data, please see Wellner and Zhang (2007) <doi:10.1214/009053607000000181>, Huang and Wellner (1997) <ISBN:978-0-387-94992-5>, and Wang and Xu (2010) <doi:10.1198/TECH.2009.08197>, respectively.",
    "version": "0.1.0",
    "maintainer": "Wang Yudong <yudongw@u.nus.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 388,
    "package_name": "BCBCSF",
    "title": "Bias-Corrected Bayesian Classification with Selected Features",
    "description": "Fully Bayesian Classification with a subset of high-dimensional features, such as expression levels of genes. The data are modeled with a hierarchical Bayesian models using heavy-tailed t distributions as priors. When a large number of features are available, one may like to select only a subset of features to use, typically those features strongly correlated with the response in training cases. Such a feature selection procedure is however invalid since the relationship between the response and the features has be exaggerated by feature selection. This package provides a way to avoid this bias and yield better-calibrated predictions for future cases when one uses F-statistic to select features. ",
    "version": "1.0-1",
    "maintainer": "Longhai Li <longhai@math.usask.ca>",
    "url": "http://www.r-project.org, http://math.usask.ca/~longhai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 392,
    "package_name": "BCDAG",
    "title": "Bayesian Structure and Causal Learning of Gaussian Directed\nGraphs",
    "description": "A collection of functions for structure learning of causal networks and estimation of joint causal effects from observational Gaussian data. Main algorithm consists of a Markov chain Monte Carlo scheme for posterior inference of causal structures, parameters and causal effects between variables.\n    References:\n    F. Castelletti and A. Mascaro (2021) <doi:10.1007/s10260-021-00579-1>,\n    F. Castelletti and A. Mascaro (2022) <doi:10.48550/arXiv.2201.12003>.",
    "version": "1.1.3",
    "maintainer": "Alessandro Mascaro <alessandro.mascaro@upf.edu>",
    "url": "https://github.com/alesmascaro/BCDAG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 395,
    "package_name": "BCEE",
    "title": "The Bayesian Causal Effect Estimation Algorithm",
    "description": "A Bayesian model averaging approach to causal effect estimation\n    based on the BCEE algorithm. Currently supports binary or continuous\n    exposures and outcomes. For more details, see \n    Talbot et al. (2015) <doi:10.1515/jci-2014-0035>\n    Talbot and Beaudoin (2022) <doi:10.1515/jci-2021-0023>.",
    "version": "1.3.2",
    "maintainer": "Denis Talbot <denis.talbot@fmed.ulaval.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 396,
    "package_name": "BCHM",
    "title": "Clinical Trial Calculation Based on BCHM Design",
    "description": "Users can estimate the treatment effect for multiple subgroups basket trials based on the Bayesian Cluster Hierarchical Model (BCHM). In this model, a Bayesian non-parametric method is applied to dynamically calculate the number of clusters by conducting the multiple cluster classification based on subgroup outcomes. Hierarchical model is used to compute the posterior probability of treatment effect with the borrowing strength determined by the Bayesian non-parametric clustering and the similarities between subgroups. To use this package, 'JAGS' software and 'rjags' package are required, and users need to pre-install them.",
    "version": "1.00",
    "maintainer": "J. Jack Lee <jjlee@mdanderson.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 399,
    "package_name": "BCgee",
    "title": "Bias-Corrected Estimates for Generalized Linear Models for\nDependent Data",
    "description": "Provides bias-corrected estimates for the regression coefficients of a marginal model estimated with generalized estimating equations. Details about the bias formula used are in Lunardon, N., Scharfstein, D. (2017) <doi:10.1002/sim.7366>.",
    "version": "0.1.1",
    "maintainer": "Nicola Lunardon <nicola.lunardon@unimib.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 400,
    "package_name": "BDAlgo",
    "title": "Bloom Detecting Algorithm",
    "description": "The Bloom Detecting Algorithm enables the detection of blooms within a time series of species abundance and extracts 22 phenological variables. For details, see Karasiewicz et al. (2022) <doi:10.3390/jmse10020174>.",
    "version": "0.1.0",
    "maintainer": "Stephane Karasiewicz <skaraz.science@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 403,
    "package_name": "BDWreg",
    "title": "Bayesian Inference for Discrete Weibull Regression",
    "description": "A Bayesian regression model for discrete response, where the conditional distribution is modelled via a discrete Weibull distribution. This package provides an implementation of Metropolis-Hastings and Reversible-Jumps algorithms to draw samples from the posterior. It covers a wide range of regularizations through any two parameter prior. Examples are Laplace (Lasso), Gaussian (ridge), Uniform, Cauchy and customized priors like a mixture of priors. An extensive visual toolbox is included to check the validity of the results as well as several measures of goodness-of-fit.",
    "version": "1.3.0",
    "maintainer": "Hamed Haselimashhadi <hamedhaseli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 404,
    "package_name": "BDgraph",
    "title": "Bayesian Structure Learning in Graphical Models using\nBirth-Death MCMC",
    "description": "Advanced statistical tools for Bayesian structure learning in undirected graphical models, accommodating continuous, ordinal, discrete, count, and mixed data. It integrates recent advancements in Bayesian graphical models as presented in the literature, including the works of Mohammadi and Wit (2015) <doi:10.1214/14-BA889>, Mohammadi et al. (2021) <doi:10.1080/01621459.2021.1996377>, Dobra and Mohammadi (2018) <doi:10.1214/18-AOAS1164>, and Mohammadi et al. (2023) <doi:10.48550/arXiv.2307.00127>. ",
    "version": "2.74",
    "maintainer": "Reza Mohammadi <a.mohammadi@uva.nl>",
    "url": "https://www.uva.nl/profile/a.mohammadi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 410,
    "package_name": "BEDASSLE",
    "title": "Quantifies Effects of Geo/Eco Distance on Genetic\nDifferentiation",
    "description": "Provides functions that allow users to quantify the relative \n\tcontributions of geographic and ecological distances to empirical patterns of genetic \n\tdifferentiation on a landscape.  Specifically, we use a custom Markov chain \n\tMonte Carlo (MCMC) algorithm, which is used to estimate the parameters of the \n\tinference model, as well as functions for performing MCMC diagnosis and assessing \n\tmodel adequacy.",
    "version": "1.6.1",
    "maintainer": "Gideon Bradburd <bradburd@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 412,
    "package_name": "BEKKs",
    "title": "Multivariate Conditional Volatility Modelling and Forecasting",
    "description": "Methods and tools for estimating, simulating and forecasting of so-called BEKK-models (named after Baba, Engle, Kraft and Kroner) based on the fast Berndt–Hall–Hall–Hausman (BHHH) algorithm described in Hafner and Herwartz (2008) <doi:10.1007/s00184-007-0130-y>. For an overview, we refer the reader to Fülle et al. (2024) <doi:10.18637/jss.v111.i04>.  ",
    "version": "1.4.6",
    "maintainer": "Markus J. Fülle <markus.fuelle@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 413,
    "package_name": "BEND",
    "title": "Bayesian Estimation of Nonlinear Data (BEND)",
    "description": "Provides a set of models to estimate nonlinear longitudinal data using Bayesian estimation methods. These models include the: 1) Bayesian Piecewise Random Effects Model (Bayes_PREM()) which estimates a piecewise random effects (mixture) model for a given number of latent classes and a latent number of possible changepoints in each class, and can incorporate class and outcome predictive covariates (see Lamm (2022) <https://hdl.handle.net/11299/252533> and Lock et al., (2018) <doi:10.1007/s11336-017-9594-5>), 2) Bayesian Crossed Random Effects Model (Bayes_CREM()) which estimates a linear, quadratic, exponential, or piecewise crossed random effects models where individuals are changing groups over time (e.g., students and schools; see Rohloff et al., (2024) <doi:10.1111/bmsp.12334>), and 3) Bayesian Bivariate Piecewise Random Effects Model (Bayes_BPREM()) which estimates a bivariate piecewise random effects model to jointly model two related outcomes (e.g., reading and math achievement; see Peralta et al., (2022) <doi:10.1037/met0000358>). ",
    "version": "1.1",
    "maintainer": "Corissa T. Rohloff <corissa.wurth@gmail.com>",
    "url": "https://github.com/crohlo/BEND",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 417,
    "package_name": "BETS",
    "title": "Brazilian Economic Time Series",
    "description": "It provides access to and information about the most important",
    "version": "0.3.2",
    "maintainer": "Pedro Costa Ferreira <pedro.guilherme@fgv.br>",
    "url": "https://github.com/pedrocostaferreira/BETS",
    "exports": [],
    "topics": ["brazil", "time-series"],
    "score": "NA",
    "stars": 30
  },
  {
    "id": 424,
    "package_name": "BFpack",
    "title": "Flexible Bayes Factor Testing of Scientific Expectations",
    "description": "Implementation of default Bayes factors\n    for testing statistical hypotheses under various statistical models. The package is\n    intended for applied quantitative researchers in the\n    social and behavioral sciences, medical research,\n    and related fields. The Bayes factor tests can be\n    executed for statistical models such as \n    univariate and multivariate normal linear models,\n    correlation analysis, generalized linear models, special cases of \n    linear mixed models, survival models, relational\n    event models. Parameters that can be tested are\n    location parameters (e.g., group means, regression coefficients),\n    variances (e.g., group variances), and measures of \n    association (e.g,. polychoric/polyserial/biserial/tetrachoric/product\n    moments correlations), among others.\n    The statistical underpinnings are\n    described in\n    O'Hagan (1995) <DOI:10.1111/j.2517-6161.1995.tb02017.x>,\n    De Santis and Spezzaferri (2001) <DOI:10.1016/S0378-3758(00)00240-8>,\n    Mulder and Xin (2022) <DOI:10.1080/00273171.2021.1904809>,\n    Mulder and Gelissen (2019) <DOI:10.1080/02664763.2021.1992360>,\n    Mulder (2016) <DOI:10.1016/j.jmp.2014.09.004>,\n    Mulder and Fox (2019) <DOI:10.1214/18-BA1115>,\n    Mulder and Fox (2013) <DOI:10.1007/s11222-011-9295-3>,\n    Boeing-Messing, van Assen, Hofman, Hoijtink, and Mulder (2017) <DOI:10.1037/met0000116>,\n    Hoijtink, Mulder, van Lissa, and Gu (2018) <DOI:10.1037/met0000201>,\n    Gu, Mulder, and Hoijtink (2018) <DOI:10.1111/bmsp.12110>,\n    Hoijtink, Gu, and Mulder (2018) <DOI:10.1111/bmsp.12145>, and\n    Hoijtink, Gu, Mulder, and Rosseel (2018) <DOI:10.1037/met0000187>. When using the\n    packages, please refer to the package Mulder et al. (2021) <DOI:10.18637/jss.v100.i18>\n    and the relevant methodological papers.",
    "version": "1.5.3",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "url": "https://github.com/jomulder/BFpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 427,
    "package_name": "BGFD",
    "title": "Bell-G and Complementary Bell-G Family of Distributions",
    "description": "Evaluates the probability density function, cumulative distribution function, quantile function, random numbers, survival function, hazard rate function, and maximum likelihood estimates for the following distributions: Bell exponential, Bell extended exponential, Bell Weibull, Bell extended Weibull, Bell-Fisk, Bell-Lomax, Bell Burr-XII, Bell Burr-X, complementary Bell exponential, complementary Bell extended exponential, complementary Bell Weibull, complementary Bell extended Weibull, complementary Bell-Fisk, complementary Bell-Lomax, complementary Bell Burr-XII and complementary Bell Burr-X distribution. Related work includes:\n     a) Fayomi A., Tahir M. H., Algarni A., Imran M. and Jamal F. (2022). \"A new useful exponential model with applications to quality control and \n        actuarial data\". Computational Intelligence and Neuroscience, 2022. <doi:10.1155/2022/2489998>.\n     b) Alanzi, A. R., Imran M., Tahir M. H., Chesneau C., Jamal F. Shakoor S. and Sami, W. (2023). \"Simulation analysis, \n        properties and applications on a new Burr XII model based on the Bell-X functionalities\". AIMS Mathematics, 8(3): 6970-7004. <doi:10.3934/math.2023352>.\n     c) Algarni A. (2022). \"Group Acceptance Sampling Plan Based on New Compounded Three-Parameter Weibull Model\". Axioms, 11(9): 438. <doi:10.3390/axioms11090438>.",
    "version": "0.1",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 429,
    "package_name": "BGGM",
    "title": "Bayesian Gaussian Graphical Models",
    "description": "Fit Bayesian Gaussian graphical models. The methods are separated into \n    two Bayesian approaches for inference: hypothesis testing and estimation. There are \n    extensions for confirmatory hypothesis testing, comparing Gaussian graphical models, \n    and node wise predictability. These methods were recently introduced in the Gaussian \n    graphical model literature, including \n    Williams (2019) <doi:10.31234/osf.io/x8dpr>, \n    Williams and Mulder (2019) <doi:10.31234/osf.io/ypxd8>,\n    Williams, Rast, Pericchi, and Mulder (2019) <doi:10.31234/osf.io/yt386>.",
    "version": "2.1.6",
    "maintainer": "Philippe Rast <rast.ph@gmail.com>",
    "url": "https://rast-lab.github.io/BGGM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 430,
    "package_name": "BGLR",
    "title": "Bayesian Generalized Linear Regression",
    "description": "Bayesian Generalized Linear Regression.",
    "version": "1.1.4",
    "maintainer": "Paulino Perez Rodriguez <perpdgo@colpos.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 431,
    "package_name": "BGPhazard",
    "title": "Markov Beta and Gamma Processes for Modeling Hazard Rates",
    "description": "Computes the hazard rate estimate as described by\n    Nieto-Barajas & Walker (2002), Nieto-Barajas (2003), Nieto-Barajas &\n    Walker (2007) and Nieto-Barajas & Yin (2008).",
    "version": "2.1.1",
    "maintainer": "Emilio Akira Morones Ishikawa <emiliomorones@gmail.com>",
    "url": "https://github.com/EAMI91/BGPhazard",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 432,
    "package_name": "BGVAR",
    "title": "Bayesian Global Vector Autoregressions",
    "description": "Estimation of Bayesian Global Vector Autoregressions (BGVAR) with different prior setups and the possibility to introduce stochastic volatility. Built-in priors include the Minnesota, the stochastic search variable selection and Normal-Gamma (NG) prior. For a reference see also Crespo Cuaresma, J., Feldkircher, M. and F. Huber (2016) \"Forecasting with Global Vector Autoregressive Models: a Bayesian Approach\", Journal of Applied Econometrics, Vol. 31(7), pp. 1371-1391 <doi:10.1002/jae.2504>. Post-processing functions allow for doing predictions, structurally identify the model with short-run or sign-restrictions and compute impulse response functions, historical decompositions and forecast error variance decompositions. Plotting functions are also available. The package has a companion paper: Boeck, M., Feldkircher, M. and F. Huber (2022) \"BGVAR: Bayesian Global Vector Autoregressions with Shrinkage Priors in R\", Journal of Statistical Software, Vol. 104(9), pp. 1-28 <doi:10.18637/jss.v104.i09>.",
    "version": "2.5.9",
    "maintainer": "Maximilian Boeck <maximilian.boeck@fau.de>",
    "url": "https://github.com/mboeck11/BGVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 437,
    "package_name": "BHMSMAfMRI",
    "title": "Bayesian Hierarchical Multi-Subject Multiscale Analysis of\nFunctional MRI (fMRI) Data",
    "description": "Package BHMSMAfMRI performs Bayesian hierarchical multi-subject multiscale analysis of fMRI data as described in Sanyal & Ferreira (2012) <DOI:10.1016/j.neuroimage.2012.08.041>, or other multiscale data, using wavelet-based prior that borrows strength across subjects and provides posterior smoothed images of the effect sizes and samples from the posterior distribution.",
    "version": "2.3",
    "maintainer": "Nilotpal Sanyal <nilotpal.sanyal@gmail.com>",
    "url": "https://nilotpalsanyal.github.io/BHMSMAfMRI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 438,
    "package_name": "BHSBVAR",
    "title": "Structural Bayesian Vector Autoregression Models",
    "description": "Provides a function for estimating the parameters of Structural Bayesian Vector Autoregression models with the method developed by Baumeister and Hamilton (2015) <doi:10.3982/ECTA12356>, Baumeister and Hamilton (2017) <doi:10.3386/w24167>, and Baumeister and Hamilton (2018) <doi:10.1016/j.jmoneco.2018.06.005>. Functions for plotting impulse responses, historical decompositions, and posterior distributions of model parameters are also provided.",
    "version": "3.1.3",
    "maintainer": "Paul Richardson <p.richardson.54391@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 447,
    "package_name": "BINCOR",
    "title": "Estimate the Correlation Between Two Irregular Time Series",
    "description": "Estimate the correlation between two irregular time series that are not necessarily sampled on identical time points. This program is also applicable to the situation of two evenly spaced time series that are not on the same time grid. 'BINCOR' is based on a novel estimation approach proposed by Mudelsee (2010, 2014) to estimate the correlation between two climate time series with different timescales. The idea is that autocorrelation (AR1 process) allows to correlate values obtained on different time points. 'BINCOR' contains four functions: bin_cor() (the main function to build the binned time series), plot_ts() (to plot and compare the irregular and binned time series, cor_ts() (to estimate the correlation between the binned time series) and ccf_ts() (to estimate the cross-correlation between the binned time series).",
    "version": "0.2.0",
    "maintainer": "Josué M. Polanco-Martínez <josue.m.polanco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 448,
    "package_name": "BINtools",
    "title": "Bayesian BIN (Bias, Information, Noise) Model of Forecasting",
    "description": "A recently proposed Bayesian BIN model disentangles the underlying processes \n    that enable forecasters and forecasting methods to improve, decomposing forecasting accuracy into \n    three components: bias, partial information, and noise. By describing the differences between two \n    groups of forecasters, the model allows the user to carry out useful inference, such as calculating \n    the posterior probabilities of the treatment reducing bias, diminishing noise, or increasing information.\n    It also provides insight into how much tamping down bias and noise in judgment or enhancing the efficient \n    extraction of valid information from the environment improves forecasting accuracy. This package provides \n    easy access to the BIN model. For further information refer to the paper Ville A. Satopää, Marat Salikhov,\n    Philip E. Tetlock, and Barbara Mellers (2021) \"Bias, Information, Noise: The BIN \n    Model of Forecasting\" <doi:10.1287/mnsc.2020.3882>. ",
    "version": "0.2.0",
    "maintainer": "Ville Satopää <ville.satopaa@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 454,
    "package_name": "BKT",
    "title": "Bayesian Knowledge Tracing Model",
    "description": "Fitting, cross-validating, and predicting with Bayesian Knowledge Tracing (BKT) models. It is designed for analyzing educational datasets to trace student knowledge over time. The package includes functions for fitting BKT models, evaluating their performance using various metrics, and making predictions on new data. It provides the similar functionality as the Python package pyBKT authored by Zachary A. Pardos (zp@berkeley.edu) at <https://github.com/CAHLR/pyBKT>.",
    "version": "0.1.0",
    "maintainer": "Yuhao Yuan <yuanyuhaoapply@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 463,
    "package_name": "BLR",
    "title": "Bayesian Linear Regression",
    "description": "Bayesian Linear Regression.",
    "version": "1.6",
    "maintainer": "Paulino Perez Rodriguez <perpdgo@colpos.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 464,
    "package_name": "BLRPM",
    "title": "Stochastic Rainfall Generator Bartlett-Lewis Rectangular Pulse\nModel",
    "description": "Due to a limited availability of observed high-resolution precipitation records with adequate length, simulations with stochastic precipitation models are used to generate series for subsequent studies [e.g. Khaliq and Cunmae, 1996, <doi:10.1016/0022-1694(95)02894-3>, Vandenberghe et al., 2011, <doi:10.1029/2009WR008388>]. This package contains an R implementation of the original Bartlett-Lewis rectangular pulse model (BLRPM), developed by Rodriguez-Iturbe et al. (1987) <doi:10.1098/rspa.1987.0039>. It contains a function for simulating a precipitation time series based on storms and cells generated by the model with given or estimated model parameters. Additionally BLRPM parameters can be estimated from a given or simulated precipitation time series. The model simulations can be plotted in a three-layer plot including an overview of generated storms and cells by the model (which can also be plotted individually), a continuous step-function and a discrete precipitation time series at a chosen aggregation level. ",
    "version": "1.0",
    "maintainer": "Christoph Ritschel <christoph.ritschel@met.fu-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 469,
    "package_name": "BMA",
    "title": "Bayesian Model Averaging",
    "description": "Package for Bayesian model averaging and variable selection for linear models,\n        generalized linear models and survival models (cox\n        regression).",
    "version": "3.18.20",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "https://github.com/hanase/BMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 470,
    "package_name": "BMAmevt",
    "title": "Multivariate Extremes: Bayesian Estimation of the Spectral\nMeasure",
    "description": "Toolkit for Bayesian estimation of the dependence structure in multivariate extreme value parametric models, following Sabourin and Naveau (2014) <doi:10.1016/j.csda.2013.04.021> and Sabourin, Naveau and Fougeres (2013) <doi:10.1007/s10687-012-0163-0>.",
    "version": "1.0.5",
    "maintainer": "Leo Belzile <belzilel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 472,
    "package_name": "BMIselect",
    "title": "Bayesian MI-LASSO for Variable Selection on Multiply-Imputed\nDatasets",
    "description": "Provides a suite of Bayesian MI-LASSO for variable selection methods for multiply-imputed datasets. The package includes four Bayesian MI-LASSO models using shrinkage (Multi-Laplace, Horseshoe, ARD) and Spike-and-Slab (Spike-and-Laplace) priors, along with tools for model fitting via MCMC, four-step projection predictive variable selection, and hyperparameter calibration. Methods are suitable for both continuous and binary covariates under missing-at-random or missing-completely-at-random assumptions. See Zou, J., Wang, S. and Chen, Q. (2025), Bayesian MI-LASSO for Variable Selection on Multiply-Imputed Data. ArXiv, 2211.00114. <doi:10.48550/arXiv.2211.00114> for more details. We also provide the frequentist`s MI-LASSO function.",
    "version": "1.0.3",
    "maintainer": "Jungang Zou <jungang.zou@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 474,
    "package_name": "BMRMM",
    "title": "An Implementation of the Bayesian Markov (Renewal) Mixed Models",
    "description": "The Bayesian Markov renewal mixed models take sequentially observed categorical data with continuous duration times, being either state duration or inter-state duration. These models comprehensively analyze the stochastic dynamics of both state transitions and duration times under the influence of multiple exogenous factors and random individual effect. The default setting flexibly models the transition probabilities using Dirichlet mixtures and the duration times using gamma mixtures. It also provides the flexibility of modeling the categorical sequences using Bayesian Markov mixed models alone, either ignoring the duration times altogether or dividing duration time into multiples of an additional category in the sequence by a user-specific unit. The package allows extensive inference of the state transition probabilities and the duration times as well as relevant plots and graphs. It also includes a synthetic data set to demonstrate the desired format of input data set and the utility of various functions. Methods for Bayesian Markov renewal mixed models are as described in: Abhra Sarkar et al., (2018) <doi:10.1080/01621459.2018.1423986> and Yutong Wu et al., (2022) <doi:10.1093/biostatistics/kxac050>.",
    "version": "1.0.1",
    "maintainer": "Yutong Wu <yutong.wu@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 475,
    "package_name": "BMS",
    "title": "Bayesian Model Averaging Library",
    "description": "Bayesian Model Averaging for linear models with a wide choice of (customizable) priors. Built-in priors include coefficient priors (fixed, hyper-g and empirical priors), 5 kinds of model priors, moreover model sampling by enumeration or various MCMC approaches. Post-processing functions allow for inferring posterior inclusion and model probabilities, various moments, coefficient and predictive densities. Plotting functions available for posterior model size, MCMC convergence, predictive and coefficient densities, best models representation, BMA comparison. Also includes Bayesian normal-conjugate linear model with Zellner's g prior, and assorted methods.",
    "version": "0.3.5",
    "maintainer": "Stefan Zeugner <stefan.zeugner@ec.europa.eu>",
    "url": "http://bms.zeugner.eu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 479,
    "package_name": "BNPTSclust",
    "title": "A Bayesian Nonparametric Algorithm for Time Series Clustering",
    "description": "Performs the algorithm for time series clustering described in Nieto-Barajas and Contreras-Cristan (2014).",
    "version": "2.0",
    "maintainer": "David Alejandro Martell Juarez <alex91599@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 480,
    "package_name": "BNPdensity",
    "title": "Ferguson-Klass Type Algorithm for Posterior Normalized Random\nMeasures",
    "description": "Bayesian nonparametric density estimation modeling mixtures by a Ferguson-Klass type algorithm for posterior normalized random measures.",
    "version": "2025.7.29",
    "maintainer": "Guillaume Kon Kam King <guillaume.konkamking.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 482,
    "package_name": "BNSL",
    "title": "Bayesian Network Structure Learning",
    "description": "From a given data frame, this package learns its Bayesian network structure based on a selected score.",
    "version": "0.1.4",
    "maintainer": "Joe Suzuki <j-suzuki@sigmath.es.osaka-u.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 483,
    "package_name": "BNSP",
    "title": "Bayesian Non- And Semi-Parametric Model Fitting",
    "description": "MCMC algorithms & processing functions for: 1. single response multiple regression, see Papageorgiou, G. (2018) <doi: 10.32614/RJ-2018-069>, 2. multivariate response multiple regression, with nonparametric models for the means, the variances and the correlation matrix, with variable selection, see Papageorgiou, G. and Marshall, B. C. (2020) <doi: 10.1080/10618600.2020.1739534>, 3. joint mean-covariance models for multivariate responses, see Papageorgiou, G. (2022) <doi: 10.1002/sim.9376>, and 4.Dirichlet process mixtures, see Papageorgiou, G. (2019) <doi: 10.1111/anzs.12273>.",
    "version": "2.2.3",
    "maintainer": "Georgios Papageorgiou <gpapageo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 489,
    "package_name": "BOP2FE",
    "title": "Bayesian Optimal Phase II Design with Futility and Efficacy\nStopping Boundaries",
    "description": "Bayesian optimal design with futility and efficacy stopping boundaries (BOP2-FE) is a novel statistical framework for single-arm Phase II clinical trials. It enables early termination for efficacy when interim data are promising, while explicitly controlling Type I and Type II error rates. The design supports a variety of endpoint structures, including single binary endpoints, nested endpoints, co-primary endpoints, and joint monitoring of efficacy and toxicity. The package provides tools for enumerating stopping boundaries prior to trial initiation and for conducting simulation studies to evaluate the design’s operating characteristics. Users can flexibly specify design parameters to suit their specific applications. For methodological details, refer to Xu et al. (2025) <doi:10.1080/10543406.2025.2558142>.",
    "version": "1.0.3",
    "maintainer": "Belay Birlie Yimer <belayabyimer@gmail.com>",
    "url": "https://github.com/belayb/BOP2FE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 493,
    "package_name": "BRACE",
    "title": "Bias Reduction Through Analysis of Competing Events (BRACE)",
    "description": "Adjusting the bias due to residual confounding (often called\n    treatment selection bias) in estimating the treatment effect in a\n    proportional hazard model, as described in Williamson et al.\n    (2022) <doi:10.1158/1078-0432.ccr-21-2468>.",
    "version": "0.1.0",
    "maintainer": "Tuo Lin <tulin@health.ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 494,
    "package_name": "BRACoD.R",
    "title": "BRACoD: Bayesian Regression Analysis of Compositional Data",
    "description": "The goal of this method is to identify associations between bacteria and an environmental variable in 16S or other compositional data. The environmental variable is any variable which is measure for each microbiome sample, for example, a butyrate measurement paired with every sample in the data. Microbiome data is compositional, meaning that the total abundance of each sample sums to 1, and this introduces severe statistical distortions. This method takes a Bayesian approach to correcting for these statistical distortions, in which the total abundance is treated as an unknown variable. This package runs the python implementation using reticulate.",
    "version": "0.0.2.0",
    "maintainer": "Adrian Verster <adrian.verster@hc-sc.gc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 505,
    "package_name": "BRcal",
    "title": "Boldness-Recalibration of Binary Events",
    "description": "Boldness-recalibration maximally spreads out probability predictions while maintaining a user specified level of calibration, facilitated the brcal() function. Supporting functions to assess calibration via Bayesian and Frequentist approaches, Maximum Likelihood Estimator (MLE) recalibration, Linear in Log Odds (LLO)-adjust via any specified parameters, and visualize results are also provided. Methodological details can be found in Guthrie & Franck (2024) <doi:10.1080/00031305.2024.2339266>. ",
    "version": "1.0.1",
    "maintainer": "Adeline P. Guthrie <apguthrie47@gmail.com>",
    "url": "https://github.com/apguthrie/BRcal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 507,
    "package_name": "BSGW",
    "title": "Bayesian Survival Model with Lasso Shrinkage Using Generalized\nWeibull Regression",
    "description": "Bayesian survival model using Weibull regression on both scale and shape parameters. Dependence of shape parameter on covariates permits deviation from proportional-hazard assumption, leading to dynamic - i.e. non-constant with time - hazard ratios between subjects. Bayesian Lasso shrinkage in the form of two Laplace priors - one for scale and one for shape coefficients - allows for many covariates to be included. Cross-validation helper functions can be used to tune the shrinkage parameters. Monte Carlo Markov Chain (MCMC) sampling using a Gibbs wrapper around Radford Neal's univariate slice sampler (R package MfUSampler) is used for coefficient estimation.",
    "version": "0.9.4",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 524,
    "package_name": "BTSPAS",
    "title": "Bayesian Time-Stratified Population Analysis",
    "description": "Provides advanced Bayesian methods to estimate\n\t     abundance and run-timing from temporally-stratified\n\t     Petersen mark-recapture experiments. Methods include\n\t     hierarchical modelling of the capture probabilities\n  \t     and spline smoothing of the daily run size. Theory\n  \t     described in Bonner and Schwarz (2011)\n         <doi:10.1111/j.1541-0420.2011.01599.x>.",
    "version": "2024.11.1",
    "maintainer": "Carl J Schwarz <cschwarz.stat.sfu.ca@gmail.com>",
    "url": "https://github.com/cschwarz-stat-sfu-ca/BTSPAS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 525,
    "package_name": "BTSR",
    "title": "Bounded Time Series Regression",
    "description": "Simulate, estimate and forecast a wide range of regression\n    based dynamic models for bounded time series, covering the most\n    commonly applied models in the literature. The main calculations are\n    done in FORTRAN, which translates into very fast algorithms.",
    "version": "1.0.0",
    "maintainer": "Taiane Schaedler Prass <taianeprass@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 537,
    "package_name": "BVARverse",
    "title": "Tidy Bayesian Vector Autoregression",
    "description": "Functions to prepare tidy objects from estimated models via 'BVAR'\n    (see Kuschnig & Vashold, 2019 <doi:10.13140/RG.2.2.25541.60643>) and\n    visualisation thereof. Bridges the gap between estimating models with 'BVAR'\n    and plotting the results in a more sophisticated way with 'ggplot2' as well\n    as passing them on in a tidy format. ",
    "version": "0.0.1",
    "maintainer": "Lukas Vashold <lukas.vashold@wu.ac.at>",
    "url": "https://github.com/nk027/bvarverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 540,
    "package_name": "BaHZING",
    "title": "Bayesian Hierarchical Zero-Inflated Negative Binomial Regression\nwith G-Computation",
    "description": "A Bayesian model for examining the association between\n    environmental mixtures and all Taxa measured in a hierarchical\n    microbiome dataset in a single integrated analysis. Compared with\n    analyzing the associations of environmental mixtures with each Taxa\n    individually, 'BaHZING' controls Type 1 error rates and provides more\n    stable effect estimates when dealing with small sample sizes.",
    "version": "1.0.0",
    "maintainer": "Jesse Goodrich <jagoodri@usc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 541,
    "package_name": "BaM",
    "title": "Functions and Datasets for \"Bayesian Methods: A Social and\nBehavioral Sciences Approach\"",
    "description": "Functions and datasets for Jeff Gill: \"Bayesian Methods: A Social and Behavioral Sciences Approach\". First, Second, and Third Edition. Published by Chapman and Hall/CRC (2002, 2007, 2014) <doi:10.1201/b17888>.",
    "version": "1.0.3",
    "maintainer": "Jeff Gill <jgill5402@mac.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 542,
    "package_name": "BaSTA",
    "title": "Age-Specific Bayesian Survival Trajectory Analysis from\nIncomplete Census or Capture-Recapture/Recovery Data",
    "description": "Estimates survival and mortality with covariates from census or capture-recapture/recovery data in a Bayesian framework when many individuals are of unknown age. It includes tools for data checking, model diagnostics and outputs such as life-tables and plots, as described in Colchero, Jones, and Rebke (2012) <doi:10.1111/j.2041-210X.2012.00186.x> and Colchero et al. (2021) <doi:10.1038/s41467-021-23894-3>.",
    "version": "2.0.2",
    "maintainer": "Fernando Colchero <fernando_colchero@eva.mpg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 543,
    "package_name": "BaSkePro",
    "title": "Bayesian Model to Archaeological Faunal Skeletal Profiles",
    "description": "Tool to perform Bayesian inference of carcass processing/transport strategy and bone attrition from archaeofaunal skeletal profiles characterized by percentages of MAU (Minimum Anatomical Units). The approach is based on a generative model for skeletal profiles that replicates the two phases of formation of any faunal assemblage: initial accumulation as a function of human transport strategies and subsequent attrition.Two parameters define this model: 1) the transport preference (alpha), which can take any value between - 1 (mostly axial contribution) and 1 (mostly appendicular contribution) following strategies constructed as a function of butchering efficiency of different anatomical elements and the results of ethnographic studies, and 2) degree of attrition (beta), which can vary between 0 (no attrition) and 10 (maximum attrition) and relates the survivorship of bone elements to their maximum bone density. Starting from uniform prior probability distribution functions of alpha and beta, a Monte Carlo Markov Chain sampling based on a random walk Metropolis-Hasting algorithm is adopted to derive the posterior probability distribution functions, which are then available for interpretation. During this process, the likelihood of obtaining the observed percentages of MAU given a pair of parameter values is estimated by the inverse of the Chi2 statistic, multiplied by the proportion of elements within a 1 percent of the observed value. See Ana B. Marin-Arroyo, David Ocio (2018).<doi:10.1080/08912963.2017.1336620>.",
    "version": "1.1.1",
    "maintainer": "Marco Vidal-Cordasco <marcovidalcordasco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 557,
    "package_name": "BaseTempSeed",
    "title": "Estimation of Seed Germination Base Temperature in Thermal\nModelling",
    "description": "All the seeds do not germinate at a single point in time due to physiological mechanisms determined by temperature which vary among individual seeds in the population. Seeds germinate by following accumulation of thermal time in degree days/hours, quantified by multiplying the time of germination with excess of base temperature required by each seed for its germination, which follows log-normal distribution. The theoretical germination course can be obtained by regressing the rate of germination at various fractions against temperature (Garcia et al., 1982), where the fraction-wise regression lines intersect the temperature axis at base temperature and the methodology of determining optimum base temperature has been described by Ellis et al. (1987). This package helps to find the base temperature of seed germination using algorithms of Garcia et al. (1982) and Ellis et al. (1982) <doi:10.1093/JXB/38.6.1033> <doi:10.1093/jxb/33.2.288>.",
    "version": "0.1.0",
    "maintainer": "Dr. Himadri Ghosh <hghosh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 560,
    "package_name": "BasketTrial",
    "title": "Bayesian Basket Trial Design and Analysis",
    "description": "Provides tools for Bayesian basket trial design and analysis using a novel three-component local power prior framework with global borrowing control, pairwise similarity assessment and a borrowing threshold. Supports simulation-based evaluation of operating characteristics and comparison with other methods. Applicable to both equal and unequal sample size settings in early-phase oncology trials. For more details see Zhou et al. (2023) <doi:10.48550/arXiv.2312.15352>.",
    "version": "0.1.0",
    "maintainer": "Haiming Zhou <haiming2019@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 566,
    "package_name": "BayLum",
    "title": "Chronological Bayesian Models Integrating Optically Stimulated\nLuminescence and Radiocarbon Age Dating",
    "description": "Bayesian analysis of luminescence data and C-14 age\n    estimates. Bayesian models are based on the following publications:\n    Combes, B. & Philippe, A. (2017) <doi:10.1016/j.quageo.2017.02.003>\n    and Combes et al. (2015) <doi:10.1016/j.quageo.2015.04.001>. This\n    includes, amongst others, data import, export, application of age\n    models and palaeodose model.",
    "version": "0.3.3",
    "maintainer": "Anne Philippe <anne.philippe@univ-nantes.fr>",
    "url": "https://CRAN.r-project.org/package=BayLum,\nhttps://crp2a.github.io/BayLum/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 567,
    "package_name": "Bayenet",
    "title": "Robust Bayesian Elastic Net",
    "description": "As heavy-tailed error distribution and outliers in the response variable widely exist, models which are robust to data contamination are highly demanded. Here, we develop a novel robust Bayesian variable selection method with elastic net penalty. In particular, the spike-and-slab priors have been incorporated to impose sparsity. An efficient Gibbs sampler has been developed to facilitate computation.The core modules of the package have been developed in 'C++' and R.",
    "version": "0.3",
    "maintainer": "Xi Lu <xilu@ksu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 568,
    "package_name": "BayesARIMAX",
    "title": "Bayesian Estimation of ARIMAX Model",
    "description": "The Autoregressive Integrated Moving Average (ARIMA) model is very popular univariate time series model. Its application has been widened by the incorporation of exogenous variable(s) (X) in the model and modified as ARIMAX by Bierens (1987) <doi:10.1016/0304-4076(87)90086-8>. In this package we estimate the ARIMAX model using Bayesian framework. ",
    "version": "0.1.1",
    "maintainer": "Achal Lama <achal.lama@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 569,
    "package_name": "BayesAT",
    "title": "Bayesian Adaptive Trial",
    "description": "Bayesian adaptive trial algorithm implements multiple-stage interim analysis. Package includes data generating function, and Bayesian hypothesis testing function.",
    "version": "0.1.0",
    "maintainer": "Yuan Zhong <aqua.zhong@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 571,
    "package_name": "BayesBP",
    "title": "Bayesian Estimation using Bernstein Polynomial Fits Rate Matrix",
    "description": "Smoothed lexis diagrams with Bayesian method specifically tailored to cancer \n             incidence data. Providing to calculating slope and constructing credible interval.\n             LC Chien et al. (2015) <doi:10.1080/01621459.2015.1042106>. \n             LH Chien et al. (2017) <doi:10.1002/cam4.1102>.",
    "version": "1.1",
    "maintainer": "Li-Syuan Hong <lisyuan@nhri.org.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 572,
    "package_name": "BayesBinMix",
    "title": "Bayesian Estimation of Mixtures of Multivariate Bernoulli\nDistributions",
    "description": "Fully Bayesian inference for estimating the number of clusters and related parameters to heterogeneous binary data.",
    "version": "1.4.1",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 574,
    "package_name": "BayesCACE",
    "title": "Bayesian Model for CACE Analysis",
    "description": "Performs CACE (Complier Average Causal Effect analysis) on either a single study or meta-analysis of datasets with binary outcomes, using either complete or incomplete noncompliance information. Our package implements the Bayesian methods proposed in Zhou et al. (2019) <doi:10.1111/biom.13028>, which introduces a Bayesian hierarchical model for estimating CACE in meta-analysis of clinical trials with noncompliance, and Zhou et al. (2021) <doi:10.1080/01621459.2021.1900859>, with an application example on Epidural Analgesia.",
    "version": "1.2.3",
    "maintainer": "Jinhui Yang <james.yangjinhui@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 576,
    "package_name": "BayesCR",
    "title": "Bayesian Analysis of Censored Regression Models Under Scale\nMixture of Skew Normal Distributions",
    "description": "Propose a parametric fit for censored linear regression models based on SMSN distributions, from a Bayesian perspective. Also, generates SMSN random variables.",
    "version": "2.1",
    "maintainer": "Aldo M. Garay <medina_garay@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 577,
    "package_name": "BayesCTDesign",
    "title": "Two Arm Bayesian Clinical Trial Design with and Without\nHistorical Control Data",
    "description": "A set of functions to help clinical trial researchers calculate power and sample size for two-arm Bayesian randomized clinical trials that do or do not incorporate historical control data.  At some point during the design process, a clinical trial researcher who is designing a basic two-arm Bayesian randomized clinical trial needs to make decisions about power and sample size within the context of hypothesized treatment effects.  Through simulation, the simple_sim() function will estimate power and other user specified clinical trial characteristics at user specified sample sizes given user defined scenarios about treatment effect,control group characteristics, and outcome.  If the clinical trial researcher has access to historical control data, then the researcher can design a two-arm Bayesian randomized clinical trial that incorporates the historical data.  In such a case, the researcher needs to work through the potential consequences of historical and randomized control differences on trial characteristics, in addition to working through issues regarding power in the context of sample size, treatment effect size, and outcome.  If a researcher designs a clinical trial that will incorporate historical control data, the researcher needs the randomized controls to be from the same population as the historical controls.  What if this is not the case when the designed trial is implemented?  During the design phase, the researcher needs to investigate the negative effects of possible historic/randomized control differences on power, type one error, and other trial characteristics.  Using this information, the researcher should design the trial to mitigate these negative effects.  Through simulation, the historic_sim() function will estimate power and other user specified clinical trial characteristics at user specified sample sizes given user defined scenarios about historical and randomized control differences as well as treatment effects and outcomes.  The results from historic_sim() and simple_sim() can be printed with print_table() and graphed with plot_table() methods.  Outcomes considered are Gaussian, Poisson, Bernoulli, Lognormal, Weibull, and Piecewise Exponential.  The methods are described in Eggleston et al. (2021) <doi:10.18637/jss.v100.i21>.  ",
    "version": "0.6.1",
    "maintainer": "Barry Eggleston <beggleston@rti.org>",
    "url": "https://github.com/begglest/BayesCTDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 579,
    "package_name": "BayesChange",
    "title": "Bayesian Methods for Change Points Analysis",
    "description": "Perform change points detection on univariate and multivariate time series according to the methods presented by Asael Fabian Martínez and Ramsés H. Mena (2014) <doi:10.1214/14-BA878> and Corradin, Danese and Ongaro (2022) <doi:10.1016/j.ijar.2021.12.019>. It also clusters different types of time dependent data with common change points, see \"Model-based clustering of time-dependent observations with common structural changes\" (Corradin,Danese,KhudaBukhsh and Ongaro, 2024) <doi:10.48550/arXiv.2410.09552> for details. ",
    "version": "2.1.3",
    "maintainer": "Luca Danese <l.danese1@campus.unimib.it>",
    "url": "https://github.com/lucadanese/BayesChange",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 580,
    "package_name": "BayesCombo",
    "title": "Bayesian Evidence Combination",
    "description": "Combine diverse evidence across multiple studies to test a high level scientific theory. The methods can also be used as an alternative to a standard meta-analysis.",
    "version": "1.0",
    "maintainer": "Stanley E. Lazic <stan.lazic@cantab.net>",
    "url": "https://github.com/stanlazic/BayesCombo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 581,
    "package_name": "BayesDA",
    "title": "Functions and Datasets for the book \"Bayesian Data Analysis\"",
    "description": "Functions for Bayesian Data Analysis, with datasets from\n        the book \"Bayesian data Analysis (second edition)\" by Gelman,\n        Carlin, Stern and Rubin. Not all datasets yet, hopefully\n        completed soon.",
    "version": "2012.04-1",
    "maintainer": "Kjetil Halvorsen <kjetil1001@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 582,
    "package_name": "BayesDIP",
    "title": "Bayesian Decreasingly Informative Priors for Early Termination\nPhase II Trials",
    "description": "Provide early termination phase II trial designs with a\n    decreasingly informative prior (DIP) or a regular Bayesian prior\n    chosen by the user. The program can determine the minimum planned\n    sample size necessary to achieve the user-specified admissible\n    designs. The program can also perform power and expected sample size\n    calculations for the tests in early termination Phase II trials.\n    See Wang C and Sabo RT (2022) <doi:10.18203/2349-3259.ijct20221110>;\n    Sabo RT (2014) <doi:10.1080/10543406.2014.888441>.",
    "version": "0.1.1",
    "maintainer": "Chen Wang <wangc10@vcu.edu>",
    "url": "<https://github.com/chenw10/BayesDIP>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 583,
    "package_name": "BayesDLMfMRI",
    "title": "Statistical Analysis for Task-Based Fmri Data",
    "description": "The 'BayesDLMfMRI' package performs statistical analysis for task-based functional magnetic resonance imaging (fMRI) data at both individual and group levels. The analysis to detect brain activation at the individual level is based on modeling the fMRI signal using Matrix-Variate Dynamic Linear Models (MDLM). The analysis for the group stage is based on posterior distributions of the state parameter obtained from the modeling at the individual level. In this way, this package offers several R functions with different algorithms to perform inference on the state parameter to assess brain activation for both individual and group stages. Those functions allow for parallel computation when the analysis is performed for the entire brain as well as analysis at specific voxels when it is required.\n             References: Cardona-Jiménez (2021) <doi:10.1016/j.csda.2021.107297>;\n             Cardona-Jiménez (2021) <arXiv:2111.01318>.",
    "version": "0.0.3",
    "maintainer": "Carlos Pérez <caaperezag@unal.edu.co>",
    "url": "https://github.com/JohnatanLAB/BayesDLMfMRI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 584,
    "package_name": "BayesDecon",
    "title": "Density Deconvolution Using Bayesian Semiparametric Methods",
    "description": "Estimates the density of a variable in a measurement error setup, potentially with an excess of zero values. For more details see Sarkar (2022) <doi:10.1080/01621459.2020.1782220>.",
    "version": "0.1.4",
    "maintainer": "Mainak Manna <mainakmanna29@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 585,
    "package_name": "BayesDesign",
    "title": "Bayesian Single-Arm Design with Survival Endpoints",
    "description": "The proposed event-driven approach for Bayesian two-stage single-arm phase II trial design is a novel clinical trial design and can be regarded as an extension of the Simon’s two-stage design with the time-to-event endpoint. This design is motivated by cancer clinical trials with immunotherapy and molecularly targeted therapy, in which time-to-event endpoint is often a desired endpoint.",
    "version": "0.1.1",
    "maintainer": "Chia-Wei Hsu <Chia-Wei.Hsu@stjude.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 586,
    "package_name": "BayesDissolution",
    "title": "Bayesian Models for Dissolution Testing",
    "description": "Fits Bayesian models (amongst others) to dissolution data sets that can be used for dissolution testing. The package was originally constructed to include only the Bayesian models outlined in Pourmohamad et al. (2022) <doi:10.1111/rssc.12535>. However, additional Bayesian and non-Bayesian models (based on bootstrapping and generalized pivotal quanties) have also been added. More models may be added over time.",
    "version": "0.2.1",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 588,
    "package_name": "BayesESS",
    "title": "Determining Effective Sample Size",
    "description": "Determines effective sample size of a parametric prior distribution\n    in Bayesian models. For a web-based Shiny application related to this package, see <https://implement.shinyapps.io/bayesess/>. ",
    "version": "0.1.19",
    "maintainer": "Jaejoon Song <jaejoonsong@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 589,
    "package_name": "BayesFBHborrow",
    "title": "Bayesian Dynamic Borrowing with Flexible Baseline Hazard\nFunction",
    "description": "Allows Bayesian borrowing from a historical dataset for time-to-\n    event data. A flexible baseline hazard function is achieved via a piecewise\n    exponential likelihood with time varying split points and smoothing prior on the\n    historic baseline hazards. The method is described in Scott and Lewin (2024) \n    <doi:10.48550/arXiv.2401.06082>, and the software paper is in Axillus et al. \n    (2024) <doi:10.48550/arXiv.2408.04327>.",
    "version": "2.0.2",
    "maintainer": "Darren Scott <darren.scott@astrazeneca.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 590,
    "package_name": "BayesFM",
    "title": "Bayesian Inference for Factor Modeling",
    "description": "Collection of procedures to perform Bayesian analysis on a variety\n    of factor models. Currently, it includes: \"Bayesian Exploratory Factor \n    Analysis\" (befa) from G. Conti, S. Frühwirth-Schnatter, J.J. Heckman, \n    R. Piatek (2014) <doi:10.1016/j.jeconom.2014.06.008>, an approach to \n    dedicated factor analysis with stochastic search on the structure of the \n    factor loading matrix. The number of latent factors, as well as the \n    allocation of the manifest variables to the factors, are not fixed a priori \n    but determined during MCMC sampling.",
    "version": "0.1.7",
    "maintainer": "Rémi Piatek <remi.piatek@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 591,
    "package_name": "BayesFactor",
    "title": "Computation of Bayes Factors for Common Designs",
    "description": "A suite of functions for computing\n    various Bayes factors for simple designs, including contingency tables,\n    one- and two-sample designs, one-way designs, general ANOVA designs, and\n    linear regression.",
    "version": "0.9.12-4.7",
    "maintainer": "Richard D. Morey <richarddmorey@gmail.com>",
    "url": "https://richarddmorey.github.io/BayesFactor/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 593,
    "package_name": "BayesGOF",
    "title": "Bayesian Modeling via Frequentist Goodness-of-Fit",
    "description": "A Bayesian data modeling scheme that performs four interconnected tasks: (i) characterizes the uncertainty of the elicited parametric prior; (ii) provides exploratory diagnostic for checking prior-data conflict; (iii) computes the final statistical prior density estimate; and (iv) executes macro- and micro-inference. Primary reference is Mukhopadhyay, S. and Fletcher, D. 2018 paper \"Generalized Empirical Bayes via Frequentist Goodness of Fit\" (<https://www.nature.com/articles/s41598-018-28130-5 >). ",
    "version": "5.2",
    "maintainer": "Doug Fletcher <tug25070@temple.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 594,
    "package_name": "BayesGP",
    "title": "Efficient Implementation of Gaussian Process in Bayesian\nHierarchical Models",
    "description": "Implements Bayesian hierarchical models with flexible Gaussian process priors, focusing on Extended Latent Gaussian Models and incorporating various Gaussian process priors for Bayesian smoothing. Computations leverage finite element approximations and adaptive quadrature for efficient inference. Methods are detailed in Zhang, Stringer, Brown, and Stafford (2023) <doi:10.1177/09622802221134172>; Zhang, Stringer, Brown, and Stafford (2024) <doi:10.1080/10618600.2023.2289532>; Zhang, Brown, and Stafford (2023) <doi:10.48550/arXiv.2305.09914>; and Stringer, Brown, and Stafford (2021) <doi:10.1111/biom.13329>.",
    "version": "0.1.3",
    "maintainer": "Ziang Zhang <ziangzhang@uchicago.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 595,
    "package_name": "BayesGPfit",
    "title": "Fast Bayesian Gaussian Process Regression Fitting",
    "description": "Bayesian inferences on nonparametric regression via Gaussian Processes with a modified exponential square kernel using a basis expansion approach.",
    "version": "1.1.0",
    "maintainer": "Jian Kang <jiankang@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 596,
    "package_name": "BayesGWQS",
    "title": "Bayesian Grouped Weighted Quantile Sum Regression",
    "description": "Fits Bayesian grouped weighted quantile sum (BGWQS) regressions for one or more chemical groups with binary outcomes. Wheeler DC et al. (2019) <doi:10.1016/j.sste.2019.100286>.",
    "version": "0.1.1",
    "maintainer": "Matthew Carli <carlimm@mymail.vcu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 597,
    "package_name": "BayesGmed",
    "title": "Bayesian Causal Mediation Analysis using 'Stan'",
    "description": "Performs parametric mediation analysis using the Bayesian g-formula approach for binary and continuous outcomes. The methodology is based on Comment (2018) <doi:10.5281/zenodo.1285275> and a demonstration of its application can be found at Yimer et al. (2022) <doi:10.48550/arXiv.2210.08499>. ",
    "version": "0.0.3",
    "maintainer": "Belay Birlie Yimer <belaybirlie.yimer@manchester.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 599,
    "package_name": "BayesHMM",
    "title": "Full Bayesian Inference for Hidden Markov Models",
    "description": "An R Package to run full Bayesian inference on Hidden Markov Models (HMM) using the probabilistic programming language Stan. The software enables users to fit HMM with time-homogeneous transitions as well as time-varying transition probabilities. Priors can be set for every model parameter. Implemented inference algorithms include forward (filtering), forward-backwards (smoothing), Viterbi (most likely hidden path), prior predictive sampling, and posterior predictive sampling. Graphs, tables and other convenience methods for convergence diagnosis, goodness of fit, and data analysis are provided.",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/luisdamiano/BayesHMM",
    "exports": [],
    "topics": ["bayesian", "bayesian-inference", "hidden-markov-models", "hmm", "machine-learning", "mcmc", "priors", "r", "rstats", "statistical-learning", "statistics", "time-series"],
    "score": "NA",
    "stars": 43
  },
  {
    "id": 601,
    "package_name": "BayesLCA",
    "title": "Bayesian Latent Class Analysis",
    "description": "Bayesian Latent Class Analysis using several different\n        methods.",
    "version": "1.9",
    "maintainer": "Arthur White <arwhite@tcd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 602,
    "package_name": "BayesLN",
    "title": "Bayesian Inference for Log-Normal Data",
    "description": "Bayesian inference under log-normality assumption must be performed very carefully. In fact, under the common priors for the variance, useful quantities in the original data scale (like mean and quantiles) do not have posterior moments that are finite (Fabrizi et al. 2012 <doi:10.1214/12-BA733>). This package allows to easily carry out a proper Bayesian inferential procedure by fixing a suitable distribution (the generalized inverse Gaussian) as prior for the variance. Functions to estimate several kind of means (unconditional, conditional and conditional under a mixed model) and quantiles (unconditional and conditional) are provided. ",
    "version": "0.2.12",
    "maintainer": "Aldo Gardini <aldo.gardini2@unibo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 603,
    "package_name": "BayesLogit",
    "title": "PolyaGamma Sampling",
    "description": "Tools for sampling from the PolyaGamma distribution based on Polson, Scott, and Windle (2013) <doi:10.1080/01621459.2013.829001>.  Useful for logistic regression.",
    "version": "2.1",
    "maintainer": "Jesse Windle <jesse.windle@gmail.com>",
    "url": "https://github.com/jwindle/BayesLogit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 605,
    "package_name": "BayesMallows",
    "title": "Bayesian Preference Learning with the Mallows Rank Model",
    "description": "An implementation of the Bayesian version of the Mallows rank model\n    (Vitelli et al., Journal of Machine Learning Research, 2018 <https://jmlr.org/papers/v18/15-481.html>;\n    Crispino et al., Annals of Applied Statistics, 2019 <doi:10.1214/18-AOAS1203>;\n    Sorensen et al., R Journal, 2020 <doi:10.32614/RJ-2020-026>;\n    Stein, PhD Thesis, 2023 <https://eprints.lancs.ac.uk/id/eprint/195759>). Both Metropolis-Hastings\n    and sequential Monte Carlo algorithms for estimating the models are available. Cayley, footrule,\n    Hamming, Kendall, Spearman, and Ulam distances are supported in the models. The rank data to be\n    analyzed can be in the form of complete rankings, top-k rankings, partially missing rankings, as well\n    as consistent and inconsistent pairwise preferences. Several functions for plotting and studying the\n    posterior distributions of parameters are provided. The package also provides functions for estimating\n    the partition function (normalizing constant) of the Mallows rank model, both with the importance\n    sampling algorithm of Vitelli et al. and asymptotic approximation with the IPFP algorithm\n    (Mukherjee, Annals of Statistics, 2016 <doi:10.1214/15-AOS1389>).",
    "version": "2.2.6",
    "maintainer": "Oystein Sorensen <oystein.sorensen.1985@gmail.com>",
    "url": "https://github.com/ocbe-uio/BayesMallows,\nhttps://ocbe-uio.github.io/BayesMallows/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 606,
    "package_name": "BayesMixSurv",
    "title": "Bayesian Mixture Survival Models using Additive\nMixture-of-Weibull Hazards, with Lasso Shrinkage and\nStratification",
    "description": "Bayesian Mixture Survival Models using Additive Mixture-of-Weibull Hazards, with Lasso Shrinkage and\n        Stratification. As a Bayesian dynamic survival model, it relaxes the proportional-hazard assumption. Lasso shrinkage controls\n        overfitting, given the increase in the number of free parameters in the model due to presence of two Weibull components\n        in the hazard function.",
    "version": "0.9.3",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 607,
    "package_name": "BayesMoFo",
    "title": "Bayesian Mortality Forecasting",
    "description": "Carry out Bayesian estimation and forecasting for a variety of stochastic mortality models using vague prior distributions. Models supported include numerous well-established approaches introduced in the actuarial and demographic literature, such as the Lee-Carter (1992) <doi:10.1080/01621459.1992.10475265>, the Cairns-Blake-Dowd (2009) <doi:10.1080/10920277.2009.10597538>, the Li-Lee (2005) <doi:10.1353/dem.2005.0021>, and the Plat (2009) <doi:10.1016/j.insmatheco.2009.08.006> models. The package is designed to analyse stratified mortality data structured as a 3-dimensional array of dimensions p × A × T (strata × age × year). Stratification can represent factors such as cause of death, country, deprivation level, sex, geographic region, insurance product, marital status, socioeconomic group, or smoking behavior. While the primary focus is on analysing stratified data (p > 1), the package can also handle mortality data that are not stratified (p = 1). Model selection via the Deviance Information Criterion (DIC) is supported.",
    "version": "0.1.0",
    "maintainer": "Jackie Siaw Tze Wong <jw19203@essex.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 610,
    "package_name": "BayesMultiMode",
    "title": "Bayesian Mode Inference",
    "description": "A two-step Bayesian approach for mode inference following \n      Cross, Hoogerheide, Labonne and van Dijk (2024) <doi:10.1016/j.econlet.2024.111579>).\n      First, a mixture distribution is fitted on the data using a sparse finite\n      mixture (SFM) Markov chain Monte Carlo (MCMC) algorithm. The number of\n      mixture components does not have to be known; the size of the mixture is\n      estimated endogenously through the SFM approach. Second, the modes of the\n      estimated mixture at each MCMC draw are retrieved using algorithms\n      specifically tailored for mode detection. These estimates are then used to\n      construct posterior probabilities for the number of modes, their locations\n      and uncertainties, providing a powerful tool for mode inference.",
    "version": "0.7.4",
    "maintainer": "Paul Labonne <labonnepaul@gmail.com>",
    "url": "https://github.com/paullabonne/BayesMultiMode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 612,
    "package_name": "BayesNetBP",
    "title": "Bayesian Network Belief Propagation",
    "description": "Belief propagation methods in Bayesian Networks to propagate evidence through the network. The implementation of these methods are based on the article: Cowell, RG (2005). Local Propagation in Conditional Gaussian Bayesian Networks <https://www.jmlr.org/papers/v6/cowell05a.html>. For details please see Yu et. al. (2020) BayesNetBP: An R Package for Probabilistic Reasoning in Bayesian Networks <doi:10.18637/jss.v094.i03>. The optional 'cyjShiny' package for running the Shiny app is available at <https://github.com/cytoscape/cyjShiny>. Please see the example in the documentation of 'runBayesNetApp' function for installing 'cyjShiny' package from GitHub. ",
    "version": "1.6.1",
    "maintainer": "Han Yu <hyu9@buffalo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 614,
    "package_name": "BayesPPD",
    "title": "Bayesian Power Prior Design",
    "description": "Bayesian power/type I error calculation and model fitting using \n  the power prior and the normalized power prior for generalized linear models.\n  Detailed examples of applying the package are available at <doi:10.32614/RJ-2023-016>.\n  Models for time-to-event outcomes are implemented in the R package 'BayesPPDSurv'.\n  The Bayesian clinical trial design methodology is described in Chen et al. (2011) \n  <doi:10.1111/j.1541-0420.2011.01561.x>, and Psioda and Ibrahim (2019) \n  <doi:10.1093/biostatistics/kxy009>. The normalized power prior is described in Duan et al. (2006) \n  <doi:10.1002/env.752> and Ibrahim et al. (2015) <doi:10.1002/sim.6728>. ",
    "version": "1.1.3",
    "maintainer": "Yueqi Shen <angieshen6@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 615,
    "package_name": "BayesPPDSurv",
    "title": "Bayesian Power Prior Design for Survival Data",
    "description": "Bayesian power/type I error calculation and model fitting using \n  the power prior and the normalized power prior for proportional hazards models\n  with piecewise constant hazard. The methodology and examples of \n  applying the package are detailed in <doi:10.48550/arXiv.2404.05118>.\n  The Bayesian clinical trial design methodology is described in \n  Chen et al. (2011) <doi:10.1111/j.1541-0420.2011.01561.x>, \n  and Psioda and Ibrahim (2019) <doi:10.1093/biostatistics/kxy009>. \n  The proportional hazards model with piecewise constant hazard is detailed in \n  Ibrahim et al. (2001) <doi:10.1007/978-1-4757-3447-8>. ",
    "version": "1.0.3",
    "maintainer": "Yueqi Shen <ys137@live.unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 617,
    "package_name": "BayesPostEst",
    "title": "Generate Postestimation Quantities for Bayesian MCMC Estimation",
    "description": "An implementation of functions to generate and plot postestimation quantities after estimating Bayesian regression models using Markov chain Monte Carlo (MCMC). Functionality includes the estimation of the Precision-Recall curves (see Beger, 2016 <doi:10.2139/ssrn.2765419>), the implementation of the observed values method of calculating predicted probabilities by Hanmer and Kalkan (2013) <doi:10.1111/j.1540-5907.2012.00602.x>, the implementation of the average value method of calculating predicted probabilities (see King, Tomz, and Wittenberg, 2000 <doi:10.2307/2669316>), and the generation and plotting of first differences to summarize typical effects across covariates (see Long 1997, ISBN:9780803973749; King, Tomz, and Wittenberg, 2000 <doi:10.2307/2669316>). This package can be used with MCMC output generated by any Bayesian estimation tool including 'JAGS', 'BUGS', 'MCMCpack', and 'Stan'.",
    "version": "0.4.0",
    "maintainer": "Shana Scogin <shanarscogin@gmail.com>",
    "url": "https://github.com/ShanaScogin/BayesPostEst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 618,
    "package_name": "BayesPower",
    "title": "Sample Size and Power Calculation for Bayesian Testing with\nBayes Factor",
    "description": "The goal of 'BayesPower' is to provide tools for Bayesian sample size determination and power analysis across a range of common hypothesis testing scenarios using Bayes factors. The main function, BayesPower_BayesFactor(), launches an interactive 'shiny' application for performing these analyses. The application also provides command-line code for reproducibility. Details of the methods are described in the tutorial by Wong, Pawel, and Tendeiro (2025) <doi:10.31234/osf.io/pgdac_v1>.",
    "version": "1.0.1",
    "maintainer": "Tsz Keung Wong <t.k.wong3004@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 620,
    "package_name": "BayesRS",
    "title": "Bayes Factors for Hierarchical Linear Models with Continuous\nPredictors",
    "description": "Runs hierarchical linear Bayesian models. Samples from the posterior\n    distributions of model parameters in JAGS (Just Another Gibbs Sampler;\n\tPlummer, 2017, <http://mcmc-jags.sourceforge.net>). Computes Bayes factors for group\n\tparameters of interest with the Savage-Dickey density ratio (Wetzels,\n\tRaaijmakers, Jakab, Wagenmakers, 2009, <doi:10.3758/PBR.16.4.752>).",
    "version": "0.1.3",
    "maintainer": "Mirko Thalmann <mirkothalmann@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 621,
    "package_name": "BayesRegDTR",
    "title": "Bayesian Regression for Dynamic Treatment Regimes",
    "description": "Methods to estimate optimal dynamic treatment regimes using Bayesian\n    likelihood-based regression approach as described in \n    Yu, W., & Bondell, H. D. (2023) <doi:10.1093/jrsssb/qkad016>\n    Uses backward induction and dynamic programming theory for computing\n    expected values. Offers options for future parallel computing.",
    "version": "1.1.2",
    "maintainer": "Weichang Yu <weichang.yu@unimelb.edu.au>",
    "url": "https://github.com/jlimrasc/BayesRegDTR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 622,
    "package_name": "BayesRep",
    "title": "Bayesian Analysis of Replication Studies",
    "description": "Provides tools for the analysis of replication studies using Bayes factors (Pawel and Held, 2022) <doi:10.1111/rssb.12491>.",
    "version": "0.42.2",
    "maintainer": "Samuel Pawel <samuel.pawel@uzh.ch>",
    "url": "https://github.com/SamCH93/BayesRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 623,
    "package_name": "BayesRepDesign",
    "title": "Bayesian Design of Replication Studies",
    "description": "Provides functionality for determining the sample size of replication studies using Bayesian design approaches in the normal-normal hierarchical model (Pawel et al., 2022) <doi:10.48550/arXiv.2211.02552>.",
    "version": "0.42",
    "maintainer": "Samuel Pawel <samuel.pawel@uzh.ch>",
    "url": "https://github.com/SamCH93/BayesRepDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 624,
    "package_name": "BayesS5",
    "title": "Bayesian Variable Selection Using Simplified Shotgun Stochastic\nSearch with Screening (S5)",
    "description": "In p >> n settings, full posterior sampling using existing Markov chain Monte\n    Carlo (MCMC) algorithms is highly inefficient and often not feasible from a practical\n    perspective. To overcome this problem, we propose a scalable stochastic search algorithm that is called the Simplified Shotgun Stochastic Search (S5) and aimed at rapidly explore interesting regions of model space and finding the maximum a posteriori(MAP) model. Also, the S5 provides an approximation of posterior probability of each model (including the marginal inclusion probabilities). This algorithm is a part of an article titled \"Scalable Bayesian Variable Selection Using Nonlocal Prior Densities in Ultrahigh-dimensional Settings\" (2018) by Minsuk Shin, Anirban Bhattacharya, and Valen E. Johnson and \"Nonlocal Functional Priors for Nonparametric Hypothesis Testing and High-dimensional Model Selection\" (2020+) by Minsuk Shin and Anirban Bhattacharya. ",
    "version": "1.41",
    "maintainer": "Minsuk Shin <minsuk000@gmail.com>",
    "url": "https://arxiv.org/abs/1507.07106v4",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 625,
    "package_name": "BayesSIM",
    "title": "Integrated Interface of Bayesian Single Index Models using\n'nimble'",
    "description": "Provides tools for fitting Bayesian single index models with\n    flexible choices of priors for both the index and the link function.\n    The package implements model estimation and posterior inference using\n    efficient MCMC algorithms built on the 'nimble' framework, allowing\n    users to specify, extend, and simulate models in a unified and\n    reproducible manner. The following methods are implemented in the\n    package: Antoniadis et al. (2004)\n    <https://www.jstor.org/stable/24307224>, Wang (2009)\n    <doi:10.1016/j.csda.2008.12.010>, Choi et al. (2011)\n    <doi:10.1080/10485251003768019>, Dhara et al. (2019)\n    <doi:10.1214/19-BA1170>, McGee et al. (2023) <doi:10.1111/biom.13569>.",
    "version": "1.0.1",
    "maintainer": "Seowoo Jung <jsw1347@ewha.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 626,
    "package_name": "BayesSUR",
    "title": "Bayesian Seemingly Unrelated Regression Models in\nHigh-Dimensional Settings",
    "description": "Bayesian seemingly unrelated regression with general variable selection and dense/sparse covariance matrix. The sparse seemingly unrelated regression is described in Bottolo et al. (2021) <doi:10.1111/rssc.12490>, the software paper is in Zhao et al. (2021) <doi:10.18637/jss.v100.i11>, and the model with random effects is described in Zhao et al. (2024) <doi:10.1093/jrsssc/qlad102>.",
    "version": "2.3-1",
    "maintainer": "Zhi Zhao <zhi.zhao@medisin.uio.no>",
    "url": "https://github.com/mbant/BayesSUR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 628,
    "package_name": "BayesSenMC",
    "title": "Different Models of Posterior Distributions of Adjusted Odds\nRatio",
    "description": "Generates different posterior distributions of adjusted odds ratio under different priors of sensitivity and specificity, and plots the models for comparison. It also provides estimations for the specifications of the models using diagnostics of exposure status with a non-linear mixed effects model. It implements the methods that are first proposed in <doi:10.1016/j.annepidem.2006.04.001> and <doi:10.1177/0272989X09353452>.",
    "version": "0.1.5",
    "maintainer": "Jinhui Yang <james.yangjinhui@gmail.com>",
    "url": "https://github.com/formidify/BayesSenMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 630,
    "package_name": "BayesSurvival",
    "title": "Bayesian Survival Analysis for Right Censored Data",
    "description": "Performs unadjusted Bayesian survival analysis for right censored time-to-event data. The main function, BayesSurv(), computes the posterior mean and a credible band for the survival function and for the cumulative hazard, as well as the posterior mean for the hazard, starting from a piecewise exponential (histogram) prior with Gamma distributed heights that are either independent, or have a Markovian dependence structure.\n    A function, PlotBayesSurv(), is provided to easily create plots of the posterior means of the hazard, cumulative hazard and survival function, with a credible band accompanying the latter two.\n    The priors and samplers are described in more detail in Castillo and Van der Pas (2020) \"Multiscale Bayesian survival analysis\" <arXiv:2005.02889>. In that paper it is also shown that the credible bands for the survival function and the cumulative hazard can be considered confidence bands (under mild conditions) and thus offer reliable uncertainty quantification.",
    "version": "0.2.0",
    "maintainer": "Stephanie van der Pas <s.l.vanderpas@amsterdamumc.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 633,
    "package_name": "BayesTree",
    "title": "Bayesian Additive Regression Trees",
    "description": "This is an implementation of BART:Bayesian Additive Regression Trees,\n        by Chipman, George, McCulloch (2010).",
    "version": "0.3-1.5",
    "maintainer": "Robert McCulloch <robert.e.mcculloch@gmail.com>",
    "url": "https://www.r-project.org, https://www.rob-mcculloch.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 634,
    "package_name": "BayesTreePrior",
    "title": "Bayesian Tree Prior Simulation",
    "description": "Provides a way to simulate from the prior distribution of Bayesian trees by Chipman et al. (1998) <DOI:10.2307/2669832>. The prior distribution of Bayesian trees is highly dependent on the design matrix X, therefore using the suggested hyperparameters by Chipman et al. (1998) <DOI:10.2307/2669832> is not recommended and could lead to unexpected prior distribution. This work is part of my master thesis (expected 2016).",
    "version": "1.0.1",
    "maintainer": "Alexia Jolicoeur-Martineau <alexia.jolicoeur-martineau@mail.mcgill.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 635,
    "package_name": "BayesTwin",
    "title": "Bayesian Analysis of Item-Level Twin Data",
    "description": "Bayesian analysis of item-level hierarchical twin data using an integrated item response theory model. Analyses are based on Schwabe & van den Berg (2014) <doi:10.1007/s10519-014-9649-7>, Molenaar & Dolan (2014) <doi:10.1007/s10519-014-9647-9>, Schwabe, Jonker & van den Berg (2016) <doi:10.1007/s10519-015-9768-9> and Schwabe, Boomsma & van den Berg (2016) <doi:10.1016/j.lindif.2017.01.018>.",
    "version": "1.0",
    "maintainer": "Inga Schwabe <bayestwin@gmail.com>",
    "url": "http://www.ingaschwabe.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 636,
    "package_name": "BayesVarSel",
    "title": "Bayes Factors, Model Choice and Variable Selection in Linear\nModels",
    "description": "Bayes factors and posterior probabilities in Linear models, \n    aimed at provide a formal Bayesian answer to testing and variable \n    selection problems. ",
    "version": "2.4.5",
    "maintainer": "Gonzalo Garcia-Donato <gonzalo.garciadonato@uclm.es>",
    "url": "https://github.com/comodin19/BayesVarSel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 637,
    "package_name": "BayesX",
    "title": "R Utilities Accompanying the Software Package BayesX",
    "description": "Functions for exploring and visualising estimation results\n  obtained with BayesX, a free software for estimating structured additive\n  regression models (<https://www.uni-goettingen.de/de/bayesx/550513.html>). In addition,\n  functions that allow to read, write and manipulate map objects that are required in spatial\n  analyses performed with BayesX.",
    "version": "0.3-3",
    "maintainer": "Nikolaus Umlauf <Nikolaus.Umlauf@uibk.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 638,
    "package_name": "BayesXsrc",
    "title": "Distribution of the 'BayesX' C++ Sources",
    "description": "'BayesX' performs Bayesian inference in structured additive regression (STAR) models.\n\tThe R package BayesXsrc provides the 'BayesX' command line tool for easy installation.\n\tA convenient R interface is provided in package R2BayesX.",
    "version": "3.0-7",
    "maintainer": "Nikolaus Umlauf <Nikolaus.Umlauf@uibk.ac.at>",
    "url": "https://www.uni-goettingen.de/de/bayesx/550513.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 640,
    "package_name": "BayesianDisaggregation",
    "title": "Bayesian Methods for Economic Data Disaggregation",
    "description": "Implements a novel Bayesian disaggregation framework that combines \n    Principal Component Analysis (PCA) and Singular Value Decomposition (SVD) \n    dimension reduction of prior weight matrices with deterministic Bayesian \n    updating rules. The method provides Markov Chain Monte Carlo (MCMC) free \n    posterior estimation with built-in diagnostic metrics. While based on \n    established PCA (Jolliffe, 2002) <doi:10.1007/b98835> and Bayesian principles \n    (Gelman et al., 2013) <doi:10.1201/b16018>, the specific integration for \n    economic disaggregation represents an original methodological contribution.",
    "version": "0.1.2",
    "maintainer": "José Mauricio Gómez Julián <isadore.nabi@pm.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 642,
    "package_name": "BayesianGLasso",
    "title": "Bayesian Graphical Lasso",
    "description": "Implements a data-augmented block Gibbs sampler for simulating the posterior distribution of concentration matrices for specifying the topology and parameterization of a Gaussian Graphical Model (GGM). This sampler was originally proposed in Wang (2012) <doi:10.1214/12-BA729>.",
    "version": "0.2.0",
    "maintainer": "Patrick Trainor <patrick.trainor@louisville.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 643,
    "package_name": "BayesianLasso",
    "title": "Bayesian Lasso Regression and Tools for the Lasso Distribution",
    "description": "Implements Bayesian Lasso regression using efficient Gibbs sampling\n               algorithms, including modified versions of the Hans and Park–Casella (PC) samplers.\n               Includes functions for working with the Lasso distribution, such as its density,\n               cumulative distribution, quantile, and random generation functions, along with moment\n               calculations. Also includes a function to compute the Mills ratio. Designed for sparse\n               linear models and suitable for high-dimensional regression problems.",
    "version": "0.3.6",
    "maintainer": "Mohammad Javad Davoudabadi <mohammad.davoudabadi@sydney.edu.au>",
    "url": "https://garthtarr.github.io/BayesianLasso/,\nhttps://github.com/garthtarr/BayesianLasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 644,
    "package_name": "BayesianLaterality",
    "title": "Predict Brain Asymmetry Based on Handedness and Dichotic\nListening",
    "description": "Functional differences between the cerebral hemispheres \n    are a fundamental characteristic of the human brain. Researchers \n    interested in studying these differences often infer underlying \n    hemispheric dominance for a certain function (e.g., language) from \n    laterality indices calculated from observed performance or brain \n    activation measures . However, any inference from observed measures \n    to latent (unobserved) classes has to consider the prior probability \n    of class membership in the population. The provided functions \n    implement a Bayesian model for predicting hemispheric dominance from\n    observed laterality indices (Sorensen and Westerhausen, Laterality: \n    Asymmetries of Body, Brain and Cognition, 2020, <doi:10.1080/1357650X.2020.1769124>).",
    "version": "0.1.2",
    "maintainer": "Oystein Sorensen <oystein.sorensen.1985@gmail.com>",
    "url": "https://github.com/LCBC-UiO/BayesianLaterality",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 646,
    "package_name": "BayesianMediationA",
    "title": "Bayesian Mediation Analysis",
    "description": "We perform general mediation analysis in the Bayesian setting using the methods described in Yu and Li (2022, ISBN:9780367365479). With the package, the mediation analysis can be performed on different types of outcomes (e.g., continuous, binary, categorical, or time-to-event), with default or user-defined priors and predictive models. The Bayesian estimates and credible sets of mediation effects are reported as analytic results.",
    "version": "1.0.1",
    "maintainer": "Qingzhao Yu <qyu@lsuhsc.edu>",
    "url": "https://cran.r-project.org/package=BayesianMediationA,\nhttps://publichealth.lsuhsc.edu/Faculty_pages/qyu/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 647,
    "package_name": "BayesianNetwork",
    "title": "Bayesian Network Modeling and Analysis",
    "description": "A \"Shiny\"\" web application for creating interactive Bayesian Network models,\n    learning the structure and parameters of Bayesian networks, and utilities for classic\n    network analysis.",
    "version": "0.3.2",
    "maintainer": "Paul Govan <paul.govan2@gmail.com>",
    "url": "https://github.com/paulgovan/bayesiannetwork,\nhttp://paulgovan.github.io/BayesianNetwork/,\nhttps://github.com/paulgovan/BayesianNetwork",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 648,
    "package_name": "BayesianPlatformDesignTimeTrend",
    "title": "Simulate and Analyse Bayesian Platform Trial with Time Trend",
    "description": "Simulating the sequential multi-arm multi-stage or platform trial with Bayesian approach using the 'rstan' package, which provides the R interface for the Stan. \n    This package supports fixed ratio and Bayesian adaptive randomization approaches for randomization. \n    Additionally, it allows for the study of time trend problems in platform trials. \n    There are demos available for a multi-arm multi-stage trial with two different null scenarios, as well as for Bayesian trial cutoff screening.\n    The Bayesian adaptive randomisation approaches are described in:\n    Trippa et al. (2012) <doi:10.1200/JCO.2011.39.8420> and\n    Wathen et al. (2017) <doi:10.1177/1740774517692302>.\n    The randomisation algorithm is described in: \n    Zhao W <doi:10.1016/j.cct.2015.06.008>.\n    The analysis methods of time trend effect in platform trial are described in:\n    Saville et al. (2022) <doi:10.1177/17407745221112013> and\n    Bofill Roig et al. (2022) <doi:10.1186/s12874-022-01683-w>.",
    "version": "1.2.3",
    "maintainer": "Ziyan Wang <zw7g20@soton.ac.uk>",
    "url": "https://github.com/ZXW834/BayesianPlatformDesignTimeTrend",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 651,
    "package_name": "BayesianTools",
    "title": "General-Purpose MCMC and SMC Samplers and Tools for Bayesian\nStatistics",
    "description": "General-purpose MCMC and SMC samplers, as well as plot and\n    diagnostic functions for Bayesian statistics, with a particular focus on\n    calibrating complex system models. Implemented samplers include various\n    Metropolis MCMC variants (including adaptive and/or delayed rejection MH), the\n    T-walk, two differential evolution MCMCs, two DREAM MCMCs, and a sequential\n    Monte Carlo (SMC) particle filter.",
    "version": "0.1.8",
    "maintainer": "Florian Hartig <florian.hartig@biologie.uni-regensburg.de>",
    "url": "https://github.com/florianhartig/BayesianTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 652,
    "package_name": "Bayesiangammareg",
    "title": "Bayesian Gamma Regression: Joint Mean and Shape Modeling",
    "description": "Adjust the Gamma regression models from a Bayesian perspective described by Cepeda and Urdinola (2012) <doi:10.1080/03610918.2011.600500>, modeling the parameters of mean and shape and using different link functions for the parameter associated to the mean. And calculates different adjustment statistics such as the Akaike information criterion and Bayesian information criterion.",
    "version": "0.1.0",
    "maintainer": "Arturo Camargo Lozano <bacamargol@unal.edu.co>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 653,
    "package_name": "Bayesiantreg",
    "title": "Bayesian t Regression for Modeling Mean and Scale Parameters",
    "description": "Performs Bayesian t Regression where mean and scale parameters are modeling by lineal regression structures, and the degrees of freedom parameters are estimated. ",
    "version": "1.0.1",
    "maintainer": "Margarita Marin <mmarinj@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 654,
    "package_name": "Bayesrel",
    "title": "Bayesian Reliability Estimation",
    "description": "Functionality for reliability estimates. For 'unidimensional' tests:\n    Coefficient alpha, 'Guttman's' lambda-2/-4/-6, the Greatest lower\n    bound and coefficient omega_u ('unidimensional') in a Bayesian and a frequentist version. \n    For multidimensional tests: omega_t (total) and omega_h (hierarchical). \n    The results include confidence and credible intervals, the \n    probability of a coefficient being larger than a cutoff, \n    and a check for the factor models, necessary for the omega coefficients. \n    The method for the Bayesian 'unidimensional' estimates, except for omega_u, \n    is sampling from the posterior inverse 'Wishart' for the \n    covariance matrix based measures (see 'Murphy', 2007, \n    <https://groups.seas.harvard.edu/courses/cs281/papers/murphy-2007.pdf>. \n    The Bayesian omegas (u, t, and h) are obtained by \n    'Gibbs' sampling from the conditional posterior distributions of \n    (1) the single factor model, (2) the second-order factor model, (3) the bi-factor model, \n    (4) the correlated factor model\n    ('Lee', 2007, <doi:10.1002/9780470024737>). ",
    "version": "0.7.8",
    "maintainer": "Julius M. Pfadt <julius.pfadt@gmail.com>",
    "url": "https://github.com/juliuspfadt/Bayesrel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 657,
    "package_name": "BeQut",
    "title": "Bayesian Estimation for Quantile Regression Mixed Models",
    "description": "Using a Bayesian estimation procedure, this package fits linear quantile regression models such as linear quantile models, linear quantile mixed models, quantile regression joint models for time-to-event and longitudinal data. The estimation procedure is based on the asymmetric Laplace distribution and the 'JAGS' software is used to get posterior samples (Yang, Luo, DeSantis (2019) <doi:10.1177/0962280218784757>).",
    "version": "0.1.0",
    "maintainer": "Antoine Barbieri <antoine.barbieri@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 660,
    "package_name": "BeastJar",
    "title": "JAR Dependency for MCMC Using 'BEAST'",
    "description": "Provides JAR to perform Markov chain Monte Carlo (MCMC) inference using\n    the popular Bayesian Evolutionary Analysis by Sampling Trees 'BEAST X' software\n    library of Baele et al (2025) <doi:10.1038/s41592-025-02751-x>. 'BEAST X' supports\n    auto-tuning Metropolis-Hastings, slice, Hamiltonian Monte Carlo and Sequential Monte\n    Carlo sampling for a large variety of composable standard and phylogenetic\n    statistical models using high performance computing.  By placing the 'BEAST X' JAR in\n    this package, we offer an efficient distribution system for 'BEAST X' use by other R\n    packages using CRAN.",
    "version": "10.5.1",
    "maintainer": "Marc A. Suchard <msuchard@ucla.edu>",
    "url": "https://github.com/beast-dev/BeastJar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 662,
    "package_name": "BeeGUTS",
    "title": "General Unified Threshold Model of Survival for Bees using\nBayesian Inference",
    "description": "Tools to calibrate, validate, and make predictions with the\n    General Unified Threshold model of Survival adapted for Bee species. The\n    model is presented in the publication from Baas, J., Goussen, B., Miles, M.,\n    Preuss, T.G., Roessing, I. (2022) <doi:10.1002/etc.5423> and \n    Baas, J., Goussen, B., Taenzler, V., Roeben, V., Miles, M., Preuss, T.G., \n    van den Berg, S., Roessink, I. (2024) <doi:10.1002/etc.5871>, and is based on the \n    GUTS framework Jager, T., Albert, C., Preuss, T.G. and Ashauer, R. (2011) \n    <doi:10.1021/es103092a>.\n    The authors are grateful to Bayer A.G. for its financial support.",
    "version": "1.4.0",
    "maintainer": "Carlo Romoli <carlo.romoli@ibacon.com>",
    "url": "https://github.com/bgoussen/BeeGUTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 665,
    "package_name": "Bergm",
    "title": "Bayesian Exponential Random Graph Models",
    "description": "Bayesian analysis for exponential random graph models using\n    advanced computational algorithms. More information can be found at:\n    <https://acaimo.github.io/Bergm/>.",
    "version": "5.0.7",
    "maintainer": "Alberto Caimo <alberto.caimo1@ucd.ie>",
    "url": "https://acaimo.github.io/Bergm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 669,
    "package_name": "Bestie",
    "title": "Bayesian Estimation of Intervention Effects",
    "description": "An implementation of intervention effect estimation for DAGs (directed acyclic graphs) learned from binary or continuous data. First, parameters are estimated or sampled for the DAG and then interventions on each node (variable) are propagated through the network (do-calculus). Both exact computation (for continuous data or for binary data up to around 20 variables) and Monte Carlo schemes (for larger binary networks) are implemented.",
    "version": "0.1.5",
    "maintainer": "Jack Kuipers <jack.kuipers@bsse.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 670,
    "package_name": "BetaBit",
    "title": "Mini Games from Adventures of Beta and Bit",
    "description": "Three games: proton, frequon and regression. Each one is a console-based data-crunching game for younger and older data scientists.\n  Act as a data-hacker and find Slawomir Pietraszko's credentials to the Proton server.\n  In proton you have to solve four data-based puzzles to find the login and password.\n  There are many ways to solve these puzzles. You may use loops, data filtering, ordering, aggregation or other tools.\n  Only basics knowledge of R is required to play the game, yet the more functions you know, the more approaches you can try.\n  In frequon you will help to perform statistical cryptanalytic attack on a corpus of ciphered messages.\n  This time seven sub-tasks are pushing the bar much higher. Do you accept the challenge?\n  In regression you will test your modeling skills in a series of eight sub-tasks.\n  Try only if ANOVA is your close friend.\n  It's a part of Beta and Bit project.\n  You will find more about the Beta and Bit project at <https://github.com/BetaAndBit/Charts>.",
    "version": "2.2",
    "maintainer": "Przemyslaw Biecek <przemyslaw.biecek@gmail.com>",
    "url": "https://github.com/BetaAndBit/Charts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 671,
    "package_name": "BetaPASS",
    "title": "Calculate Power and Sample Size with Beta Regression",
    "description": "Power calculations are a critical component of any research study to determine the \n\tminimum sample size necessary to detect differences between multiple groups. \n\tResearchers often work with data taking the form of proportions that can be modeled with \n\ta beta distribution. Here we present an R package, 'BetaPASS', that perform power and \n\tsample size calculations for data following a beta distribution with comparative \n\tnonparametric output. This package allows flexibility with multiple options for link \n\tfunctions to fit the data and graphing functionality for visual comparisons.",
    "version": "1.1-2",
    "maintainer": "Jinpu Li <lijinp@health.missouri.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 679,
    "package_name": "BiDAG",
    "title": "Bayesian Inference for Directed Acyclic Graphs",
    "description": "Implementation of a collection of MCMC methods for Bayesian structure learning\n    of directed acyclic graphs (DAGs), both from continuous and discrete data. For efficient\n    inference on larger DAGs, the space of DAGs is pruned according to the data. To filter\n    the search space, the algorithm employs a hybrid approach, combining constraint-based \n    learning with search and score. A reduced search space is initially defined on the basis\n    of a skeleton obtained by means of the PC-algorithm, and then iteratively improved with\n    search and score. Search and score is then performed following two approaches:\n    Order MCMC, or Partition MCMC.\n    The BGe score is implemented for continuous data and the BDe score is implemented\n    for binary data or categorical data. The algorithms may provide the maximum a posteriori \n    (MAP) graph or a sample (a collection of DAGs) from the posterior distribution given the data.\n    All algorithms are also applicable for structure learning and sampling for dynamic Bayesian networks.\n    References:\n    J. Kuipers, P. Suter, G. Moffa (2022) <doi:10.1080/10618600.2021.2020127>,\n    N. Friedman and D. Koller (2003) <doi:10.1023/A:1020249912095>, \n    J. Kuipers and G. Moffa (2017) <doi:10.1080/01621459.2015.1133426>, \n    M. Kalisch et al. (2012) <doi:10.18637/jss.v047.i11>,\n    D. Geiger and D. Heckerman (2002) <doi:10.1214/aos/1035844981>, \n    P. Suter, J. Kuipers, G. Moffa, N.Beerenwinkel (2023) <doi:10.18637/jss.v105.i09>.",
    "version": "2.1.4",
    "maintainer": "Polina Suter <polina.suter@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 680,
    "package_name": "BiDimRegression",
    "title": "Calculates the Bidimensional Regression Between Two 2D\nConfigurations",
    "description": "Calculates the bidimensional regression between two 2D configurations following the approach by Tobler (1965).",
    "version": "2.0.1",
    "maintainer": "Alexander Pastukhov <pastukhov.alexander@gmail.com>",
    "url": "https://CRAN.R-project.org/package=BiDimRegression/,\nhttps://github.com/alexander-pastukhov/bidim-regression/,\nhttps://alexander-pastukhov.github.io/bidim-regression/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 698,
    "package_name": "BigVAR",
    "title": "Dimension Reduction Methods for Multivariate Time Series",
    "description": "Estimates VAR and VARX models with Structured Penalties.",
    "version": "1.1.4",
    "maintainer": "Will Nicholson <wbn8@cornell.edu>",
    "url": "https://github.com/wbnicholson/BigVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 705,
    "package_name": "BinSegBstrap",
    "title": "Piecewise Smooth Regression by Bootstrapped Binary Segmentation",
    "description": "Provides methods for piecewise smooth regression. A piecewise smooth signal is estimated by applying a bootstrapped test recursively (binary segmentation approach). Each bootstrapped test decides whether the underlying signal is smooth on the currently considered subsegment or contains at least one further change-point.",
    "version": "1.0-1",
    "maintainer": "Pein Florian <f.pein@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 708,
    "package_name": "BinaryEPPM",
    "title": "Mean and Scale-Factor Modeling of Under- And Over-Dispersed\nBinary Data",
    "description": "Under- and over-dispersed binary data are modeled using an extended Poisson \n process model (EPPM) appropriate for binary data. A feature of the model is that the \n under-dispersion relative to the binomial distribution only needs to be greater than\n zero, but the over-dispersion is  restricted compared to other distributional models  \n such as the beta and correlated binomials. Because of this, the examples focus on \n under-dispersed data and how, in combination with the beta or correlated distributions,\n flexible models can be fitted to data displaying both under- and over-dispersion. Using\n Generalized Linear Model (GLM)  terminology, the functions utilize linear predictors for\n the probability of success and scale-factor with various link functions for p, and log \n link for scale-factor, to fit a variety of models relevant to areas such as bioassay. \n Details of the EPPM are in Faddy and Smith (2012) <doi:10.1002/bimj.201100214> and \n Smith and Faddy (2019) <doi:10.18637/jss.v090.i08>.",
    "version": "3.0",
    "maintainer": "David M. Smith <dmccsmith@verizon.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 725,
    "package_name": "BioPETsurv",
    "title": "Biomarker Prognostic Enrichment Tool for Time-to-Event Trial",
    "description": "Prognostic Enrichment is a strategy of enriching a clinical trial for testing an intervention intended to prevent or delay an unwanted clinical event.  A prognostically enriched trial enrolls only patients who are more likely to experience the unwanted clinical event than the broader patient population (R. Temple (2010) <doi:10.1038/clpt.2010.233>). By testing the intervention in an enriched study population, the trial may be adequately powered with a smaller sample size, which can have both practical and ethical advantages.\n    This package provides tools to evaluate biomarkers for prognostic enrichment of clinical trials with survival/time-to-event outcomes.",
    "version": "0.1.0",
    "maintainer": "Si Cheng <chengsi@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 770,
    "package_name": "BivGeo",
    "title": "Basu-Dhar Bivariate Geometric Distribution",
    "description": "Provides functions to compute the joint probability mass function (pmf), cumulative distribution function (cdf), and survival function (sf) of the Basu-Dhar bivariate geometric distribution. Additional functionalities include the calculation of the correlation coefficient, covariance, and cross-factorial moments, as well as the generation of random variates. The package also implements parameter estimation based on the method of moments.",
    "version": "2.1.1",
    "maintainer": "Ricardo Puziol de Oliveira <rpuziol.oliveira@gmail.com>",
    "url": "https://doi.org/10.1285/i20705948v11n1p108",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 772,
    "package_name": "BivRegBLS",
    "title": "Tolerance Interval and EIV Regression - Method Comparison\nStudies",
    "description": "Assess the agreement in method comparison studies by tolerance intervals and errors-in-variables (EIV) regressions. The Ordinary Least Square regressions (OLSv and OLSh), the Deming Regression (DR), and the (Correlated)-Bivariate Least Square regressions (BLS and CBLS) can be used with unreplicated or replicated data. The BLS() and CBLS() are the two main functions to estimate a regression line, while XY.plot() and MD.plot() are the two main graphical functions to display, respectively an (X,Y) plot or (M,D) plot with the BLS or CBLS results. Four hyperbolic statistical intervals are provided: the Confidence Interval (CI), the Confidence Bands (CB), the Prediction Interval and the Generalized prediction Interval. Assuming no proportional bias, the (M,D) plot (Band-Altman plot) may be simplified by calculating univariate tolerance intervals (beta-expectation (type I) or beta-gamma content (type II)). Major updates from last version 1.0.0 are: title shortened, include the new functions BLS.fit() and CBLS.fit() as shortcut of the, respectively, functions BLS() and CBLS(). References: B.G. Francq, B. Govaerts (2016) <doi:10.1002/sim.6872>, B.G. Francq, B. Govaerts (2014) <doi:10.1016/j.chemolab.2014.03.006>, B.G. Francq, B. Govaerts (2014) <http://publications-sfds.fr/index.php/J-SFdS/article/view/262>, B.G. Francq (2013), PhD Thesis, UCLouvain, Errors-in-variables regressions to assess equivalence in method comparison studies, <https://dial.uclouvain.be/pr/boreal/object/boreal%3A135862/datastream/PDF_01/view>.",
    "version": "1.1.1",
    "maintainer": "Bernard G Francq <BivRegBLS@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 788,
    "package_name": "Bolstad",
    "title": "Functions for Elementary Bayesian Inference",
    "description": "A set of R functions and data sets for the book Introduction to Bayesian Statistics, Bolstad, W.M. (2017), John Wiley & Sons ISBN 978-1-118-09156-2.",
    "version": "0.2.42",
    "maintainer": "James Curran <j.curran@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 789,
    "package_name": "Bolstad2",
    "title": "Bolstad Functions",
    "description": "A set of R functions and data sets for the book \"Understanding Computational Bayesian Statistics.\" This book was written by Bill (WM) Bolstad and published in 2009 by John Wiley & Sons (ISBN 978-0470046098).",
    "version": "1.0-29",
    "maintainer": "James Curran <j.curran@auckland.ac.nz>",
    "url": "https://github.com/jmcurran/Bolstad2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 797,
    "package_name": "Boom",
    "title": "Bayesian Object Oriented Modeling",
    "description": "A C++ library for Bayesian modeling, with an emphasis on Markov\n   chain Monte Carlo.  Although boom contains a few R utilities (mainly plotting\n   functions), its primary purpose is to install the BOOM C++ library on your\n   system so that other packages can link against it.",
    "version": "0.9.16",
    "maintainer": "Steven L. Scott <steve.the.bayesian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 798,
    "package_name": "BoomSpikeSlab",
    "title": "MCMC for Spike and Slab Regression",
    "description": "Spike and slab regression with a variety of residual error\n  distributions corresponding to Gaussian, Student T, probit, logit, SVM, and a\n  few others.  Spike and slab regression is Bayesian regression with prior\n  distributions containing a point mass at zero.  The posterior updates the\n  amount of mass on this point, leading to a posterior distribution that is\n  actually sparse, in the sense that if you sample from it many coefficients are\n  actually zeros.  Sampling from this posterior distribution is an elegant way\n  to handle Bayesian variable selection and model averaging.  See\n  <DOI:10.1504/IJMMNO.2014.059942> for an explanation of the Gaussian case.",
    "version": "1.2.7",
    "maintainer": "Steven L. Scott <steve.the.bayesian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 801,
    "package_name": "BootPR",
    "title": "Bootstrap Prediction Intervals and Bias-Corrected Forecasting",
    "description": "Contains functions for bias-Corrected Forecasting and Bootstrap Prediction Intervals for Autoregressive Time Series.",
    "version": "1.0",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 802,
    "package_name": "BootWPTOS",
    "title": "Test Stationarity using Bootstrap Wavelet Packet Tests",
    "description": "Provides significance tests for second-order stationarity\n\tfor time series using bootstrap wavelet packet tests.\n\tProvides functionality to visualize the time series with\n\tthe results of the hypothesis tests superimposed.\n\tThe methodology is described in Cardinali, A and Nason, G P (2016)\n\t\"Practical powerful wavelet packet tests for second-order stationarity.\"\n\tApplied and Computational Harmonic Analysis, 44, 558-585\n\t<doi:10.1016/j.acha.2016.06.006>.",
    "version": "1.2.1",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 804,
    "package_name": "BootstrapTests",
    "title": "Bootstrap-Based Hypothesis Testing using Different Resampling\nSchemes",
    "description": "Perform bootstrap-based hypothesis testing procedures on\n    three statistical problems. In particular, it covers independence\n    testing, testing the slope in a linear regression setting, and\n    goodness-of-fit testing, following (Derumigny, Galanis, Schipper and Van der Vaart, 2025)\n    <doi:10.48550/arXiv.2512.10546>.",
    "version": "0.1.0",
    "maintainer": "Wieger Schipper <W.R.Schipper@tudelft.nl>",
    "url": "https://github.com/AlexisDerumigny/BootstrapTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 805,
    "package_name": "Boptbd",
    "title": "Bayesian Optimal Block Designs",
    "description": "Computes Bayesian A- and D-optimal block designs under the linear mixed effects model settings using block/array exchange algorithm of Debusho, Gemechu and Haines (2018) <doi:10.1080/03610918.2018.1429617> and Gemechu, Debusho and Haines (2025) <doi:10.5539/ijsp.v14n1p50> where the interest is in a \n\tcomparison of all possible elementary treatment contrasts. The package also provides an optional \n\tmethod of using the graphical user interface (GUI) R package 'tcltk' to ensure that it is user friendly.",
    "version": "1.0.7",
    "maintainer": "Dibaba Bayisa Gemechu <diboobayu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 812,
    "package_name": "BrainCon",
    "title": "Inference the Partial Correlations Based on Time Series Data",
    "description": "A statistical tool to inference the multi-level partial correlations based on multi-subject time series data, especially for brain functional connectivity. It combines both individual and population level inference by using  the methods of Qiu and Zhou. (2021)<DOI: 10.1080/01621459.2021.1917417> and Genovese and Wasserman. (2006)<DOI: 10.1198/016214506000000339>. It realizes two reliable estimation methods of partial correlation coefficients, using scaled lasso and lasso. It can be used to estimate individual- or population-level partial correlations, identify nonzero ones, and find out unequal partial correlation coefficients between two populations.",
    "version": "0.3.0",
    "maintainer": "Yunhaonan Yang <haonan_yy@pku.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 822,
    "package_name": "BrokenAdaptiveRidge",
    "title": "Broken Adaptive Ridge Regression with Cyclops",
    "description": "Approximates best-subset selection (L0) regression with\n   an iteratively adaptive Ridge (L2) penalty for large-scale models.\n   This package uses Cyclops for an efficient implementation and the\n   iterative method is described in Kawaguchi et al (2020)\n   <doi:10.1002/sim.8438> and Li et al (2021)\n   <doi:10.1016/j.jspi.2020.12.001>.",
    "version": "1.0.2",
    "maintainer": "Marc A. Suchard <msuchard@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 825,
    "package_name": "Brq",
    "title": "Bayesian Analysis of Quantile Regression Models",
    "description": "Bayesian estimation and variable selection for quantile regression models.",
    "version": "3.0",
    "maintainer": "Rahim Alhamzawi  (University of Al-Qadisiyah) <rahim.alhamzawi@qu.edu.iq>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 837,
    "package_name": "BuyseTest",
    "title": "Generalized Pairwise Comparisons",
    "description": "Implementation of the Generalized Pairwise Comparisons (GPC) as defined in Buyse (2010) <doi:10.1002/sim.3923> for complete observations, and extended in Peron (2018) <doi:10.1177/0962280216658320> to deal with right-censoring. GPC compare two groups of observations (intervention vs. control group) regarding several prioritized endpoints to estimate the probability that a random observation drawn from one group performs better/worse/equivalently than a random observation drawn from the other group. Summary statistics such as the net treatment benefit, win ratio, or win odds are then deduced from these probabilities. Confidence intervals and p-values are obtained based on asymptotic results (Ozenne 2021 <doi:10.1177/09622802211037067>), non-parametric bootstrap, or permutations. The software enables the use of thresholds of minimal importance difference, stratification, non-prioritized endpoints (O Brien test), and can handle right-censoring and competing-risks.",
    "version": "3.3.4",
    "maintainer": "Brice Ozenne <brice.mh.ozenne@gmail.com>",
    "url": "https://github.com/bozenne/BuyseTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 839,
    "package_name": "BwQuant",
    "title": "Bandwidth Selectors for Local Linear Quantile Regression",
    "description": "Bandwidth selectors for local linear quantile regression, including cross-validation and plug-in methods. The local linear quantile regression estimate is also implemented.",
    "version": "0.1.0",
    "maintainer": "Mercedes Conde-Amboage <mercedes.amboage@usc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 846,
    "package_name": "CADFtest",
    "title": "A Package to Perform Covariate Augmented Dickey-Fuller Unit Root\nTests",
    "description": "Hansen's (1995) Covariate-Augmented\n        Dickey-Fuller (CADF) test. The only required argument is y, the\n        Tx1 time series to be tested. If no stationary covariate X is\n        passed to the procedure, then an ordinary ADF test is\n        performed. The p-values of the test are computed using the\n        procedure illustrated in Lupi (2009).",
    "version": "0.3-3",
    "maintainer": "Claudio Lupi <lupi@unimol.it>",
    "url": "http://www.jstatsoft.org/v32/i02",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 852,
    "package_name": "CAGR",
    "title": "Compound Annual Growth Rate",
    "description": "A time series usually does not have a uniform growth rate. Compound Annual Growth Rate measures the average annual growth over a given period. More details can be found in Bardhan et al. (2022) <DOI:10.18805/ag.D-5418>.",
    "version": "1.1.1",
    "maintainer": "Debopam Rakshit <rakshitdebopam@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 862,
    "package_name": "CARBayesST",
    "title": "Spatio-Temporal Generalised Linear Mixed Models for Areal Unit\nData",
    "description": "Implements a class of univariate and multivariate spatio-temporal generalised linear mixed models for areal unit data, with inference in a Bayesian setting using Markov chain Monte Carlo (MCMC) simulation. The response variable can be binomial, Gaussian, or Poisson, but for some models only the binomial and Poisson data likelihoods are available. The spatio-temporal autocorrelation is modelled by  random effects, which are assigned conditional autoregressive (CAR) style prior distributions. A number of different random effects structures are available, including models similar to Rushworth et al. (2014) <doi:10.1016/j.sste.2014.05.001>. Full details are given in the vignette accompanying this package. The creation and development of this package was supported by the Engineering and Physical Sciences Research Council  (EPSRC) grants EP/J017442/1 and EP/T004878/1 and the Medical Research Council (MRC) grant MR/L022184/1.",
    "version": "4.0",
    "maintainer": "Duncan Lee <Duncan.Lee@glasgow.ac.uk>",
    "url": "https://github.com/duncanplee/CARBayesST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 863,
    "package_name": "CARBayesdata",
    "title": "Data Used in the Vignettes Accompanying the CARBayes and\nCARBayesST Packages",
    "description": "Spatio-temporal data from Scotland used in the vignettes accompanying the CARBayes (spatial modelling) and CARBayesST (spatio-temporal modelling) packages. Most of the data relate to the set of 271 Intermediate Zones (IZ)  that make up the 2001 definition of the  Greater Glasgow and Clyde health board. ",
    "version": "3.0",
    "maintainer": "Duncan Lee <Duncan.Lee@glasgow.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 868,
    "package_name": "CARRoT",
    "title": "Predicting Categorical and Continuous Outcomes Using One in Ten\nRule",
    "description": "Predicts categorical or continuous outcomes while concentrating on a number of key points. These are Cross-validation, Accuracy, Regression and Rule of Ten or \"one in ten rule\" (CARRoT), and, in addition to it R-squared statistics, prior knowledge on the dataset etc. It performs the cross-validation specified number of times by partitioning the input into training and test set and fitting linear/multinomial/binary regression models to the training set. All regression models satisfying chosen constraints are fitted and the ones with the best predictive power are given as an output. Best predictive power is understood as highest accuracy in case of binary/multinomial outcomes, smallest absolute and relative errors in case of continuous outcomes. For binary case there is also an option of finding a regression model which gives the highest AUROC (Area Under Receiver Operating Curve) value. The option of parallel toolbox is also available. Methods are described in Peduzzi et al. (1996) <doi:10.1016/S0895-4356(96)00236-3> , Rhemtulla et al. (2012) <doi:10.1037/a0029315>, Riley et al. (2018) <doi:10.1002/sim.7993>, Riley et al. (2019) <doi:10.1002/sim.7992>.",
    "version": "3.0.2",
    "maintainer": "Alina Bazarova <al.bazarova@fz-juelich.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 886,
    "package_name": "CBPS",
    "title": "Covariate Balancing Propensity Score",
    "description": "Implements the covariate balancing propensity score (CBPS) proposed\n    by Imai and Ratkovic (2014) <DOI:10.1111/rssb.12027>. The propensity score is\n    estimated such that it maximizes the resulting covariate balance as well as the\n    prediction of treatment assignment. The method, therefore, avoids an iteration\n    between model fitting and balance checking.  The package also implements optimal\n    CBPS from Fan et al. (in-press) <DOI:10.1080/07350015.2021.2002159>,  \n    several extensions of the CBPS beyond the cross-sectional, binary treatment setting.\n    They include the CBPS for longitudinal settings so that it can be used in \n    conjunction with marginal structural models from Imai and Ratkovic (2015) \n    <DOI:10.1080/01621459.2014.956872>, treatments with three- and four-valued treatment \n    variables, continuous-valued treatments from Fong, Hazlett, and Imai (2018) \n    <DOI:10.1214/17-AOAS1101>, propensity score estimation with a large number of \n    covariates from Ning, Peng, and Imai (2020) <DOI:10.1093/biomet/asaa020>, and the situation \n    with multiple distinct binary treatments administered simultaneously. In the future \n    it will be extended to other settings including the generalization of experimental \n    and instrumental variable estimates. ",
    "version": "0.24",
    "maintainer": "Christian Fong <cjfong@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 887,
    "package_name": "CBRT",
    "title": "CBRT Data on Turkish Economy",
    "description": "The Central Bank of the Republic of Turkey (CBRT) provides one of \n  the most comprehensive time series databases on the Turkish economy. The 'CBRT' \n  package provides functions for accessing the CBRT's electronic data delivery \n  system <https://evds2.tcmb.gov.tr/>. It contains the lists of all data \n  categories and data groups for searching the available variables (data series).  \n  As of November 3, 2024, there were 40,826 variables in the dataset. The lists \n  of data categories and data groups can be updated by the user at any time. A \n  specific variable, a group of variables, or all variables in a data group can \n  be downloaded at different frequencies using a variety of aggregation methods.",
    "version": "0.1.1",
    "maintainer": "Erol Taymaz <etaymaz@metu.edu.tr>",
    "url": "https://github.com/etaymaz/CBRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 891,
    "package_name": "CBnetworkMA",
    "title": "Contrast-Based Bayesian Network Meta Analysis",
    "description": "A function that facilitates fitting three types of models \n    for contrast-based Bayesian Network Meta Analysis.  The first model is that which\n    is described in Lu and Ades (2006) <doi:10.1198/016214505000001302>.  The other two \n    models are based on a Bayesian nonparametric methods that permit ties when comparing \n    treatment or for a treatment effect to be exactly equal to zero. In addition to the \n    model fits, the package provides a summary of the interplay  between treatment \n    effects based on the procedure described in Barrientos, Page, and Lin (2023)\n    <doi:10.48550/arXiv.2207.06561>.",
    "version": "0.1.0",
    "maintainer": "Garritt L. Page <page@stat.byu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 902,
    "package_name": "CCWeights",
    "title": "Perform Weighted Linear Regression for Calibration Curve",
    "description": "Automated assessment and selection of weighting factors for accurate quantification using linear calibration curve. \n In addition, a 'shiny' App is provided, allowing users to analyze their data using an interactive graphical user interface, without any programming requirements.",
    "version": "0.1.6",
    "maintainer": "Yonghui Dong <yonghui.dong@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 906,
    "package_name": "CDFt",
    "title": "Downscaling and Bias Correction via Non-Parametric CDF-Transform",
    "description": "Statistical downscaling and bias correction (model output statistics) method based on cumulative distribution functions (CDF) transformation. See Michelangeli, Vrac, Loukos (2009) Probabilistic downscaling approaches: Application to wind cumulative distribution functions. Geophysical Research Letters, 36, L11708, <doi:10.1029/2009GL038401>. ; and Vrac, Drobinski, Merlo, Herrmann, Lavaysse, Li, Somot (2012) Dynamical and statistical downscaling of the French Mediterranean climate: uncertainty assessment. Nat. Hazards Earth Syst. Sci., 12, 2769-2784, www.nat-hazards-earth-syst-sci.net/12/2769/2012/, <doi:10.5194/nhess-12-2769-2012>.",
    "version": "1.2",
    "maintainer": "Mathieu Vrac <mathieu.vrac@lsce.ipsl.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 916,
    "package_name": "CDatanet",
    "title": "Econometrics of Network Data",
    "description": "Simulating and estimating peer effect models and network formation models. The class of peer effect models includes linear-in-means models (Lee, 2004; <doi:10.1111/j.1468-0262.2004.00558.x>), Tobit models (Xu and Lee, 2015; <doi:10.1016/j.jeconom.2015.05.004>), and discrete numerical data models (Houndetoungan, 2025; <doi:10.48550/arXiv.2405.17290>). The network formation models include pair-wise regressions with degree heterogeneity (Graham, 2017; <doi:10.3982/ECTA12679>) and exponential random graph models (Mele, 2017; <doi:10.3982/ECTA10400>).",
    "version": "2.2.2",
    "maintainer": "Aristide Houndetoungan <ahoundetoungan@ecn.ulaval.ca>",
    "url": "https://github.com/ahoundetoungan/CDatanet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 925,
    "package_name": "CFAcoop",
    "title": "Colony Formation Assay: Taking into Account Cellular Cooperation",
    "description": "Cellular cooperation compromises the plating efficiency-based \n    analysis of clonogenic survival data. This tool provides functions that \n    enable a robust analysis of colony formation assay (CFA) data in presence \n    or absence of cellular cooperation. \n    The implemented method has been described \n    in Brix et al. (2020). (Brix, N., Samaga, D., Hennel, R. et al. \n    \"The clonogenic assay: robustness of plating efficiency-based analysis is \n    strongly compromised by cellular cooperation.\" Radiat Oncol 15, 248 (2020).\n    <doi:10.1186/s13014-020-01697-y>)\n        Power regression for parameter estimation, calculation of survival\n    fractions, uncertainty analysis and plotting functions are provided.",
    "version": "1.0.0",
    "maintainer": "Daniel Samaga <daniel.samaga@helmholtz-muenchen.de>",
    "url": "https://github.com/ZytoHMGU/CFAcoop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 927,
    "package_name": "CFC",
    "title": "Cause-Specific Framework for Competing-Risk Analysis",
    "description": "Numerical integration of cause-specific survival curves to arrive at cause-specific cumulative incidence functions,\n    with three usage modes: 1) Convenient API for parametric survival regression followed by competing-risk analysis, 2) API for\n    CFC, accepting user-specified survival functions in R, and 3) Same as 2, but accepting survival functions in C++. For \n    mathematical details and software tutorial, see Mahani and Sharabiani (2019) <DOI:10.18637/jss.v089.i09>. ",
    "version": "1.2.1",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 943,
    "package_name": "CGR",
    "title": "Compound Growth Rate for Capturing the Growth Rate Over the\nPeriod",
    "description": "The compound growth rate indicates the percentage change of a specific variable over a defined period. It is calculated using non-linear models, particularly the exponential model. To estimate the compound growth rates, the growth model is first converted to semilog form and then analyzed using Ordinary Least Squares (OLS) regression. This package has been developed using concept of Shankar et al. (2022)<doi:10.3389/fsufs.2023.1208898>.",
    "version": "0.1.0",
    "maintainer": "Dr. S. Vishnu Shankar <S.vishnushankar55@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 952,
    "package_name": "CIAAWconsensus",
    "title": "Isotope Ratio Meta-Analysis",
    "description": "Calculation of consensus values for atomic weights, isotope amount ratios, and isotopic abundances with the associated uncertainties using multivariate meta-regression approach for consensus building.",
    "version": "1.3",
    "maintainer": "Juris Meija <juris.meija@nrc-cnrc.gc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 956,
    "package_name": "CIEE",
    "title": "Estimating and Testing Direct Effects in Directed Acyclic Graphs\nusing Estimating Equations",
    "description": "In many studies across different disciplines, detailed measures of the variables of interest are available. If assumptions can be made regarding the direction of effects between the assessed variables, this has to be considered in the analysis. The functions in this package implement the novel approach CIEE (causal inference using estimating equations; Konigorski et al., 2018, <DOI:10.1002/gepi.22107>) for estimating and testing the direct effect of an exposure variable on a primary outcome, while adjusting for indirect effects of the exposure on the primary outcome through a secondary intermediate outcome and potential factors influencing the secondary outcome. The underlying directed acyclic graph (DAG) of this considered model is described in the vignette. CIEE can be applied to studies in many different fields, and it is implemented here for the analysis of a continuous primary outcome and a time-to-event primary outcome subject to censoring. CIEE uses estimating equations to obtain estimates of the direct effect and robust sandwich standard error estimates. Then, a large-sample Wald-type test statistic is computed for testing the absence of the direct effect. Additionally, standard multiple regression, regression of residuals, and the structural equation modeling approach are implemented for comparison. ",
    "version": "0.1.1",
    "maintainer": "Stefan Konigorski <stefan.konigorski@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 967,
    "package_name": "CIS.DGLM",
    "title": "Covariates, Interaction, and Selection for DGLM",
    "description": "An implementation of double generalized linear model (DGLM) building with variable selection procedures and handling of interaction terms and other complex situations. We also provide a method of handling convergence issues within the dglm() function. The package offers a simulation function for generating simulated data for testing purposes and utilizes the forward stepwise variable selection procedure in model-building. It also provides a new custom bootstrap function for mean and standard deviation estimation and functions for building crossplots and squareplots from a data set.",
    "version": "0.1.0",
    "maintainer": "Yishi Wang <wangy@uncw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 975,
    "package_name": "CKAT",
    "title": "Composite Kernel Association Test for Pharmacogenetics Studies",
    "description": "Composite Kernel Association Test (CKAT) is a flexible and robust kernel machine based approach to jointly test the genetic main effect and gene-treatment interaction effect for a set of single-nucleotide polymorphisms (SNPs) in pharmacogenetics (PGx) assessments embedded within randomized clinical trials.",
    "version": "0.1.0",
    "maintainer": "Hong Zhang <hzhang@wpi.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 976,
    "package_name": "CKLRT",
    "title": "Composite Kernel Machine Regression Based on Likelihood Ratio\nTest",
    "description": "Composite Kernel Machine Regression based on Likelihood Ratio Test (CKLRT): in this package, we develop a kernel machine regression framework to model the overall genetic effect of a SNP-set, considering the possible GE interaction. Specifically, we use a composite kernel to specify the overall genetic effect via a nonparametric function and we model additional covariates parametrically within the regression framework. The composite kernel is constructed as a weighted average of two kernels, one corresponding to the genetic main effect and one corresponding to the GE interaction effect. We propose a likelihood ratio test (LRT) and a restricted likelihood ratio test (RLRT) for statistical significance. We derive a Monte Carlo approach for the finite sample distributions of LRT and RLRT statistics. (N. Zhao, H. Zhang, J. Clark, A. Maity, M. Wu. Composite Kernel Machine Regression based on Likelihood Ratio Test with Application for Combined Genetic and Gene-environment Interaction Effect (Submitted).) ",
    "version": "0.2.3",
    "maintainer": "Haoyu Zhang <andrew.haoyu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 979,
    "package_name": "CLIC",
    "title": "The LIC for Distributed Cosine Regression Analysis",
    "description": "This comprehensive framework for periodic time series modeling is designated as \"CLIC\" (The LIC for Distributed Cosine Regression Analysis) analysis. It is predicated on the assumption that the underlying data exhibits complex periodic structures beyond simple harmonic components. The philosophy of the method is articulated in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>.",
    "version": "0.1",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 988,
    "package_name": "CMF",
    "title": "Collective Matrix Factorization",
    "description": "Collective matrix factorization (CMF) finds joint low-rank\n\trepresentations for a collection of matrices with shared row or column\n\tentities. This code learns a variational Bayesian approximation for CMF,\n\tsupporting multiple likelihood potentials and missing data, while\n\tidentifying both factors shared by multiple matrices and factors private\n\tfor each matrix. For further details on the method see\n\tKlami et al. (2014) <arXiv:1312.5921>.\n\tThe package can also be used to learn Bayesian canonical correlation\n\tanalysis (CCA) and group factor analysis (GFA) models, both of which are\n\tspecial cases of CMF. This is likely to be useful for people looking for\n\tCCA and GFA solutions supporting missing data and non-Gaussian likelihoods.\n\tSee Klami et al. (2013) <https://research.cs.aalto.fi/pml/online-papers/klami13a.pdf>\n\tand\tVirtanen et al. (2012) <http://proceedings.mlr.press/v22/virtanen12.html>\n\tfor details on Bayesian CCA and GFA, respectively.",
    "version": "1.0.3",
    "maintainer": "Felix Held <felix.held@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 997,
    "package_name": "CMTFtoolbox",
    "title": "Create (Advanced) Coupled Matrix and Tensor Factorization Models",
    "description": "Creation and selection of (Advanced) Coupled Matrix and Tensor Factorization (ACMTF) and ACMTF-Regression (ACMTF-R) models. Selection of the optimal number of components can be done using 'ACMTF_modelSelection()' and 'ACMTFR_modelSelection()'. The CMTF and ACMTF methods were originally described by Acar et al., 2011 <doi:10.48550/arXiv.1105.3422> and Acar et al., 2014 <doi:10.1186/1471-2105-15-239>, respectively.",
    "version": "1.0.1",
    "maintainer": "Geert Roelof van der Ploeg <g.r.ploeg@uva.nl>",
    "url": "https://grvanderploeg.com/CMTFtoolbox/,\nhttps://github.com/GRvanderPloeg/CMTFtoolbox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1006,
    "package_name": "CNLTtsa",
    "title": "Complex-Valued Wavelet Lifting for Univariate and Bivariate Time\nSeries Analysis",
    "description": "Implementations of recent complex-valued wavelet spectral procedures for analysis of irregularly sampled signals, see Hamilton et al (2018) <doi:10.1080/00401706.2017.1281846>.",
    "version": "0.1-2",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1027,
    "package_name": "COINT",
    "title": "Unit Root Tests with Structural Breaks and Fully-Modified\nEstimators",
    "description": "Procedures include Phillips (1995) FMVAR <doi:10.2307/2171721>,  Kitamura and Phillips (1997) FMGMM <doi:10.1016/S0304-4076(97)00004-3>, Park (1992) CCR <doi:10.2307/2951679>, and so on. Tests with 1 or 2 structural breaks include Gregory and Hansen (1996) <doi:10.1016/0304-4076(69)41685-7>, Zivot and Andrews (1992) <doi:10.2307/1391541>, and Kurozumi (2002) <doi:10.1016/S0304-4076(01)00106-3>.",
    "version": "0.0.2",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1032,
    "package_name": "COMIX",
    "title": "Coarsened Mixtures of Hierarchical Skew Kernels",
    "description": "Bayesian fit of a Dirichlet Process Mixture with hierarchical multivariate skew normal kernels and coarsened posteriors. For more information, see Gorsky, Chan and Ma (2024) <doi:10.1214/22-BA1356>.",
    "version": "1.0.1",
    "maintainer": "S. Gorsky <sgorsky@umass.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1035,
    "package_name": "COMPoissonReg",
    "title": "Conway-Maxwell Poisson (COM-Poisson) Regression",
    "description": "Fit Conway-Maxwell Poisson (COM-Poisson or CMP) regression models\n    to count data (Sellers & Shmueli, 2010) <doi:10.1214/09-AOAS306>. The\n    package provides functions for model estimation, dispersion testing, and\n    diagnostics. Zero-inflated CMP regression (Sellers & Raim, 2016)\n    <doi:10.1016/j.csda.2016.01.007> is also supported.",
    "version": "0.8.1",
    "maintainer": "Andrew Raim <andrew.raim@gmail.com>",
    "url": "https://github.com/lotze/COMPoissonReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1044,
    "package_name": "COST",
    "title": "Copula-Based Semiparametric Models for Spatio-Temporal Data",
    "description": "Parameter estimation, one-step ahead forecast and new location\n    prediction methods for spatio-temporal data.",
    "version": "0.1.0",
    "maintainer": "Yanlin Tang <yanlintang2018@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1046,
    "package_name": "COUNT",
    "title": "Functions, Data and Code for Count Data",
    "description": "Functions, data and code for Hilbe, J.M. 2011. Negative Binomial Regression, 2nd Edition (Cambridge University Press) and Hilbe, J.M. 2014. Modeling Count Data (Cambridge University Press).",
    "version": "1.3.5",
    "maintainer": "Andrew Robinson <apro@unimelb.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1050,
    "package_name": "CP",
    "title": "Conditional Power Calculations",
    "description": "Functions for calculating the conditional power\n             for different models in survival time analysis\n             within randomized clinical trials\n             with two different treatments to be compared\n             and survival as an endpoint.",
    "version": "1.8",
    "maintainer": "Andreas Kuehnapfel <andreas.kuehnapfel@imise.uni-leipzig.de>",
    "url": "https://www.genstat.imise.uni-leipzig.de/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1051,
    "package_name": "CPBayes",
    "title": "Bayesian Meta Analysis for Studying Cross-Phenotype Genetic\nAssociations",
    "description": "A Bayesian meta-analysis method for studying cross-phenotype\n    genetic associations. It uses summary-level data across multiple phenotypes to\n    simultaneously measure the evidence of aggregate-level pleiotropic association\n    and estimate an optimal subset of traits associated with the risk locus. CPBayes\n    is based on a spike and slab prior. The methodology is available from: A Majumdar, T Haldar, S Bhattacharya, JS Witte (2018) <doi:10.1371/journal.pgen.1007139>.",
    "version": "1.1.0",
    "maintainer": "Arunabha Majumdar <statgen.arunabha@gmail.com>",
    "url": "https://github.com/ArunabhaCodes/CPBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1054,
    "package_name": "CPE",
    "title": "Concordance Probability Estimates in Survival Analysis",
    "description": "Concordance probability estimate (CPE) is a commonly used performance measure in survival analysis that evaluates the predictive accuracy of a survival model.  It measures how well a model can distinguish between pairs of individuals with different survival times.  Specifically, it calculate the proportion of all pairs of individuals whose predicted survival times are correctly ordered.",
    "version": "1.6.3",
    "maintainer": "Qianxing Mo  <qianxing.mo@moffitt.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1060,
    "package_name": "CPsurv",
    "title": "Nonparametric Change Point Estimation for Survival Data",
    "description": "Nonparametric change point estimation for survival data based on p-values of exact binomial tests.",
    "version": "1.0.0",
    "maintainer": "Stefanie Krügel <stefanie.kruegel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1065,
    "package_name": "CRE",
    "title": "Interpretable Discovery and Inference of Heterogeneous Treatment\nEffects",
    "description": "Provides a new method for interpretable heterogeneous \n    treatment effects characterization in terms of decision rules \n    via an extensive exploration of heterogeneity patterns by an \n    ensemble-of-trees approach, enforcing high stability in the \n    discovery. It relies on a two-stage pseudo-outcome regression, and \n    it is supported by theoretical convergence guarantees. Bargagli-Stoffi, \n    F. J., Cadei, R., Lee, K., & Dominici, F. (2023) Causal rule ensemble: \n    Interpretable Discovery and Inference of Heterogeneous Treatment Effects.  \n    arXiv preprint <doi:10.48550/arXiv.2009.09036>. ",
    "version": "0.2.7",
    "maintainer": "Falco Joannes Bargagli Stoffi <fbargaglistoffi@hsph.harvard.edu>",
    "url": "https://github.com/NSAPH-Software/CRE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1072,
    "package_name": "CRPClustering",
    "title": "Bayesian Nonparametric Chinese Restaurant Process Clustering with Entropy",
    "description": "Chinese restaurant process [Pitman (1995) <doi:10.1007/BF01213386>] is used in order to compose Dirichlet process [Ferguson (1973) <doi:10.1214/aos/1176342360>]. The clustering which uses Chinese restaurant process does not need to decide the number of clusters in advance. This algorithm automatically adjusts it. And this package calculates the ambiguity of clusters as entropy [Yngvason (1999) <doi:10.1016/S0370-1573(98)00082-9>].",
    "version": "1.4",
    "maintainer": "Masashi Okada <okadaalgorithm@gmail.com>",
    "url": "https://github.com/jirotubuyaki/ThunderBayesR",
    "exports": [],
    "topics": ["bayes", "bayesian", "bayesian-methods", "bayesian-nonparametric-models", "bayesian-nonparametrics", "bayesian-statistics", "cluster", "clustering", "clustering-algorithm", "clustering-methods", "crp", "data-analysis", "data-clustering-algorithm", "data-science", "dirichlet-process", "machine-learning", "nonparametric", "r", "r-package", "simulation"],
    "score": "NA",
    "stars": 16
  },
  {
    "id": 1088,
    "package_name": "CSTools",
    "title": "Assessing Skill of Climate Forecasts on Seasonal-to-Decadal\nTimescales",
    "description": "Exploits dynamical seasonal forecasts in order to provide\n    information relevant to stakeholders at the seasonal timescale. The package\n    contains process-based methods for forecast calibration, bias correction,\n    statistical and stochastic downscaling, optimal forecast combination and\n    multivariate verification, as well as basic and advanced tools to obtain\n    tailored products. This package was developed in the context of the ERA4CS \n    project MEDSCOPE and the H2020 S2S4E project and includes contributions from \n    ArticXchange project founded by EU-PolarNet 2. Implements methods described in\n    Pérez-Zanón et al. (2022) <doi:10.5194/gmd-15-6115-2022>,\n    Doblas-Reyes et al. (2005) <doi:10.1111/j.1600-0870.2005.00104.x>,\n    Mishra et al. (2018) <doi:10.1007/s00382-018-4404-z>,\n    Sanchez-Garcia et al. (2019) <doi:10.5194/asr-16-165-2019>,\n    Straus et al. (2007) <doi:10.1175/JCLI4070.1>,\n    Terzago et al. (2018) <doi:10.5194/nhess-18-2825-2018>,\n    Torralba et al. (2017) <doi:10.1175/JAMC-D-16-0204.1>,\n    D'Onofrio et al. (2014) <doi:10.1175/JHM-D-13-096.1>,\n    Verfaillie et al. (2017) <doi:10.5194/gmd-10-4257-2017>,\n    Van Schaeybroeck et al. (2019) <doi:10.1016/B978-0-12-812372-0.00010-8>,\n    Yiou et al. (2013) <doi:10.1007/s00382-012-1626-3>.",
    "version": "5.3.0",
    "maintainer": "Theertha Kariyathan <theertha.kariyathan@bsc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1089,
    "package_name": "CSclone",
    "title": "Bayesian Nonparametric Modeling in R",
    "description": "Germline and somatic locus data which contain the total read depth and B allele \n    read depth using Bayesian model (Dirichlet Process) to cluster. Meanwhile, the cluster \n    model can deal with the SNVs mutation and the CNAs mutation.",
    "version": "1.0",
    "maintainer": "Peter Wu <peter123wu0@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1126,
    "package_name": "CalSim",
    "title": "The Calibration Simplex",
    "description": "Generates the calibration simplex (a generalization of the reliability diagram) for three-category probability forecasts, as proposed by Wilks (2013) <doi:10.1175/WAF-D-13-00027.1>.",
    "version": "0.5.4",
    "maintainer": "Johannes Resin <johannes.resin@h-its.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1148,
    "package_name": "CaseBasedReasoning",
    "title": "Case Based Reasoning",
    "description": "Case-based reasoning is a problem-solving methodology that involves solving a new problem by referring to the solution of a similar problem in a large set of previously solved problems. The key aspect of Case Based Reasoning is to determine the problem that \"most closely\" matches the new problem at hand. This is achieved by defining a family of distance functions and using these distance functions as parameters for local averaging regression estimates of the final result. The optimal distance function is chosen based on a specific error measure used in regression estimation. This approach allows for efficient problem-solving by leveraging past experiences and adapting solutions from similar cases. The underlying concept is inspired by the work of Dippon J. (2002) <doi:10.1016/S0167-9473(02)00058-0>.",
    "version": "0.3",
    "maintainer": "Simon Mueller <simon.mueller@muon-stat.com>",
    "url": "https://github.com/sipemu/case-based-reasoning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1149,
    "package_name": "CaseCohortCoxSurvival",
    "title": "Case-Cohort Cox Survival Inference",
    "description": "Cox model inference for relative hazard and covariate-specific pure risk estimated \n             from stratified and unstratified case-cohort data as described in \n             Etievant, L., Gail, M.H. (Lifetime Data Analysis, 2024) <doi:10.1007/s10985-024-09621-2>.",
    "version": "0.0.36",
    "maintainer": "Lola Etievant <lola.etievant@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1158,
    "package_name": "CausalGAM",
    "title": "Estimation of Causal Effects with Generalized Additive Models",
    "description": "Implements various estimators for average\n  treatment effects - an inverse probability weighted (IPW) estimator, \n  an augmented inverse probability weighted (AIPW) estimator, and a standard\n  regression estimator - that make use of generalized additive models for\n  the treatment assignment model and/or outcome model. See: Glynn, Adam N.\n  and Kevin M. Quinn. 2010. \"An Introduction to the Augmented Inverse\n  Propensity Weighted Estimator.\" Political Analysis. 18: 36-56.",
    "version": "0.1-4",
    "maintainer": "Kevin Quinn <kmq@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1159,
    "package_name": "CausalGPS",
    "title": "Matching on Generalized Propensity Scores with Continuous\nExposures",
    "description": "Provides a framework for estimating causal effects of a continuous \n    exposure using observational data, and implementing matching and weighting \n    on the generalized propensity score.\n    Wu, X., Mealli, F., Kioumourtzoglou, M.A., Dominici, F. and Braun, D., 2022. \n    Matching on generalized propensity scores with continuous exposures. Journal \n    of the American Statistical Association, pp.1-29.",
    "version": "0.5.0",
    "maintainer": "Naeem Khoshnevis <nkhoshnevis@g.harvard.edu>",
    "url": "https://github.com/NSAPH-Software/CausalGPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1160,
    "package_name": "CausalImpact",
    "title": "Inferring Causal Effects using Bayesian Structural Time-Series\nModels",
    "description": "Implements a Bayesian approach to causal impact estimation in time\n  series, as described in Brodersen et al. (2015) <DOI:10.1214/14-AOAS788>.\n  See the package documentation on GitHub\n  <https://google.github.io/CausalImpact/> to get started.",
    "version": "1.4.1",
    "maintainer": "Alain Hauser <alhauser@google.com>",
    "url": "https://google.github.io/CausalImpact/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1161,
    "package_name": "CausalMBSTS",
    "title": "MBSTS Models for Causal Inference and Forecasting",
    "description": "Infers the causal effect of an intervention on a multivariate response through the use of Multivariate \n    Bayesian Structural Time Series models (MBSTS) as described in Menchetti & Bojinov (2020) <arXiv:2006.12269>. \n    The package also includes functions for model building and forecasting.  ",
    "version": "0.1.1",
    "maintainer": "Fiammetta Menchetti <fiammetta.menchetti@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1164,
    "package_name": "CausalQueries",
    "title": "Make, Update, and Query Binary Causal Models",
    "description": "Users can declare causal models over binary nodes, update beliefs about causal types given data, and calculate arbitrary queries.  Updating is implemented in 'stan'. See Humphreys and Jacobs, 2023, Integrated Inferences (<DOI: 10.1017/9781316718636>) and Pearl, 2009 Causality (<DOI:10.1017/CBO9780511803161>).",
    "version": "1.4.4",
    "maintainer": "Till Tietz <ttietz2014@gmail.com>",
    "url": "https://integrated-inferences.github.io/CausalQueries/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1179,
    "package_name": "CenBAR",
    "title": "Broken Adaptive Ridge AFT Model with Censored Data",
    "description": "Broken adaptive ridge estimator for censored data is used to\n  select variables and estimate their coefficients in the semi-parametric \n  accelerated failure time model for right-censored survival data.",
    "version": "0.1.1",
    "maintainer": "Zhihua Sun <zhihuasun@ouc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1181,
    "package_name": "CensRegSMSN",
    "title": "Censored Linear Regression Models under Heavy‑tailed\nDistributions",
    "description": "Functions for fitting univariate linear regression models under Scale Mixtures of Skew-Normal (SMSN) distributions, considering left, right or interval censoring and missing responses. Estimation is performed via an EM-type algorithm. Includes selection criteria, sample generation and envelope. For details, see Gil, Y.A., Garay, A.M., and Lachos, V.H. (2025) \n    <doi:10.1007/s10260-025-00797-x>.",
    "version": "0.0.1",
    "maintainer": "Yessenia Alvarez Gil <yessenia.alvarez@ufpe.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1183,
    "package_name": "Census2016",
    "title": "Data from the Australian Census 2016",
    "description": "Contains selected variables from the time series profiles for statistical areas level 2 from the 2006, 2011, and 2016 censuses of population and housing, Australia. Also provides methods for viewing the questions asked for convenience during analysis.",
    "version": "0.2.0",
    "maintainer": "Hugh Parsonage <hugh.parsonage@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1184,
    "package_name": "CepReg",
    "title": "A Cepstral Model for Covariate-Dependent Time Series",
    "description": "Modeling associations between covariates and power spectra of replicated time series using a cepstral-based semiparametric framework. Implements a fast two-stage estimation procedure via Whittle likelihood and multivariate regression.The methodology is based on Li and Dong (2025) <doi:10.1080/10618600.2025.2473936>. ",
    "version": "0.1.3",
    "maintainer": "Qi Xia <xiaqi1010@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1226,
    "package_name": "ChiRP",
    "title": "Chinese Restaurant Process Mixtures of Regressions",
    "description": "What the package does (one paragraph).",
    "version": "1.0.0",
    "maintainer": "",
    "url": "https://github.com/stablemarkets/ChiRP",
    "exports": [],
    "topics": ["bayesian", "bayesian-statistics", "nonparametric-regression", "nonparametric-statistics", "nonparametricbayes", "r"],
    "score": "NA",
    "stars": 12
  },
  {
    "id": 1233,
    "package_name": "ChoiceModelR",
    "title": "Choice Modeling in R",
    "description": "Implements an MCMC algorithm to estimate a hierarchical multinomial logit model with a normal heterogeneity distribution. The algorithm uses a hybrid Gibbs Sampler with a random walk metropolis step for the MNL coefficients for each unit. Dependent variable may be discrete or continuous. Independent variables may be discrete or continuous with optional order constraints. Means of the distribution of heterogeneity can optionally be modeled as a linear function of unit characteristics variables.",
    "version": "1.3.1",
    "maintainer": "John V Colias <jcolias@decisionanalyst.com>",
    "url": "https://www.decisionanalyst.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1244,
    "package_name": "CircOutlier",
    "title": "Detection of Outliers in Circular-Circular Regression",
    "description": "Detection of outliers in circular-circular regression models, modifying its and estimating of models parameters.",
    "version": "3.2.3",
    "maintainer": "Azade Ghazanfarihesari <azade_ghazanfari@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1257,
    "package_name": "ClamR",
    "title": "Time Series Modeling for Climate Change Proxies",
    "description": "Implementation of the Wilkinson and Ivany (2002) approach to paleoclimate analysis, applied to isotope data extracted from clams.",
    "version": "2.1-3",
    "maintainer": "Jonathan M. Lees<jonathan.lees@unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1269,
    "package_name": "CliftLRD",
    "title": "Complex-Valued Wavelet Lifting Estimators of the Hurst Exponent\nfor Irregularly Sampled Time Series",
    "description": "Implementation of Hurst exponent estimators based on complex-valued lifting wavelet energy from Knight, M. I and Nunes, M. A. (2018) <doi:10.1007/s11222-018-9820-8>. ",
    "version": "0.1-2",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1271,
    "package_name": "ClimInd",
    "title": "Climate Indices",
    "description": "Computes 138 standard climate indices at monthly, seasonal and annual resolution. These indices were selected, based on their direct and significant impacts on target sectors, after a thorough review of the literature in the field of extreme weather events and natural hazards. Overall, the selected indices characterize different aspects of the frequency, intensity and duration of extreme events, and are derived from a broad set of climatic variables, including surface air temperature, precipitation, relative humidity, wind speed, cloudiness, solar radiation, and snow cover. The 138 indices have been classified as follow: Temperature based indices (42), Precipitation based indices (22), Bioclimatic indices (21), Wind-based indices (5), Aridity/ continentality indices (10), Snow-based indices (13), Cloud/radiation based indices (6), Drought indices (8), Fire indices (5), Tourism indices (5).",
    "version": "0.1-3",
    "maintainer": "Fergus Reig-Gracia <fergusrg@gmail.com>",
    "url": "https://gitlab.com/indecis-eu/indecis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1273,
    "package_name": "ClimProjDiags",
    "title": "Set of Tools to Compute Various Climate Indices",
    "description": "Set of tools to compute metrics and indices for climate analysis.\n    The package provides functions to compute extreme indices, evaluate the\n    agreement between models and combine theses models into an ensemble. Multi-model\n    time series of climate indices can be computed either after averaging the 2-D\n    fields from different models provided they share a common grid or by combining\n    time series computed on the model native grid. Indices can be assigned weights\n    and/or combined to construct new indices. The package makes use of some of the\n    methods described in:\n    N. Manubens et al. (2018) <doi:10.1016/j.envsoft.2018.01.018>.",
    "version": "0.3.5",
    "maintainer": "Victòria Agudetse <victoria.agudetse@bsc.es>",
    "url": "https://gitlab.earth.bsc.es/es/ClimProjDiags",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1277,
    "package_name": "ClinicalTrialSummary",
    "title": "Summary Measures for Clinical Trials with Survival Outcomes",
    "description": "Provides estimates of several summary measures for clinical trials including\n    the average hazard ratio, the weighted average hazard ratio, the restricted \n    superiority probability ratio, the restricted mean survival difference and \n    the ratio of restricted mean times lost, based on the short-term and \n    long-term hazard ratio model (Yang, 2005 <doi:10.1093/biomet/92.1.1>) \n    which accommodates various non-proportional hazards scenarios. The inference\n    procedures and the asymptotic results for the summary measures are discussed \n    in Yang (2018, <doi:10.1002/sim.7676>).",
    "version": "1.1.1",
    "maintainer": "Daewoo Pak <heavyrain.pak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1314,
    "package_name": "CoDaImpact",
    "title": "Interpreting CoDa Regression Models",
    "description": "\n    Provides methods for interpreting CoDa (Compositional Data) regression models along the lines of \"Pairwise share ratio interpretations of compositional regression models\" (Dargel and Thomas-Agnan 2024) <doi:10.1016/j.csda.2024.107945>.\n    The new methods include variation scenarios, elasticities, elasticity differences and share ratio elasticities.\n    These tools are independent of log-ratio transformations and allow an interpretation in the original space of shares.\n    'CoDaImpact' is designed to be used with the 'compositions' package and its ecosystem.",
    "version": "0.1.0",
    "maintainer": "Lukas Dargel <lukas.dargel@mailbox.org>",
    "url": "https://github.com/LukeCe/CoDaImpact,\nhttps://lukece.github.io/CoDaImpact/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1315,
    "package_name": "CoDaLoMic",
    "title": "Compositional Models to Longitudinal Microbiome Data",
    "description": "Implementation of models to analyse compositional microbiome time series taking into account the interaction between groups of bacteria. The models implemented are described in Creus-Martí et al (2018, ISBN:978-84-09-07541-6), Creus-Martí et al (2021) <doi:10.1155/2021/9951817> and Creus-Martí et al (2022) <doi:10.1155/2022/4907527>.",
    "version": "0.1.1",
    "maintainer": "Irene Creus Martí <ircrmar@mat.upv.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1319,
    "package_name": "CoMiRe",
    "title": "Convex Mixture Regression",
    "description": "Posterior inference under the convex mixture regression (CoMiRe) models introduced by Canale, Durante, and Dunson (2018) <doi:10.1111/biom.12917>.",
    "version": "0.8",
    "maintainer": "Antonio Canale <antonio.canale@unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1323,
    "package_name": "CoSMoS",
    "title": "Complete Stochastic Modelling Solution",
    "description": "Makes univariate, multivariate, or random fields simulations precise and simple. Just select the desired time series or random fields’ properties and it will do the rest. CoSMoS is based on the framework described in Papalexiou (2018, <doi:10.1016/j.advwatres.2018.02.013>), extended for random fields in Papalexiou and Serinaldi (2020, <doi:10.1029/2019WR026331>), and further advanced in Papalexiou et al. (2021, <doi:10.1029/2020WR029466>) to allow fine-scale space-time simulation of storms (or even cyclone-mimicking fields).",
    "version": "2.1.1",
    "maintainer": "Kevin Shook <kevin.shook@usask.ca>",
    "url": "https://github.com/TycheLab/CoSMoS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1334,
    "package_name": "CohortSurvival",
    "title": "Estimate Survival from Common Data Model Cohorts",
    "description": "Estimate survival using data mapped to the Observational Medical Outcomes Partnership common data model. Survival can be estimated based on user-defined study cohorts.",
    "version": "1.1.0",
    "maintainer": "Kim López-Güell <kim.lopez@spc.ox.ac.uk>",
    "url": "https://darwin-eu-dev.github.io/CohortSurvival/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1336,
    "package_name": "CoinMinD",
    "title": "Simultaneous Confidence Intervals for Multinomial Proportions",
    "description": "Several authors have proposed methods for constructing simultaneous confidence intervals for multinomial proportions. The package implements seven classical approaches—Wilson, Quesenberry and Hurst, Goodman, Wald (with and without continuity correction), Fitzpatrick and Scott, and Sison and Glaz—along with Bayesian methods based on Dirichlet models. Both equal and unequal Dirichlet priors are supported, providing a broad framework for inference, data analysis, and sensitivity evaluation.",
    "version": "1.2.1",
    "maintainer": "Subbiah M <sisufive@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1337,
    "package_name": "Coinprofile",
    "title": "Coincident Profile",
    "description": "Builds the \n  coincident profile proposed by Martinez, W and Nieto, Fabio H and Poncela, P (2016) \n  <doi:10.1016/j.spl.2015.11.008>.\n  This methodology studies the relationship between a couple of\n  time series based on the the set of turning points of each\n  time series. The coincident profile establishes if two time\n  series are coincident, or one of them leads the second.",
    "version": "0.1.9",
    "maintainer": "Wilmer Martinez <womartin@asu.edu>",
    "url": "https://github.com/WilmerMartinezR/Coinprofile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1343,
    "package_name": "Colossus",
    "title": "\"Risk Model Regression and Analysis with Complex Non-Linear\nModels\"",
    "description": "Performs survival analysis using general non-linear models. Risk models can be the sum or product of terms. Each term is the product of exponential/linear functions of covariates. Additionally sub-terms can be defined as a sum of exponential, linear threshold, and step functions. Cox Proportional hazards <https://en.wikipedia.org/wiki/Proportional_hazards_model>, Poisson <https://en.wikipedia.org/wiki/Poisson_regression>, and Fine-Gray competing risks <https://www.publichealth.columbia.edu/research/population-health-methods/competing-risk-analysis> regression are supported. This work was sponsored by NASA Grants 80NSSC19M0161 and 80NSSC23M0129 through a subcontract from the National Council on Radiation Protection and Measurements (NCRP). The computing for this project was performed on the Beocat Research Cluster at Kansas State University, which is funded in part by NSF grants CNS-1006860, EPS-1006860, EPS-0919443, ACI-1440548, CHE-1726332, and NIH P20GM113109.",
    "version": "1.4.6",
    "maintainer": "Eric Giunta <egiunta@ksu.edu>",
    "url": "https://ericgiunta.github.io/Colossus/,\nhttps://github.com/ericgiunta/Colossus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1361,
    "package_name": "CompModels",
    "title": "Pseudo Computer Models for Optimization",
    "description": "A suite of computer model test functions that can be used to test and evaluate algorithms for Bayesian (also known as sequential) optimization. Some of the functions have known functional forms, however, most are intended to serve as black-box functions where evaluation requires running computer code that reveals little about the functional forms of the objective and/or constraints. The primary goal of the package is to provide users (especially those who do not have access to real computer models) a source of reproducible and shareable examples that can be used for benchmarking algorithms. The package is a living repository, and so more functions will be added over time. For function suggestions, please do contact the author of the package.",
    "version": "0.3.0",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1365,
    "package_name": "CompareCausalNetworks",
    "title": "Interface to Diverse Estimation Methods of Causal Networks",
    "description": "Unified interface for the estimation of causal networks, including\n    the methods 'backShift' (from package 'backShift'), 'bivariateANM' (bivariate\n    additive noise model), 'bivariateCAM' (bivariate causal additive model),\n    'CAM' (causal additive model) (from package 'CAM'; the package is \n    temporarily unavailable on the CRAN repository; formerly available versions \n    can be obtained from the archive), 'hiddenICP' (invariant\n    causal prediction with hidden variables), 'ICP' (invariant causal prediction)\n    (from package 'InvariantCausalPrediction'), 'GES' (greedy equivalence\n    search), 'GIES' (greedy interventional equivalence search), 'LINGAM', 'PC' (PC\n    Algorithm), 'FCI' (fast causal inference), \n    'RFCI' (really fast causal inference) (all from package 'pcalg') and\n    regression.",
    "version": "0.2.6.2",
    "maintainer": "Christina Heinze-Deml <heinzedeml@stat.math.ethz.ch>",
    "url": "https://github.com/christinaheinze/CompareCausalNetworks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1385,
    "package_name": "ConSpline",
    "title": "Partial Linear Least-Squares Regression using Constrained\nSplines",
    "description": "Given response y, continuous predictor x, and covariate matrix, the relationship between E(y) and x is estimated with a shape constrained regression spline.  Function outputs fits and various types of inference.",
    "version": "1.2",
    "maintainer": "Mary C Meyer <meyer@stat.colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1407,
    "package_name": "ConsReg",
    "title": "Fits Regression & ARMA Models Subject to Constraints to the\nCoefficient",
    "description": "Fits or generalized linear models either a regression with Autoregressive moving-average (ARMA) errors for time series data. \n       The package makes it easy to incorporate constraints into the model's coefficients. \n          The model is specified by an objective function (Gaussian, Binomial or Poisson) or an ARMA order (p,q), \n          a vector of bound constraints \n          for the coefficients (i.e beta1 > 0) and the possibility to incorporate restrictions\n          among coefficients (i.e beta1 > beta2).\n          The references of this packages are the same as 'stats' package for glm() and arima() functions.\n          See Brockwell, P. J. and Davis, R. A. (1996, ISBN-10: 9783319298528).\n          For the different optimizers implemented, it is recommended to consult the documentation of the corresponding packages. ",
    "version": "0.1.0",
    "maintainer": "Josep Puig <puigjos@gmail.com>",
    "url": "https://github.com/puigjos/ConsReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1411,
    "package_name": "ContRespPP",
    "title": "Predictive Probability for a Continuous Response with an ANOVA\nStructure",
    "description": "A Bayesian approach to using \n    predictive probability in an ANOVA construct with a continuous normal response, \n    when threshold values must be obtained for the question of interest to be \n    evaluated as successful (Sieck and Christensen (2021) <doi:10.1002/qre.2802>). \n    The Bayesian Mission Mean (BMM) is used to evaluate a question \n    of interest (that is, a mean that randomly selects combination of factor levels \n    based on their probability of occurring instead of averaging over the factor \n    levels, as in the grand mean). Under this construct, in contrast to a Gibbs \n    sampler (or Metropolis-within-Gibbs sampler), a two-stage sampling method is \n    required. The nested sampler determines the conditional posterior distribution \n    of the model parameters, given Y, and the outside sampler determines the marginal \n    posterior distribution of Y (also commonly called the predictive distribution for Y). \n    This approach provides a sample from the joint posterior distribution of Y and \n    the model parameters, while also accounting for the threshold value that must be \n    obtained in order for the question of interest to be evaluated as successful.",
    "version": "0.4.2",
    "maintainer": "Victoria Sieck <vcarrillo314@gmail.com>",
    "url": "https://github.com/jcliff89/ContRespPP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1423,
    "package_name": "CopCTS",
    "title": "Copula-Based Semiparametric Analysis for Time Series Data with\nDetection Limits",
    "description": "Semiparametric estimation for censored time series\n    with lower detection limit. The latent response is a sequence of\n    stationary process with Markov property\n    of order one.\n    Estimation of copula parameter(COPC) and Conditional quantile estimation\n    are included for five available copula functions.\n    Copula selection methods based on L2 distance from empirical copula function\n    are also included.",
    "version": "1.0.0",
    "maintainer": "Fuyuan David Li <LFY@gwmail.gwu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1427,
    "package_name": "Copula.Markov",
    "title": "Copula-Based Estimation and Statistical Process Control for\nSerially Correlated Time Series",
    "description": "Estimation and statistical process control are performed under\n copula-based time-series models.\n Available are statistical methods in Long and Emura (2014 JCSA),\n Emura et al. (2017 Commun Stat-Simul) <DOI:10.1080/03610918.2015.1073303>,\n Huang and Emura (2021 Commun Stat-Simul) <DOI:10.1080/03610918.2019.1602647>,\n Lin et al. (2021 Comm Stat-Simul) <DOI:10.1080/03610918.2019.1652318>,\n Sun et al. (2020 JSS Series in Statistics)<DOI:10.1007/978-981-15-4998-4>,\n and Huang and Emura (2021, in revision).",
    "version": "2.9",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1429,
    "package_name": "Copula.surv",
    "title": "Analysis of Bivariate Survival Data Based on Copulas",
    "description": "Simulating bivariate survival data from copula models.\n Estimation of the association parameter in copula models.\n Two different ways to estimate the association parameter in copula models are implemented.\n A goodness-of-fit test for a given copula model is implemented.\n See Emura, Lin and Wang (2010) <doi:10.1016/j.csda.2010.03.013> for details.",
    "version": "3.0",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1431,
    "package_name": "CopulaGAMM",
    "title": "Copula-Based Mixed Regression Models",
    "description": "Estimation of 2-level factor copula-based regression models  for clustered data where the response variable can be either discrete or continuous. ",
    "version": "0.6.5",
    "maintainer": "Bruno N Remillard <bruno.remillard@hec.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1448,
    "package_name": "Counterfactual",
    "title": "Estimation and Inference Methods for Counterfactual Analysis",
    "description": "Implements the estimation and inference methods for counterfactual analysis described in Chernozhukov, Fernandez-Val and Melly (2013) <DOI:10.3982/ECTA10582> \"Inference on Counterfactual Distributions,\" Econometrica, 81(6). The counterfactual distributions considered are the result of changing either the marginal distribution of covariates related to the outcome variable of interest, or the conditional distribution of the outcome given the covariates. They can be applied to estimate quantile treatment effects and wage decompositions.",
    "version": "1.2",
    "maintainer": "Ivan Fernandez-Val <ivanf@bu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1450,
    "package_name": "Countr",
    "title": "Flexible Univariate Count Models Based on Renewal Processes",
    "description": "Flexible univariate count models based on renewal processes. The\n    models may include covariates and can be specified with familiar formula\n    syntax as in glm() and package 'flexsurv'.  The methodology is described by\n    Kharrat et all (2019) <doi:10.18637/jss.v090.i13> (included as vignette\n    'Countr_guide' in the package).",
    "version": "3.6",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/Countr/ (doc),\nhttps://CRAN.R-project.org/package=Rdpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1453,
    "package_name": "CovEsts",
    "title": "Nonparametric Estimators for Covariance Functions",
    "description": "Several nonparametric estimators of autocovariance functions. Procedures for constructing their confidence regions by using bootstrap techniques. Methods to correct autocovariance estimators and several tools for analysing and comparing them. Supplementary functions, including kernel computations and discrete cosine Fourier transforms. For more details see Bilchouris and Olenko (2025) <doi:10.17713/ajs.v54i1.1975>.",
    "version": "1.0.0",
    "maintainer": "Adam Bilchouris <adam.bilchouris@gmail.com>",
    "url": "https://github.com/AdamBilchouris/CovEsts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1460,
    "package_name": "CoxBoost",
    "title": "Cox Models by Likelihood Based Boosting for a Single Survival\nEndpoint or Competing Risks",
    "description": "Provides routines for fitting Cox models by likelihood based\n    boosting for single event survival data with right censoring or in the\n    presence of competing risks. The methodology is described in Binder\n    and Schumacher (2008) <doi:10.1186/1471-2105-9-14> and Binder et al.\n    (2009) <doi:10.1093/bioinformatics/btp088>.",
    "version": "1.5.1",
    "maintainer": "John Zobolas <bblodfon@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1461,
    "package_name": "CoxICPen",
    "title": "Variable Selection for Cox's Model with Interval-Censored Data",
    "description": "Perform variable selection for Cox regression model with interval-censored data. Can deal with both low-dimensional and high-dimensional data. Case-cohort design can be incorporated. Two sets of covariates scenario can also be considered. The references are listed in the URL below.",
    "version": "1.1.0",
    "maintainer": "Qiwei Wu <qw235@mail.missouri.edu>",
    "url": "https://doi.org/10.1080/01621459.2018.1537922,\nhttps://doi.org/10.1002/sim.8594,\nhttps://doi.org/10.1002/bimj.201900180",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1463,
    "package_name": "CoxPlus",
    "title": "Cox Regression (Proportional Hazards Model) with Multiple Causes\nand Mixed Effects",
    "description": "Extends the Cox model to events with more than one causes. Also supports random and fixed effects, tied events, and time-varying variables. Model details are provided in Peng et al. (2018) <doi:10.1509/jmr.14.0643>.",
    "version": "1.5.7",
    "maintainer": "Jing Peng <jing.peng@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1464,
    "package_name": "CoxR2",
    "title": "R-Squared Measure Based on Partial LR Statistic, for the Cox PH\nRegression Model",
    "description": "Calculate the R-squared, aka explained randomness, based on the partial likelihood ratio statistic under the Cox Proportional Hazard model [J O'Quigley, R Xu, J Stare (2005) <doi:10.1002/sim.1946>].",
    "version": "1.0",
    "maintainer": "Hyeri You <h2you@health.ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1488,
    "package_name": "CureDepCens",
    "title": "Dependent Censoring Regression Models with Cure Fraction",
    "description": "Cure dependent censoring regression models for long-term survival multivariate data. These models are based on extensions of the frailty models, capable to accommodating the cure fraction and the dependence between failure and censoring times, with Weibull and piecewise exponential marginal distributions. Theoretical details regarding the models implemented in the package can be found in Schneider et al. (2022) <doi:10.1007/s10651-022-00549-0>.",
    "version": "0.1.0",
    "maintainer": "Silvana Schneider <silvana.schneider@ufrgs.br>",
    "url": "https://github.com/GabrielGrandemagne/CureDepCens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1494,
    "package_name": "CutpointsOEHR",
    "title": "Optimal Equal-HR Method to Find Two Cutpoints for U-Shaped\nRelationships in Cox Model",
    "description": "Use optimal equal-HR method to determine two optimal cutpoints of a continuous predictor that has a U-shaped relationship with survival outcomes based on Cox regression model. The optimal equal-HR method estimates two optimal cut-points that have approximately the same log hazard value based on Cox regression model and divides individuals into different groups according to their HR values.",
    "version": "0.1.2",
    "maintainer": "Yimin Chen <chenyimin0226@126.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1497,
    "package_name": "Cyclops",
    "title": "Cyclic Coordinate Descent for Logistic, Poisson and Survival\nAnalysis",
    "description": "This model fitting tool incorporates cyclic coordinate descent and\n    majorization-minimization approaches to fit a variety of regression models\n    found in large-scale observational healthcare data.  Implementations focus\n    on computational optimization and fine-scale parallelization to yield\n    efficient inference in massive datasets.  Please see:\n    Suchard, Simpson, Zorych, Ryan and Madigan (2013) <doi:10.1145/2414416.2414791>.",
    "version": "3.6.0",
    "maintainer": "Marc A. Suchard <msuchard@ucla.edu>",
    "url": "https://github.com/ohdsi/cyclops",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1513,
    "package_name": "DAAG",
    "title": "Data Analysis and Graphics Data and Functions",
    "description": "Functions and data sets used in examples and exercises in the\n        text Maindonald, J.H. and Braun, W.J. (2003, 2007, 2010) \"Data\n        Analysis and Graphics Using R\", and in an upcoming Maindonald,\n        Braun, and Andrews text that builds on this earlier text.",
    "version": "1.25.7",
    "maintainer": "W. John Braun <john.braun@ubc.ca>",
    "url": "https://gitlab.com/daagur/DAAG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1529,
    "package_name": "DATAstudio",
    "title": "The Research Data Warehouse of Miguel de Carvalho",
    "description": "Pulls together a collection of datasets from Miguel de Carvalho research articles. Including, for example:\n    - de Carvalho (2012) <doi:10.1016/j.jspi.2011.08.016>;\n    - de Carvalho et al (2012) <doi:10.1080/03610926.2012.709905>;\n    - de Carvalho et al (2012) <doi:10.1016/j.econlet.2011.09.007>);\n    - de Carvalho and Davison (2014) <doi:10.1080/01621459.2013.872651>;\n    - de Carvalho and Rua (2017) <doi:10.1016/j.ijforecast.2015.09.004>;\n    - de Carvalho et al (2023) <doi:10.1002/sta4.560>;\n    - de Carvalho et al (2022) <doi:10.1007/s13253-021-00469-9>;\n    - Palacios et al (2024) <doi:10.1214/24-BA1420>.",
    "version": "1.2.1",
    "maintainer": "Miguel de Carvalho <Miguel.deCarvalho@ed.ac.uk>",
    "url": "https://www.maths.ed.ac.uk/~mdecarv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1536,
    "package_name": "DBModelSelect",
    "title": "Distribution-Based Model Selection",
    "description": "Perform model selection using distribution and probability-based methods,\n\tincluding standardized AIC, BIC, and AICc. These standardized information criteria\n\tallow one to perform model selection in a way similar to the prevalent \"Rule of 2\"\n\tmethod, but formalize the method to rely on probability theory. A novel goodness-of-fit\n\tprocedure for assessing linear regression models is also available. This test relies on\n\ttheoretical properties of the estimated error variance for a normal linear regression\n\tmodel, and employs a bootstrap procedure to assess the null hypothesis that the fitted\n\tmodel shows no lack of fit. For more information, see Koeneman and Cavanaugh (2023)\n\t<arXiv:2309.10614>. Functionality to perform all subsets linear or generalized linear\n\tregression is also available.",
    "version": "0.2.0",
    "maintainer": "Scott H. Koeneman <Scott.Koeneman@jefferson.edu>",
    "url": "https://github.com/shkoeneman/DBModelSelect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1538,
    "package_name": "DBR",
    "title": "Discrete Beta Regression",
    "description": "Bayesian Beta Regression, adapted for bounded discrete responses, commonly seen in survey responses.\n  Estimation is done via Markov Chain Monte Carlo sampling, using a Gibbs wrapper around univariate slice sampler \n  (Neal (2003) <DOI:10.1214/aos/1056562461>), as implemented in the R package MfUSampler \n  (Mahani and Sharabiani (2017) <DOI: 10.18637/jss.v078.c01>).",
    "version": "1.4.1",
    "maintainer": "Alireza Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1541,
    "package_name": "DBfit",
    "title": "A Double Bootstrap Method for Analyzing Linear Models with\nAutoregressive Errors",
    "description": "Computes the double bootstrap as discussed in McKnight, McKean, and Huitema (2000) <doi:10.1037/1082-989X.5.1.87>. \n              The double bootstrap method provides a better fit for a linear model with autoregressive errors than ARIMA when the sample size is small.",
    "version": "2.0",
    "maintainer": "Shaofeng Zhang <shaofeng.zhang@wmich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1549,
    "package_name": "DCL",
    "title": "Claims Reserving under the Double Chain Ladder Model",
    "description": "Statistical modelling and forecasting in claims reserving in non-life insurance under the Double Chain Ladder framework by Martinez-Miranda, Nielsen and Verrall (2012).",
    "version": "0.1.2",
    "maintainer": "Maria Dolores Martinez-Miranda <mmiranda@ugr.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1553,
    "package_name": "DChaos",
    "title": "Chaotic Time Series Analysis",
    "description": "Chaos theory has been hailed as a revolution of thoughts and attracting ever increasing \n    attention of many scientists from diverse disciplines. Chaotic systems are nonlinear deterministic \n    dynamic systems which can behave like an erratic and apparently random motion. A relevant field\n    inside chaos theory and nonlinear time series analysis is the detection of a chaotic behaviour \n    from empirical time series data. One of the main features of chaos is the well known initial value \n    sensitivity property. Methods and techniques related to test the hypothesis of chaos try to quantify \n    the initial value sensitive property estimating the Lyapunov exponents. The DChaos package \n    provides different useful tools and efficient algorithms which test robustly the hypothesis of chaos \n    based on the Lyapunov exponent in order to know if the data generating process behind time series \n    behave chaotically or not.",
    "version": "0.1-7",
    "maintainer": "Julio E. Sandubete <jsandube@ucm.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1559,
    "package_name": "DDL",
    "title": "Doubly Debiased Lasso (DDL)",
    "description": "Statistical inference for the regression coefficients in high-dimensional linear models with hidden confounders. The Doubly Debiased Lasso method was proposed in <arXiv:2004.03758>.",
    "version": "1.0.2",
    "maintainer": "Zijian Guo <zijguo@stat.rutgers.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1563,
    "package_name": "DDPstar",
    "title": "Density Regression via Dirichlet Process Mixtures of Normal\nStructured Additive Regression Models",
    "description": "Implements a flexible, versatile, and computationally tractable model for density regression based on a single-weights dependent Dirichlet process mixture of normal distributions model for univariate continuous responses. The model assumes an additive structure for the mean of each mixture component and the effects of continuous covariates are captured through smooth nonlinear functions. The key components of our modelling approach are penalised B-splines and their bivariate tensor product extension. The proposed method can also easily deal with parametric effects of categorical covariates, linear effects of continuous covariates, interactions between categorical and/or continuous covariates, varying coefficient terms, and random effects. Please see Rodriguez-Alvarez, Inacio et al. (2025) for more details.",
    "version": "1.0-1",
    "maintainer": "Maria Xose Rodriguez-Alvarez <mxrodriguez@uvigo.gal>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1565,
    "package_name": "DEBBI",
    "title": "Differential Evolution-Based Bayesian Inference",
    "description": "Bayesian inference algorithms based on the population-based \"differential evolution\" (DE) algorithm. Users can obtain posterior mode (MAP) estimates via DEMAP, posterior samples via DEMCMC, and variational approximations via DEVI. ",
    "version": "0.1.0",
    "maintainer": "Brendan Matthew Galdo <Brendan.m.galdo@gmail.com>",
    "url": "https://github.com/bmgaldo/DEBBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1580,
    "package_name": "DENDRO",
    "title": "Dna based EvolutioNary tree preDiction by scRna-seq technOlogy",
    "description": "We propose DENDRO, a statistical framework takes scRNA-seq data",
    "version": "0.2.2",
    "maintainer": "Zilu Zhou <zhouzilu@pennmedicine.upenn.edu>",
    "url": "https://github.com/zhouzilu/DENDRO",
    "exports": [],
    "topics": ["bioinformatics", "computational-biology", "single-cell", "statistics", "tumor-heterogeneity"],
    "score": "NA",
    "stars": 35
  },
  {
    "id": 1605,
    "package_name": "DFBA",
    "title": "Distribution-Free Bayesian Analysis",
    "description": "A set of functions to perform distribution-free Bayesian analyses. \n             Included are Bayesian analogues to the frequentist Mann-Whitney U \n             test, the Wilcoxon Signed-Ranks test, Kendall's Tau Rank \n             Correlation Coefficient, Goodman and Kruskal's Gamma, McNemar's\n             Test, the binomial test, the sign test, the median test, as well as \n             distribution-free methods for testing contrasts among condition and \n             for computing Bayes factors for hypotheses. The package also\n             includes procedures to estimate the power of distribution-free\n             Bayesian tests based on data simulations using various probability \n             models for the data. The set of functions provide data analysts \n             with a set of Bayesian procedures that avoids requiring parametric \n             assumptions about measurement error and is robust to problem of \n             extreme outlier scores.",
    "version": "0.1.0",
    "maintainer": "Daniel H. Barch <daniel.barch@tufts.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1613,
    "package_name": "DGLMExtPois",
    "title": "Double Generalized Linear Models Extending Poisson Regression",
    "description": "Model estimation, dispersion testing and diagnosis of hyper-Poisson\n    Saez-Castillo, A.J. and Conde-Sanchez, A. (2013) \n    <doi:10.1016/j.csda.2012.12.009> and Conway-Maxwell-Poisson Huang, A. (2017)\n    regression models.",
    "version": "0.2.4",
    "maintainer": "Francisco Martinez <fmartin@ujaen.es>",
    "url": "https://github.com/franciscomartinezdelrio/DGLMExtPois",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1614,
    "package_name": "DGP4LCF",
    "title": "Dependent Gaussian Processes for Longitudinal Correlated Factors",
    "description": "Functionalities for analyzing high-dimensional and longitudinal biomarker data to facilitate precision medicine, using a joint model of Bayesian sparse factor analysis and dependent Gaussian processes. This paper illustrates the method in detail: J Cai, RJB Goudie, C Starr, BDM Tom (2023) <doi:10.48550/arXiv.2307.02781>.",
    "version": "1.0.0.1",
    "maintainer": "Jiachen Cai <jiachen.cai@mrc-bsu.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1635,
    "package_name": "DIMORA",
    "title": "Diffusion Models R Analysis",
    "description": "The implemented methods are: Standard Bass model, Generalized Bass model (with rectangular shock, exponential shock, and mixed shock. You can choose to add from 1 to 3 shocks), Guseo-Guidolin model and Variable Potential Market model, and UCRCD model. The Bass model consists of a simple differential equation that describes the process of how new products get adopted in a population, the Generalized Bass model is a generalization of the Bass model in which there is a \"carrier\" function x(t) that allows to change the speed of time sliding. In some real processes the reachable potential of the resource available in a temporal instant may appear to be not constant over time, because of this we use Variable Potential Market model, in which the Guseo-Guidolin has a particular specification for the market function. The UCRCD model (Unbalanced Competition and Regime Change Diachronic) is a diffusion model used to capture the dynamics of the competitive or collaborative transition.",
    "version": "0.3.6",
    "maintainer": "Savio Andrea <svandr97@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1636,
    "package_name": "DIRECT",
    "title": "Bayesian Clustering of Multivariate Data Under the\nDirichlet-Process Prior",
    "description": "A Bayesian clustering method for replicated time series or replicated measurements from multiple experimental conditions, e.g., time-course gene expression data.  It estimates the number of clusters directly from the data using a Dirichlet-process prior.  See Fu, A. Q., Russell, S., Bray, S. and Tavare, S. (2013) Bayesian clustering of replicated time-course gene expression data with weak signals. The Annals of Applied Statistics. 7(3) 1334-1361. <doi:10.1214/13-AOAS650>.",
    "version": "1.1.0",
    "maintainer": "Audrey Q. Fu <audreyqyfu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1647,
    "package_name": "DJL",
    "title": "Distance Measure Based Judgment and Learning",
    "description": "Implements various decision support tools related to the Econometrics & Technometrics.\n             Subroutines include correlation reliability test, Mahalanobis distance measure for outlier detection, combinatorial search (all possible subset regression), non-parametric efficiency analysis measures: DDF (directional distance function), DEA (data envelopment analysis), HDF (hyperbolic distance function), SBM (slack-based measure), and SF (shortage function), benchmarking, Malmquist productivity analysis, risk analysis, technology adoption model, new product target setting, network DEA, dynamic DEA, intertemporal budgeting, etc.",
    "version": "3.9",
    "maintainer": "Dong-Joon Lim <tgno3.com@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1651,
    "package_name": "DLMRMV",
    "title": "Distributed Linear Regression Models with Response Missing\nVariables",
    "description": "As a distributed imputation strategy, the Distributed full\n        information Multiple Imputation method is developed to impute\n        missing response variables in distributed linear regression.\n        The philosophy of the package is described in 'Guo' (2025)\n        <doi:10.1038/s41598-025-93333-6>.",
    "version": "1.0.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1678,
    "package_name": "DNetFinder",
    "title": "Estimating Differential Networks under Semiparametric Gaussian\nGraphical Models",
    "description": "Provides a modified hierarchical test (Liu (2017) <doi:10.1214/17-AOS1539>) for detecting the structural difference between two Semiparametric Gaussian graphical models. The multiple testing procedure asymptotically controls the false discovery rate (FDR) at a user-specified level. To construct the test statistic, a truncated estimator is used to approximate the transformation functions and two R functions including lassoGGM() and lassoNPN() are provided to compute the lasso estimates of the regression coefficients. ",
    "version": "1.1",
    "maintainer": "Qingyang Zhang <qz008@uark.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1689,
    "package_name": "DOvalidation",
    "title": "Kernel Hazard Estimation with Best One-Sided and Double\nOne-Sided Cross-Validation",
    "description": "Local linear hazard estimator and its multiplicatively bias correction, including three bandwidth selection methods: best one-sided cross-validation, double one-sided cross-validation, and standard cross-validation.",
    "version": "1.1.0",
    "maintainer": "Maria Dolores Martinez-Miranda <mmiranda@ugr.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1692,
    "package_name": "DPP",
    "title": "Inference of Parameters of Normal Distributions from a Mixture\nof Normals",
    "description": "This MCMC method takes a data numeric vector (Y) and assigns the elements of Y\n  to a (potentially infinite) number of normal distributions. The individual normal distributions from a mixture of normals can be inferred.\n  Following the method described in Escobar (1994) <doi:10.2307/2291223> we use a Dirichlet Process Prior (DPP) to describe stochastically our prior assumptions about the dimensionality of the data.",
    "version": "0.1.2",
    "maintainer": "Luis M. Avila <lmavila@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1695,
    "package_name": "DPTM",
    "title": "Dynamic Panel Multiple Threshold Model with Fixed Effects",
    "description": "Compute the fixed effects dynamic panel threshold model suggested by Ramírez-Rondán (2020) <doi:10.1080/07474938.2019.1624401>, and dynamic panel linear model suggested by Hsiao et al. (2002) <doi:10.1016/S0304-4076(01)00143-9>, where maximum likelihood type estimators are used. Multiple thresholds estimation based on Markov Chain Monte Carlo (MCMC) is allowed, and model selection of linear model, threshold model and multiple threshold model is also allowed.",
    "version": "3.0.2",
    "maintainer": "Bai Hujie <hujiebai@163.com>",
    "url": "https://github.com/HujieBai/DPTM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1697,
    "package_name": "DPtree",
    "title": "Dirichlet-Based Polya Tree",
    "description": "Contains functions to perform copula estimation \n\tby the non-parametric Bayesian method, \n\tDirichlet-based Polya Tree. See Ning (2018) <doi:10.1080/00949655.2017.1421194>.",
    "version": "1.0.1",
    "maintainer": "Shaoyang Ning <shaoyangning@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1703,
    "package_name": "DRDID",
    "title": "Doubly Robust Difference-in-Differences Estimators",
    "description": "Implements the locally efficient doubly robust difference-in-differences (DiD)\n    estimators for the average treatment effect proposed by Sant'Anna and Zhao (2020)\n    <doi:10.1016/j.jeconom.2020.06.003>. The estimator combines inverse probability weighting and outcome\n    regression estimators (also implemented in the package) to form estimators with\n    more attractive statistical properties. Two different estimation methods can be used\n    to estimate the nuisance functions.",
    "version": "1.2.3",
    "maintainer": "Pedro H. C. Sant'Anna <pedrohcgs@gmail.com>",
    "url": "https://psantanna.com/DRDID/, https://github.com/pedrohcgs/DRDID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1705,
    "package_name": "DREGAR",
    "title": "Regularized Estimation of Dynamic Linear Regression in the\nPresence of Autocorrelated Residuals (DREGAR)",
    "description": "A penalized/non-penalized implementation for dynamic regression in the presence of autocorrelated residuals (DREGAR) using iterative penalized/ordinary least squares. It applies Mallows CP, AIC, BIC and GCV to select the tuning parameters.",
    "version": "0.1.4.0",
    "maintainer": "Hamed Haselimashhadi <hamedhaseli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1708,
    "package_name": "DRIP",
    "title": "Discontinuous Regression and Image Processing",
    "description": "A collection of functions that perform jump regression\n\t     and image analysis such as denoising, deblurring and\n\t     jump detection. The implemented methods are based on\n\t     the following research: Qiu, P. (1998) <doi:10.1214/aos/1024691468>,\n\t     Qiu, P. and Yandell, B. (1997) <doi: 10.1080/10618600.1997.10474746>,\n\t     Qiu, P. (2009) <doi: 10.1007/s10463-007-0166-9>,\n\t     Kang, Y. and Qiu, P. (2014) <doi: 10.1080/00401706.2013.844732>,\n\t     Qiu, P. and Kang, Y. (2015) <doi: 10.5705/ss.2014.054>,\n\t     Kang, Y., Mukherjee, P.S. and Qiu, P. (2018) <doi: 10.1080/00401706.2017.1415975>,\n\t     Kang, Y. (2020) <doi: 10.1080/10618600.2019.1665536>.",
    "version": "2.3",
    "maintainer": "Yicheng Kang <kangyicheng0527@gmail.com>",
    "url": "https://miamioh.edu/fsb/directory/?up=/directory/kangy10,\nhttps://github.com/kangy10/DRIP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1711,
    "package_name": "DRR",
    "title": "Dimensionality Reduction via Regression",
    "description": "An Implementation of Dimensionality Reduction\n    via Regression using Kernel Ridge Regression.",
    "version": "0.0.4",
    "maintainer": "Guido Kraemer <gkraemer@bgc-jena.mpg.de>",
    "url": "https://github.com/gdkrmr/DRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1716,
    "package_name": "DRsurvCRT",
    "title": "Doubly-Robust Estimation for Survival Outcomes in\nCluster-Randomized Trials",
    "description": "Cluster-randomized trials (CRTs) assign treatment to groups rather than individuals, so valid analyses must distinguish cluster-level and individual-level effects and define estimands within a potential-outcomes framework. This package supports right-censored survival outcomes for both single-state (binary) and multi-state settings. For single-state outcomes, it provides estimands based on stage-specific survival contrasts (SPCE) and restricted mean survival time (RMST). For multi-state outcomes, it provides SPCE as well as a generalized win-based restricted mean time-in-favor estimand (RMT-IF). The package implements doubly robust estimators that accommodate covariate-dependent censoring and remain consistent if either the outcome model or the censoring model is correctly specified. Users can choose marginal Cox or gamma-frailty Cox working models for nuisance estimation, and inference is supported via leave-one-cluster-out jackknife variance and confidence interval estimation. Methods are described in Fang et al. (2025) \"Estimands and doubly robust estimation for cluster-randomized trials with survival outcomes\" <doi:10.48550/arXiv.2510.08438>.",
    "version": "0.0.1",
    "maintainer": "Xi Fang <x.fang@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1721,
    "package_name": "DSBayes",
    "title": "Bayesian Subgroup Analysis in Clinical Trials",
    "description": "Calculate posterior modes and credible intervals of parameters of the Dixon-Simon model for subgroup analysis (with binary covariates) in clinical trials. For details of the methodology, please refer to D.O. Dixon and R. Simon (1991), Biometrics, 47: 871-881.",
    "version": "2023.1.0",
    "maintainer": "Ravi Varadhan <ravi.varadhan@jhu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1736,
    "package_name": "DTAXG",
    "title": "Diagnostic Test Assessment in the Absence of Gold Standard",
    "description": "To calculate the sensitivity and specificity in the absence of gold standard using the Bayesian method.\n    The Bayesian method can be referenced at Haiyan Gu and Qiguang Chen (1999) <doi:10.3969/j.issn.1002-3674.1999.04.004>.",
    "version": "0.1.0",
    "maintainer": "Zhicheng Du <dgdzc@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1737,
    "package_name": "DTAplots",
    "title": "Creates Plots Accompanying Bayesian Diagnostic Test Accuracy\nMeta-Analyses",
    "description": "Function to create forest plots. Functions to use posterior samples from Bayesian bivariate meta-analysis model, Bayesian hierarchical summary receiver operating characteristic (HSROC) meta-analysis model or Bayesian latent class (LC) meta-analysis model to create Summary Receiver Operating Characteristic (SROC) plots using methods described by Harbord et al (2007)<doi:10.1093/biostatistics/kxl004>.",
    "version": "1.0.2.5",
    "maintainer": "Ian Schiller <ian.schiller@rimuhc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1739,
    "package_name": "DTDA",
    "title": "Doubly Truncated Data Analysis",
    "description": "Implementation of different algorithms for analyzing\n        randomly truncated data, one-sided and two-sided (i.e. doubly)\n        truncated data. It serves to compute empirical cumulative \n        distributions and also kernel density and hazard functions \n        using different bandwidth selectors.\n      Several real data sets are included.",
    "version": "3.0.1",
    "maintainer": "Carla Moreira <carlamgmm@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1743,
    "package_name": "DTEBOP2",
    "title": "Bayesian Optimal Phase II Randomized Clinical Trial Design with\nDelayed Outcomes",
    "description": "Implements a Bayesian Optimal Phase II design (DTE-BOP2) for trials with delayed treatment effects, particularly relevant to immunotherapy studies where treatment benefits may emerge after a delay. The method builds upon the BOP2 framework and incorporates uncertainty in the delay timepoint through a truncated gamma prior, informed by expert knowledge or default settings. Supports two-arm trial designs with functionality for sample size determination, interim and final analyses, and comprehensive simulation under various delay and design scenarios. Ensures rigorous type I and II error control while improving trial efficiency and power when the delay effect is present. A manuscript describing the methodology is under development and will be formally referenced upon publication.",
    "version": "1.0.3",
    "maintainer": "Zhongheng Cai <zhonghengcai123@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1746,
    "package_name": "DTRlearn2",
    "title": "Statistical Learning Methods for Optimizing Dynamic Treatment\nRegimes",
    "description": "We provide a comprehensive software to estimate general K-stage DTRs from SMARTs with Q-learning and a variety of outcome-weighted learning methods. Penalizations are allowed for variable selection and model regularization. With the outcome-weighted learning scheme, different loss functions - SVM hinge loss, SVM ramp loss, binomial deviance loss, and L2 loss - are adopted to solve the weighted classification problem at each stage; augmentation in the outcomes is allowed to improve efficiency. The estimated DTR can be easily applied to a new sample for individualized treatment recommendations or DTR evaluation.",
    "version": "1.1",
    "maintainer": "Yuan Chen <irene.yuan.chen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1747,
    "package_name": "DTRreg",
    "title": "DTR Estimation and Inference via G-Estimation, Dynamic WOLS,\nQ-Learning, and Dynamic Weighted Survival Modeling (DWSurv)",
    "description": "Dynamic treatment regime estimation and inference via G-estimation, \n  dynamic weighted ordinary least squares (dWOLS) and Q-learning. Inference via \n  bootstrap and recursive sandwich estimation. Estimation and \n  inference for survival outcomes via Dynamic Weighted Survival Modeling (DWSurv). \n  Extension to continuous treatment variables. Wallace et al. (2017) \n  <DOI:10.18637/jss.v080.i02>; Simoneau et al. (2020) \n  <DOI:10.1080/00949655.2020.1793341>.",
    "version": "2.3",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1750,
    "package_name": "DTSg",
    "title": "A Class for Working with Time Series Data Based on 'data.table'\nand 'R6' with Largely Optional Reference Semantics",
    "description": "Basic time series functionalities such as listing of missing\n    values, application of arbitrary aggregation as well as rolling (asymmetric)\n    window functions and automatic detection of periodicity. As it is mainly\n    based on 'data.table', it is fast and (in combination with the 'R6' package)\n    offers reference semantics. In addition to its native R6 interface, it\n    provides an S3 interface for those who prefer the latter. Finally yet\n    importantly, its functional approach allows for incorporating\n    functionalities from many other packages.",
    "version": "2.0.0",
    "maintainer": "Gerold Hepp <gisler@hepp.cc>",
    "url": "https://gisler.github.io/DTSg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1752,
    "package_name": "DTWBI",
    "title": "Imputation of Time Series Based on Dynamic Time Warping",
    "description": "Functions to impute large gaps within time series based on Dynamic Time Warping methods. It contains all required functions to create large missing consecutive values within time series and to fill them, according to the paper Phan et al. (2017), <DOI:10.1016/j.patrec.2017.08.019>. Performance criteria are added to compare similarity between two signals (query and reference).",
    "version": "1.1",
    "maintainer": "Emilie Poisson-Caillault <emilie.poisson@univ-littoral.fr>",
    "url": "http://mawenzi.univ-littoral.fr/DTWBI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1753,
    "package_name": "DTWUMI",
    "title": "Imputation of Multivariate Time Series Based on Dynamic Time\nWarping",
    "description": "Functions to impute large gaps within multivariate time series based on Dynamic Time Warping methods. Gaps of size 1 or inferior to a defined threshold are filled using simple average and weighted moving average respectively. Larger gaps are filled using the methodology provided by Phan et al. (2017) <DOI:10.1109/MLSP.2017.8168165>: a query is built immediately before/after a gap and a moving window is used to find the most similar sequence to this query using Dynamic Time Warping. To lower the calculation time, similar sequences are pre-selected using global features. Contrary to the univariate method (package 'DTWBI'), these global features are not estimated over the sequence containing the gap(s), but a feature matrix is built to summarize general features of the whole multivariate signal. Once the most similar sequence to the query has been identified, the adjacent sequence to this window is used to fill the gap considered. This function can deal with multiple gaps over all the sequences componing the input multivariate signal. However, for better consistency, large gaps at the same location over all sequences should be avoided.",
    "version": "1.0",
    "maintainer": "POISSON-CAILLAULT Emilie <emilie.poisson@univ-littoral.fr>",
    "url": "http://mawenzi.univ-littoral.fr/DTWUMI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1760,
    "package_name": "DWaveNARDL",
    "title": "Dual Wavelet Based NARDL Model",
    "description": "Dual Wavelet based Nonlinear Autoregressive Distributed Lag model has been developed for noisy time series analysis. This package is designed to capture both short-run and long-run relationships in time series data, while incorporating wavelet transformations. The methodology combines the NARDL model with wavelet decomposition to better capture the nonlinear dynamics of the series and exogenous variables. The package is useful for analyzing economic and financial time series data that exhibit both long-term trends and short-term fluctuations. This package has been developed using algorithm of Jammazi et al. <doi:10.1016/j.intfin.2014.11.011>.",
    "version": "0.1.0",
    "maintainer": "Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1761,
    "package_name": "DWreg",
    "title": "Parametric Regression for Discrete Response",
    "description": "Regression for a discrete response, where the conditional distribution is modelled via a discrete Weibull distribution.",
    "version": "3.0",
    "maintainer": "Veronica Vinciotti <veronica.vinciotti@unitn.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1769,
    "package_name": "Dark",
    "title": "The Analysis of Dark Adaptation Data",
    "description": "The recovery of visual sensitivity in a dark environment is known\n    as dark adaptation. In a clinical or research setting the recovery is typically\n    measured after a dazzling flash of light and can be described by the Mahroo,\n    Lamb and Pugh (MLP) model of dark adaptation. The functions in this package take\n    dark adaptation data and use nonlinear regression to find the parameters of the\n    model that 'best' describe the data. They do this by firstly, generating rapid\n    initial objective estimates of data adaptation parameters, then a multi-start\n    algorithm is used to reduce the possibility of a local minimum. There is also a\n    bootstrap method to calculate parameter confidence intervals. The functions rely\n    upon a 'dark' list or object. This object is created as the first step in the\n    workflow and parts of the object are updated as it is processed.",
    "version": "0.9.9",
    "maintainer": "Jeremiah MF Kelly <emkayoh@mac.com>",
    "url": "https://github.com/emkayoh/Dark",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1781,
    "package_name": "DataSetsUni",
    "title": "A Collection of Univariate Data Sets",
    "description": "A collection of widely used univariate data sets of various applied domains on applications of distribution theory. The functions allow researchers and practitioners to quickly, easily, and efficiently access and use these data sets. The data are related to different applied domains and as follows: Bio-medical, survival analysis, medicine, reliability analysis, hydrology, actuarial science, operational research, meteorology, extreme values, quality control, engineering, finance, sports and economics. The total 100 data sets are documented along with associated references for further details and uses.     ",
    "version": "0.1",
    "maintainer": "Muhammad Imran <imranshakoor84@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1782,
    "package_name": "DataSetsVerse",
    "title": "A Metapackage for Thematic and Domain-Specific Datasets",
    "description": "A metapackage that brings together a curated collection \n    of R packages containing domain-specific datasets. It includes time series data, \n    educational metrics, crime records, medical datasets, and oncology research data. \n    Designed to provide researchers, analysts, educators, and data scientists with \n    centralized access to structured and well-documented datasets, this metapackage \n    facilitates reproducible research, data exploration, and teaching applications across \n    a wide range of domains.\n    Included packages:\n    - 'timeSeriesDataSets': Time series data from economics, finance, energy, and healthcare.\n    - 'educationR': Datasets related to education, learning outcomes, and school metrics.\n    - 'crimedatasets': Datasets on global and local crime and criminal behavior.\n    - 'MedDataSets': Datasets related to medicine, public health, treatments, and clinical trials.\n    - 'OncoDataSets': Datasets focused on cancer research, survival, genetics, and biomarkers.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "url": "https://github.com/lightbluetitan/datasetsverse,\nhttps://lightbluetitan.github.io/datasetsverse/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1797,
    "package_name": "DeCAFS",
    "title": "Detecting Changes in Autocorrelated and Fluctuating Signals",
    "description": "Detect abrupt changes in time series with local fluctuations as a random walk process and autocorrelated noise as an AR(1) process. See Romano, G., Rigaill, G., Runge, V., Fearnhead, P. (2021) <doi:10.1080/01621459.2021.1909598>.",
    "version": "3.3.5",
    "maintainer": "Gaetano Romano <g.romano@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1803,
    "package_name": "DebiasInfer",
    "title": "Efficient Inference on High-Dimensional Linear Model with\nMissing Outcomes",
    "description": "A statistically and computationally efficient debiasing method for conducting valid inference on the high-dimensional linear regression function with missing outcomes.\n    The reference paper is Zhang, Giessing, and Chen (2023) <arXiv:2309.06429>. ",
    "version": "0.2",
    "maintainer": "Yikun Zhang <yikunzhang@foxmail.com>",
    "url": "https://github.com/zhangyk8/Debias-Infer/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1831,
    "package_name": "DepCens",
    "title": "Dependent Censoring Regression Models",
    "description": "Dependent censoring regression models for survival multivariate data. These models are based on extensions of the frailty models, capable to accommodating the dependence between failure and censoring times, with Weibull and piecewise exponential marginal distributions. Theoretical details regarding the models implemented in the package can be found in Schneider et al. (2019) <doi:10.1002/bimj.201800391>.",
    "version": "0.2.3",
    "maintainer": "Silvana Schneider <silvana.schneider@ufrgs.br>",
    "url": "https://github.com/GabrielGrandemagne/DepCens,\nhttps://gabrielgrandemagne.github.io/DepCens/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1835,
    "package_name": "DepthProc",
    "title": "Statistical Depth Functions for Multivariate Analysis",
    "description": "Data depth concept offers a variety of powerful and user friendly\n    tools for robust exploration and inference for multivariate data. The offered\n    techniques may be successfully used in cases of lack of our knowledge on\n    parametric models generating data due to their nature. The\n    package consist of among others implementations of several data depth techniques\n    involving multivariate quantile-quantile plots, multivariate scatter estimators,\n    multivariate Wilcoxon tests and robust regressions.",
    "version": "2.1.6",
    "maintainer": "Zygmunt Zawadzki <zygmunt@zstat.pl>",
    "url": "https://www.depthproc.zstat.pl/,\nhttps://github.com/zzawadz/DepthProc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1848,
    "package_name": "DevTreatRules",
    "title": "Develop Treatment Rules with Observational Data",
    "description": "Develop and evaluate treatment rules based on: (1) the standard indirect approach of split-regression, which fits regressions separately in both treatment groups and assigns an individual to the treatment option under which predicted outcome is more desirable; (2) the direct approach of outcome-weighted-learning proposed by Yingqi Zhao, Donglin Zeng, A. John Rush, and Michael Kosorok (2012) <doi:10.1080/01621459.2012.695674>; (3) the direct approach, which we refer to as direct-interactions, proposed by Shuai Chen, Lu Tian, Tianxi Cai, and Menggang Yu (2017) <doi:10.1111/biom.12676>. Please see the vignette for a walk-through of how to start with an observational dataset whose design is understood scientifically and end up with a treatment rule that is trustworthy statistically, along with an estimation of rule benefit in an independent sample.",
    "version": "1.1.0",
    "maintainer": "Jeremy Roth <jhroth@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1855,
    "package_name": "DiPs",
    "title": "Directional Penalties for Optimal Matching in Observational\nStudies",
    "description": "Improves the balance of optimal matching with near-fine balance by giving penalties on the unbalanced covariates with the unbalanced directions. Many directional penalties can also be viewed as Lagrange multipliers, pushing a matched sample in the direction of satisfying a linear constraint that would not be satisfied without penalization.\n    Yu and Rosenbaum (2019) <doi:10.1111/biom.13098>. ",
    "version": "0.6.4",
    "maintainer": "Ruoqi Yu <ruoqiyu125@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1878,
    "package_name": "Dire",
    "title": "Linear Regressions with a Latent Outcome Variable",
    "description": "Fit latent variable linear models, estimating score distributions for groups of people, following Cohen and Jiang (1999) <doi:10.2307/2669917>. In this model, a latent distribution is conditional on students item response, item characteristics, and conditioning variables the user includes. This latent trait is then integrated out. This software is intended to fit the same models as the existing software 'AM' <https://am.air.org/>. As of version 2, also allows the user to draw plausible values.",
    "version": "2.2.0",
    "maintainer": "Paul Bailey <pbailey@air.org>",
    "url": "https://american-institutes-for-research.github.io/Dire/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1880,
    "package_name": "DirectEffects",
    "title": "Estimating Controlled Direct Effects for Explaining Causal\nFindings",
    "description": "A set of functions to estimate the controlled direct effect of treatment fixing a potential mediator to a specific value. Implements the sequential g-estimation estimator described in Vansteelandt (2009) <doi:10.1097/EDE.0b013e3181b6f4c9> and Acharya, Blackwell, and Sen (2016) <doi:10.1017/S0003055416000216> and the telescope matching estimator described in Blackwell and Strezhnev (2020) <doi:10.1111/rssa.12759>.  ",
    "version": "0.3",
    "maintainer": "Matthew Blackwell <mblackwell@gmail.com>",
    "url": "https://mattblackwell.github.io/DirectEffects/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1886,
    "package_name": "DirichletReg",
    "title": "Dirichlet Regression",
    "description": "Implements Dirichlet regression models.",
    "version": "0.7-2",
    "maintainer": "Marco Johannes Maier <marco_maier@posteo.de>",
    "url": "https://github.com/maiermarco/DirichletReg\nhttps://CRAN.R-project.org/package=DirichletReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1892,
    "package_name": "DiscreteDLM",
    "title": "Bayesian Distributed Lag Model Fitting for Binary and Count\nResponse Data",
    "description": "Tools for fitting Bayesian Distributed Lag Models (DLMs) to longitudinal response data that is a count or binary. Count data is fit using negative binomial regression and binary is fit using quantile regression. The contribution of the lags are fit via b-splines. In addition, infers the predictor inclusion uncertainty. Multimomial models are not supported. Based on Dempsey and Wyse (2025) <doi:10.48550/arXiv.2403.03646>.",
    "version": "1.0.0",
    "maintainer": "Daniel Dempsey <daniel.dempsey0@gmail.com>",
    "url": "https://github.com/DanDempsey/DiscreteDLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1907,
    "package_name": "Distributacalcul",
    "title": "Probability Distribution Functions",
    "description": "Calculates expected values, variance, different moments (kth \n    moment, truncated mean), stop-loss, mean excess loss, Value-at-Risk (VaR)\n    and Tail Value-at-Risk (TVaR) as well as some density and cumulative \n    (survival) functions of continuous, discrete and compound distributions. \n    This package also includes a visual 'Shiny' component to enable students \n    to visualize distributions and understand the impact of their parameters.\n    This package is intended to expand the 'stats' package so as\n    to enable students to develop an intuition for probability.",
    "version": "0.4.0",
    "maintainer": "Alec James van Rassel <alec.van-rassel.1@ulaval.ca>",
    "url": "https://alec42.github.io/Distributacalcul_Package/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1935,
    "package_name": "DrBats",
    "title": "Data Representation: Bayesian Approach That's Sparse",
    "description": "Feed longitudinal data into a Bayesian Latent Factor Model to obtain \n  a low-rank representation. Parameters are estimated using a Hamiltonian \n  Monte Carlo algorithm with STAN. See G. Weinrott, B. Fontez, N. Hilgert and \n  S. Holmes, \"Bayesian Latent Factor Model for Functional Data Analysis\", \n  Actes des JdS 2016.",
    "version": "0.1.6",
    "maintainer": "Benedicte Fontez <benedicte.fontez@supagro.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1960,
    "package_name": "DynNom",
    "title": "Visualising Statistical Models using Dynamic Nomograms",
    "description": "Demonstrate the results of a statistical model object as a dynamic nomogram in an RStudio panel or web browser. The package provides two generics functions: DynNom, which display statistical model objects as a dynamic nomogram; DNbuilder, which builds required scripts to publish a dynamic nomogram on a web server such as the <https://www.shinyapps.io/>. Current version of 'DynNom' supports stats::lm, stats::glm, survival::coxph, rms::ols, rms::Glm, rms::lrm, rms::cph, and mgcv::gam model objects.",
    "version": "5.1",
    "maintainer": "Amirhossein Jalali <amir.jalali@ul.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1961,
    "package_name": "DynTxRegime",
    "title": "Methods for Estimating Optimal Dynamic Treatment Regimes",
    "description": "Methods to estimate dynamic treatment regimes using Interactive\n  Q-Learning, Q-Learning, weighted learning, and value-search methods based on \n  Augmented Inverse Probability Weighted Estimators and Inverse Probability\n  Weighted Estimators. Dynamic Treatment Regimes: Statistical Methods for \n  Precision Medicine, Tsiatis, A. A., Davidian, M. D., Holloway, S. T., and Laber, E. B., \n  Chapman & Hall/CRC Press, 2020, ISBN:978-1-4987-6977-8.",
    "version": "4.16",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1969,
    "package_name": "EBASE",
    "title": "Estuarine Bayesian Single-Station Estimation Method for\nEcosystem Metabolism",
    "description": "Estimate ecosystem metabolism in a Bayesian framework for\n  individual water quality monitoring stations with continuous dissolved\n  oxygen time series. A mass balance equation is used that provides\n  estimates of parameters for gross primary production, respiration,\n  and gas exchange. Methods adapted from Grace et al. (2015)\n  <doi:10.1002/lom3.10011> and Wanninkhof (2014) <doi:10.4319/lom.2014.12.351>.\n  Details in Beck et al. (2024) <doi:10.1002/lom3.10620>.",
    "version": "1.1.0",
    "maintainer": "Marcus Beck <mbeck@tbep.org>",
    "url": "https://fawda123.github.io/EBASE/,\nhttps://github.com/fawda123/EBASE/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1970,
    "package_name": "EBCHS",
    "title": "An Empirical Bayes Method for Chi-Squared Data",
    "description": "We provide the main R functions to compute the posterior interval for the noncentrality parameter of the chi-squared distribution. The skewness estimate of the posterior distribution is also available to improve the coverage rate of posterior intervals. Details can be found in Du and Hu (2020) <doi:10.1080/01621459.2020.1777137>.  ",
    "version": "0.1.0",
    "maintainer": "Lilun Du <dulilun@ust.hk>",
    "url": "https://github.com/dulilun/EBCHS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1971,
    "package_name": "EBEN",
    "title": "Empirical Bayesian Elastic Net",
    "description": "Provides the Empirical Bayesian Elastic Net for handling multicollinearity in generalized linear regression models.  As a special case of the 'EBglmnet'\n  package (also available on CRAN), this package encourages a grouping effects to select relevant variables and estimate the corresponding non-zero effects. ",
    "version": "5.2",
    "maintainer": "Anhui Huang <anhuihuang@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1973,
    "package_name": "EBMAforecast",
    "title": "Estimate Ensemble Bayesian Model Averaging Forecasts using Gibbs\nSampling or EM-Algorithms",
    "description": "Create forecasts from multiple predictions using ensemble Bayesian model averaging (EBMA). EBMA models can be estimated using an expectation maximization (EM) algorithm or as fully Bayesian models via Gibbs sampling. The methods in this package are Montgomery, Hollenbach, and Ward (2015) <doi:10.1016/j.ijforecast.2014.08.001> and Montgomery, Hollenbach, and Ward (2012) <doi:10.1093/pan/mps002>.",
    "version": "1.0.32",
    "maintainer": "Florian M. Hollenbach <fho.egb@cbs.dk>",
    "url": "https://github.com/fhollenbach/EBMA/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1980,
    "package_name": "EBrank",
    "title": "Empirical Bayes Ranking",
    "description": "Empirical Bayes ranking applicable to parallel-estimation settings where the estimated parameters are asymptotically unbiased and normal, with known standard errors.  A mixture normal prior for each parameter is estimated using Empirical Bayes methods, subsequentially ranks for each parameter are simulated from the resulting joint posterior over all parameters (The marginal posterior densities for each parameter are assumed independent). Finally, experiments are ordered by expected posterior rank, although computations minimizing other plausible rank-loss functions are also given.  ",
    "version": "1.0.0",
    "maintainer": "John Ferguson <john.ferguson@nuigalway.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 1987,
    "package_name": "ECTSVR",
    "title": "Cointegration Based Support Vector Regression Model",
    "description": "The cointegration based support vector regression model enables researchers to use data obtained from the cointegrating vector as input in the support vector regression model.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2013,
    "package_name": "EGAnet",
    "title": "Exploratory Graph Analysis – a Framework for Estimating the\nNumber of Dimensions in Multivariate Data using Network\nPsychometrics",
    "description": "Implements the Exploratory Graph Analysis (EGA) framework for dimensionality\n             and psychometric assessment. EGA estimates the number of dimensions in\n\t     \t psychological data using network estimation methods and community detection\n             algorithms. A bootstrap method is provided to assess the stability of dimensions\n\t     \t and items. Fit is evaluated using the Entropy Fit family of indices. Unique \n             Variable Analysis evaluates the extent to which items are locally dependent (or\n             redundant). Network loadings provide similar information to factor loadings and\n\t     \t can be used to compute network scores. A bootstrap and permutation approach are\n             available to assess configural and metric invariance. Hierarchical structures\n             can be detected using Hierarchical EGA. Time series and intensive longitudinal \n\t     \t data can be analyzed using Dynamic EGA, supporting individual, group, and \n             population level assessments.",
    "version": "2.4.0",
    "maintainer": "Hudson Golino <hfg9s@virginia.edu>",
    "url": "https://r-ega.net",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2015,
    "package_name": "EGRET",
    "title": "Exploration and Graphics for RivEr Trends",
    "description": "Statistics and graphics for streamflow history,\n    water quality trends, and the statistical modeling algorithm: Weighted\n    Regressions on Time, Discharge, and Season (WRTDS). ",
    "version": "3.0.11",
    "maintainer": "Laura DeCicco <ldecicco@usgs.gov>",
    "url": "https://pubs.usgs.gov/tm/04/a10/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2016,
    "package_name": "EGRETci",
    "title": "Exploration and Graphics for RivEr Trends Confidence Intervals",
    "description": "Collection of functions to evaluate uncertainty of results from\n    water quality analysis using the Weighted Regressions on Time Discharge and\n    Season (WRTDS) method. This package is an add-on to the EGRET package that\n    performs the WRTDS analysis. The WRTDS modeling\n    method was initially introduced and discussed in Hirsch et al. (2010) <doi:10.1111/j.1752-1688.2010.00482.x>,\n    and expanded in Hirsch and De Cicco (2015) <doi:10.3133/tm4A10>. The \n    paper describing the uncertainty and confidence interval calculations \n    is Hirsch et al. (2015) <doi:10.1016/j.envsoft.2015.07.017>.",
    "version": "2.0.5",
    "maintainer": "Laura DeCicco <ldecicco@usgs.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2025,
    "package_name": "EKMCMC",
    "title": "MCMC Procedures for Estimating Enzyme Kinetics Constants",
    "description": "Functions for estimating catalytic constant and Michaelis-Menten constant\n            for enzyme kinetics model using Metropolis-Hasting algorithm within Gibbs \n            sampler based on the Bayesian framework. ",
    "version": "1.1.2",
    "maintainer": "Hyukpyo Hong <hphong@kaist.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2027,
    "package_name": "EL2Surv",
    "title": "Empirical Likelihood (EL) for Comparing Two Survival Functions",
    "description": "Functions for computing critical values and implementing the one-sided/two-sided EL tests.",
    "version": "1.1",
    "maintainer": "Guo-You Lan <jj6020770416jj@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2034,
    "package_name": "ELYP",
    "title": "Empirical Likelihood Analysis for the Cox Model and\nYang-Prentice (2005) Model",
    "description": "Empirical likelihood ratio tests for the Yang and Prentice (short/long term hazards ratio) model. \n             Empirical likelihood tests within a Cox model, for parameters defined via \n\t\t\t both baseline hazard function and regression parameters.",
    "version": "0.7-6",
    "maintainer": "Mai Zhou <maizhou@gmail.com>",
    "url": "http://www.ms.uky.edu/~mai/EmpLik.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2037,
    "package_name": "EMAR",
    "title": "Empirical Model Assessment",
    "description": "A tool that allows users to generate various indices for evaluating statistical models. The fitstat() function computes indices based on the fitting data. The valstat() function computes indices based on the validation data set. Both fitstat() and valstat() will return 16 indices SSR: residual sum of squares, TRE: total relative error, Bias: mean bias, MRB: mean relative bias, MAB: mean absolute bias, MAPE: mean absolute percentage error, MSE: mean squared\terror, RMSE: root mean square error, Percent.RMSE: percentage root mean squared error, R2: coefficient of determination, R2adj: adjusted coefficient of determination, APC: Amemiya's prediction criterion, logL: Log-likelihood, AIC: Akaike information criterion, AICc: corrected Akaike information criterion, BIC: Bayesian information criterion, HQC: Hannan-Quin information criterion. The\tlower the better for the SSR, TRE, Bias, MRB, MAB, MAPE, MSE, RMSE, Percent.RMSE, APC, AIC, AICc, BIC and HQC indices. The higher the better for R2 and R2adj indices. Petre Stoica, P., Selén, Y. (2004) <doi:10.1109/MSP.2004.1311138>\\n Zhou et al. (2023) <doi:10.3389/fpls.2023.1186250>\\n Ogana, F.N., Ercanli, I. (2021) <doi:10.1007/s11676-021-01373-1>\\n Musabbikhah et al. (2019) <doi:10.1088/1742-6596/1175/1/012270>.",
    "version": "1.0.0",
    "maintainer": "Friday Nwabueze Ogana <ogana_fry@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2039,
    "package_name": "EMC2",
    "title": "Bayesian Hierarchical Analysis of Cognitive Models of Choice",
    "description": "Fit Bayesian (hierarchical) cognitive models\n    using a linear modeling language interface using particle Metropolis Markov\n    chain Monte Carlo sampling with Gibbs steps. The diffusion decision model (DDM), \n    linear ballistic accumulator model (LBA), racing diffusion model (RDM), and the lognormal\n    race model (LNR) are supported. Additionally, users can specify their own likelihood\n    function and/or choose for non-hierarchical\n    estimation, as well as for a diagonal, blocked or full multivariate normal\n    group-level distribution to test individual differences. Prior specification \n    is facilitated through methods that visualize the (implied) prior. \n    A wide range of plotting functions assist in assessing model convergence and\n    posterior inference. Models can be easily evaluated using functions\n    that plot posterior predictions or using relative model comparison metrics \n    such as information criteria or Bayes factors.\n    References: Stevenson et al. (2024) <doi:10.31234/osf.io/2e4dq>.",
    "version": "3.3.0",
    "maintainer": "Niek Stevenson <niek.stevenson@gmail.com>",
    "url": "https://ampl-psych.github.io/EMC2/,\nhttps://github.com/ampl-psych/EMC2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2041,
    "package_name": "EMD",
    "title": "Empirical Mode Decomposition and Hilbert Spectral Analysis",
    "description": "For multiscale analysis, this package carries out empirical mode decomposition and Hilbert spectral\n        analysis. For usage of EMD, see Kim and Oh, 2009 (Kim, D and Oh, H.-S. (2009) EMD: A Package for Empirical \n        Mode Decomposition and Hilbert Spectrum, The R Journal, 1, 40-46). ",
    "version": "1.5.9",
    "maintainer": "Donghoh Kim <donghoh.kim@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2043,
    "package_name": "EMDSVRhybrid",
    "title": "Empirical Mode Decomposition Based Support Vector Regression\nModel",
    "description": "Description: Application of empirical mode decomposition based support vector regression model for nonlinear and non stationary univariate time series forecasting. For method details see (i) Choudhury (2019) <http://krishi.icar.gov.in/jspui/handle/123456789/44873>; (ii) Das (2020) <http://krishi.icar.gov.in/jspui/handle/123456789/43174>; (iii) Das (2023) <http://krishi.icar.gov.in/jspui/handle/123456789/77772>.",
    "version": "0.2.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2045,
    "package_name": "EMGCR",
    "title": "Fit a Mixture Cure Rate Model with Custom Link Function",
    "description": "Tools to fit Mixture Cure Rate models via the Expectation-Maximization (EM) algorithm, allowing for flexible link functions in the cure component and various survival distributions in the latency part. The package supports user-specified link functions, includes methods for parameter estimation and model diagnostics, and provides residual analysis tailored for cure models. The classical theory methods used are described in Berkson, J. and Gage, R. P. (1952) <doi:10.2307/2281318>, Dempster, A. P., Laird, N. M. and Rubin, D. B. (1977) <https://www.jstor.org/stable/2984875>, Bazán, J., Torres-Avilés, F., Suzuki, A. and Louzada, F. (2017)<doi:10.1002/asmb.2215>.",
    "version": "0.2.0",
    "maintainer": "Jalmar M. F. Carrasco <carrasco.jalmar@ufba.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2046,
    "package_name": "EMJMCMC",
    "title": "Evolutionary Mode Jumping Markov Chain Monte Carlo Expert\nToolbox",
    "description": "Implementation of the Mode Jumping Markov Chain Monte Carlo algorithm from Hubin, A., Storvik, G. (2018) <doi:10.1016/j.csda.2018.05.020>, Genetically Modified Mode Jumping Markov Chain Monte Carlo from Hubin, A., Storvik, G., & Frommlet, F. (2020) <doi:10.1214/18-BA1141>, Hubin, A., Storvik, G., & Frommlet, F. (2021) <doi:10.1613/jair.1.13047>, and Hubin, A., Heinze, G., & De Bin, R. (2023) <doi:10.3390/fractalfract7090641>, and Reversible Genetically Modified Mode Jumping Markov Chain Monte Carlo from Hubin, A., Frommlet, F., & Storvik, G. (2021) <doi:10.48550/arXiv.2110.05316>, which allow for estimating posterior model probabilities and Bayesian model averaging across a wide set of Bayesian models including linear, generalized linear, generalized linear mixed, generalized nonlinear, generalized nonlinear mixed, and logic regression models.",
    "version": "1.5.0",
    "maintainer": "Waldir Leoncio <w.l.netto@medisin.uio.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2054,
    "package_name": "EMMREML",
    "title": "Fitting Mixed Models with Known Covariance Structures",
    "description": "The main functions are 'emmreml', and 'emmremlMultiKernel'. 'emmreml' solves a mixed model with known covariance structure using the 'EMMA' algorithm.  'emmremlMultiKernel' is a wrapper for 'emmreml' to handle multiple random components with known covariance structures. The function 'emmremlMultivariate' solves a multivariate gaussian mixed model with known covariance structure using the 'ECM' algorithm.",
    "version": "3.1",
    "maintainer": "Deniz Akdemir <deniz.akdemir.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2055,
    "package_name": "EMOTIONS",
    "title": "'EMOTIONS: Ensemble Models for Lactation Curves'",
    "description": "Lactation curve modeling plays a central role in dairy production, supporting management decisions and the selection of animals with superior productivity and resilience. The package 'EMOTIONS' fits 47 models for lactation curves and creates ensemble models using model averaging based on Akaike information criterion, Bayesian information criterion, root mean square percentage error, and mean squared error, variance of the predictions, cosine similarity for each model's predictions, and Bayesian Model Average. The daily production values predicted through the ensemble models can be used to estimate resilience indicators in the package. Additionally, the package allows the graphical visualization of the model ranks and the predicted lactation curves.",
    "version": "1.0",
    "maintainer": "Pablo Fonseca <p.fonseca@csic.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2069,
    "package_name": "EPLSIM",
    "title": "Partial Linear Single Index Models for Environmental Mixture\nAnalysis",
    "description": "Collection of ancillary functions and utilities for Partial Linear Single Index Models for Environmental mixture analyses, which currently provides functions for scalar outcomes. The outputs of these functions include the single index function, single index coefficients, partial linear coefficients, mixture overall effect, exposure main and interaction effects, and differences of quartile effects. In the future, we will add functions for binary, ordinal, Poisson, survival, and longitudinal outcomes, as well as models for time-dependent exposures. See Wang et al (2020) <doi:10.1186/s12940-020-00644-4> for an overview. ",
    "version": "0.1.1",
    "maintainer": "Yuyan Wang <yuyan.wang@nyumc.org>",
    "url": "https://github.com/YuyanWangSixTwo/EPLSIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2077,
    "package_name": "EQUALrepeat",
    "title": "Algorithm Driven Time Series Analysis for Researchers without\nCoding Skills",
    "description": "Support functions for R-based 'EQUAL-STATS' software which automatically classifies the data and performs appropriate statistical tests. 'EQUAL-STATS' software is a shiny application with an user-friendly interface to perform complex statistical analysis. Gurusamy,K (2024)<doi:10.5281/zenodo.13354162>.",
    "version": "0.4.0",
    "maintainer": "Kurinchi Gurusamy <k.gurusamy@ucl.ac.uk>",
    "url": "https://sites.google.com/view/equal-group/home",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2083,
    "package_name": "ERPeq",
    "title": "Probabilistic Hazard Assessment",
    "description": "Computes the probability density and cumulative distribution functions of fourteen distributions used for the probabilistic hazard assessment. Estimates the model parameters of the distributions using the maximum likelihood and reports the goodness-of-fit statistics. The recurrence interval estimations of earthquakes are computed for each distribution.",
    "version": "0.1.0",
    "maintainer": "Emrah Altun <emrahaltun123@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2084,
    "package_name": "ERSA",
    "title": "Exploratory Regression 'Shiny' App",
    "description": "Constructs a 'shiny' app function with interactive displays for summary and analysis of variance regression tables, and parallel coordinate plots of data and residuals.",
    "version": "0.1.4",
    "maintainer": "Catherine B. Hurley <catherine.hurley@mu.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2086,
    "package_name": "ES",
    "title": "Edge Selection",
    "description": "Implementation of the Edge Selection Algorithm for undirected graph selection.  The least angle regression-based algorithm selects edges of an undirected graph based on the projection of the current residuals on the two dimensional edge-planes.  The algorithm selects symmetric adjacency matrix, which many other regression-based undirected graph selection procedures cannot do. ",
    "version": "1.1",
    "maintainer": "Sanjay Chaudhuri <schaudhuri2@unl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2088,
    "package_name": "ESTER",
    "title": "Efficient Sequential Testing with Evidence Ratios",
    "description": "An implementation of sequential testing that uses evidence ratios\n    computed from the weights of a set of models. These weights correspond either\n    to Akaike weights computed from the Akaike Information Criterion (AIC) or the\n    Bayesian Information Criterion (BIC) and following Burnham & Anderson\n    (2004, <doi:10.1177/0049124104268644>) recommendations, or to pseudo-BMA\n    weights computed from the WAIC or the LOO-IC of models fitted\n    with 'brms' and following Yao et al. (2017, <arXiv:1704.02030v3>).",
    "version": "0.2.0",
    "maintainer": "Ladislas Nalborczyk <ladislas.nalborczyk@gmail.com>",
    "url": "https://github.com/lnalborczyk/ESTER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2098,
    "package_name": "EValue",
    "title": "Sensitivity Analyses for Unmeasured Confounding and Other Biases\nin Observational Studies and Meta-Analyses",
    "description": "Conducts sensitivity analyses for unmeasured confounding, selection bias, and measurement error (individually or in combination; VanderWeele & Ding (2017) <doi:10.7326/M16-2607>; Smith & VanderWeele (2019) <doi:10.1097/EDE.0000000000001032>; VanderWeele & Li (2019) <doi:10.1093/aje/kwz133>; Smith, Mathur, & VanderWeele (2021) <doi:10.1097/EDE.0000000000001380>). Also conducts sensitivity analyses for unmeasured confounding in meta-analyses (Mathur & VanderWeele (2020a) <doi:10.1080/01621459.2018.1529598>; Mathur & VanderWeele (2020b) <doi:10.1097/EDE.0000000000001180>) and for additive measures of effect modification (Mathur et al., <doi:10.1093/ije/dyac073>).  ",
    "version": "4.1.4",
    "maintainer": "Maya B. Mathur <mmathur@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2103,
    "package_name": "EWS",
    "title": "Early Warning System",
    "description": "The purpose of Early Warning Systems (EWS) is to detect accurately the occurrence of a crisis, which is represented by a binary variable which takes the value of one when the event occurs, and the value of zero otherwise. EWS are a toolbox for policymakers to prevent or attenuate the impact of economic downturns. Modern EWS are based on the econometric framework of Kauppi and Saikkonen (2008) <doi:10.1162/rest.90.4.777>. Specifically, this framework includes four dichotomous models, relying on a logit approach to model the relationship between yield spreads and future recessions, controlling for recession risk factors. These models can be estimated in a univariate or a balanced panel framework as in Candelon, Dumitrescu and Hurlin (2014) <doi:10.1016/j.ijforecast.2014.03.015>. This package provides both methods for estimating these models and a dataset covering 13 OECD countries over a period of 45 years. In addition, this package also provides methods for the analysis of the propagation mechanisms of an exogenous shock, as well as robust confidence intervals for these response functions using a block-bootstrap method as in Lajaunie (2021). This package constitutes a useful toolbox (data and functions) for scholars as well as policymakers.",
    "version": "0.2.0",
    "maintainer": "Quentin Lajaunie <quentin_lajaunie@hotmail.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2105,
    "package_name": "EXPAR",
    "title": "Fitting of Exponential Autoregressive (EXPAR) Model",
    "description": "The amplitude-dependent exponential autoregressive (EXPAR) time series model, initially proposed by Haggan and Ozaki (1981) <doi:10.2307/2335819> has been implemented in this package. Throughout various studies, the model has been found to adequately capture the cyclical nature of datasets. Parameter estimation of such family of models has been tackled by the approach of minimizing the residual sum of squares (RSS). Model selection among various candidate orders has been implemented using various information criteria, viz., Akaike information criteria (AIC), corrected Akaike information criteria (AICc) and Bayesian information criteria (BIC). An illustration utilizing data of egg price indices has also been provided.",
    "version": "0.1.0",
    "maintainer": "Saikath Das <saikathdas007@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2106,
    "package_name": "EXPARMA",
    "title": "Fitting of Exponential Autoregressive Moving Average (EXPARMA)\nModel",
    "description": "The amplitude-dependent autoregressive time series model (EXPAR) proposed by Haggan and Ozaki (1981) <doi:10.2307/2335819> was improved by incorporating the moving average (MA) framework for capturing the variability efficiently. Parameters of the EXPARMA model can be estimated using this package. The user is provided with the best fitted EXPARMA model for the data set under consideration.",
    "version": "0.1.0",
    "maintainer": "Bishal Gurung <Bishal.Gurung@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2107,
    "package_name": "EXRQ",
    "title": "Extreme Regression of Quantiles",
    "description": "Estimation for high conditional quantiles based on quantile regression.",
    "version": "1.0",
    "maintainer": "Huixia Judy Wang <judywang@gwu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2117,
    "package_name": "EbayesThresh",
    "title": "Empirical Bayes Thresholding and Related Methods",
    "description": "Empirical Bayes thresholding using the methods developed\n    by I. M. Johnstone and B. W. Silverman. The basic problem is to\n    estimate a mean vector given a vector of observations of the mean\n    vector plus white noise, taking advantage of possible sparsity in\n    the mean vector. Within a Bayesian formulation, the elements of\n    the mean vector are modelled as having, independently, a\n    distribution that is a mixture of an atom of probability at zero\n    and a suitable heavy-tailed distribution. The mixing parameter can\n    be estimated by a marginal maximum likelihood approach. This leads\n    to an adaptive thresholding approach on the original data.\n    Extensions of the basic method, in particular to wavelet\n    thresholding, are also implemented within the package.",
    "version": "1.4-12",
    "maintainer": "Peter Carbonetto <peter.carbonetto@gmail.com>",
    "url": "https://github.com/stephenslab/EbayesThresh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2118,
    "package_name": "Ecdat",
    "title": "Data Sets for Econometrics",
    "description": "Data sets for econometrics, including political science.",
    "version": "0.4.7",
    "maintainer": "Spencer Graves <spencer.graves@effectivedefense.org>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2121,
    "package_name": "EcoDiet",
    "title": "Estimating a Diet Matrix from Biotracer and Stomach Content Data",
    "description": "Biotracers and stomach content analyses are combined in a Bayesian hierarchical model\n    to estimate a probabilistic topology matrix (all trophic link probabilities) and a diet matrix \n    (all diet proportions).\n    The package relies on the JAGS software and the 'jagsUI' package to run a Markov chain Monte Carlo \n    approximation of the different variables.",
    "version": "2.0.1",
    "maintainer": "Pierre-Yves Hernvann <pierre.yves.hernvann@gmail.com>",
    "url": "https://github.com/pyhernvann/EcoDiet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2124,
    "package_name": "EcoMetrics",
    "title": "Econometrics Model Building",
    "description": "An intuitive and user-friendly package designed to aid undergraduate students in understanding and applying econometric methods in their studies,\n    Tailored specifically for Econometrics and Regression Modeling courses, it provides a practical toolkit for modeling and analyzing econometric data with detailed inference capabilities.",
    "version": "0.1.1",
    "maintainer": "Mutua Kilai <kilaimutua@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2129,
    "package_name": "EconCausal",
    "title": "Causal Analysis for Macroeconomic Time Series (ECM-MARS, BSTS,\nBayesian GLM-AR(1))",
    "description": "Implements three complementary pipelines for causal analysis on macroeconomic time series:\n    (1) Error-Correction Models with Multivariate Adaptive Regression Splines (ECM-MARS),\n    (2) Bayesian Structural Time Series (BSTS), and \n    (3) Bayesian GLM with AR(1) errors validated with Leave-Future-Out (LFO). \n    Heavy backends (Stan) are optional and never used in examples or tests.",
    "version": "1.0.2",
    "maintainer": "José Mauricio Gómez Julián <isadore.nabi@pm.me>",
    "url": "https://github.com/IsadoreNabi/EconCausal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2135,
    "package_name": "EffectLiteR",
    "title": "Average and Conditional Effects",
    "description": "Use structural equation modeling to estimate average and\n    conditional effects of a treatment variable on an outcome variable, taking into\n    account multiple continuous and categorical covariates.",
    "version": "0.5-1",
    "maintainer": "Axel Mayer <amayer2010@gmail.com>",
    "url": "https://github.com/amayer2010/EffectLiteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2136,
    "package_name": "EffectStars",
    "title": "Visualization of Categorical Response Models",
    "description": "Notice: The package EffectStars2 provides a more up-to-date implementation of effect stars! EffectStars provides functions to visualize regression models with categorical response as proposed by Tutz and Schauberger (2013) <doi:10.1080/10618600.2012.701379>. The effects of the variables are plotted with star plots in order to allow for an optical impression of the fitted model.",
    "version": "1.9-1",
    "maintainer": "Gunther Schauberger <gunther.schauberger@tum.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2137,
    "package_name": "EffectStars2",
    "title": "Effect Stars",
    "description": "Provides functions for the method of effect stars as proposed by Tutz and Schauberger (2013) <doi:10.1080/10618600.2012.701379>. Effect stars can be used to visualize estimates of parameters corresponding to different groups, for example in multinomial logit models. Beside the main function 'effectstars' there exist methods for special objects, for example for 'vglm' objects from the 'VGAM' package.",
    "version": "0.1-3",
    "maintainer": "Gunther Schauberger <gunther.schauberger@tum.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2138,
    "package_name": "EffectTreat",
    "title": "Prediction of Therapeutic Success",
    "description": "In personalized medicine, one wants to know, for a given patient and his or her outcome for a predictor (pre-treatment variable), how likely it is that a treatment will be more beneficial than an alternative treatment. This package allows for the quantification of the predictive causal association (i.e., the association between the predictor variable and the individual causal effect of the treatment) and related metrics. Part of this software has been developed using funding provided from the European Union's 7th Framework Programme for research, technological development and demonstration under Grant Agreement no 602552.",
    "version": "1.1",
    "maintainer": "Wim Van der Elst <Wim.vanderelst@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2149,
    "package_name": "EloSteepness",
    "title": "Bayesian Dominance Hierarchy Steepness via Elo Rating and\nDavid's Scores",
    "description": "Obtain Bayesian posterior distributions of dominance hierarchy steepness (Neumann and Fischer (2023) <doi:10.1111/2041-210X.14021>). Steepness estimation is based on Bayesian implementations of either Elo-rating or David's scores. ",
    "version": "0.5.0",
    "maintainer": "Christof Neumann <christofneumann1@gmail.com>",
    "url": "https://github.com/gobbios/EloSteepness",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2155,
    "package_name": "EncompassTest",
    "title": "Direct Multi-Step Forecast Based Comparison of Nested Models via\nan Encompassing Test",
    "description": "The encompassing test is developed based on multi-step-ahead predictions of two nested models as in Pitarakis, J. (2023) <doi:10.48550/arXiv.2312.16099>. The statistics are standardised to a normal distribution, and the null hypothesis is that the larger model contains no additional useful information. P-values will be provided in the output.",
    "version": "0.22",
    "maintainer": "Rong Peng <r.peng@soton.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2214,
    "package_name": "EstimationTools",
    "title": "Maximum Likelihood Estimation for Probability Functions from\nData Sets",
    "description": "Total Time on Test plot and routines for parameter estimation of\n    any lifetime distribution implemented in R via maximum likelihood (ML) given\n    a data set. It is implemented thinking on parametric survival analysis, but\n    it feasible to use in parameter estimation of probability density or mass\n    functions in any field. The main routines 'maxlogL' and 'maxlogLreg' are\n    wrapper functions specifically developed for ML estimation. There are\n    included optimization procedures such as 'nlminb' and 'optim' from base\n    package, and 'DEoptim' Mullen (2011) <doi:10.18637/jss.v040.i06>. Standard\n    errors are estimated with 'numDeriv' Gilbert (2011)\n    <https://CRAN.R-project.org/package=numDeriv> or the option 'Hessian = TRUE'\n    of 'optim' function.",
    "version": "4.3.1",
    "maintainer": "Jaime Mosquera <jmosquerag@unal.edu.co>",
    "url": "https://jaimemosg.github.io/EstimationTools/,\nhttps://github.com/Jaimemosg/EstimationTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2229,
    "package_name": "EventWinRatios",
    "title": "Event-Specific Win Ratios for Terminal and Non-Terminal Events",
    "description": "Provides several confidence interval and testing procedures using\n    event-specific win ratios for semi-competing risks data with non-terminal\n    and terminal events, as developed in Yang et al. (2021<doi:10.1002/sim.9266>). \n    Compared with conventional methods for survival data, these procedures are \n    designed to utilize more data for improved inference procedures with \n    semi-competing risks data. The event-specific win ratios were introduced in \n    Yang and Troendle (2021<doi:10.1177/1740774520972408>). In this package, \n    the event-specific win ratios and confidence intervals are obtained for each \n    event type, and several testing procedures are developed for the global null \n    of no treatment effect on either terminal or non-terminal events. Furthermore,\n    a test of proportional hazard assumptions, under which the event-specific win \n    ratios converge to the hazard ratios, and a test of equal hazard ratios are \n    provided. For summarizing the treatment effect on all events, confidence \n    intervals for linear combinations of the event-specific win ratios are available\n    using pre-determined or data-driven weights. Asymptotic properties of these \n    inference procedures are discussed in Yang et al (2021<doi:10.1002/sim.9266>). \n    Also, transformations are used to yield better control of the type one error \n    rates for moderately sized data sets.",
    "version": "1.0.0",
    "maintainer": "Daewoo Pak <dpak@yonsei.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2237,
    "package_name": "ExGaussEstim",
    "title": "Quantile Maximization Likelihood Estimation and Bayesian\nEx-Gaussian Estimation",
    "description": "Presents two methods to estimate the parameters 'mu', 'sigma', and 'tau' of an ex-Gaussian distribution. Those methods are Quantile Maximization Likelihood Estimation ('QMLE') and Bayesian. The 'QMLE' method allows a choice between three different estimation algorithms for these parameters : 'neldermead' ('NEMD'), 'fminsearch' ('FMIN'), and 'nlminb' ('NLMI'). For more details about the methods you can refer at the following list: Brown, S., & Heathcote, A. (2003) <doi:10.3758/BF03195527>; McCormack, P. D., & Wright, N. M. (1964) <doi:10.1037/h0083285>; Van Zandt, T. (2000) <doi:10.3758/BF03214357>; El Haj, A., Slaoui, Y., Solier, C., & Perret, C. (2021) <doi:10.19139/soic-2310-5070-1251>; Gilks, W. R., Best, N. G., & Tan, K. K. C. (1995) <doi:10.2307/2986138>.",
    "version": "0.1.2",
    "maintainer": "Jean DUMONCEL <jean.dumoncel@univ-poitiers.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2244,
    "package_name": "ExactCox",
    "title": "Exact Test and Exact Confidence Interval for the Cox Model",
    "description": "Performs the exact test on whether there is a difference between two survival curves. \n  Exact confidence interval for the hazard ratio can also be generated for the Cox model.",
    "version": "0.1.0",
    "maintainer": "Yongwu Shao <ywshao@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2245,
    "package_name": "ExactMed",
    "title": "Exact Mediation Analysis for Binary Outcomes",
    "description": "A tool for conducting exact parametric regression-based causal mediation analysis of binary outcomes \n    as described in Samoilenko, Blais and Lefebvre (2018) <doi:10.1353/obs.2018.0013>;\n    Samoilenko, Lefebvre (2021) <doi:10.1093/aje/kwab055>; and Samoilenko, Lefebvre (2023) <doi:10.1002/sim.9621>.",
    "version": "0.3.0",
    "maintainer": "Miguel Caubet <miguelcaubet@gmail.com>",
    "url": "https://caubm.github.io/ExactMed/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2255,
    "package_name": "ExpAnalysis3d",
    "title": "Pacote Para Analise De Experimentos Com Graficos De Superficie\nResposta",
    "description": "Pacote para a analise de experimentos havendo duas variaveis\n    explicativas quantitativas e uma variavel dependente quantitativa. Os\n    experimentos podem ser sem repeticoes ou com delineamento estatistico.\n    Sao ajustados 12 modelos de regressao multipla e plotados graficos de\n    superficie resposta (Hair JF, 2016) <ISBN:13:978-0138132637>.(Package\n    for the analysis of experiments having two explanatory quantitative\n    variables and one quantitative dependent variable. The experiments can\n    be without repetitions or with a statistical design. Twelve multiple\n    regression models are fitted and response surface graphs are plotted\n    (Hair JF, 2016) <ISBN:13:978-0138132637>).",
    "version": "0.1.3",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2258,
    "package_name": "ExpDes",
    "title": "Experimental Designs Package",
    "description": "Package for analysis of simple experimental designs (CRD, RBD and LSD), experiments in double factorial schemes (in CRD and RBD), experiments in a split plot in time schemes (in CRD and RBD), experiments in double factorial schemes with an additional treatment (in CRD and RBD), experiments in triple factorial scheme (in CRD and RBD) and experiments in triple factorial schemes with an additional treatment (in CRD and RBD), performing the analysis of variance and means comparison by fitting regression models until the third power (quantitative treatments) or by a multiple comparison test, Tukey test, test  of Student-Newman-Keuls (SNK), Scott-Knott, Duncan test, t test (LSD) and Bonferroni t test (protected LSD) - for qualitative treatments; residual analysis (Ferreira, Cavalcanti and Nogueira, 2014) <doi:10.4236/am.2014.519280>.",
    "version": "1.2.2",
    "maintainer": "Eric Batista Ferreira <eric.ferreira@unifal-mg.edu.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2272,
    "package_name": "ExtendedABSurvTDC",
    "title": "Survival Analysis using Indicators under Time Dependent\nCovariates",
    "description": "Survival analysis is employed to model time-to-event data. This package examines the relationship between survival and one or more predictors, termed as covariates, which can include both treatment variables (e.g., season of birth, represented by indicator functions) and continuous variables. To this end, the Cox-proportional hazard (Cox-PH) model, introduced by Cox in 1972, is a widely applicable and commonly used method for survival analysis. This package enables the estimation of the effect of randomization for the treatment variable to account for potential confounders, providing adjustment when estimating the association with exposure. It accommodates both fixed and time-dependent covariates and computes survival probabilities for lactation periods in dairy animals. The package is built upon the algorithm developed by Klein and Moeschberger (2003) <DOI:10.1007/b97377>.",
    "version": "0.1.0",
    "maintainer": "Dr. Himadri Ghosh <hghosh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2277,
    "package_name": "ExtremeBounds",
    "title": "Extreme Bounds Analysis (EBA)",
    "description": "An implementation of Extreme Bounds Analysis (EBA), a global sensitivity analysis that examines the robustness of determinants in regression models. The package supports both Leamer's and Sala-i-Martin's versions of EBA, and allows users to customize all aspects of the analysis.",
    "version": "0.1.7",
    "maintainer": "Marek Hlavac <mhlavac@alumni.princeton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2280,
    "package_name": "FABInference",
    "title": "FAB p-Values and Confidence Intervals",
    "description": "Frequentist assisted by Bayes (FAB) p-values and confidence \n interval construction. See \n Hoff (2019) <arXiv:1907.12589> \n \"Smaller p-values via indirect information\",\n Hoff and Yu (2019) <doi:10.1214/18-EJS1517> \n \"Exact adaptive confidence intervals for linear regression coefficients\", and\n Yu and Hoff (2018) <doi:10.1093/biomet/asy009> \n \"Adaptive multigroup confidence intervals with constant coverage\".",
    "version": "0.1",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2290,
    "package_name": "FARS",
    "title": "Factor-Augmented Regression Scenarios",
    "description": "Provides a comprehensive framework in R for modeling and forecasting economic scenarios based on multi-level dynamic factor model. The package enables users to: (i) extract global and group-specific factors using a flexible multi-level factor structure; (ii) compute asymptotically valid confidence regions for the estimated factors, accounting for uncertainty in the factor loadings; (iii) obtain estimates of the parameters of the factor-augmented quantile regressions together with their standard deviations; (iv) recover full predictive conditional densities from estimated quantiles; (v) obtain risk measures based on extreme quantiles of the conditional densities; (vi) estimate the conditional density and the corresponding extreme quantiles when the factors are stressed.",
    "version": "0.7.1",
    "maintainer": "Gian Pietro Bellocca <gbellocc@est-econ.uc3m.es>",
    "url": "https://arxiv.org/abs/2507.10679",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2291,
    "package_name": "FAS",
    "title": "Factor-Augmented Sparse Regression Tuning-Free Testing",
    "description": "The 'FAS' package implements the bootstrap method for the tuning parameter selection and tuning-free inference on sparse regression coefficient vectors. Currently, the test could be applied to linear and factor-augmented sparse regressions, see Lederer & Vogt (2021, JMLR) <https://www.jmlr.org/papers/volume22/20-539/20-539.pdf> and Beyhum & Striaukas (2023) <arXiv:2307.13364>. ",
    "version": "1.0.0",
    "maintainer": "Jonas Striaukas <jonas.striaukas@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2293,
    "package_name": "FASeg",
    "title": "Joint Segmentation of Correlated Time Series",
    "description": "It contains a function designed to the joint segmentation in the mean of several correlated series. The method is described in the paper X. Collilieux, E. Lebarbier and S. Robin. A factor model approach for the joint segmentation with between-series correlation (2015) <arXiv:1505.05660>.",
    "version": "0.1.9",
    "maintainer": "Emilie Lebarbier <emilie.lebarbier@agroparistech.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2295,
    "package_name": "FAVAR",
    "title": "Bayesian Analysis of a FAVAR Model",
    "description": "Estimate a FAVAR model by a Bayesian method, based on Bernanke et al. (2005) <DOI:10.1162/0033553053327452>.",
    "version": "0.1.3",
    "maintainer": "Pu Chen <shengnehs@qq.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2298,
    "package_name": "FAmle",
    "title": "Maximum Likelihood and Bayesian Estimation of Univariate\nProbability Distributions",
    "description": "Estimate parameters of univariate probability distributions \n  with maximum likelihood and Bayesian methods.",
    "version": "1.3.7",
    "maintainer": "Thomas Petzoldt <thomas.petzoldt@tu-dresden.de>",
    "url": "https://github.com/tpetzoldt/FAmle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2300,
    "package_name": "FBCRM",
    "title": "Phase I Optimal Dose Assignment using the FBCRM and MFBCRM\nMethods",
    "description": "Performs dose assignment and trial simulation for the FBCRM (Fully Bayesian Continual Reassessment Method) and MFBCRM (Mixture Fully Bayesian Continual Reassessment Method) phase I clinical trial designs. These trial designs extend the Continual Reassessment Method (CRM) and Bayesian Model Averaging Continual Reassessment Method (BMA-CRM) by allowing the prior toxicity skeleton itself to be random, with posterior distributions obtained from Markov Chain Monte Carlo. On average, the FBCRM and MFBCRM methods outperformed the CRM and BMA-CRM methods in terms of selecting an optimal dose level across thousands of randomly generated simulation scenarios. Details on the methods and results of this simulation study are available on request, and the manuscript is currently under review.",
    "version": "1.1",
    "maintainer": "Andrew G Chapple <achapp@lsuhsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2301,
    "package_name": "FBFsearch",
    "title": "Algorithm for Searching the Space of Gaussian Directed Acyclic\nGraph Models Through Moment Fractional Bayes Factors",
    "description": "We propose an objective Bayesian algorithm for searching the space\n of Gaussian directed acyclic graph (DAG) models. The algorithm uses moment\n fractional Bayes factors (MFBF) and is suitable for learning sparse graphs.\n The algorithm is implemented using Armadillo, an open-source C++ linear\n algebra library.",
    "version": "1.3",
    "maintainer": "Davide Altomare <davide.altomare@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2302,
    "package_name": "FBMS",
    "title": "Flexible Bayesian Model Selection and Model Averaging",
    "description": "Implements the Mode Jumping Markov Chain Monte Carlo algorithm described in <doi:10.1016/j.csda.2018.05.020> and its Genetically Modified counterpart described in <doi:10.1613/jair.1.13047> as well as the sub-sampling versions described in <doi:10.1016/j.ijar.2022.08.018> for flexible Bayesian model selection and model averaging.",
    "version": "1.3",
    "maintainer": "Jon Lachmann <jon@lachmann.nu>",
    "url": "https://github.com/jonlachmann/FBMS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2306,
    "package_name": "FCUSUM",
    "title": "Fourier CUSUM Cointegration Test",
    "description": "Implements the Fourier cumulative sum (CUSUM) cointegration test \n    for detecting cointegration relationships in time series data with \n    structural breaks. The test uses Fourier approximations to capture smooth \n    structural changes and CUSUM statistics to test for cointegration stability.\n    Based on methodology described in Zaghdoudi (2025) \n    <doi:10.46557/001c.144076>. The corrected Akaike Information Criterion \n    (AICc) is used for optimal frequency selection.",
    "version": "1.0.0",
    "maintainer": "Taha Zaghdoudi <zedtaha@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2307,
    "package_name": "FCVAR",
    "title": "Estimation and Inference for the Fractionally Cointegrated VAR",
    "description": "Estimation and inference using the Fractionally Cointegrated \n    Vector Autoregressive (VAR) model. It includes functions for model specification, \n    including lag selection and cointegration rank selection, as well as a comprehensive\n    set of options for hypothesis testing, including tests of hypotheses on the \n    cointegrating relations, the adjustment coefficients and the fractional \n    differencing parameters. \n    An article describing the FCVAR model with examples is available on the Webpage \n    <https://sites.google.com/view/mortennielsen/software>.",
    "version": "0.1.4",
    "maintainer": "Lealand Morin <lealand.morin@ucf.edu>",
    "url": "https://github.com/LeeMorinUCF/FCVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2324,
    "package_name": "FFdownload",
    "title": "Download Data from Kenneth French's Website",
    "description": "Downloads all the datasets (you can exclude the daily ones or specify a list of those you are targeting specifically) from Kenneth French's Website at <https://mba.tuck.dartmouth.edu/pages/faculty/ken.french/data_library.html>, process them and convert them to list of 'xts' (time series).",
    "version": "1.1.1",
    "maintainer": "Sebastian Stoeckl <sebastian.stoeckl@uni.li>",
    "url": "https://github.com/sstoeckl/ffdownload,\nhttps://sstoeckl.github.io/ffdownload/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2330,
    "package_name": "FHtest",
    "title": "Tests for Right and Interval-Censored Survival Data Based on the\nFleming-Harrington Class",
    "description": "Functions to compare two or more survival curves with:\n             a) The Fleming-Harrington test for right-censored data based on permutations and on counting processes.\n             b) An extension of the Fleming-Harrington test for interval-censored data based on a permutation distribution and on a score vector distribution.",
    "version": "1.5.1",
    "maintainer": "Ramon Oller <ramon.oller@uvic.cat>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2335,
    "package_name": "FIND",
    "title": "Objective Comparison of Phase I Dose-Finding Designs",
    "description": "Generate decision tables and simulate operating characteristics\n    for phase I dose-finding designs to enable objective comparison across\n    methods. Supported designs include the traditional 3+3, Bayesian Optimal\n    Interval (BOIN) (Liu and Yuan (2015) <doi:10.1158/1078-0432.CCR-14-1526>),\n    modified Toxicity Probability Interval-2 (mTPI-2) (Guo et al. (2017)\n    <doi:10.1002/sim.7185>), interval 3+3 (i3+3) (Liu et al. (2020)\n    <doi:10.1177/0962280220939123>), and Generalized 3+3 (G3). Provides\n    visualization tools for comparing decision rules and operating\n    characteristics across multiple designs simultaneously.",
    "version": "0.1.1",
    "maintainer": "Yunxuan Zhang <yunxuanz@uchicago.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2338,
    "package_name": "FKF",
    "title": "Fast Kalman Filter",
    "description": "This is a fast and flexible implementation of the Kalman\n        filter and smoother, which can deal with NAs. It is entirely written in C and relies fully on linear algebra subroutines contained in\n        BLAS and LAPACK. Due to the speed of the filter, the fitting of\n        high-dimensional linear state space models to large datasets\n        becomes possible. This package also contains a plot function\n        for the visualization of the state vector and graphical\n        diagnostics of the residuals.",
    "version": "0.2.6",
    "maintainer": "Paul Smith <paul@waternumbers.co.uk>",
    "url": "https://waternumbers.github.io/FKF/,\nhttps://github.com/waternumbers/FKF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2339,
    "package_name": "FKF.SP",
    "title": "Fast Kalman Filtering Through Sequential Processing",
    "description": "Fast and flexible Kalman filtering and smoothing implementation utilizing sequential processing, designed for efficient parameter estimation through maximum likelihood estimation. Sequential processing is a univariate treatment of a multivariate series of observations and can benefit from computational efficiency over traditional Kalman filtering when independence is assumed in the variance of the disturbances of the measurement equation. Sequential processing is described in the textbook of Durbin and Koopman (2001, ISBN:978-0-19-964117-8). 'FKF.SP' was built upon the existing 'FKF' package and is, in general, a faster Kalman filter/smoother.",
    "version": "0.3.4",
    "maintainer": "Thomas Aspinall <tomaspinall2512@gmail.com>",
    "url": "https://github.com/TomAspinall/FKF.SP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2343,
    "package_name": "FLAME",
    "title": "Interpretable Matching for Causal Inference",
    "description": "Efficient implementations of the algorithms in the \n    Almost-Matching-Exactly framework for interpretable matching in causal\n    inference. These algorithms match units via a learned, weighted Hamming\n    distance that determines which covariates are more important to match on.\n    For more information and examples, see the Almost-Matching-Exactly website. ",
    "version": "2.1.1",
    "maintainer": "Vittorio Orlandi <almost.matching.exactly@gmail.com>",
    "url": "https://almost-matching-exactly.github.io,https://vittorioorlandi.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2347,
    "package_name": "FLORAL",
    "title": "Fit Log-Ratio Lasso Regression for Compositional Data",
    "description": "Log-ratio Lasso regression for continuous, binary, and survival outcomes with (longitudinal) compositional features. See Fei and others (2024) <doi:10.1016/j.crmeth.2024.100899>.",
    "version": "0.6.0",
    "maintainer": "Teng Fei <feit1@mskcc.org>",
    "url": "https://vdblab.github.io/FLORAL/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2354,
    "package_name": "FMCensSkewReg",
    "title": "Finite Mixture of Censored Regression Models with Skewed\nDistributions",
    "description": "Provides an implementation of finite mixture regression models for censored\n    data under four distributional families: Normal (FM-NCR), Student t (FM-TCR),\n    skew-Normal (FM-SNCR), and skew-t (FM-STCR). The package enables flexible\n    modeling of skewness and heavy tails often observed in real-world data, while\n    explicitly accounting for censoring. Functions are included for parameter\n    estimation via the Expectation-Maximization (EM) algorithm, computation of standard errors, and model\n    comparison criteria such as the Akaike Information Criterion (AIC), the Bayesian Information Criterion (BIC), \n    and the Efficient Determination Criterion (EDC). The underlying methodology is described in Park et al. (2024)\n    <doi:10.1007/s00180-024-01459-4>.",
    "version": "0.1.1",
    "maintainer": "Jiwon Park <pcjylove87@gmail.com>",
    "url": "https://github.com/JiwonPark41/FMCensSkewReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2356,
    "package_name": "FMM",
    "title": "Rhythmic Patterns Modeling by FMM Models",
    "description": "Provides a collection of functions to fit and explore single, multi-component and restricted Frequency Modulated Moebius (FMM) models. 'FMM' is a nonlinear parametric regression model capable of fitting non-sinusoidal shapes in rhythmic patterns. Details about the mathematical formulation of 'FMM' models can be found in Rueda et al. (2019) <doi:10.1038/s41598-019-54569-1>.",
    "version": "0.4.1",
    "maintainer": "Itziar Fernandez <itziar.fernandez@uva.es>",
    "url": "https://github.com/FMMGroupVa/FMM-base-R-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2360,
    "package_name": "FMsmsnReg",
    "title": "Regression Models with Finite Mixtures of Skew Heavy-Tailed\nErrors",
    "description": "Fit linear regression models where the random errors follow a finite mixture of of Skew Heavy-Tailed Errors.",
    "version": "1.0",
    "maintainer": "Luis Benites Sanchez <lbenitesanchez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2372,
    "package_name": "FRApp",
    "title": "FRAP Data Analysis Using Nonlinear Mixed Effect Models with\n'shiny'",
    "description": "Analysis of Fluorescence Recovery After Photobleaching (FRAP) experiments using nonlinear mixed-effects regression models and analysis of the results. 'FRApp' is not limited to the analysis of FRAP experiments only. Any nonlinear mixed-effects models with an asymptotic exponential functional relationship to hierarchical data in various domains can be fitted. \n  The analysis of data available in the package is presented in Di Credico, G., Pelucchi, S., Pauli, F. et al. (2025) <doi:10.1038/s41598-025-87154-w>.",
    "version": "1.0.0",
    "maintainer": "Gioia Di Credico <gioia.dicredico@deams.units.it>",
    "url": "https://github.com/gioiadc/FRApp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2393,
    "package_name": "FTSgof",
    "title": "White Noise and Goodness-of-Fit Tests for Functional Time Series",
    "description": "It offers comprehensive tools for the analysis of functional\n    time series data, focusing on white noise hypothesis testing and\n    goodness-of-fit evaluations, alongside functions for\n    simulating data and advanced visualization techniques, such as 3D\n    rainbow plots. These methods are described in Kokoszka, Rice, and Shang (2017)  <doi:10.1016/j.jmva.2017.08.004>, \n    Yeh, Rice, and Dubin (2023) <doi:10.1214/23-EJS2112>, Kim, Kokoszka, and Rice (2023) <doi:10.1214/23-ss143>, and \n    Rice, Wirjanto, and Zhao (2020) <doi:10.1111/jtsa.12532>.",
    "version": "1.0.0",
    "maintainer": "Mihyun Kim <mihyun.kim@mail.wvu.edu>",
    "url": "https://github.com/veritasmih/FTSgof",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2395,
    "package_name": "FVDDPpkg",
    "title": "Implement Fleming-Viot-Dependent Dirichlet Processes",
    "description": "\n      A Bayesian Nonparametric model for the study of time-evolving frequencies, which has become renowned in the study of population genetics.\n      The model consists of a Hidden Markov Model (HMM) in which the latent signal is a distribution-valued stochastic process that takes the form of a finite mixture of Dirichlet Processes, indexed by vectors that count how many times each value is observed in the population.\n      The package implements methodologies presented in Ascolani, Lijoi and Ruggiero (2021) <doi:10.1214/20-BA1206> and Ascolani, Lijoi and Ruggiero (2023) <doi:10.3150/22-BEJ1504> that make it possible to study the process at the time of data collection or to predict its evolution in future or in the past.",
    "version": "0.1.2",
    "maintainer": "Stefano Damato <stefano.damato@idsia.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2396,
    "package_name": "FWDselect",
    "title": "Selecting Variables in Regression Models",
    "description": "A simple method\n    to select the best model or best subset of variables using\n    different types of data (binary, Gaussian or Poisson) and\n    applying it in different contexts (parametric or non-parametric).",
    "version": "2.1.0",
    "maintainer": "Marta Sestelo <sestelo@uvigo.es>",
    "url": "http://cran.r-project.org/package=FWDselect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2413,
    "package_name": "FamEvent",
    "title": "Family Age-at-Onset Data Simulation and Penetrance Estimation",
    "description": "Simulates age-at-onset traits associated with a segregating major gene in family data \n     obtained from population-based, clinic-based, or multi-stage designs. Appropriate ascertainment \n     correction is utilized to estimate age-dependent penetrance functions either parametrically from \n     the fitted model or nonparametrically from the data. The Expectation and Maximization algorithm \n     can infer missing genotypes and carrier probabilities estimated from family's genotype and\n     phenotype information or from a fitted model. Plot functions include pedigrees of simulated \n     families and predicted penetrance curves based on specified parameter values.\n     For more information see Choi, Y.-H., Briollais, L., He, W. and Kopciuk, K. (2021) FamEvent: An \n     R Package for Generating and Modeling Time-to-Event Data in Family Designs, \n     Journal of Statistical Software 97 (7), 1-30.",
    "version": "3.2",
    "maintainer": "Yun-Hee Choi <yun-hee.choi@schulich.uwo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2418,
    "package_name": "FastGP",
    "title": "Efficiently Using Gaussian Processes with Rcpp and RcppEigen",
    "description": "Contains Rcpp and RcppEigen implementations of matrix operations useful for Gaussian process models, such as the inversion of a symmetric Toeplitz matrix, sampling from multivariate normal distributions, evaluation of the log-density of a multivariate normal vector, and Bayesian inference for latent variable Gaussian process models with elliptical slice sampling (Murray, Adams, and MacKay 2010).",
    "version": "1.2",
    "maintainer": "Giri Gopalan <gopalan88@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2422,
    "package_name": "FastJM",
    "title": "Semi-Parametric Joint Modeling of Longitudinal and Survival Data",
    "description": "A joint model for large-scale, competing risks time-to-event data with singular or multiple longitudinal biomarkers, implemented with the efficient algorithms developed by Li and colleagues (2022) <doi:10.1155/2022/1362913> and <doi:10.48550/arXiv.2506.12741>.\n             The time-to-event data is modelled using a (cause-specific) Cox \n             proportional hazards regression model with time-fixed covariates. \n             The longitudinal biomarkers are modelled using a linear mixed \n             effects model. The association between the longitudinal submodel \n             and the survival submodel is captured through shared random \n             effects. It allows researchers to analyze large-scale data to \n             model biomarker trajectories, estimate their effects on event \n             outcomes, and dynamically predict future events from patients’ \n             past histories. A function for simulating survival and longitudinal \n             data for multiple biomarkers is also included alongside built-in \n             datasets.",
    "version": "1.5.3",
    "maintainer": "Shanpeng Li <lishanpeng0913@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2423,
    "package_name": "FastKM",
    "title": "A Fast Multiple-Kernel Method Based on a Low-Rank Approximation",
    "description": "A computationally efficient and statistically rigorous fast \n    Kernel Machine method for multi-kernel analysis. The approach is based on \n    a low-rank approximation to the nuisance effect kernel matrices. The \n    algorithm is applicable to continuous, binary, and survival traits and \n    is implemented using the existing single-kernel analysis software 'SKAT' \n    and 'coxKM'. 'coxKM' can be obtained from \n    <https://github.com/lin-lab/coxKM>.",
    "version": "1.2",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2425,
    "package_name": "FastKRR",
    "title": "Kernel Ridge Regression using 'RcppArmadillo'",
    "description": "Provides core computational operations in C++ via 'RcppArmadillo', enabling faster performance than pure R, improved numerical stability, and parallel execution with OpenMP where available. On systems without OpenMP support, the package automatically falls back to single-threaded execution with no user configuration required. For efficient model selection, it integrates with 'CVST' to provide sequential-testing cross-validation that identifies competitive hyperparameters without exhaustive grid search. The package offers a unified interface for exact kernel ridge regression and three scalable approximations—Nyström, Pivoted Cholesky, and Random Fourier Features—allowing analyses with substantially larger sample sizes than are feasible with exact KRR. It also integrates with the 'tidymodels' ecosystem via the 'parsnip' model specification 'krr_reg', and the S3 method tunable.krr_reg(). To understand the theoretical background, one can refer to Wainwright (2019) <doi:10.1017/9781108627771>.",
    "version": "0.1.2",
    "maintainer": "Kwan-Young Bak <kybak@sungshin.ac.kr>",
    "url": "https://github.com/kybak90/FastKRR, https://www.tidymodels.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2428,
    "package_name": "FastRCS",
    "title": "Fits the FastRCS Robust Multivariable Linear Regression Model",
    "description": "The FastRCS algorithm of Vakili and Schmitt (2014) for robust fit of the multivariable linear regression model and outliers detection.",
    "version": "0.0.9",
    "maintainer": "Kaveh Vakili <vakili.kaveh.email@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2453,
    "package_name": "FinAna",
    "title": "Financial Analysis and Regression Diagnostic Analysis",
    "description": "Functions for financial analysis and financial modeling, \n             including batch graphs generation, beta calculation, \n             descriptive statistics, annuity calculation, bond pricing \n             and financial data download.",
    "version": "0.1.2",
    "maintainer": "Xuanhua(Peter) Yin <peteryin.sju@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2454,
    "package_name": "FinCal",
    "title": "Time Value of Money, Time Series Analysis and Computational\nFinance",
    "description": "Package for time value of money calculation, time series analysis and computational finance.",
    "version": "0.6.3",
    "maintainer": "Felix Yanhui Fan <nolanfyh@gmail.com>",
    "url": "http://felixfan.github.io/FinCal/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2457,
    "package_name": "FinTS",
    "title": "Companion to Tsay (2005) Analysis of Financial Time Series",
    "description": "R companion to Tsay (2005) Analysis of Financial Time\n   Series, second edition (Wiley).  Includes data sets, functions and\n   script files required to work some of the examples.  Version 0.3-x\n   includes R objects for all data files used in the text and script\n   files to recreate most of the analyses in chapters 1-3 and 9 plus\n   parts of chapters 4 and 11.",
    "version": "0.4-9",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/FinTSDoc/ (doc),\nhttps://r-forge.r-project.org/projects/fints/ (devel)",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2460,
    "package_name": "FindIt",
    "title": "Finding Heterogeneous Treatment Effects",
    "description": "The heterogeneous treatment effect estimation procedure \n        proposed by Imai and Ratkovic (2013)<DOI: 10.1214/12-AOAS593>.  \n        The proposed method is applicable, for\n        example, when selecting a small number of most (or least)\n        efficacious treatments from a large number of alternative\n        treatments as well as when identifying subsets of the\n        population who benefit (or are harmed by) a treatment of\n        interest. The method adapts the Support Vector Machine\n        classifier by placing separate LASSO constraints over the\n        pre-treatment parameters and causal heterogeneity parameters of\n        interest. This allows for the qualitative distinction between\n        causal and other parameters, thereby making the variable\n        selection suitable for the exploration of causal heterogeneity. \t\n        The package also contains a class of functions, CausalANOVA, \n        which estimates the average marginal interaction effects (AMIEs)\n        by a regularized ANOVA as proposed by Egami and Imai (2019). \n        It contains a variety of regularization techniques to facilitate \n\tanalysis of large factorial experiments. ",
    "version": "1.3.0",
    "maintainer": "Naoki Egami <naoki.egami5@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2461,
    "package_name": "FinePop",
    "title": "Fine-Scale Population Analysis",
    "description": "Statistical tool set for population genetics. The package provides following functions: 1) empirical Bayes estimator of Fst and other measures of genetic differentiation, 2) regression analysis of environmental effects on genetic differentiation using bootstrap method, 3) interfaces to read and manipulate 'GENEPOP' format data files and allele/haplotype frequency format files.",
    "version": "1.5.2",
    "maintainer": "Reiichiro Nakamichi <nakamichi_reiichiro33@fra.go.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2462,
    "package_name": "FinePop2",
    "title": "Fine-Scale Population Analysis (Rewrite for\nGene-Trait-Environment Interaction Analysis)",
    "description": "Statistical tool set for population genetics. The package provides following functions: 1) estimators of genetic differentiation (FST), 2) regression analysis of environmental effects on genetic differentiation using generalized least squares (GLS) method, 3) interfaces to read and manipulate 'GENEPOP' format data files). For more information, see Kitada, Nakamichi and Kishino (2020) <doi:10.1101/2020.01.30.927186>.",
    "version": "0.4",
    "maintainer": "Reiichiro Nakamichi <nakamichi@affrc.go.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2477,
    "package_name": "FlexReg",
    "title": "Regression Models for Bounded Continuous and Discrete Responses",
    "description": "Functions to fit regression models for bounded continuous and discrete responses. In case of bounded continuous  responses (e.g., proportions and rates), available models are the flexible beta (Migliorati, S., Di Brisco, A. M., Ongaro, A. (2018) <doi:10.1214/17-BA1079>), the variance-inflated beta (Di Brisco, A. M., Migliorati, S., Ongaro, A. (2020) <doi:10.1177/1471082X18821213>), the beta (Ferrari, S.L.P., Cribari-Neto, F. (2004) <doi:10.1080/0266476042000214501>), and their augmented versions to handle the presence of zero/one values (Di Brisco, A. M., Migliorati, S. (2020) <doi:10.1002/sim.8406>) are implemented. In case of bounded discrete responses (e.g., bounded counts, such as the number of successes in n trials),  available models are the flexible beta-binomial (Ascari, R., Migliorati, S. (2021) <doi:10.1002/sim.9005>), the beta-binomial, and the binomial are implemented. Inference is dealt with a Bayesian approach based on the Hamiltonian Monte Carlo (HMC) algorithm (Gelman, A., Carlin, J. B., Stern, H. S., Rubin, D. B. (2014) <doi:10.1201/b16018>). Besides, functions to compute residuals, posterior predictives, goodness of fit measures, convergence diagnostics, and graphical representations are provided.",
    "version": "1.4.1",
    "maintainer": "Roberto Ascari <roberto.ascari@unimib.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2479,
    "package_name": "FlexVarJM",
    "title": "Estimate Joint Models with Subject-Specific Variance",
    "description": "Estimation of mixed models including a subject-specific variance which can be time and covariate dependent. In the joint model framework, the package handles left truncation and allows a flexible dependence structure between the competing events and the longitudinal marker. The estimation is performed under the frequentist framework, using the Marquardt-Levenberg algorithm. (Courcoul, Tzourio, Woodward, Barbieri, Jacqmin-Gadda (2023) <arXiv:2306.16785>).",
    "version": "0.1.0",
    "maintainer": "Léonie Courcoul <leonie.courcoul@u-bordeaux.fr>",
    "url": "https://github.com/LeonieCourcoul/FlexVarJM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2484,
    "package_name": "FlowScreen",
    "title": "Daily Streamflow Trend and Change Point Screening",
    "description": "Screens daily streamflow time series for temporal trends and\n    change-points. This package has been primarily developed for assessing the\n    quality of daily streamflow time series. It also contains tools for plotting\n    and calculating many different streamflow metrics. The package can be used to\n    produce summary screening plots showing change-points and significant temporal\n    trends for high flow, low flow, and/or baseflow statistics, or it can be used\n    to perform more detailed hydrological time series analyses. The package was\n    designed for screening daily streamflow time series from Water Survey Canada\n    and the United States Geological Survey but will also work with streamflow time\n    series from many other agencies.  \n    Package update to version 2.0 made updates to read.flows function to allow \n    loading of GRDC and ROBIN streamflow record formats.\n    This package uses the `changepoint` package for change point detection.\n    For more information on change point methods, see the changepoint \n    package at <https://cran.r-project.org/package=changepoint>.",
    "version": "2.1",
    "maintainer": "Jennifer Dierauer <jen.r.brand@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2487,
    "package_name": "FluxPoint",
    "title": "Change Point Detection for Non-Stationary and Cross-Correlated\nTime Series",
    "description": "Implements methods for multiple change point detection in multivariate\n    time series with non-stationary dynamics and cross-correlations. The methodology\n    is based on a model in which each component has a fluctuating mean represented by\n    a random walk with occasional abrupt shifts, combined with a stationary vector\n    autoregressive structure to capture temporal and cross-sectional dependence. The\n    framework is broadly applicable to correlated multivariate sequences in which\n    large, sudden shifts occur in all or subsets of components and are the primary\n    targets of interest, whereas small, smooth fluctuations are not. Although random\n    walks are used as a modeling device, they provide a flexible approximation for a\n    wide class of slowly varying or locally smooth dynamics, enabling robust\n    performance beyond the strict random walk setting.",
    "version": "0.1.2",
    "maintainer": "Yuhan Tian <yuhan.tian@fau.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2493,
    "package_name": "ForCausality",
    "title": "A Curated Collection of 'Causal Inference' Datasets and Tools",
    "description": "Provides a comprehensive set of datasets and tools for 'causal inference' research. \n    The package includes data from clinical trials, cancer studies, epidemiological surveys, environmental exposures, and health-related observational studies.\n    Designed to facilitate causal analysis, risk assessment, and advanced statistical modeling, \n    it leverages datasets from packages such as 'causalOT', 'survival', 'causalPAF', 'evident', 'melt', and 'sanon'. \n    The package is inspired by the foundational work of Pearl (2009) <doi:10.1017/CBO9780511803161> on causal inference frameworks.",
    "version": "0.1.0",
    "maintainer": "Tomás Valderrama <tomasvm2004@gmail.com>",
    "url": "https://github.com/Toby-codigos/ForCausality,\nhttps://toby-codigos.github.io/ForCausality/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2498,
    "package_name": "ForeComp",
    "title": "Size-Power Tradeoff Visualization for Equal Predictive Ability\nof Two Forecasts",
    "description": "Offers a set of tools for visualizing and analyzing size and power properties of the test for equal predictive accuracy, the Diebold-Mariano test that is based on heteroskedasticity and autocorrelation-robust (HAR) inference. A typical HAR inference is involved with non-parametric estimation of the long-run variance, and one of its tuning parameters, the truncation parameter, trades off a size and power. Lazarus, Lewis, and Stock (2021)<doi:10.3982/ECTA15404> theoretically characterize the size-power frontier for the Gaussian multivariate location model. 'ForeComp' computes and visualizes the finite-sample size-power frontier of the Diebold-Mariano test based on fixed-b asymptotics together with the Bartlett kernel. To compute the finite-sample size and power, it works with the best approximating ARMA process to the given dataset. It informs the user how their choice of the truncation parameter performs and how robust the testing outcomes are.",
    "version": "0.9.0",
    "maintainer": "Minchul Shin <visiblehand@gmail.com>",
    "url": "https://github.com/mcmcs/ForeComp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2499,
    "package_name": "ForecastBenchmark",
    "title": "Libra: A Benchmark for Time Series Forecasting Methods",
    "description": "Libra, a forecasting benchmark, automatically evaluates forecasting methods based on their performance in a diverse set of evaluation scenarios. The benchmark comprises four different use cases, each covering 100 heterogeneous time series taken from different domains.",
    "version": "1.0.0",
    "maintainer": "Andre Bauer <andre.bauer@uni-wuerzburg.de>",
    "url": "https://github.com/DescartesResearch/ForecastBenchmark",
    "exports": [],
    "topics": ["benchmark", "evaluation", "forecasting", "time-series"],
    "score": "NA",
    "stars": 16
  },
  {
    "id": 2500,
    "package_name": "ForecastCombinations",
    "title": "Forecast Combinations",
    "description": "Aim: Supports the most frequently used methods to combine forecasts. Among others: Simple average, Ordinary Least Squares, Least Absolute Deviation, Constrained Least Squares, Variance-based, Best Individual model, Complete subset regressions and Information-theoretic (information criteria based).",
    "version": "1.1",
    "maintainer": "Eran Raviv  <eeraviv@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2502,
    "package_name": "ForecastingEnsembles",
    "title": "Time Series Forecasting Using 23 Individual Models",
    "description": "Runs multiple individual time series models, and combines them into an ensembles of time series models. This is mainly used to predict the results of the monthly labor market report from the \n    United States Bureau of Labor Statistics for virtually any part of the economy reported by the Bureau of Labor Statistics, but it can be easily modified to work with other types of time series data.\n    For example, the package was used to predict the winning men's and women's time for the 2024 London Marathon.",
    "version": "0.5.1",
    "maintainer": "Russ Conte <russconte@mac.com>",
    "url": "https://github.com/InfiniteCuriosity/ForecastingEnsembles",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2506,
    "package_name": "ForestFit",
    "title": "Statistical Modelling for Plant Size Distributions",
    "description": "Developed for the following tasks. 1 ) Computing the probability density function,\n             cumulative distribution function, random generation, and estimating the parameters\n \t\t\t of the eleven mixture models. 2 ) Point estimation of the parameters of two - \n\t\t\t parameter Weibull distribution using twelve methods and three - parameter Weibull \n\t\t\t distribution using nine methods. 3 ) The Bayesian inference for the three - \n\t\t\t parameter Weibull distribution. 4 ) Estimating parameters of the three - parameter\n\t\t\t Birnbaum - Saunders, generalized exponential, and Weibull distributions fitted to\n\t\t\t grouped data using three methods including approximated maximum likelihood, \n\t\t\t expectation maximization, and maximum likelihood. 5 ) Estimating the parameters\n\t\t\t of the gamma, log-normal, and Weibull mixture models fitted to the grouped data\n\t\t\t through the EM algorithm, 6 ) Estimating parameters of the nonlinear height curve\n\t\t\t fitted to the height - diameter observation, 7 ) Estimating parameters, computing\n\t\t\t probability density function, cumulative distribution function, and generating\n\t\t\t realizations from gamma shape mixture model introduced by Venturini et al. (2008)\n\t\t\t <doi:10.1214/07-AOAS156> , 8 ) The Bayesian inference, computing probability\n\t\t\t density function, cumulative distribution function, and generating realizations\n\t\t\t from univariate and bivariate Johnson SB distribution, 9 ) Robust multiple linear\n\t\t\t regression analysis when error term follows skewed t distribution, 10 ) Estimating \n\t\t\t parameters of a given distribution fitted to grouped data using method of maximum\n\t\t\t likelihood, and 11 ) Estimating parameters of the Johnson SB distribution through \n\t\t\t the Bayesian, method of moment, conditional maximum likelihood, and two - percentile \n\t\t\t method.",
    "version": "2.4.3",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2510,
    "package_name": "FormulR",
    "title": "Comprehensive Tools for Drug Formulation Analysis and\nVisualization",
    "description": "This presents a comprehensive set of tools for the analysis and visualization of drug formulation data. It includes functions for statistical analysis, regression modeling, hypothesis testing, and comparative analysis to assess the impact of formulation parameters on drug release and other critical attributes. Additionally, the package offers a variety of data visualization functions, such as scatterplots, histograms, and boxplots, to facilitate the interpretation of formulation data. With its focus on usability and efficiency, this package aims to streamline the drug formulation process and aid researchers in making informed decisions during formulation design and optimization.",
    "version": "1.0.0",
    "maintainer": "Oche Ambrose George <ocheab1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2528,
    "package_name": "FunSurv",
    "title": "Modeling Time-to-Event Data with Functional Predictors",
    "description": "A collection of methods for modeling time-to-event data using both functional and scalar predictors. It implements functional data analysis techniques for estimation and inference, allowing researchers to assess the impact of functional covariates on survival outcomes, including time-to-single event and recurrent event outcomes.",
    "version": "1.0.0",
    "maintainer": "Zifang Kong <zkong@smu.edu>",
    "url": "https://github.com/zifangkong/FunSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2533,
    "package_name": "FunctanSNP",
    "title": "Functional Analysis (with Interactions) for Dense SNP Data",
    "description": "An implementation of revised functional regression models for multiple genetic variation data, such as single nucleotide polymorphism (SNP) data, which provides revised functional linear regression models, partially functional interaction regression analysis with penalty-based techniques and corresponding drawing functions, etc.(Ruzong Fan, Yifan Wang, James L. Mills, Alexander F. Wilson, Joan E. Bailey-Wilson, and Momiao Xiong (2013) <doi:10.1002/gepi.21757>).",
    "version": "0.1.0",
    "maintainer": "Rui Ren <xmurr@stu.xmu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2559,
    "package_name": "GACE",
    "title": "Generalized Adaptive Capped Estimator for Time Series\nForecasting",
    "description": "\n    Provides deterministic forecasting for weekly, monthly, quarterly, and yearly\n    time series using the Generalized Adaptive Capped Estimator. The method\n    includes preprocessing for missing and extreme values, extraction of multiple\n    growth components (including long-term, short-term, rolling, and drift-based\n    signals), volatility-aware asymmetric capping, optional seasonal adjustment\n    via damped and normalized seasonal factors, and a recursive forecast\n    formulation with moderated growth. The package includes a user-facing\n    forecasting interface and a plotting helper for visualization. Related\n    forecasting background is discussed in Hyndman and Athanasopoulos (2021)\n    <https://otexts.com/fpp3/> and Hyndman and Khandakar (2008)\n    <doi:10.18637/jss.v027.i03>. The method extends classical extrapolative\n    forecasting approaches and is suited for operational and business planning\n    contexts where stability and interpretability are important.",
    "version": "1.0.0",
    "maintainer": "Vinodhkumar Gunasekaran <vinoalles@gmail.com>",
    "url": "https://github.com/vinoalles/GACE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2561,
    "package_name": "GAD",
    "title": "Analysis of Variance from General Principles",
    "description": "Analysis of complex ANOVA models with any combination of orthogonal/nested and fixed/random factors, as described by Underwood (1997). There are two restrictions: (i) data must be balanced; (ii) fixed nested factors are not allowed. Homogeneity of variances is checked using Cochran's C test and 'a posteriori' comparisons of means are done using Student-Newman-Keuls (SNK) procedure. For those terms with no denominator in the F-ratio calculation, pooled mean squares and quasi F-ratios are provided. Magnitute of effects are assessed by components of variation.",
    "version": "2.0",
    "maintainer": "Leonardo Sandrini-Neto <leonardosandrini@ufpr.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2568,
    "package_name": "GAMLj3",
    "title": "GAMLj Suite for Linear Models",
    "description": "A suite for estimating linear models, such as the general linear model, linear mixed models,",
    "version": "3.6.5",
    "maintainer": "Marcello Gallucci <mcfanda@gmail.com>",
    "url": "https://github.com/gamlj/gamlj",
    "exports": [],
    "topics": ["anova", "estimates", "jamovi", "linear-models", "post-hoc", "r", "regression", "slopes", "statistics"],
    "score": "NA",
    "stars": 40
  },
  {
    "id": 2574,
    "package_name": "GARCHIto",
    "title": "Class of GARCH-Ito Models",
    "description": "Provides functions to estimate model parameters and forecast future volatilities using the Unified GARCH-Ito [Kim and Wang (2016) <doi:10.1016/j.jeconom.2016.05.003>] and Realized GARCH-Ito [Song et. al. (2020) <doi:10.1016/j.jeconom.2020.07.007>] models. Optimization is done using augmented Lagrange multiplier method.  ",
    "version": "0.1.0",
    "maintainer": "Xinyu Song <song.xinyu@mail.shufe.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2583,
    "package_name": "GBClust",
    "title": "Generalized Bayes clustering",
    "description": "This package is an implementation of several generalized Bayes clustering methods.",
    "version": "0.0.2",
    "maintainer": "Blinded <blinded@gmail.com>",
    "url": "https://github.com/tommasorigon/GBClust",
    "exports": [],
    "topics": ["bayesian", "clustering", "r-package", "robustness"],
    "score": "NA",
    "stars": 6
  },
  {
    "id": 2585,
    "package_name": "GBOP2",
    "title": "Generalized Bayesian Optimal Phase II Design (G-BOP2)",
    "description": "Provides functions for implementing the Generalized Bayesian Optimal Phase II (G-BOP2) design using various Particle Swarm Optimization (PSO) algorithms, including:\n    - PSO-Default, based on Kennedy and Eberhart (1995) <doi:10.1109/ICNN.1995.488968>, \"Particle Swarm Optimization\";\n    - PSO-Quantum, based on Sun, Xu, and Feng (2004) <doi:10.1109/ICCIS.2004.1460396>, \"A Global Search Strategy of Quantum-Behaved Particle Swarm Optimization\";\n    - PSO-Dexp, based on Stehlík et al. (2024) <doi:10.1016/j.asoc.2024.111913>, \"A Double Exponential Particle Swarm Optimization with Non-Uniform Variates as Stochastic Tuning and Guaranteed Convergence to a Global Optimum with Sample Applications to Finding Optimal Exact Designs in Biostatistics\";\n    - and PSO-GO.",
    "version": "0.1.4",
    "maintainer": "Wanni Lei <wanni.lei17@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2590,
    "package_name": "GCPBayes",
    "title": "Bayesian Meta-Analysis of Pleiotropic Effects Using Group\nStructure",
    "description": "Run a Gibbs sampler for a multivariate Bayesian sparse group selection model with Dirac, continuous and hierarchical spike prior for detecting pleiotropy on the traits. This package is designed for summary statistics containing estimated regression coefficients and its estimated covariance matrix. The methodology is available from: Baghfalaki, T., Sugier, P. E., Truong, T., Pettitt, A. N., Mengersen, K., & Liquet, B. (2021) <doi:10.1002/sim.8855>.",
    "version": "4.3.0",
    "maintainer": "Yazdan Asgari <yazdan.asgari@inserm.fr>",
    "url": "https://github.com/tbaghfalaki/GCPBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2620,
    "package_name": "GENMETA",
    "title": "Implements Generalized Meta-Analysis Using Iterated Reweighted\nLeast Squares Algorithm",
    "description": "Generalized meta-analysis is a technique for estimating parameters associated with a multiple regression model through meta-analysis of studies which may have information only on partial sets of the regressors. It estimates the effects of each variable while fully adjusting for all other variables that are measured in at least one of the studies. Using algebraic relationships between regression parameters in different dimensions, a set of moment equations is specified for estimating the parameters of a maximal model through information available on sets of parameter estimates from a series of reduced models available from the different studies. The specification of the equations requires a reference dataset to estimate the joint distribution of the covariates. These equations are solved using the generalized method of moments approach, with the optimal weighting of the equations taking into account uncertainty associated with estimates of the parameters of the reduced models. The proposed framework is implemented using iterated reweighted least squares algorithm for fitting generalized linear regression models. For more details about the method, please see pre-print version of the manuscript on generalized meta-analysis by Prosenjit Kundu, Runlong Tang and Nilanjan Chatterjee (2018) <doi:10.1093/biomet/asz030>.The current version (0.2.0) is updated to address some of the stability issues in the previous version (0.1).",
    "version": "0.2.0",
    "maintainer": "Prosenjit Kundu <28pkundu1992@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2628,
    "package_name": "GET",
    "title": "Global Envelopes",
    "description": "Implementation of global envelopes for a set of general d-dimensional vectors T\n    in various applications. A 100(1-alpha)% global envelope is a band bounded by two\n    vectors such that the probability that T falls outside this envelope in any of the d\n    points is equal to alpha. Global means that the probability is controlled simultaneously\n    for all the d elements of the vectors. The global envelopes can be used for graphical\n    Monte Carlo and permutation tests where the test statistic is a multivariate vector or\n    function (e.g. goodness-of-fit testing for point patterns and random sets, functional\n    analysis of variance, functional general linear model, n-sample test of correspondence\n    of distribution functions), for central regions of functional or multivariate data (e.g.\n    outlier detection, functional boxplot) and for global confidence and prediction bands\n    (e.g. confidence band in polynomial regression, Bayesian posterior prediction). See\n    Myllymäki and Mrkvička (2024) <doi:10.18637/jss.v111.i03>,\n    Myllymäki et al. (2017) <doi:10.1111/rssb.12172>,\n    Mrkvička and Myllymäki (2023) <doi:10.1007/s11222-023-10275-7>,\n    Mrkvička et al. (2016) <doi:10.1016/j.spasta.2016.04.005>,\n    Mrkvička et al. (2017) <doi:10.1007/s11222-016-9683-9>,\n    Mrkvička et al. (2020) <doi:10.14736/kyb-2020-3-0432>,\n    Mrkvička et al. (2021) <doi:10.1007/s11009-019-09756-y>,\n    Myllymäki et al. (2021) <doi:10.1016/j.spasta.2020.100436>,\n    Mrkvička et al. (2022) <doi:10.1002/sim.9236>,\n    Dai et al. (2022) <doi:10.5772/intechopen.100124>,\n    Dvořák and Mrkvička (2022) <doi:10.1007/s00180-021-01134-y>,\n    Mrkvička et al. (2023) <doi:10.48550/arXiv.2309.04746>, and\n    Konstantinou et al. (2024) <doi: 10.1007/s00180-024-01569-z>.",
    "version": "1.0-7",
    "maintainer": "Mari Myllymäki <mari.myllymaki@luke.fi>",
    "url": "https://github.com/myllym/GET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2633,
    "package_name": "GEint",
    "title": "Misspecified Models for Gene-Environment Interaction",
    "description": "The first major functionality is to compute the bias in regression coefficients of misspecified linear gene-environment interaction models. The most generalized function for this objective is GE_bias(). However GE_bias() requires specification of many higher order moments of covariates in the model. If users are unsure about how to calculate/estimate these higher order moments, it may be easier to use GE_bias_normal_squaredmis(). This function places many more assumptions on the covariates (most notably that they are all jointly generated from a multivariate normal distribution) and is thus able to automatically calculate many of the higher order moments automatically, necessitating only that the user specify some covariances. There are also functions to solve for the bias through simulation and non-linear equation solvers; these can be used to check your work. Second major functionality is to implement the Bootstrap Inference with Correct Sandwich (BICS) testing procedure, which we have found to provide better finite-sample performance than other inference procedures for testing GxE interaction. More details on these functions are available in Sun, Carroll, Christiani, and Lin (2018) <doi:10.1111/biom.12813>.",
    "version": "1.1",
    "maintainer": "Ryan Sun <ryansun.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2634,
    "package_name": "GEmetrics",
    "title": "Best Linear Unbiased Prediction of Genotype-by-Environment\nMetrics",
    "description": "Provides functions to calculate the best linear unbiased prediction of genotype-by-environment metrics: ecovalence, environmental variance, Finlay and Wilkinson regression and Lin and Binns superiority measure, based on a multi-environment genomic prediction model.",
    "version": "1.0.0",
    "maintainer": "Simon Rio <simon.rio@cirad.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2638,
    "package_name": "GFDrmst",
    "title": "Multiple RMST-Based Tests in General Factorial Designs",
    "description": "We implemented multiple tests based on the restricted mean survival time (RMST) for general factorial designs as described in Munko et al. (2024) <doi:10.1002/sim.10017>. Therefore, an asymptotic test, a groupwise bootstrap test, and a permutation test are incorporated with a Wald-type test statistic. The asymptotic and groupwise bootstrap test take the asymptotic exact dependence structure of the test statistics into account to gain more power. Furthermore, confidence intervals for RMST contrasts can be calculated and plotted and a stepwise extension that can improve the power of the multiple tests is available.",
    "version": "0.1.1",
    "maintainer": "Merle Munko <merle.munko@ovgu.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2640,
    "package_name": "GFDsurv",
    "title": "Tests for Survival Data in General Factorial Designs",
    "description": "Implemented are three Wald-type statistic and respective\n  permuted versions for null hypotheses formulated in terms of cumulative hazard rate functions, medians and the concordance measure, respectively, in the general framework of survival factorial designs with possibly heterogeneous survival and/or censoring distributions, for crossed designs with an arbitrary number of factors and nested designs with up to three factors.\n\tDitzhaus, Dobler and Pauly (2020) <doi:10.1177/0962280220980784> \n\tDitzhaus, Janssen, Pauly (2020) <arXiv: 2004.10818v2>\n\tDobler and Pauly (2019) <doi:10.1177/0962280219831316>.",
    "version": "0.1.1",
    "maintainer": "Merle Munko <merle.munko@ovgu.de>",
    "url": "https://github.com/PhilippSteinhauer/GFDsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2651,
    "package_name": "GGRidge",
    "title": "Graphical Group Ridge",
    "description": "The Graphical Group Ridge 'GGRidge' package package classifies ridge regression predictors in disjoint groups of conditionally correlated variables and derives different penalties (shrinkage parameters) for these groups of predictors. It combines the ridge regression method with the graphical model for high-dimensional data (i.e. the number of predictors exceeds the number of cases) or ill-conditioned data (e.g. in the presence of multicollinearity among predictors). The package reduces the mean square errors and the extent of over-shrinking of predictors as compared to the ridge method.Aldahmani, S. and Zoubeidi, T. (2020) <DOI:10.1080/00949655.2020.1803320>.",
    "version": "1.1.0",
    "maintainer": "Saeed Aldahmani <saldahmani@uaeu.ac.ae>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2659,
    "package_name": "GHS",
    "title": "Graphical Horseshoe MCMC Sampler Using Data Augmented Block\nGibbs Sampler",
    "description": "Draw posterior samples to estimate the precision matrix for multivariate Gaussian data. Posterior means of the samples is the graphical horseshoe estimate by Li, Bhadra and Craig(2017) <arXiv:1707.06661>. The function uses matrix decomposition and variable change from the Bayesian graphical lasso by Wang(2012) <doi:10.1214/12-BA729>, and the variable augmentation for sampling under the horseshoe prior by Makalic and Schmidt(2016) <arXiv:1508.03884>. Structure of the graphical horseshoe function was inspired by the Bayesian graphical lasso function using blocked sampling, authored by Wang(2012) <doi:10.1214/12-BA729>.",
    "version": "0.1",
    "maintainer": "Ashutosh Srivastava<srivas48@purdue.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2665,
    "package_name": "GIMMEgVAR",
    "title": "Group Iterative Multiple Model Estimation with 'graphicalVAR'",
    "description": "Data-driven approach for arriving at person-specific time series models from within a Graphical Vector Autoregression (VAR) framework. The method first identifies which relations replicate across the majority of individuals to detect signal from noise. These group-level relations are then used as a foundation for starting the search for person-specific (or individual-level) relations. All estimates are obtained uniquely for each individual in the final models. The method for the 'graphicalVAR' approach is found in Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>. ",
    "version": "0.1.0",
    "maintainer": "Sandra Williams Lee <wsandra@live.unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2673,
    "package_name": "GJRM",
    "title": "Generalised Joint Regression Modelling",
    "description": "Routines for fitting various joint (and univariate) regression models, with several types of covariate effects, in the presence of equations' errors association.",
    "version": "0.2-6.8",
    "maintainer": "Giampiero Marra <giampiero.marra@ucl.ac.uk>",
    "url": "https://www.ucl.ac.uk/statistics/people/giampieromarra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2674,
    "package_name": "GJRM.data",
    "title": "Data Sets for Copula Additive Distributional Regression Using R",
    "description": "Data sets used in the book Marra and Radice (2025, ISBN:9781032973111) \"Copula Additive Distributional Regression Using R\", for illustrating the fitting of various joint (and univariate) regression models, with several types of covariate effects, in the presence of equations' errors association.",
    "version": "0.1-1",
    "maintainer": "Giampiero Marra <giampiero.marra@ucl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2678,
    "package_name": "GLDEX",
    "title": "Fitting Single and Mixture of Generalised Lambda Distributions",
    "description": "The fitting algorithms considered in this package have two major objectives. One is to provide a smoothing device to fit distributions to data using the weight and unweighted discretised approach based on the bin width of the histogram. The other is to provide a definitive fit to the data set using the maximum likelihood and quantile matching estimation. Other methods such as moment matching, starship method, L moment matching are also provided. Diagnostics on goodness of fit can be done via qqplots, KS-resample tests and comparing mean, variance, skewness and kurtosis of the data with the fitted distribution. References include the following: Karvanen and Nuutinen (2008) \"Characterizing the generalized lambda distribution by L-moments\" <doi:10.1016/j.csda.2007.06.021>, King and MacGillivray (1999) \"A starship method for fitting the generalised lambda distributions\" <doi:10.1111/1467-842X.00089>, Su (2005) \"A Discretized Approach to Flexibly Fit Generalized Lambda Distributions to Data\" <doi:10.22237/jmasm/1130803560>, Su (2007) \"Nmerical Maximum Log Likelihood Estimation for Generalized Lambda Distributions\" <doi:10.1016/j.csda.2006.06.008>, Su (2007) \"Fitting Single and Mixture of Generalized Lambda Distributions to Data via Discretized and Maximum Likelihood Methods: GLDEX in R\" <doi:10.18637/jss.v021.i09>, Su (2009) \"Confidence Intervals for Quantiles Using Generalized Lambda Distributions\" <doi:10.1016/j.csda.2009.02.014>, Su (2010) \"Chapter 14: Fitting GLDs and Mixture of GLDs to Data using Quantile Matching Method\" <doi:10.1201/b10159>, Su (2010) \"Chapter 15: Fitting GLD to data using GLDEX 1.0.4 in R\" <doi:10.1201/b10159>, Su (2015)   \"Flexible Parametric Quantile Regression Model\" <doi:10.1007/s11222-014-9457-1>, Su (2021) \"Flexible parametric accelerated failure time model\"<doi:10.1080/10543406.2021.1934854>.",
    "version": "2.0.0.9.4",
    "maintainer": "Steve Su <allegro.su@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2679,
    "package_name": "GLDreg",
    "title": "Fit GLD Regression/Quantile/AFT Model to Data",
    "description": "Owing to the rich shapes of Generalised Lambda Distributions (GLDs), GLD standard/quantile/Accelerated Failure Time (AFT) regression is a competitive flexible model compared to standard/quantile/AFT regression. The proposed method has some major advantages: 1) it provides a reference line which is very robust to outliers with the attractive property of zero mean residuals and 2) it gives a unified, elegant quantile regression model from the reference line with smooth regression coefficients across different quantiles. For AFT model, it also eliminates the needs to try several different AFT models, owing to the flexible shapes of GLD. The goodness of fit of  the proposed model can be assessed via QQ plots and Kolmogorov-Smirnov tests and data driven smooth test, to ensure the appropriateness of the statistical inference under consideration. Statistical distributions of coefficients of the GLD regression line are obtained using simulation, and interval estimates are obtained directly from simulated data.  References include the following: Su (2015) \"Flexible Parametric Quantile Regression Model\" <doi:10.1007/s11222-014-9457-1>, Su (2021) \"Flexible parametric accelerated failure time model\"<doi:10.1080/10543406.2021.1934854>.",
    "version": "1.1.2",
    "maintainer": "Steve Su <allegro.su@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2681,
    "package_name": "GLMMadaptive",
    "title": "Generalized Linear Mixed Models using Adaptive Gaussian\nQuadrature",
    "description": "Fits generalized linear mixed models for a single grouping factor under\n    maximum likelihood approximating the integrals over the random effects with an \n    adaptive Gaussian quadrature rule; Jose C. Pinheiro and Douglas M. Bates (1995) \n    <doi:10.1080/10618600.1995.10474663>.  ",
    "version": "0.9-7",
    "maintainer": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "url": "https://drizopoulos.github.io/GLMMadaptive/,\nhttps://github.com/drizopoulos/GLMMadaptive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2688,
    "package_name": "GLSME",
    "title": "Generalized Least Squares with Measurement Error",
    "description": "Performs linear regression with correlated predictors, responses and correlated measurement errors in predictors and responses, correcting for biased caused by these.",
    "version": "1.0.5",
    "maintainer": "Krzysztof Bartoszek <krzbar@protonmail.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2692,
    "package_name": "GMDH",
    "title": "Short Term Forecasting via GMDH-Type Neural Network Algorithms",
    "description": "Group method of data handling (GMDH) - type neural network algorithm is the heuristic self-organization method for modelling the complex systems. In this package, GMDH-type neural network algorithms are applied to make short term forecasting for a univariate time series. ",
    "version": "1.6",
    "maintainer": "Osman Dag <osman.dag@hacettepe.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2701,
    "package_name": "GNAR",
    "title": "Methods for Fitting Network Time Series Models",
    "description": "Simulation of, and fitting models for, Generalised Network Autoregressive (GNAR) time series models which take account of network structure, potentially with exogenous variables.  Such models are described in Knight et al. (2020) <doi:10.18637/jss.v096.i05> and Nason and Wei (2021) <doi:10.1111/rssa.12875>.  Diagnostic tools for GNAR(X) models can be found in Nason et al. (2023) <doi:10.48550/arXiv.2312.00530>.",
    "version": "1.1.4",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2710,
    "package_name": "GORCure",
    "title": "Fit Generalized Odds Rate Mixture Cure Model with Interval\nCensored Data",
    "description": "Generalized Odds Rate Mixture Cure (GORMC) model is a flexible model of fitting survival data with a cure fraction, including the Proportional Hazards Mixture Cure (PHMC) model and the Proportional Odds Mixture Cure Model as special cases. This package fit the GORMC model with interval censored data.",
    "version": "2.0",
    "maintainer": "Jie Zhou <zhoujie02569@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2724,
    "package_name": "GPCERF",
    "title": "Gaussian Processes for Estimating Causal Exposure Response\nCurves",
    "description": "Provides a non-parametric Bayesian framework based on Gaussian process priors for estimating causal effects of a continuous exposure and detecting change points in the causal exposure response curves using observational data. Ren, B., Wu, X., Braun, D., Pillai, N., & Dominici, F.(2021). \"Bayesian modeling for exposure response curve via gaussian processes: Causal effects of exposure to air pollution on health outcomes.\" arXiv preprint <doi:10.48550/arXiv.2105.03454>.",
    "version": "0.2.4",
    "maintainer": "Boyu Ren <bren@mgb.org>",
    "url": "https://github.com/NSAPH-Software/GPCERF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2728,
    "package_name": "GPFDA",
    "title": "Gaussian Process for Functional Data Analysis",
    "description": "Functionalities for modelling functional data with \n    multidimensional inputs, multivariate functional data, and non-separable \n    and/or non-stationary covariance structure of function-valued processes. \n    In addition, there are functionalities for functional regression models where \n    the mean function depends on scalar and/or functional covariates and \n    the covariance structure depends on functional covariates. \n    The development version of the package can be found on \n    <https://github.com/gpfda/GPFDA-dev>.",
    "version": "3.1.3",
    "maintainer": "Evandro Konzen <gpfda.r@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2731,
    "package_name": "GPLTR",
    "title": "Generalized Partially Linear Tree-Based Regression Model",
    "description": "Combining a generalized linear model with an additional tree part \n          on the same scale. A four-step procedure is proposed to fit the model and test \n          the joint effect of the selected tree part while adjusting on confounding factors. \n          We also proposed an ensemble procedure based on the bagging to improve prediction \n          accuracy and computed several scores of importance for variable selection.\n          See 'Cyprien Mbogning et al.'(2014)<doi:10.1186/2043-9113-4-6> and \n         'Cyprien Mbogning et al.'(2015)<doi:10.1159/000380850> \n          for an overview of all the methods implemented in this package.",
    "version": "1.5",
    "maintainer": "Cyprien Mbogning <cyprien.mbogning@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2737,
    "package_name": "GPTreeO",
    "title": "Dividing Local Gaussian Processes for Online Learning Regression",
    "description": "We implement and extend the Dividing Local Gaussian Process \n    algorithm by Lederer et al. (2020) <doi:10.48550/arXiv.2006.09446>. Its\n    main use case is in online learning where it is used to train a network of\n    local GPs (referred to as tree) by cleverly partitioning the input space. \n    In contrast to a single GP, 'GPTreeO' is able to deal with larger amounts of\n    data. The package includes methods to create the tree and set its\n    parameter, incorporating data points from a data stream as well as making\n    joint predictions based on all relevant local GPs.",
    "version": "1.0.1",
    "maintainer": "Timo Braun <gptreeo.timo.braun@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2738,
    "package_name": "GPareto",
    "title": "Gaussian Processes for Pareto Front Estimation and Optimization",
    "description": "Gaussian process regression models, a.k.a. Kriging models, are\n    applied to global multi-objective optimization of black-box functions.\n    Multi-objective Expected Improvement and Step-wise Uncertainty Reduction\n    sequential infill criteria are available. A quantification of uncertainty\n    on Pareto fronts is provided using conditional simulations.",
    "version": "1.1.9",
    "maintainer": "Mickael Binois <mickael.binois@inria.fr>",
    "url": "https://github.com/mbinois/GPareto",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2743,
    "package_name": "GPvam",
    "title": "Maximum Likelihood Estimation of Multiple Membership Mixed\nModels Used in Value-Added Modeling",
    "description": "An EM algorithm, Karl et al. (2013) <doi:10.1016/j.csda.2012.10.004>, is used to estimate the generalized, variable, and complete persistence models, Mariano et al. (2010) <doi:10.3102/1076998609346967>. These are multiple-membership linear mixed models with teachers modeled as \"G-side\" effects and students modeled with either \"G-side\" or \"R-side\" effects.",
    "version": "3.2-0",
    "maintainer": "Andrew Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2748,
    "package_name": "GRCRegression",
    "title": "Modified Poisson Regression of Grouped and Right-Censored Counts",
    "description": "Implement maximum likelihood estimation for Poisson generalized\n  linear models with grouped and right-censored count data. Intended to be used\n  for analyzing grouped and right-censored data, which is widely applied in\n  many branches of social sciences. The algorithm implemented is described\n  in Fu et al., (2021) <doi:10.1111/rssa.12678>.",
    "version": "1.0",
    "maintainer": "Xin Guo <xin.guo@uq.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2757,
    "package_name": "GROAN",
    "title": "Genomic Regression Workbench",
    "description": "Workbench for testing genomic regression accuracy on\n    (optionally noisy) phenotypes.",
    "version": "1.3.1",
    "maintainer": "Nelson Nazzicari <nelson.nazzicari@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2779,
    "package_name": "GSM",
    "title": "Gamma Shape Mixture",
    "description": "Implementation of a Bayesian approach for estimating a mixture of gamma distributions in which the mixing occurs over the shape parameter. This family provides a flexible and novel approach for modeling heavy-tailed distributions, it is computationally efficient, and it only requires to specify a prior distribution for a single parameter.",
    "version": "1.3.2",
    "maintainer": "Sergio Venturini <sergio.venturini@unibocconi.it>",
    "url": "http://projecteuclid.org/euclid.aoas/1215118537",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2784,
    "package_name": "GSSE",
    "title": "Genotype-Specific Survival Estimation",
    "description": "We propose a fully efficient sieve maximum likelihood method to estimate genotype-specific distribution of time-to-event outcomes under a nonparametric model. We can handle missing genotypes in pedigrees.  We estimate the time-dependent hazard ratio between two genetic mutation groups using B-splines, while applying nonparametric maximum likelihood estimation to the reference baseline hazard function.  The estimators are calculated via an expectation-maximization algorithm.",
    "version": "0.1",
    "maintainer": "Baosheng Liang <liangbsunc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2795,
    "package_name": "GUD",
    "title": "Bayesian Modal Regression Based on the GUD Family",
    "description": "Provides probability density functions and sampling algorithms for three key distributions from the General Unimodal Distribution (GUD) family: the Flexible Gumbel (FG) distribution, the Double Two-Piece (DTP) Student-t distribution, and the Two-Piece Scale (TPSC) Student-t distribution. Additionally, this package includes a function for Bayesian linear modal regression, leveraging these three distributions for model fitting. The details of the Bayesian modal regression model based on the GUD family can be found at Liu, Huang, and Bai (2024) <doi:10.1016/j.csda.2024.108012>.",
    "version": "1.0.2",
    "maintainer": "Qingyang Liu <qingyang@email.sc.edu>",
    "url": "https://github.com/rh8liuqy/Bayesian_modal_regression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2801,
    "package_name": "GVARX",
    "title": "Perform Global Vector Autoregression Estimation and Inference",
    "description": "Light procedures for learning Global Vector Autoregression model (GVAR) of Pesaran, Schuermann and Weiner (2004) <DOI:10.1198/073500104000000019> and Dees, di Mauro, Pesaran and Smith (2007) <DOI:10.1002/jae.932>.",
    "version": "1.4",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2814,
    "package_name": "GWRM",
    "title": "Generalized Waring Regression Model for Count Data",
    "description": "Statistical functions to fit, validate and describe a Generalized\n    Waring Regression Model (GWRM).",
    "version": "2.1.0.4",
    "maintainer": "Silverio Vilchez-Lopez <svilchez@ujaen.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2817,
    "package_name": "GWlasso",
    "title": "Geographically Weighted Lasso",
    "description": "Performs geographically weighted Lasso regressions. Find optimal bandwidth, fit a geographically weighted lasso or ridge regression, and make predictions.\n    These methods are specially well suited for ecological inferences. Bandwidth selection algorithm is from A. Comber and P. Harris (2018) <doi:10.1007/s10109-018-0280-7>.",
    "version": "1.0.2",
    "maintainer": "Matthieu Mulot <matthieu.mulot@gmail.com>",
    "url": "https://github.com/nibortolum/GWlasso,\nhttps://nibortolum.github.io/GWlasso/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2823,
    "package_name": "GaSP",
    "title": "Train and Apply a Gaussian Stochastic Process Model",
    "description": "Train a Gaussian stochastic process model of an unknown function, possibly observed with error, via maximum likelihood or maximum a posteriori (MAP) estimation, run model diagnostics, and make predictions, following Sacks, J., Welch, W.J., Mitchell, T.J., and Wynn, H.P. (1989) \"Design and Analysis of Computer Experiments\", Statistical Science, <doi:10.1214/ss/1177012413>.  Perform sensitivity analysis and visualize low-order effects, following Schonlau, M. and Welch, W.J. (2006), \"Screening the Input Variables to a Computer Model Via Analysis of Variance and Visualization\", <doi:10.1007/0-387-28014-6_14>.",
    "version": "1.0.6",
    "maintainer": "William J. Welch <will@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2825,
    "package_name": "Gammareg",
    "title": "Classic Gamma Regression: Joint Modeling of Mean and Shape\nParameters",
    "description": "Performs Gamma regression, where both mean and shape parameters follows lineal regression structures. ",
    "version": "3.0.1",
    "maintainer": "Martha Corrales <martha.corrales@usa.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2829,
    "package_name": "GaussianHMM1d",
    "title": "Inference, Goodness-of-Fit and Forecast for Univariate Gaussian\nHidden Markov Models",
    "description": "Inference, goodness-of-fit test, and prediction densities and intervals for univariate Gaussian Hidden Markov Models (HMM). The goodness-of-fit is based on a Cramer-von Mises statistic and uses parametric bootstrap to estimate the p-value. The description of the methodology is taken from Chapter 10.2 of Remillard (2013) <doi:10.1201/b14285>.",
    "version": "1.1.2",
    "maintainer": "Bouchra R. Nasri <bouchra.nasri@umontreal.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2830,
    "package_name": "GeDS",
    "title": "Geometrically Designed Spline Regression",
    "description": "Spline regression, generalized additive models and\n    component-wise gradient boosting utilizing geometrically designed\n    (GeD) splines. GeDS regression is a non-parametric method inspired by\n    geometric principles, for fitting spline regression models with\n    variable knots in one or two independent variables. It efficiently\n    estimates the number of knots and their positions, as well as the\n    spline order, assuming the response variable follows a distribution\n    from the exponential family. GeDS models integrate the broader\n    category of generalized (non-)linear models, offering a flexible\n    approach to model complex relationships. A description of the\n    method can be found in Kaishev et al. (2016)\n    <doi:10.1007/s00180-015-0621-7> and Dimitrova et al. (2023)\n    <doi:10.1016/j.amc.2022.127493>. Further extending its capabilities,\n    GeDS's implementation includes generalized additive models (GAM) and\n    functional gradient boosting (FGB), enabling versatile multivariate\n    predictor modeling, as discussed in the forthcoming work of Dimitrova\n    et al. (2025).",
    "version": "0.3.3",
    "maintainer": "Emilio L. Sáenz Guillén\n<Emilio.Saenz-Guillen@citystgeorges.ac.uk>",
    "url": "https://github.com/emilioluissaenzguillen/GeDS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2908,
    "package_name": "GeodRegr",
    "title": "Geodesic Regression",
    "description": "Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.",
    "version": "0.2.0",
    "maintainer": "Ha-Young Shin <hayoung.shin@gmail.com>",
    "url": "https://github.com/hayoungshin1/GeodRegr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2925,
    "package_name": "GiRaF",
    "title": "Gibbs Random Fields Analysis",
    "description": "Allows calculation on, and\n sampling from Gibbs Random Fields, and more precisely general\n homogeneous Potts model. The primary tool is the exact computation of\n the intractable normalising constant for small rectangular lattices.\n Beside the latter function, it contains method that give exact sample from the likelihood\n for small enough rectangular lattices or approximate sample from the\n likelihood using MCMC samplers for large lattices.",
    "version": "1.0.2",
    "maintainer": "Julien Stoehr <stoehr@ceremade.dauphine.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2936,
    "package_name": "GlarmaVarSel",
    "title": "Variable Selection in Sparse GLARMA Models",
    "description": "Performs variable selection in high-dimensional sparse GLARMA models. For further details we refer the reader to the paper Gomtsyan et al. (2020), <arXiv:2007.08623v1>.",
    "version": "1.0",
    "maintainer": "Marina Gomtsyan <marina.gomtsyan@agroparistech.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2947,
    "package_name": "GmptzCurve",
    "title": "Gompertz Curve Fitting",
    "description": "A system for fitting Gompertz Curve in a Time Series Data.",
    "version": "0.1.0",
    "maintainer": "Arnab Roy <arnabroy7640@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2962,
    "package_name": "Grace",
    "title": "Graph-Constrained Estimation and Hypothesis Tests",
    "description": "Use the graph-constrained estimation (Grace) procedure (Zhao and Shojaie, 2016 <doi:10.1111/biom.12418>) to estimate graph-guided linear regression coefficients and use the Grace/GraceI/GraceR tests to perform graph-guided hypothesis tests on the association between the response and the predictors.",
    "version": "0.5.3",
    "maintainer": "Sen Zhao <sen-zhao@sen-zhao.com>",
    "url": "http://onlinelibrary.wiley.com/doi/10.1111/biom.12418/abstract",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2970,
    "package_name": "GreedyEPL",
    "title": "Greedy Expected Posterior Loss",
    "description": "Summarises a collection of partitions into a single optimal partition. The objective function is the expected posterior loss, and the minimisation is performed through a greedy algorithm described in Rastelli, R. and Friel, N. (2017) \"Optimal Bayesian estimators for latent variable cluster models\" <DOI:10.1007/s11222-017-9786-y>.",
    "version": "1.3",
    "maintainer": "Riccardo Rastelli <riccardoras@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2977,
    "package_name": "GreyModel",
    "title": "Fitting and Forecasting of Grey Model",
    "description": "Testing, Implementation and Forecasting of Grey Model (GM(1, 1)). For method details see Hsu, L. and Wang, C. (2007). <doi:10.1016/j.techfore.2006.02.005>. ",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 2984,
    "package_name": "GroupTest",
    "title": "Multiple Testing Procedure for Grouped Hypotheses",
    "description": "Contains functions for a two-stage multiple testing procedure for grouped hypothesis, aiming at controlling both the total posterior false discovery rate and within-group false discovery rate. ",
    "version": "1.0.1",
    "maintainer": "Zhigen Zhao <zhaozhg@temple.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3000,
    "package_name": "HBSTM",
    "title": "Hierarchical Bayesian Space-Time Models for Gaussian Space-Time\nData",
    "description": "Fits Hierarchical Bayesian space-Time models for Gaussian data. Furthermore, its functions have been implemented for analysing the fitting qualities of those models.",
    "version": "1.0.2",
    "maintainer": "Alberto Lopez Moreno <bertolomo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3005,
    "package_name": "HCTDesign",
    "title": "Group Sequential Design for Historical Control Trial with\nSurvival Outcome",
    "description": "It provides functions to design historical controlled trials with survival outcome by group sequential method. The options for interim look boundaries are efficacy only, efficacy & futility or futility only. It also provides the function to monitor the trial for any unplanned look. The package is based on Jianrong Wu, Xiaoping Xiong (2016) <doi:10.1002/pst.1756> and Jianrong Wu, Yimei Li (2020) <doi:10.1080/10543406.2019.1684305>.",
    "version": "0.7.4",
    "maintainer": "Tushar Patni <tushar.patni006@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3006,
    "package_name": "HCTR",
    "title": "Higher Criticism Tuned Regression",
    "description": "A novel searching scheme for tuning parameter in high-dimensional \n             penalized regression. We propose a new estimate of the regularization\n             parameter based on an estimated lower bound of the proportion of false \n             null hypotheses (Meinshausen and Rice (2006) <doi:10.1214/009053605000000741>).\n             The bound is estimated by applying the empirical null distribution of the higher \n             criticism statistic, a second-level significance testing, which is constructed\n             by dependent p-values from a multi-split regression and aggregation method\n             (Jeng, Zhang and Tzeng (2019) <doi:10.1080/01621459.2018.1518236>). An estimate \n             of tuning parameter in penalized regression is decided corresponding to the lower \n             bound of the proportion of false null hypotheses. Different penalized \n             regression methods are provided in the multi-split algorithm. ",
    "version": "0.1.1",
    "maintainer": "Tao Jiang <tjiang8@ncsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3008,
    "package_name": "HCmodelSets",
    "title": "Regression with a Large Number of Potential Explanatory\nVariables",
    "description": "Software for performing the reduction, exploratory and model selection phases of the procedure proposed by Cox, D.R. and Battey, H.S. (2017) <doi:10.1073/pnas.1703764114> for sparse regression when the number of potential explanatory variables far exceeds the sample size. The software supports linear regression, likelihood-based fitting of generalized linear regression models and the proportional hazards model fitted by partial likelihood.",
    "version": "1.1.3",
    "maintainer": "H. Battey <h.battey@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3010,
    "package_name": "HDBRR",
    "title": "High Dimensional Bayesian Ridge Regression without MCMC",
    "description": "Ridge regression provide biased estimators of the regression parameters with lower variance. The HDBRR (\"High Dimensional Bayesian Ridge Regression\") function fits Bayesian Ridge regression without MCMC, this one uses the SVD or QR decomposition for the posterior computation.",
    "version": "1.1.4",
    "maintainer": "Blanca Monroy-Castillo Developer <blancamonroy.96@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3012,
    "package_name": "HDCI",
    "title": "High Dimensional Confidence Interval Based on Lasso and\nBootstrap",
    "description": "Fits regression models on high dimensional data to estimate coefficients and use bootstrap method to obtain confidence intervals. Choices for regression models are Lasso, Lasso+OLS, Lasso partial ridge, Lasso+OLS partial ridge. ",
    "version": "1.0-2",
    "maintainer": "Xin Xu <xin.xu@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3016,
    "package_name": "HDInterval",
    "title": "Highest (Posterior) Density Intervals",
    "description": "A generic function and a set of methods to calculate highest density intervals for a variety of classes of objects which can specify a probability density distribution, including MCMC output, fitted density objects, and functions.",
    "version": "0.2.4",
    "maintainer": "Ngumbang Juat <ngumbangjuat@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3017,
    "package_name": "HDJM",
    "title": "Penalized High-Dimensional Joint Model",
    "description": "Joint models have been widely used to study the associations between longitudinal biomarkers and a survival outcome. However, existing joint models only consider one or a few longitudinal \n    biomarkers and cannot deal with high-dimensional longitudinal biomarkers. This package can be used to fit our recently developed penalized joint model that can handle high-dimensional longitudinal biomarkers. \n    Specifically, an adaptive lasso penalty is imposed on the parameters for the effects of the longitudinal biomarkers on the survival outcome, which allows for variable selection. \n    Also, our algorithm is computationally efficient, which is based on the Gaussian variational approximation method.",
    "version": "0.1.0",
    "maintainer": "Jiehuan Sun <jiehuan.sun@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3020,
    "package_name": "HDMFA",
    "title": "High-Dimensional Matrix Factor Analysis",
    "description": "High-dimensional matrix factor models have drawn much attention in view of the fact that observations are usually well structured to be an array such as in macroeconomics and finance. In addition, data often exhibit heavy-tails and thus it is also important to develop robust procedures. We aim to address this issue by replacing the least square loss with Huber loss function. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's Frobenius norm, which leads to a weighted iterative projection approach to compute and learn the parameters and thereby named as Robust-Matrix-Factor-Analysis (RMFA), see the details in He et al. (2023)<doi:10.1080/07350015.2023.2191676>. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm (IHR), see the details in He et al. (2023) <arXiv:2306.03317>. In this package, we also provide the algorithm for alpha-PCA by Chen & Fan (2021) <doi:10.1080/01621459.2021.1970569>, the Projected estimation (PE) method by Yu et al. (2022)<doi:10.1016/j.jeconom.2021.04.001>. In addition, the methods for determining the pair of factor numbers are also given.",
    "version": "0.1.1",
    "maintainer": "Ran Zhao <Zhaoran@mail.sdu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3024,
    "package_name": "HDRFA",
    "title": "High-Dimensional Robust Factor Analysis",
    "description": "Factor models have been widely applied in areas such as economics and finance, and the well-known heavy-tailedness of macroeconomic/financial data should be taken into account when conducting factor analysis. We propose two algorithms to do robust factor analysis by considering the Huber loss. One is based on minimizing the Huber loss of the idiosyncratic error's L2 norm, which turns out to do Principal Component Analysis (PCA) on the weighted sample covariance matrix and thereby named as Huber PCA. The other one is based on minimizing the element-wise Huber loss, which can be solved by an iterative Huber regression algorithm. In this package we also provide the code for traditional PCA, the Robust Two Step (RTS) method by He et al. (2022) and the Quantile Factor Analysis (QFA) method by Chen et al. (2021) and He et al. (2023).",
    "version": "0.1.5",
    "maintainer": "Dong Liu <liudong_stat@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3030,
    "package_name": "HDTSA",
    "title": "High Dimensional Time Series Analysis Tools",
    "description": "An implementation for high-dimensional time series analysis methods, including factor model for vector time series \n      proposed by Lam and Yao (2012) <doi:10.1214/12-AOS970> and Chang, Guo and Yao (2015)\n      <doi:10.1016/j.jeconom.2015.03.024>, martingale difference test proposed by \n      Chang, Jiang and Shao (2023) <doi:10.1016/j.jeconom.2022.09.001>, principal \n      component analysis for vector time series proposed by Chang, Guo and Yao (2018) <doi:10.1214/17-AOS1613>,\n      cointegration analysis proposed by Zhang, Robinson and Yao (2019)\n      <doi:10.1080/01621459.2018.1458620>, unit root test proposed by Chang, Cheng and Yao (2022)\n      <doi:10.1093/biomet/asab034>, white noise test proposed by Chang, Yao and Zhou (2017)\n      <doi:10.1093/biomet/asw066>, CP-decomposition for matrix time \n      series proposed by Chang et al. (2023) <doi:10.1093/jrsssb/qkac011> and\n      Chang et al. (2024) <doi:10.48550/arXiv.2410.05634>, and statistical inference for\n      spectral density matrix proposed by Chang et al. (2022) \n      <doi:10.48550/arXiv.2212.13686>.",
    "version": "1.0.5-1",
    "maintainer": "Chen Lin <linchen@smail.swufe.edu.cn>",
    "url": "https://github.com/Linc2021/HDTSA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3048,
    "package_name": "HGSL",
    "title": "Heterogeneous Group Square-Root Lasso",
    "description": "Estimation of high-dimensional multi-response regression with heterogeneous noises under Heterogeneous group square-root Lasso penalty. For details see: Ren, Z., Kang, Y., Fan, Y. and Lv, J. (2018)<arXiv:1606.03803>.",
    "version": "1.0.0",
    "maintainer": "Yongjian Kang <yongjiak@usc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3082,
    "package_name": "HQM",
    "title": "Superefficient Estimation of Future Conditional Hazards Based on\nMarker Information",
    "description": "Provides univariate and indexed (multivariate) nonparametric smoothed kernel estimators for the future conditional hazard rate function when time-dependent covariates are present, a bandwidth selector for the estimator's implementation and pointwise and uniform confidence bands. Methods used in the package refer to Bagkavos, Isakson, Mammen, Nielsen and Proust-Lima (2025)  <doi:10.1093/biomet/asaf008>.",
    "version": "2.0",
    "maintainer": "Dimitrios Bagkavos <dimitrios.bagkavos@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3084,
    "package_name": "HRW",
    "title": "Datasets, Functions and Scripts for Semiparametric Regression\nSupporting Harezlak, Ruppert & Wand (2018)",
    "description": "The book \"Semiparametric Regression with R\" by J. Harezlak, D. Ruppert & M.P. Wand (2018, Springer; ISBN: 978-1-4939-8851-8) makes use of datasets and scripts to explain semiparametric regression concepts. Each of the book's scripts are contained in this package as well as datasets that are not within other R packages. Functions that aid semiparametric regression analysis are also included.",
    "version": "1.0-5",
    "maintainer": "Matt P. Wand <matt.wand@uts.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3090,
    "package_name": "HSPOR",
    "title": "Hidden Smooth Polynomial Regression for Rupture Detection",
    "description": "Several functions that allow by different methods to infer a piecewise polynomial regression model under regularity constraints, namely continuity or differentiability of the link function. The implemented functions are either specific to data with two regimes, or generic for any number of regimes, which can be given by the user or learned by the algorithm. A paper describing all these methods will be submitted soon. The reference will be added to this file as soon as available. ",
    "version": "1.1.9",
    "maintainer": "Florine Greciet <florine.greciet@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3096,
    "package_name": "HTRX",
    "title": "Haplotype Trend Regression with eXtra Flexibility (HTRX)",
    "description": "Detection of haplotype patterns that include single nucleotide polymorphisms (SNPs) and non-contiguous haplotypes that are associated with a phenotype. Methods for implementing HTRX are described in Yang Y, Lawson DJ (2023) <doi:10.1093/bioadv/vbad038> and Barrie W, Yang Y, Irving-Pease E.K, et al (2024) <doi:10.1038/s41586-023-06618-z>.",
    "version": "1.2.4",
    "maintainer": "Yaoling Yang <yaoling.yang@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3106,
    "package_name": "HWEintrinsic",
    "title": "Objective Bayesian Testing for the Hardy-Weinberg Equilibrium\nProblem",
    "description": "General (multi-allelic) Hardy-Weinberg equilibrium problem from an objective Bayesian testing standpoint. This aim is achieved through the identification of a class of priors specifically designed for this testing problem. A class of intrinsic priors under the full model is considered. This class is indexed by a tuning quantity, the training sample size, as discussed in Consonni, Moreno and Venturini (2010). These priors are objective, satisfy Savage's continuity condition and have proved to behave extremely well for many statistical testing problems.",
    "version": "1.2.3",
    "maintainer": "Sergio Venturini <sergio.venturini@unicatt.it>",
    "url": "https://onlinelibrary.wiley.com/doi/10.1002/sim.4084/abstract",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3113,
    "package_name": "HanStat",
    "title": "Package for Easy Interpretation of Statistical Methods",
    "description": "A simple and time saving multiple linear regression function (OLS) with interpretation, optional bootstrapping, effect size calculation and all tested requirements.",
    "version": "0.90.0",
    "maintainer": "Konrad Krahl <Beratung@Hanseatic-Statistics.de>",
    "url": "https://github.com/KonradKrahl/HanStat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3125,
    "package_name": "Hassani.SACF",
    "title": "Computing Lower Bound of Ljung-Box Test",
    "description": "The Ljung-Box test is one of the most important tests for time series diagnostics and model selection. \n    The  Hassani SACF (Sum of the Sample Autocorrelation Function) Theorem , however, indicates that the sum of sample autocorrelation function is always fix for \n    any stationary time series with arbitrary length. This package confirms for sensitivity of the Ljung-Box test to \n    the number of lags involved in the test and therefore it should be used with extra caution.\n    The Hassani SACF Theorem has been described in : Hassani, Yeganegi and M. R. (2019) <doi:10.1016/j.physa.2018.12.028>.",
    "version": "2.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3126,
    "package_name": "Hassani.Silva",
    "title": "A Test for Comparing the Predictive Accuracy of Two Sets of\nForecasts",
    "description": "A non-parametric test founded upon the principles of the Kolmogorov-Smirnov (KS) \n  test, referred to as the KS Predictive Accuracy (KSPA) test. The KSPA test is able to serve\n  two distinct purposes. Initially, the test seeks to determine whether there exists a \n  statistically significant difference between the distribution of forecast errors, and \n  secondly it exploits the principles of stochastic dominance to determine whether the \n  forecasts with the lower error also reports a stochastically smaller error than forecasts \n  from a competing model, and thereby enables distinguishing between the predictive accuracy \n  of forecasts. KSPA test has been described in : Hassani and Silva (2015) <doi:10.3390/econometrics3030590>.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3127,
    "package_name": "HazardDiff",
    "title": "Conditional Treatment Effect for Competing Risks",
    "description": "The conditional treatment effect for competing risks data in observational studies is estimated. While it is described as a constant difference between the hazard functions given the covariates, we do not assume specific functional forms for the covariates. Rava, D. and Xu, R. (2021) <arXiv:2112.09535>.",
    "version": "0.1.0",
    "maintainer": "Denise Rava <drava@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3131,
    "package_name": "HeckmanStan",
    "title": "Heckman Selection Models Based on Bayesian Analysis",
    "description": "Implements Heckman selection models using a Bayesian approach via 'Stan' and compares the performance of normal, Student’s t, and contaminated normal distributions in addressing complexities and selection bias (Heeju Lim, Victor E. Lachos, and Victor H. Lachos, Bayesian analysis of flexible Heckman selection models using Hamiltonian Monte Carlo, 2025, under submission).",
    "version": "1.0.0",
    "maintainer": "Heeju Lim <heeju.lim@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3134,
    "package_name": "HelpersMG",
    "title": "Tools for Various R Functions Helpers",
    "description": "Contains miscellaneous functions useful for managing 'NetCDF' files (see <https://en.wikipedia.org/wiki/NetCDF>), get moon phase and time for sun rise and fall, tide level, analyse and reconstruct periodic time series of temperature with irregular sinusoidal pattern, show scales and wind rose in plot with change of color of text, Metropolis-Hastings algorithm for Bayesian MCMC analysis, plot graphs or boxplot with error bars, search files in disk by there names or their content, read the contents of all files from a folder at one time.",
    "version": "2025.12.22",
    "maintainer": "Marc Girondot <marc.girondot@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3158,
    "package_name": "HiddenMarkov",
    "title": "Hidden Markov Models",
    "description": "Contains functions for the analysis of Discrete Time Hidden Markov Models, Markov Modulated GLMs and the Markov Modulated Poisson Process. It includes functions for simulation, parameter estimation, and the Viterbi algorithm. See the topic \"HiddenMarkov\" for an introduction to the package, and \"Change Log\" for a list of recent changes. The algorithms are based of those of Walter Zucchini.",
    "version": "1.8-14",
    "maintainer": "David Harte <d.s.harte@gmail.com>",
    "url": "https://www.statsresearch.co.nz/dsh/sslib/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3169,
    "package_name": "Hmisc",
    "title": "Harrell Miscellaneous",
    "description": "Contains many functions useful for data\n\tanalysis, high-level graphics, utility operations, functions for\n\tcomputing sample size and power, simulation, importing and annotating datasets,\n\timputing missing values, advanced table making, variable clustering,\n\tcharacter string manipulation, conversion of R objects to LaTeX and html code,\n\trecoding variables, caching, simplified parallel computing, encrypting and decrypting data using a safe workflow, general moving window statistical estimation, and assistance in interpreting principal component analysis.",
    "version": "5.2-5",
    "maintainer": "Frank E Harrell Jr <fh@fharrell.com>",
    "url": "https://hbiostat.org/R/Hmisc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3170,
    "package_name": "Hmsc",
    "title": "Hierarchical Model of Species Communities",
    "description": "Hierarchical Modelling of Species Communities (HMSC) is\n   a model-based approach for analyzing community ecological data. \n   This package implements it in the Bayesian framework with Gibbs\n   Markov chain Monte Carlo (MCMC) sampling (Tikhonov et al. (2020)\n   <doi:10.1111/2041-210X.13345>).",
    "version": "3.3-7",
    "maintainer": "Otso Ovaskainen <otso.t.ovaskainen@jyu.fi>",
    "url": "https://www.helsinki.fi/en/researchgroups/statistical-ecology/software/hmsc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3171,
    "package_name": "HoRM",
    "title": "Supplemental Functions and Datasets for \"Handbook of Regression\nMethods\"",
    "description": "Supplement for the book \"Handbook of Regression Methods\" by D. S. Young.  Some datasets used in the book are included and documented.  Wrapper functions are included that simplify the examples in the textbook, such as code for constructing a regressogram and expanding ANOVA tables to reflect the total sum of squares.",
    "version": "0.1.4",
    "maintainer": "Derek S. Young <derek.young@uky.edu>",
    "url": "https://github.com/dsy109/HoRM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3178,
    "package_name": "HoneyBADGER",
    "title": "HMM-integrated Bayesian approach for detecting CNV and LOH events using single-cell RNA-seq data",
    "description": "The HoneyBADGER package implements a set of statistical methods for detecting copy number variation and loss of heterozygosity events using single-cell RNA-seq data.",
    "version": "0.1",
    "maintainer": "Jean Fan <jeanfan@fas.harvard.edu>",
    "url": "https://github.com/JEFworks-Lab/HoneyBADGER",
    "exports": [],
    "topics": ["bayesian", "bioinformatics", "cnv-detection", "heterogeneity", "hmm", "single-cell-analysis", "single-cell-rna-seq", "subpopulation", "transcriptomics"],
    "score": "NA",
    "stars": 99
  },
  {
    "id": 3179,
    "package_name": "Horsekicks",
    "title": "Provide Extensions to the Prussian Army Death by Horsekick Data",
    "description": "We provide extensions to the classical dataset \"Example 4:\n             Death by the kick of a horse in the Prussian Army\" first\n\t           used by Ladislaus von Bortkeiwicz in his treatise on\n\t           the Poisson distribution \"Das Gesetz der kleinen Zahlen\",\n\t           <DOI:10.1017/S0370164600019453>.  As well as an\n\t           extended time series for the horse-kick death data, we also\n             provide, in parallel, deaths by falling from a horse and by\n\t     drowning.",
    "version": "1.0.2",
    "maintainer": "Bill Venables <bill.venables@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3185,
    "package_name": "HuraultMisc",
    "title": "Guillem Hurault Functions' Library",
    "description": "Contains various functions for data analysis, notably helpers and diagnostics for Bayesian modelling using Stan.",
    "version": "1.2.1",
    "maintainer": "Guillem Hurault <ghurault.dev@outlook.com>",
    "url": "https://ghurault.github.io/HuraultMisc/,\nhttps://github.com/ghurault/HuraultMisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3212,
    "package_name": "IBMPopSim",
    "title": "Individual Based Model Population Simulation",
    "description": "Simulation of the random evolution of heterogeneous populations using stochastic Individual-Based Models (IBMs) <doi:10.48550/arXiv.2303.06183>. \n    The package enables users to simulate population evolution, in which individuals are characterized by their age and some characteristics, and the population is modified by different types of events, including births/arrivals, death/exit events, or changes of characteristics. The frequency at which an event can occur to an individual can depend on their age and characteristics, but also on the characteristics of other individuals (interactions). \n    Such models have a wide range of applications. For instance, IBMs can be used for simulating the evolution of a heterogeneous insurance portfolio with selection or for validating  mortality forecasts. \n    This package overcomes the limitations of time-consuming IBMs simulations by implementing new efficient algorithms  based on thinning methods, which are compiled using the 'Rcpp' package while providing a user-friendly interface.",
    "version": "1.1.0",
    "maintainer": "Daphné Giorgi <daphne.giorgi@sorbonne-universite.fr>",
    "url": "https://github.com/DaphneGiorgi/IBMPopSim,\nhttps://DaphneGiorgi.github.io/IBMPopSim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3221,
    "package_name": "ICCier",
    "title": "Computes ICCs, per person, or per observation, using the Bayesian mixed effects location scale model",
    "description": "Computes intraclass correlations (ICCs). This can be in general (classic ICC), per person, or per observation within persons.",
    "version": "0.0.0.9200",
    "maintainer": "",
    "url": "https://github.com/stephensrmmartin/ICCier",
    "exports": [],
    "topics": ["bayesian", "intraclass-correlation-coefficient", "multilevel-models", "psychometrics", "statistics"],
    "score": "NA",
    "stars": 4
  },
  {
    "id": 3228,
    "package_name": "ICGOR",
    "title": "Fit Generalized Odds Rate Hazards Model with Interval Censored\nData",
    "description": "Generalized Odds Rate Hazards (GORH) model is a flexible model of fitting survival data, including the Proportional Hazards (PH) model and the Proportional Odds (PO) Model as special cases. This package fit the GORH model with interval censored data.",
    "version": "2.0",
    "maintainer": "Jie Zhou <zhoujie02569@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3236,
    "package_name": "ICSS",
    "title": "ICSS Algorithm by Inclan/Tiao (1994)",
    "description": "The Iterative Cumulative Sum of Squares (ICSS) algorithm by Inclan/Tiao (1994) <https://www.jstor.org/stable/2290916> detects multiple change points, i.e. structural break points, in the variance of a sequence of independent observations. For series of moderate size (i.e. 200 observations and beyond), the ICSS algorithm offers results comparable to those obtained by a Bayesian approach or by likelihood ration tests, without the heavy computational burden required by these approaches.",
    "version": "1.1",
    "maintainer": "Siegfried Köstlmeier <siegfried.koestlmeier@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3238,
    "package_name": "ICSsmoothing",
    "title": "Data Smoothing by Interpolating Cubic Splines",
    "description": "We construct the explicit form of clamped cubic interpolating spline (both uniform - knots are equidistant and non-uniform - knots are arbitrary). Using this form, we propose a linear regression model suitable for real data smoothing.",
    "version": "1.2.10",
    "maintainer": "Lubomir Antoni <lubomir.antoni@upjs.sk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3240,
    "package_name": "ICcalib",
    "title": "Cox Model with Interval-Censored Starting Time of a Covariate",
    "description": "Calibration and risk-set calibration methods for fitting Cox proportional hazard model when a binary covariate is measured intermittently. Methods include functions to fit calibration models from interval-censored data and modified partial likelihood for the proportional hazard model, Nevo et al. (2018+) <arXiv:1801.01529>.",
    "version": "1.0.8",
    "maintainer": "Daniel Nevo <danielnevo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3241,
    "package_name": "ICcforest",
    "title": "An Ensemble Method for Interval-Censored Survival Data",
    "description": "Implements the conditional inference forest approach \n  to modeling interval-censored survival data. It also provides \n  functions to tune the parameters and evaluate the model fit. See \n  Yao et al. (2019) <arXiv:1901.04599>.",
    "version": "0.5.1",
    "maintainer": "Weichi Yao <wy635@stern.nyu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3242,
    "package_name": "ICglm",
    "title": "Information Criteria for Generalized Linear Regression",
    "description": "Calculate various information criteria in literature for \"lm\" and \"glm\" objects.",
    "version": "0.1.0",
    "maintainer": "Fatih Saglam <fatih.saglam@omu.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3244,
    "package_name": "ICsurv",
    "title": "Semiparametric Regression Analysis of Interval-Censored Data",
    "description": "Currently using the proportional hazards (PH) model. More methods under other semiparametric regression models will be included in later versions. ",
    "version": "1.0.1",
    "maintainer": "Lianming Wang <wangl@stat.sc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3251,
    "package_name": "IDF",
    "title": "Estimation and Plotting of IDF Curves",
    "description": "Intensity-duration-frequency (IDF) curves are a widely used analysis-tool\n              in hydrology to assess extreme values of precipitation\n              [e.g. Mailhot et al., 2007, <doi:10.1016/j.jhydrol.2007.09.019>].\n              The package 'IDF' provides functions to estimate IDF parameters for given\n              precipitation time series on the basis of a duration-dependent\n              generalized extreme value distribution\n              [Koutsoyiannis et al., 1998, <doi:10.1016/S0022-1694(98)00097-3>].",
    "version": "2.1.3",
    "maintainer": "Felix S. Fauer <felix.fauer@met.fu-berlin.de>",
    "url": "https://gitlab.met.fu-berlin.de/Rpackages/idf_package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3252,
    "package_name": "IDLFM",
    "title": "Individual Dynamic Latent Factor Model",
    "description": "A personalized dynamic latent factor model (Zhang et al. (2024) <doi:10.1093/biomet/asae015>) for irregular multi-resolution time series data, to interpolate unsampled measurements from low-resolution time series.",
    "version": "1.0.0",
    "maintainer": "Siyang Liu <liusiyang.lucia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3254,
    "package_name": "IDPSurvival",
    "title": "Imprecise Dirichlet Process for Survival Analysis",
    "description": "Functions to perform robust\n\t\t nonparametric survival analysis with right censored \n\t\t data using a prior near-ignorant Dirichlet Process.\n\t\t Mangili, F., Benavoli, A., de Campos, C.P., Zaffalon, M. (2015) <doi:10.1002/bimj.201500062>.",
    "version": "1.2.2",
    "maintainer": "Francesca Mangili <francesca@idsia.ch>",
    "url": "https://ipg.idsia.ch/software.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3268,
    "package_name": "IETD",
    "title": "Inter-Event Time Definition",
    "description": "Computes characteristics of independent rainfall events (duration, total rainfall depth, and intensity) \n  extracted from a sub-daily rainfall time series based on the inter-event time definition (IETD) method. To have a \n  reference value of IETD, it also analyzes/computes IETD values through three methods: autocorrelation analysis, the \n  average annual number of events analysis, and coefficient of variation analysis. Ideal for analyzing the sensitivity \n  of IETD to characteristics of independent rainfall events.\n  Adams B, Papa F (2000) <ISBN: 978-0-471-33217-6>.\n  Joo J et al. (2014) <doi:10.3390/w6010045>.\n  Restrepo-Posada P, Eagleson P (1982) <doi:10.1016/0022-1694(82)90136-6>.",
    "version": "1.0.0",
    "maintainer": "Luis F. Duque <lfduquey@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3281,
    "package_name": "IIVpredictor",
    "title": "Modeling Within Individual Variability as Predictor",
    "description": "Time parceling method and Bayesian variability modeling methods for modeling within individual variability indicators as predictors.For more details, see <https://github.com/xliu12/IIVpredicitor>.  ",
    "version": "0.1.0",
    "maintainer": "Xiao Liu <xliu19@nd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3304,
    "package_name": "INLAjoint",
    "title": "Multivariate Joint Modeling for Longitudinal and Time-to-Event\nOutcomes with 'INLA'",
    "description": "Estimation of joint models for multivariate longitudinal markers (with various distributions available) and survival outcomes (possibly accounting for competing risks) with Integrated Nested Laplace Approximations (INLA). The flexible and user friendly function joint() facilitates the use of the fast and reliable inference technique implemented in the 'INLA' package for joint modeling. More details are given in the help page of the joint() function (accessible via ?joint in the R console) and the vignette associated to the joint() function (accessible via vignette(\"INLAjoint\") in the R console).",
    "version": "25.11.10",
    "maintainer": "Denis Rustand <INLAjoint@gmail.com>",
    "url": "https://github.com/DenisRustand/INLAjoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3309,
    "package_name": "INQC",
    "title": "Quality Control of Climatological Daily Time Series",
    "description": "Collection of functions for quality control (QC) of climatological daily time series (e.g. the ECA&D station data).",
    "version": "2.0.5",
    "maintainer": "Enric Aguilar <enric.aguilar@urv.cat>",
    "url": "https://github.com/INDECIS-Project/INQC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3315,
    "package_name": "IOLS",
    "title": "Iterated Ordinary Least Squares Regression",
    "description": "Addresses the 'log of zero' by developing a new family of \n    estimators called iterated Ordinary Least Squares. \n    This family nests standard approaches such as log-linear and \n    Poisson regressions, offers several computational advantages, \n    and corresponds to the correct way to perform the popular \n    log(Y + 1) transformation. For more details about how to use it, \n    see the notebook at: <https://www.davidbenatia.com/>.",
    "version": "0.1.4",
    "maintainer": "Nassim Zbalah <nas66.nz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3318,
    "package_name": "IPCWK",
    "title": "Kendall's Tau Partial Corr. for Survival Trait and Biomarkers",
    "description": "We propose the inverse probability-of-censoring weighted (IPCW) Kendall's tau to measure the association of the survival trait with biomarkers and Kendall's partial correlation to reflect the relationship of the survival trait with interaction variable conditional on main effects, as described in Wang and Chen (2020) <doi:10.1093/bioinformatics/btaa017>. ",
    "version": "1.0",
    "maintainer": "Jie-Huei Wang <jhwang@mail.fcu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3320,
    "package_name": "IPEC",
    "title": "Root Mean Square Curvature Calculation",
    "description": "Calculates the RMS intrinsic and parameter-effects curvatures of a nonlinear regression model. The curvatures are global measures of assessing whether a model/data set combination is close-to-linear or not. See Bates and Watts (1980) <doi:10.1002/9780470316757> and Ratkowsky and Reddy (2017) <doi:10.1093/aesa/saw098> for details.  ",
    "version": "1.1.2",
    "maintainer": "Peijian Shi <pjshi@njfu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3359,
    "package_name": "ISR",
    "title": "The Iterated Score Regression-Based Estimation",
    "description": "We use the  ISR to handle with PCA-based missing data with high  correlation,  and the DISR to handle with distributed PCA-based missing data.  The philosophy of the package is described in Guo G. (2024)  <doi:10.1080/03610918.2022.2091779>. ",
    "version": "2025.5.16",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3387,
    "package_name": "ImpShrinkage",
    "title": "Improved Shrinkage Estimations for Multiple Linear Regression",
    "description": "A variety of improved shrinkage estimators in the area of statistical analysis: unrestricted; restricted; preliminary test; improved preliminary test; Stein; and positive-rule Stein. More details can be found in chapter 7 of Saleh, A. K. Md. E. (2006) <ISBN: 978-0-471-56375-4>.",
    "version": "1.0.0",
    "maintainer": "Mina Norouzirad <mina.norouzirad@gmail.com>",
    "url": "https://github.com/mnrzrad/ImpShrinkage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3390,
    "package_name": "ImportanceIndice",
    "title": "Analyzing Data Through of Percentage of Importance Indice and\nIts Derivations",
    "description": "The Percentage of Importance Indice (Percentage_I.I.) bases in magnitudes, frequencies, and distributions of occurrence of an event (DEMOLIN-LEITE, 2021) <http://cjascience.com/index.php/CJAS/article/view/1009/1350>. This index can detect the key loss sources (L.S) and solution sources (S.S.), classifying them according to their importance in terms of loss or income gain, on the productive system. The Percentage_I.I. = [(ks1 x c1 x ds1)/SUM (ks1 x c1 x ds1) + (ks2 x c2 x ds2) + (ksn x cn x dsn)] x 100. key source (ks) is obtained using simple regression analysis and magnitude (abundance). Constancy (c) is SUM of occurrence of L.S. or S.S. on the samples (absence = 0 or presence = 1), and distribution source (ds) is obtained using chi-square test. This index has derivations: i.e., i) Loss estimates and solutions effectiveness and ii) Attention and non-attention levels (DEMOLIN-LEITE,2024) <DOI: 10.1590/1519-6984.253215>.",
    "version": "0.0.2",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3397,
    "package_name": "IncDTW",
    "title": "Incremental Calculation of Dynamic Time Warping",
    "description": "The Dynamic Time Warping (DTW) distance measure for time series allows non-linear alignments of time series to match  similar patterns in time series of different lengths and or different speeds. IncDTW is characterized by (1) the incremental calculation of DTW (reduces runtime complexity to a linear level for updating the DTW distance) - especially for life data streams or subsequence matching, (2) the vector based implementation of DTW which is faster because no matrices are allocated (reduces the space complexity from a quadratic to a linear level in the number of observations) - for all runtime intensive DTW computations, (3) the subsequence matching algorithm runDTW, that efficiently finds the k-NN to a query pattern in a long time series, and (4) C++ in the heart. For details about DTW see the original paper \"Dynamic programming algorithm optimization for spoken word recognition\" by Sakoe and Chiba (1978) <DOI:10.1109/TASSP.1978.1163055>. For details about this package, Dynamic Time Warping and Incremental Dynamic Time Warping please see \"IncDTW: An R Package for Incremental Calculation of Dynamic Time Warping\" by Leodolter et al. (2021) <doi:10.18637/jss.v099.i09>.",
    "version": "1.1.4.6",
    "maintainer": "Maximilian Leodolter <maximilian.leodolter@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3400,
    "package_name": "IndGenErrors",
    "title": "Tests of Independence Between Innovations of Generalized Error\nModels",
    "description": "Computation of test statistics of independence between  (continuous) innovations of time series. They can be used with stochastic volatility models and Hidden Markov Models (HMM). This improves the results in Duchesne, Ghoudi & Remillard (2012) <doi:10.1002/cjs.11141>.",
    "version": "0.1.6",
    "maintainer": "Bruno N Remillard <bruno.remillard@hec.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3403,
    "package_name": "IndexConstruction",
    "title": "Index Construction for Time Series Data",
    "description": "Derivation of indexes for benchmarking purposes. A methodology with flexible number of constituents is implemented. Also functions for market capitalization and volume weighted indexes with fixed number of constituents are available. The main function of the package, indexComp(), provides the derived index, suitable for analysis purposes. The functions indexUpdate(), indexMemberSelection() and indexMembersUpdate() are components of indexComp() and enable one to construct and continuously update an index, e.g. for display on a website. The methodology behind the functions provided gets introduced in Trimborn and Haerdle (2018) <doi:10.1016/j.jempfin.2018.08.004>.",
    "version": "0.1-3",
    "maintainer": "Simon Trimborn <trimborn.econometrics@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3411,
    "package_name": "InferenceSMR",
    "title": "Inference About the Standardized Mortality Ratio when Evaluating\nthe Effect of a Screening Program on Survival",
    "description": "Functions to make inference about the \n        standardized mortality ratio (SMR) when evaluating the\n        effect of a screening program. The package is\n        based on methods described in Sasieni (2003) \n        <doi: 10.1097/00001648-200301000-00026> and \n        Talbot et al. (2011) <doi: 10.1002/sim.4334>.",
    "version": "1.0.2",
    "maintainer": "Denis Talbot <denis.talbot@fmed.ulaval.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3417,
    "package_name": "InformativeCensoring",
    "title": "Multiple Imputation for Informative Censoring",
    "description": "Multiple Imputation for Informative Censoring.\n    This package implements two methods. Gamma Imputation\n    described in <DOI:10.1002/sim.6274> and Risk Score Imputation\n    described in <DOI:10.1002/sim.3480>.",
    "version": "0.3.6",
    "maintainer": "Jonathan Bartlett <jonathan.bartlett1@lshtm.ac.uk>",
    "url": "https://github.com/jwb133/InformativeCensoring",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3422,
    "package_name": "InspectChangepoint",
    "title": "High-Dimensional Changepoint Estimation via Sparse Projection",
    "description": "Provides a data-driven projection-based method for estimating changepoints in high-dimensional time series. Multiple changepoints are estimated using a (wild) binary segmentation scheme.",
    "version": "1.2",
    "maintainer": "Tengyao Wang <t.wang59@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3436,
    "package_name": "InteractionPoweR",
    "title": "Power Analyses for Interaction Effects in Cross-Sectional\nRegressions",
    "description": "Power analysis for regression models which test the interaction of\n    two or three independent variables on a single dependent variable. Includes options \n    for correlated interacting variables and specifying variable reliability. \n    Two-way interactions can include continuous, binary, or ordinal variables.\n    Power analyses can be done either analytically or via simulation.  Includes \n    tools for simulating single data sets and visualizing power analysis results.\n    The primary functions are power_interaction_r2() and power_interaction() for two-way\n    interactions, and power_interaction_3way_r2() for three-way interactions. \n    Please cite as: Baranger DAA, Finsaas MC, Goldstein BL, Vize CE, Lynam DR,\n    Olino TM (2023). \"Tutorial: Power analyses for interaction effects in \n    cross-sectional regressions.\" <doi:10.1177/25152459231187531>. ",
    "version": "0.2.2",
    "maintainer": "David Baranger <dbaranger@gmail.com>",
    "url": "https://dbaranger.github.io/InteractionPoweR/,\nhttps://doi.org/10.1177/25152459231187531",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3446,
    "package_name": "InvStablePrior",
    "title": "Inverse Stable Prior for Widely-Used Exponential Models",
    "description": "Contains functions that allow Bayesian inference on a parameter of some widely-used exponential models. The functions can generate independent samples from the closed-form posterior distribution using the inverse stable prior. Inverse stable is a non-conjugate prior for a parameter of an exponential subclass of discrete and continuous data distributions (e.g. Poisson, exponential, inverse gamma, double exponential (Laplace), half-normal/half-Gaussian, etc.). The prior class provides flexibility in capturing a wide array of prior beliefs (right-skewed and left-skewed) as modulated by a parameter that is bounded in (0,1). The generated samples can be used to simulate the prior and posterior predictive distributions. More details can be found in Cahoy and Sedransk (2019)  <doi:10.1007/s42519-018-0027-2>. The package can also be used as a teaching demo for introductory Bayesian courses.",
    "version": "0.1.1",
    "maintainer": "Dexter Cahoy <dexter.cahoy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3447,
    "package_name": "InvariantCausalPrediction",
    "title": "Invariant Causal Prediction",
    "description": "Confidence intervals for causal effects, using data collected in different experimental or environmental conditions. Hidden variables can be included in the model with a more experimental version. ",
    "version": "0.8",
    "maintainer": "Nicolai Meinshausen <meinshausen@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3456,
    "package_name": "Iso",
    "title": "Functions to Perform Isotonic Regression",
    "description": "Linear order and unimodal order (univariate)\n\t     isotonic regression; bivariate isotonic regression\n\t     with linear order on both variables.",
    "version": "0.0-21",
    "maintainer": "Rolf Turner <rolfturner@posteo.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3469,
    "package_name": "IterativeHardThresholding",
    "title": "Iterative Hard Thresholding Extensions to Cyclops",
    "description": "Fits large-scale regression models with a penalty that \n  restricts the maximum number of non-zero regression coefficients\n  to a prespecified value.  While Chu et al (2020) <doi:10.1093/gigascience/giaa044>\n  describe the basic algorithm, this package uses Cyclops for an efficient implementation.",
    "version": "1.0.3",
    "maintainer": "Marc A. Suchard <msuchard@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3477,
    "package_name": "JFE",
    "title": "Tools for Analyzing Time Series Data of Just Finance and\nEconometrics",
    "description": "Offer procedures to download financial-economic time series data and enhanced procedures for computing the investment performance indices of Bacon (2004) <DOI:10.1002/9781119206309>.",
    "version": "2.5.11",
    "maintainer": "Ho Tsung-wu <tsungwu@ntnu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3480,
    "package_name": "JLPM",
    "title": "Joint Latent Process Models",
    "description": "Estimation of extended joint models with shared random effects. Longitudinal data are handled in latent process models for continuous (Gaussian or curvilinear) and ordinal outcomes while proportional hazard models are used for the survival part. We propose a frequentist approach using maximum likelihood estimation. See Saulnier et al, 2022 <doi:10.1016/j.ymeth.2022.03.003>.",
    "version": "1.0.2",
    "maintainer": "Viviane Philipps <Viviane.Philipps@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3481,
    "package_name": "JM",
    "title": "Joint Modeling of Longitudinal and Survival Data",
    "description": "Shared parameter models for the joint modeling of longitudinal and time-to-event data. ",
    "version": "1.5-2",
    "maintainer": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "url": "http://jmr.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3482,
    "package_name": "JMH",
    "title": "Joint Model of Heterogeneous Repeated Measures and Survival Data",
    "description": "Maximum likelihood estimation for the semi-parametric joint modeling of competing risks and longitudinal data in the presence of heterogeneous within-subject variability, proposed by Li and colleagues (2023) <arXiv:2301.06584>.\n             The proposed method models the within-subject variability of the biomarker and associates it with the risk of the competing risks event. The time-to-event data is modeled using a (cause-specific) Cox proportional hazards regression model with time-fixed covariates. \n             The longitudinal outcome is modeled using a mixed-effects location and scale model. The association is captured by shared random effects. The model \n             is estimated using an Expectation Maximization algorithm.",
    "version": "1.0.3",
    "maintainer": "Shanpeng Li <lishanpeng0913@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3485,
    "package_name": "JMbayes2",
    "title": "Extended Joint Models for Longitudinal and Time-to-Event Data",
    "description": "Fit joint models for longitudinal and time-to-event data under the Bayesian approach. Multiple longitudinal outcomes of mixed type (continuous/categorical) and multiple event times (competing risks and multi-state processes) are accommodated. Rizopoulos (2012, ISBN:9781439872864).",
    "version": "0.5-7",
    "maintainer": "Dimitris Rizopoulos <d.rizopoulos@erasmusmc.nl>",
    "url": "https://drizopoulos.github.io/JMbayes2/,\nhttps://github.com/drizopoulos/JMbayes2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3486,
    "package_name": "JMbdirect",
    "title": "Joint Model for Longitudinal and Multiple Time to Events Data",
    "description": "Provides model fitting, prediction, and plotting for joint models of longitudinal and multiple time-to-event data, including methods from Rizopoulos (2012) <doi:10.1201/b12208>. Useful for handling complex survival and longitudinal data in clinical research.",
    "version": "0.1.0",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3487,
    "package_name": "JMdesign",
    "title": "Joint Modeling of Longitudinal and Survival Data - Power\nCalculation",
    "description": "Performs power calculations for joint modeling of longitudinal \n  and survival data with k-th order trajectories when the variance-covariance \n  matrix, Sigma_theta, is unknown.",
    "version": "1.6",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3492,
    "package_name": "JPSurv",
    "title": "Joinpoint Model for Relative and Cause-Specific Survival",
    "description": "Contains functions for fitting a joinpoint proportional hazards model to relative survival or cause-specific survival data, including estimates of joinpoint years at which survival trends have changed and trend measures in the hazard and cumulative survival scale. See Yu et al.(2009) <doi:10.1111/j.1467-985X.2009.00580.x>.",
    "version": "3.0.20",
    "maintainer": "Bill Wheeler <wheelerb@imsweb.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3518,
    "package_name": "JuliaFormulae",
    "title": "Translate R Regression Model Formulae to 'Julia' Syntax",
    "description": "Metaprogramming utilities for converting R regression model\n    formulae to equivalents in 'Julia' <doi:10.1137/141000671>, via\n    modifications to the abstract syntax tree. Supports translations in\n    zero correlation random effects syntax, protection of expressions to\n    be evaluated as-is, interaction terms, and more. Accepts strings or R\n    formula objects and returns modified R formula objects where\n    possible (or a modified string, if not a valid formula in R).",
    "version": "0.1.0",
    "maintainer": "June Choe <jchoe001@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3525,
    "package_name": "KDEmcmc",
    "title": "Kernel Density Estimation with a Markov Chain Monte Carlo Sample",
    "description": "Provides methods for selecting the optimal bandwidth in kernel density estimation for dependent samples, such as those generated by Markov chain Monte Carlo (MCMC). Implements a modified biased cross-validation (mBCV) approach that accounts for sample dependence, improving the accuracy of estimated density functions.",
    "version": "0.0.2",
    "maintainer": "Juhee Lee <ljh988488@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3530,
    "package_name": "KERE",
    "title": "Expectile Regression in Reproducing Kernel Hilbert Space",
    "description": "An efficient algorithm inspired by majorization-minimization principle for solving the entire solution path of a flexible nonparametric expectile regression estimator constructed in a reproducing kernel Hilbert space.",
    "version": "1.0.0",
    "maintainer": "Yi Yang <yiyang@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3534,
    "package_name": "KGode",
    "title": "Kernel Based Gradient Matching for Parameter Inference in\nOrdinary Differential Equations",
    "description": "The kernel ridge regression and the gradient matching algorithm proposed in Niu et al. (2016) <https://proceedings.mlr.press/v48/niu16.html> and the warping algorithm proposed in Niu et al. (2017) <DOI:10.1007/s00180-017-0753-z> are implemented for parameter inference in differential equations. Four schemes are provided for improving parameter estimation in odes by using the odes regularisation and warping.",
    "version": "1.0.5",
    "maintainer": "Mu Niu <mu.niu@glasgow.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3542,
    "package_name": "KMsurv",
    "title": "Datasets from Klein and Moeschberger (1997), Survival Analysis",
    "description": "Datasets and functions for Klein and Moeschberger (1997),\n        \"Survival Analysis, Techniques for Censored and Truncated\n        Data\", Springer.",
    "version": "0.1-6",
    "maintainer": "Jun Yan <jun.yan@uconn.edu>",
    "url": "https://github.com/jun-yan/KMsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3545,
    "package_name": "KOFM",
    "title": "Test the Kronecker Product Structure in Tensor Factor Models",
    "description": "To test if a tensor time series following a Tucker-decomposition factor model has a Kronecker product structure. Supplementary functions for tensor reshape and its reversal are also included.",
    "version": "0.1.1",
    "maintainer": "Zetai Cen <z.cen@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3548,
    "package_name": "KONPsurv",
    "title": "KONP Tests: Powerful K-Sample Tests for Right-Censored Data",
    "description": "The K-sample omnibus non-proportional hazards (KONP) tests are powerful non-parametric tests for comparing K (>=2) hazard functions based on right-censored data (Gorfine, Schlesinger and Hsu, 2020, <doi:10.1177/0962280220907355>). These tests are consistent against any differences between the hazard functions of the groups. The KONP tests are often more powerful than other existing tests, especially under non-proportional hazard functions.",
    "version": "1.0.4",
    "maintainer": "Matan Schlesinger <matan.schles@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3554,
    "package_name": "KRMM",
    "title": "Kernel Ridge Mixed Model",
    "description": "Solves kernel ridge regression, within the the mixed model framework, for the linear, polynomial, Gaussian, Laplacian and ANOVA kernels. The model components (i.e. fixed and random effects) and variance parameters are estimated using the expectation-maximization (EM) algorithm. All the estimated components and parameters, e.g. BLUP of dual variables and BLUP of random predictor effects for the linear kernel (also known as RR-BLUP), are available. The kernel ridge mixed model (KRMM) is described in Jacquin L, Cao T-V and Ahmadi N (2016) A Unified and Comprehensible View of Parametric and Kernel Methods for Genomic Prediction with Application to Rice. Front. Genet. 7:145. <doi:10.3389/fgene.2016.00145>.",
    "version": "1.0",
    "maintainer": "Laval Jacquin <jacquin.julien@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3566,
    "package_name": "KarsTS",
    "title": "An Interface for Microclimate Time Series Analysis",
    "description": "An R code with a GUI for microclimate time series, with an emphasis on underground environments. 'KarsTS' provides linear and nonlinear methods, including recurrence analysis (Marwan et al. (2007) <doi:10.1016/j.physrep.2006.11.001>) and filling methods (Moffat et al. (2007) <doi:10.1016/j.agrformet.2007.08.011>), as well as tools to manipulate easily time series and gap sets.",
    "version": "2.4.1",
    "maintainer": "Marina Saez <marinasaez_andreu@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3571,
    "package_name": "Keng",
    "title": "Knock Errors Off Nice Guesses",
    "description": "Miscellaneous functions and data used in psychological research and teaching. Keng \n    currently has a built-in dataset depress, and could (1) scale a vector; (2) compute the cut-off \n    values of Pearson's r with known sample size; (3) test the significance and compute the post-hoc\n    power for Pearson's r with known sample size; (4) conduct a priori power analysis and plan the \n    sample size for Pearson's r; (5) compare lm()'s fitted outputs using R-squared, f_squared, \n    post-hoc power, and PRE (Proportional Reduction in Error, also called partial R-squared or \n    partial Eta-squared); (6) calculate PRE from partial correlation, Cohen's f, or f_squared; \n    (7) conduct a priori power analysis and plan the sample size for one or a set of predictors in \n    regression analysis; (8) conduct post-hoc power analysis for one or a set of predictors in \n    regression analysis with known sample size; (9) randomly pick numbers for Chinese Super Lotto\n    and Double Color Balls; (10) assess course objective achievement in Outcome-Based Education.",
    "version": "2025.10.8",
    "maintainer": "Qingyao Zhang <qingyaozhang@outlook.com>",
    "url": "https://github.com/qyaozh/Keng",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3575,
    "package_name": "Kernelheaping",
    "title": "Kernel Density Estimation for Heaped and Rounded Data",
    "description": "In self-reported or anonymised data the user often encounters\n    heaped data, i.e. data which are rounded (to a possibly different degree\n    of coarseness). While this is mostly a minor problem in parametric density\n    estimation the bias can be very large for non-parametric methods such as kernel\n    density estimation. This package implements a partly Bayesian algorithm treating\n    the true unknown values as additional parameters and estimates the rounding\n    parameters to give a corrected kernel density estimate. It supports various\n    standard bandwidth selection methods. Varying rounding probabilities (depending\n    on the true value) and asymmetric rounding is estimable as well: Gross, M. and Rendtel, U. (2016) (<doi:10.1093/jssam/smw011>).\n    Additionally, bivariate non-parametric density estimation for rounded data, Gross, M. et al. (2016) (<doi:10.1111/rssa.12179>),\n    as well as data aggregated on areas is supported.",
    "version": "2.3.0",
    "maintainer": "Marcus Gross <marcus.gross@inwt-statistics.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3576,
    "package_name": "Keyboard",
    "title": "Bayesian Designs for Early Phase Clinical Trials",
    "description": "We developed a package 'Keyboard' for designing single-agent, drug-combination, or phase I/II dose-finding clinical trials. The 'Keyboard' designs are novel early phase trial designs that can be implemented simply and transparently, similar to the 3+3 design, but yield excellent performance, comparable to those of more-complicated, model-based designs (Yan F, Mandrekar SJ, Yuan Y (2017) <doi:10.1158/1078-0432.CCR-17-0220>, Li DH, Whitmore JB, Guo W, Ji Y. (2017) <doi:10.1158/1078-0432.CCR-16-1125>,  Liu S, Johnson VE (2016) <doi:10.1093/biostatistics/kxv040>, Zhou Y, Lee JJ, Yuan Y (2019) <doi:10.1002/sim.8475>, Pan H, Lin R, Yuan Y (2020) <doi:10.1016/j.cct.2020.105972>). The 'Keyboard' package provides tools for designing, conducting, and analyzing single-agent, drug-combination, and phase I/II dose-finding clinical trials. For more details about how to use this packge, please refer to Li C, Sun H, Cheng C, Tang L, and Pan H. (2022) \"A software tool for both the maximum tolerated dose and the optimal biological dose finding trials in early phase designs\". Manuscript submitted for publication.",
    "version": "0.1.3",
    "maintainer": "Xiaomeng Yuan <xiaomeng.yuan@stjude.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3584,
    "package_name": "KnapsackSampling",
    "title": "Generate Feasible Samples of a Knapsack Problem",
    "description": "The sampl.mcmc function creates samples of the feasible region of a knapsack problem with both equalities and inequalities constraints.",
    "version": "0.1.1",
    "maintainer": "Chin Soon Lim <chinsoon12@hotmail.com>",
    "url": "https://github.com/chinsoon12/KnapsackSampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3602,
    "package_name": "L2E",
    "title": "Robust Structured Regression via the L2 Criterion",
    "description": "An implementation of a computational framework for performing robust structured regression with the L2 criterion \n             from Chi and Chi (2021+). Improvements using the majorization-minimization (MM) principle from Liu, Chi, and \n             Lange (2022+) added in Version 2.0.",
    "version": "2.0",
    "maintainer": "Jocelyn Chi <jocetchi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3610,
    "package_name": "LARF",
    "title": "Local Average Response Functions for Instrumental Variable\nEstimation of Treatment Effects",
    "description": "Provides instrumental variable estimation of treatment effects when both the endogenous treatment and its instrument are binary. Applicable to both binary and continuous outcomes.",
    "version": "1.4",
    "maintainer": "Weihua An <weihuaan@indiana.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3613,
    "package_name": "LAWBL",
    "title": "Latent (Variable) Analysis with Bayesian Learning",
    "description": "A variety of models to analyze latent variables based on Bayesian learning: the partially CFA (Chen, Guo, Zhang, & Pan, 2020) <DOI: 10.1037/met0000293>; generalized PCFA; partially confirmatory IRM (Chen, 2020) <DOI: 10.1007/s11336-020-09724-3>; Bayesian regularized EFA <DOI: 10.1080/10705511.2020.1854763>; Fully and partially EFA.",
    "version": "1.5.0",
    "maintainer": "Jinsong Chen <jinsong.chen@live.com>",
    "url": "https://github.com/Jinsong-Chen/LAWBL,\nhttps://jinsong-chen.github.io/LAWBL/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3618,
    "package_name": "LBPG",
    "title": "The Length-Biased Power Garima Distribution",
    "description": "The Length-Biased Power Garima distribution for computes the probability density,\n    the cumulative density distribution and the quantile function of the distribution, \n    and generates sample values with random variables based on Kittipong and Sirinapa(2021)<DOI: 10.14456/sjst-psu.2021.89>.",
    "version": "0.1.2",
    "maintainer": "Kittipong Klinjan <kittipong_k@rmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3621,
    "package_name": "LCAextend",
    "title": "Latent Class Analysis (LCA) with Familial Dependence in Extended\nPedigrees",
    "description": "Latent Class Analysis of\n        phenotypic measurements in pedigrees and model selection\n        based on one of two methods: likelihood-based cross-validation\n        and Bayesian Information Criterion. Computation of individual\n        and triplet child-parents weights in a pedigree is performed using an\n        upward-downward algorithm. The model takes into account the familial\n        dependence defined by the pedigree structure by considering\n        that a class of a child depends on his parents classes via\n        triplet-transition probabilities of the classes. The package\n        handles the case where measurements are available on all\n        subjects and the case where measurements are available only on\n        symptomatic (i.e. affected) subjects. Distributions for\n        discrete (or ordinal) and continuous data are currently\n        implemented. The package can deal with missing data.",
    "version": "1.3",
    "maintainer": "Alexandre BUREAU <alexandre.bureau@msp.ulaval.ca>",
    "url": "https://CRAN.R-project.org/package=LCAextend",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3625,
    "package_name": "LCMCR",
    "title": "Bayesian Non-Parametric Latent-Class Capture-Recapture",
    "description": "Bayesian population size estimation using non parametric latent-class models.",
    "version": "0.4.14",
    "maintainer": "Daniel Manrique-Vallier <dmanriqu@indiana.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3627,
    "package_name": "LCTMtools",
    "title": "Latent Class Trajectory Models: Tools for checking adequacy",
    "description": "A selection of model adequacy tests for Latent Class Trajectory Models (LCTMs)",
    "version": "0.1.3",
    "maintainer": "",
    "url": "https://github.com/hlennon/LCTMtools",
    "exports": [],
    "topics": ["cluster-analysis", "growth-curves", "latent-class", "longitudinal-data", "r", "software-development", "statistical-models", "statistics", "trajectories", "trajectory-models", "unsupervised-learning", "vignette"],
    "score": "NA",
    "stars": 39
  },
  {
    "id": 3628,
    "package_name": "LDATS",
    "title": "Latent Dirichlet Allocation Coupled with Time Series Analyses",
    "description": "Combines Latent Dirichlet Allocation (LDA) and Bayesian multinomial time series methods in a two-stage analysis to quantify dynamics in high-dimensional temporal data. LDA decomposes multivariate data into lower-dimension latent groupings, whose relative proportions are modeled using generalized Bayesian time series models that include abrupt changepoints and smooth dynamics. The methods are described in Blei et al. (2003) <doi:10.1162/jmlr.2003.3.4-5.993>, Western and Kleykamp (2004) <doi:10.1093/pan/mph023>, Venables and Ripley (2002, ISBN-13:978-0387954578), and Christensen et al. (2018) <doi:10.1002/ecy.2373>.",
    "version": "0.3.0",
    "maintainer": "Juniper L. Simonis <juniper.simonis@weecology.org>",
    "url": "https://weecology.github.io/LDATS/,\nhttps://github.com/weecology/LDATS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3656,
    "package_name": "LLMAgentR",
    "title": "Language Model Agents in R for AI Workflows and Research",
    "description": "Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.",
    "version": "0.3.0",
    "maintainer": "Kwadwo Daddy Nyame Owusu Boakye <kwadwo.owusuboakye@outlook.com>",
    "url": "https://github.com/knowusuboaky/LLMAgentR,\nhttps://knowusuboaky.github.io/LLMAgentR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3665,
    "package_name": "LMMstar",
    "title": "Repeated Measurement Models for Discrete Times",
    "description": "Companion R package for the course \"Statistical analysis of correlated and repeated measurements for health science researchers\"\n\t     taught by the section of Biostatistics of the University of Copenhagen.\n\t     It implements linear mixed models where the model for the variance-covariance of the residuals is specified via patterns (compound symmetry, toeplitz, unstructured, ...).\n\t     Statistical inference for mean, variance, and correlation parameters is performed based on the observed information and a Satterthwaite approximation of the degrees of freedom.\n\t     Normalized residuals are provided to assess model misspecification.\n\t     Statistical inference can be performed for arbitrary linear or non-linear combination(s) of model coefficients.\n\t     Predictions can be computed conditional to covariates only or also to outcome values. ",
    "version": "1.1.0",
    "maintainer": "Brice Ozenne <brice.mh.ozenne@gmail.com>",
    "url": "https://github.com/bozenne/LMMstar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3666,
    "package_name": "LMN",
    "title": "Inference for Linear Models with Nuisance Parameters",
    "description": "Efficient Frequentist profiling and Bayesian marginalization of parameters for which the conditional likelihood is that of a multivariate linear regression model.  Arbitrary inter-observation error correlations are supported, with optimized calculations provided for independent-heteroskedastic and stationary dependence structures.",
    "version": "1.1.3",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "https://github.com/mlysy/LMN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3682,
    "package_name": "LPDynR",
    "title": "Land Productivity Dynamics Indicator",
    "description": "It uses 'phenological' and productivity-related variables derived from time series of vegetation \n    indexes, such as the Normalized Difference Vegetation Index, to assess ecosystem dynamics and change, which \n    eventually might drive to land degradation. The final result of the Land Productivity Dynamics indicator \n    is a categorical map with 5 classes of land productivity dynamics, ranging from declining to increasing \n    productivity. See www.sciencedirect.com/science/article/pii/S1470160X21010517/ for a description \n    of the methods used in the package to calculate the indicator.",
    "version": "1.0.5",
    "maintainer": "Xavier Rotllan-Puig <xavier.rotllan.puig@aster-projects.cat>",
    "url": "https://github.com/xavi-rp/LPDynR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3688,
    "package_name": "LPWC",
    "title": "Lag Penalized Weighted Correlation for Time Series Clustering",
    "description": "Computes a time series distance measure for clustering based on weighted correlation and introduction of lags. The lags capture delayed responses in a time series dataset. The timepoints must be specified. T. Chandereng, A. Gitter (2020) <doi:10.1186/s12859-019-3324-1>.",
    "version": "1.0.0",
    "maintainer": "",
    "url": "https://github.com/gitter-lab/LPWC",
    "exports": [],
    "topics": ["bioinformatics", "clustering", "time-series"],
    "score": "NA",
    "stars": 21
  },
  {
    "id": 3708,
    "package_name": "LSEbootLS",
    "title": "Bootstrap Methods for Regression Models with Locally Stationary\nErrors",
    "description": "Implements bootstrap methods for linear regression models with errors following a time-varying process, focusing on approximating the distribution of the least-squares estimator for regression models with locally stationary errors. It enables the construction of bootstrap and classical confidence intervals for regression coefficients, leveraging intensive simulation studies and real data analysis.",
    "version": "0.1.0",
    "maintainer": "Nicolas Loyola <nloyola2016@udec.cl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3716,
    "package_name": "LSTS",
    "title": "Locally Stationary Time Series",
    "description": "A set of functions that allow stationary analysis and locally stationary time series analysis.",
    "version": "2.1",
    "maintainer": "Mauricio Vargas <mavargas11@uc.cl>",
    "url": "https://pacha.dev/LSTS/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3719,
    "package_name": "LSWPlib",
    "title": "Simulation and Spectral Estimation of Locally Stationary Wavelet\nPacket Processes",
    "description": "Library of functions for the statistical analysis and simulation of Locally Stationary Wavelet Packet (LSWP) processes.  The methods implemented by this library are described in Cardinali and Nason (2017) <doi:10.1111/jtsa.12230>.",
    "version": "0.1.0",
    "maintainer": "Alessandro Cardinali <alessandro.cardinali@plymouth.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3726,
    "package_name": "LTRCtrees",
    "title": "Survival Trees to Fit Left-Truncated and Right-Censored and\nInterval-Censored Survival Data",
    "description": "Recursive partition algorithms designed for fitting survival trees with left-truncated and right-censored (LTRC) data, as well as interval-censored data.\n    The LTRC trees can also be used to fit survival trees with time-varying covariates.",
    "version": "1.1.2",
    "maintainer": "Wenbo Jing <wj2093@stern.nyu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3736,
    "package_name": "LadderFuelsR",
    "title": "Automated Tool for Vertical Fuel Continuity Analysis using\nAirborne Laser Scanning Data",
    "description": "Set of tools for analyzing vertical fuel continuity at the tree level using Airborne Laser Scanning data. The workflow consisted of: 1) calculating the vertical height profiles of each segmented tree; 2) identifying gaps and fuel layers; 3) estimating the distance between fuel layers; and 4) retrieving the fuel layers base height and depth. Additionally, other functions recalculate previous metrics after considering distances greater than certain threshold. Moreover, the package calculates: i) the percentage of Leaf Area Density comprised in each fuel layer, ii) remove fuel layers with Leaf Area Density (LAD) percentage less than 10, and iii) recalculate the distances among the reminder ones. On the other hand, it identifies the crown base height (CBH) based on different criteria: the fuel layer with the highest LAD percentage and the fuel layers located at the largest- and at the last-distance. When there is only one fuel layer, it also identifies the CBH performing a segmented linear regression (breaking points) on the cumulative sum of LAD as a function of height. Finally, a collection of plotting functions is developed to represent: i) the initial gaps and fuel layers; ii) the fuels base height, depths and gaps with distances greater than certain threshold and, iii) the CBH based on different criteria. The methods implemented in this package are original and have not been published elsewhere.",
    "version": "0.0.7",
    "maintainer": "Olga Viedma <olga.viedma@uclm.es>",
    "url": "https://github.com/olgaviedma/LadderFuelsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3739,
    "package_name": "LakeMetabolizer",
    "title": "Tools for the Analysis of Ecosystem Metabolism",
    "description": "A collection of tools for the calculation of freewater metabolism\n    from in situ time series of dissolved oxygen, water temperature, and,\n    optionally, additional environmental variables. LakeMetabolizer implements\n    5 different metabolism models with diverse statistical underpinnings:\n    bookkeeping, ordinary least squares, maximum likelihood, Kalman filter,\n    and Bayesian. Each of these 5 metabolism models can be combined with\n    1 of 7 models for computing the coefficient of gas exchange across the\n    air–water interface (k). LakeMetabolizer also features a variety of\n    supporting functions that compute conversions and implement calculations\n    commonly applied to raw data prior to estimating metabolism (e.g., oxygen\n    saturation and optical conversion models).",
    "version": "1.5.6",
    "maintainer": "Jacob Zwart <jayzlimno@gmail.com>",
    "url": "https://www.tandfonline.com/doi/abs/10.1080/IW-6.4.883",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3742,
    "package_name": "Landmarking",
    "title": "Analysis using Landmark Models",
    "description": "The landmark approach allows survival predictions to be\n\tupdated dynamically as new measurements from an individual are recorded.\n\tThe idea is to set predefined time points, known as \"landmark times\",\n\tand form a model at each landmark time using only the individuals in the\n\trisk set. This package allows the longitudinal data to be modelled\n\teither using the last observation carried forward or linear mixed\n\teffects modelling. There is also the option to model competing risks,\n\teither through cause-specific Cox regression or Fine-Gray regression.\n\tTo find out more about the methods in this package, please see \n\t<https://isobelbarrott.github.io/Landmarking/articles/Landmarking>.",
    "version": "1.0.0",
    "maintainer": "Isobel Barrott <isobel.barrott@gmail.com>",
    "url": "https://github.com/isobelbarrott/Landmarking/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3743,
    "package_name": "Langevin",
    "title": "Langevin Analysis in One and Two Dimensions",
    "description": "Estimate drift and diffusion functions from time series and\n    generate synthetic time series from given drift and diffusion coefficients.",
    "version": "1.3.3",
    "maintainer": "Philip Rinn <philip.rinn@uni-oldenburg.de>",
    "url": "https://gitlab.uni-oldenburg.de/TWiSt/Langevin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3744,
    "package_name": "LaplacesDemon",
    "title": "Complete Environment for Bayesian Inference",
    "description": "Provides a complete environment for Bayesian inference using a variety of different samplers (see ?LaplacesDemon for an overview).",
    "version": "16.1.6",
    "maintainer": "Henrik Singmann <singmann+LaplacesDemon@gmail.com>",
    "url": "https://github.com/LaplacesDemonR/LaplacesDemon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3745,
    "package_name": "Largevars",
    "title": "Testing Large VARs for the Presence of Cointegration",
    "description": "Conducts a cointegration test for high-dimensional vector autoregressions (VARs) of order k based on the large N,T asymptotics of Bykhovskaya and Gorin, 2022 (<doi:10.48550/arXiv.2202.07150>). The implemented test is a modification of the Johansen likelihood ratio test. In the absence of cointegration the test converges to the partial sum of the Airy-1 point process. This package contains simulated quantiles of the first ten partial sums of the Airy-1 point process that are precise up to the first three digits.",
    "version": "1.0.3",
    "maintainer": "Eszter Kiss <ekiss2803@gmail.com>",
    "url": "https://github.com/eszter-kiss/Largevars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3747,
    "package_name": "LassoNet",
    "title": "3CoSE Algorithm",
    "description": "Contains functions to estimate a penalized regression model using 3CoSE algorithm, see Weber, Striaukas, Schumacher Binder (2018) <doi:10.2139/ssrn.3211163>.",
    "version": "0.8.3",
    "maintainer": "Jonas Striaukas <jonas.striaukas@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3748,
    "package_name": "LassoSIR",
    "title": "Sparsed Sliced Inverse Regression via Lasso",
    "description": "Estimate the sufficient dimension reduction space using sparsed sliced inverse regression via Lasso (Lasso-SIR) introduced in Lin, Zhao, and Liu (2019) <doi:10.1080/01621459.2018.1520115>. The Lasso-SIR is consistent and achieve the optimal convergence rate under certain sparsity conditions for the multiple index models.",
    "version": "1.0",
    "maintainer": "Zhigen Zhao <zhigen.zhao@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3760,
    "package_name": "LearnBayes",
    "title": "Functions for Learning Bayesian Inference",
    "description": "A collection of functions helpful in learning the basic tenets of Bayesian statistical inference.  It contains functions for summarizing basic one and two parameter posterior distributions and predictive distributions.  It contains MCMC algorithms for summarizing posterior distributions defined by the user.  It also contains functions for regression models, hierarchical models, Bayesian tests, and illustrations of Gibbs sampling.",
    "version": "2.15.1",
    "maintainer": "Jim Albert <albert@bgsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3766,
    "package_name": "LearnVizLMM",
    "title": "Learning and Communicating Linear Mixed Models Without Data",
    "description": "Summarizes characteristics of linear mixed effects models without \n    data or a fitted model by converting code for fitting lmer() from 'lme4' \n    and lme() from 'nlme' into tables, equations, and visuals. Outputs can be \n    used to learn how to fit linear mixed effects models in 'R' and to \n    communicate about these models in presentations, manuscripts, and analysis \n    plans. ",
    "version": "1.0.0",
    "maintainer": "Katherine Zavez <Katherine.Zavez@uconn.edu>",
    "url": "https://github.com/kzavez/LearnVizLMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3768,
    "package_name": "LearningStats",
    "title": "Elemental Descriptive and Inferential Statistics",
    "description": "Provides tools to teach students elemental statistics. The main topics covered are descriptive statistics, probability models (discrete and continuous variables) and statistical inference (confidence intervals and hypothesis tests). One of the main advantages of this package is that allows the user to read quite a variety of types of data files with one unique command. Moreover it includes shortcuts to simple but up-to-now not in R descriptive features such a complete frequency table or an histogram with the optimal number of intervals. Related to model distributions (both discrete and continuous), the package allows the student to easy plot the mass/density function, distribution function and quantile function just detailing as input arguments the known population parameters. The inference related tools are basically confidence interval and hypothesis testing. Having defined independent commands for these two tools makes it easier for the student to understand what the software is performing, and it also helps the student to have a better knowledge on which specific tool they need to use in each situation. Moreover, the hypothesis testing commands provide not only the numeric result on the screen but also a very intuitive graph (which includes the statistic distribution, the observed value of the statistic, the rejection area and the p-value) that is very useful for the student to visualise the process. The regression section includes up to now, a simple linear model, with one single command the student can obtain the numeric summary as well as the corresponding diagram with the adjusted regression model and a legend with basic information (formula of the adjusted model and R-squared).",
    "version": "0.1.0",
    "maintainer": "María Isabel Borrajo-García <mariaisabel.borrajo@usc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3772,
    "package_name": "LexisPlotR",
    "title": "Plot Lexis Diagrams for Demographic Purposes",
    "description": "Plots empty Lexis grids, adds lifelines and highlights certain areas of the grid, like cohorts and age groups.",
    "version": "0.4.0",
    "maintainer": "Philipp Ottolinger <philipp@ottolinger.de>",
    "url": "https://github.com/ottlngr/LexisPlotR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3788,
    "package_name": "LinCal",
    "title": "Static Univariate Frequentist and Bayesian Linear Calibration",
    "description": "Estimate and confidence/credible intervals for an unknown\n    regressor x0 given an observed y0.",
    "version": "1.0.1",
    "maintainer": "Derick L. Rivers <riversdl@alumni.vcu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3789,
    "package_name": "LinRegInteractive",
    "title": "Interactive Interpretation of Linear Regression Models",
    "description": "Interactive visualization of effects, response functions \n    and marginal effects for different kinds of regression models. In this version \n    linear regression models, generalized linear models, generalized additive\n    models and linear mixed-effects models are supported.  \n    Major features are the interactive approach and the handling of the effects of categorical covariates: \n    if two or more factors are used as covariates every combination of the levels of each \n    factor is treated separately. The automatic calculation of \n    marginal effects and a number of possibilities to customize the graphical output \n    are useful features as well.",
    "version": "0.3-4",
    "maintainer": "Martin Meermeyer <m.meermeyer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3792,
    "package_name": "LindleyPowerSeries",
    "title": "Lindley Power Series Distribution",
    "description": "Computes the probability density function, the cumulative distribution function, the hazard rate function, the quantile function and random generation for Lindley Power Series distributions, see Nadarajah and Si (2018) <doi:10.1007/s13171-018-0150-x>.",
    "version": "1.0.1",
    "maintainer": "Yuancheng Si <siyuanchengman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3793,
    "package_name": "LinearRegressionMDE",
    "title": "Minimum Distance Estimation in Linear Regression Model",
    "description": "Consider linear regression model Y = Xb + error where the distribution function of errors is unknown, but errors are independent and symmetrically distributed. The package contains a function named LRMDE which takes Y and X as input and returns minimum distance estimator of parameter b in the model. ",
    "version": "1.0",
    "maintainer": "Jiwoong Kim <kimjiwo2@stt.msu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3807,
    "package_name": "LoTTA",
    "title": "Bayesian Inference in Regression Discontinuity Designs",
    "description": "Implementation of the LoTTA (Local Trimmed Taylor Approximation) model \n    described in \"Bayesian Regression Discontinuity Design with Unknown Cutoff\" \n    by Kowalska, van de Wiel, van der Pas (2024) <doi:10.48550/arXiv.2406.11585>. ",
    "version": "0.1.1",
    "maintainer": "Julia Kowalska <j.m.kowalska@vu.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3817,
    "package_name": "LogicReg",
    "title": "Logic Regression",
    "description": "Routines for fitting Logic Regression models. Logic Regression is described\n\tin Ruczinski, Kooperberg, and LeBlanc (2003) <DOI:10.1198/1061860032238>. Monte\n        Carlo Logic Regression is described in and Kooperberg and Ruczinski (2005)\n        <DOI:10.1002/gepi.20042>.",
    "version": "1.6.6",
    "maintainer": "Charles Kooperberg <clk@fredhutch.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3822,
    "package_name": "LongCART",
    "title": "Recursive Partitioning for Longitudinal Data and Right Censored\nData Using Baseline Covariates",
    "description": "Constructs tree for continuous longitudinal data and survival data using baseline covariates as partitioning variables according to the 'LongCART' and 'SurvCART' algorithm, respectively. Later also included functions to calculate conditional power and predictive power of success based on interim results and probability of success for a prospective trial.",
    "version": "3.2",
    "maintainer": "Madan G Kundu <madan_g.kundu@yahoo.com>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3831,
    "package_name": "LorenzRegression",
    "title": "Lorenz and Penalized Lorenz Regressions",
    "description": "Inference for the Lorenz and penalized Lorenz regressions. More broadly, the package proposes functions to assess inequality and graphically represent it. The Lorenz Regression procedure is introduced in Heuchenne and Jacquemain (2022) <doi:10.1016/j.csda.2021.107347> and in Jacquemain, A., C. Heuchenne, and E. Pircalabelu (2024) <doi:10.1214/23-EJS2200>.",
    "version": "2.3.0",
    "maintainer": "Alexandre Jacquemain <aljacquemain@gmail.com>",
    "url": "https://github.com/AlJacq/LorenzRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3848,
    "package_name": "MADPop",
    "title": "MHC Allele-Based Differencing Between Populations",
    "description": "Tools for the analysis of population differences\n    using the Major Histocompatibility Complex (MHC) genotypes of samples\n    having a variable number of alleles (1-4) recorded for each\n    individual.  A hierarchical Dirichlet-Multinomial model on the\n    genotype counts is used to pool small samples from multiple\n    populations for pairwise tests of equality.  Bayesian inference is\n    implemented via the 'rstan' package.  Bootstrapped and posterior\n    p-values are provided for chi-squared and likelihood ratio tests of\n    equal genotype probabilities.",
    "version": "1.1.7",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "https://github.com/mlysy/MADPop",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3859,
    "package_name": "MAICtools",
    "title": "Performing Matched-Adjusted Indirect Comparisons (MAIC)",
    "description": "A generalised workflow for Matching-Adjusted Indirect\n    Comparison (MAIC) analysis, which supports both anchored and\n    non-anchored MAIC methods.  In MAIC, unbiased trial outcome comparison\n    is achieved by weighting the subject-level outcomes of the\n    intervention trial so that the weighted aggregate measures of\n    prognostic or effect-modifying variables match those of the comparator\n    trial. Measurements supported include time-to-event (e.g., overall\n    survival) and binary (e.g., objective tumor response). The method is\n    described in Signorovitch et al. (2010)\n    <doi:10.2165/11538370-000000000-00000> and Signorovitch et al. (2012)\n    <doi:10.1016/j.jval.2012.05.004>.",
    "version": "0.1.1",
    "maintainer": "Xiao Qi <qixiao2024@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3868,
    "package_name": "MANCIE",
    "title": "Matrix Analysis and Normalization by Concordant Information\nEnhancement",
    "description": "High-dimensional data integration is a critical but difficult problem in genomics research because of potential biases from high-throughput experiments. We present MANCIE, a computational method for integrating two genomic data sets with homogenous dimensions from different sources based on a PCA procedure as an approximation to a Bayesian approach.",
    "version": "1.4",
    "maintainer": "Tao Wang <tao.wang@utsouthwestern.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3872,
    "package_name": "MAPA",
    "title": "Multiple Aggregation Prediction Algorithm",
    "description": "Functions and wrappers for using the Multiple Aggregation Prediction Algorithm (MAPA) for time series forecasting. MAPA models and forecasts time series at multiple temporal aggregation levels, thus strengthening and attenuating the various time series components for better holistic estimation of its structure. For details see Kourentzes et al. (2014) <doi:10.1016/j.ijforecast.2013.09.006>.",
    "version": "2.0.7",
    "maintainer": "Nikolaos Kourentzes <nikolaos@kourentzes.com>",
    "url": "https://kourentzes.com/forecasting/2014/04/19/multiple-aggregation-prediction-algorithm-mapa/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3873,
    "package_name": "MAPCtools",
    "title": "Multivariate Age-Period-Cohort (MAPC) Modeling for Health Data",
    "description": "Bayesian multivariate age-period-cohort (MAPC) models for analyzing health data, with support for model fitting, visualization, stratification, and model comparison. Inference focuses on identifiable cross-strata differences, as described by Riebler and Held (2010) <doi:10.1093/biostatistics/kxp037>. Methods for handling complex survey data via the 'survey' package are included, as described in Mercer et al. (2014) <doi:10.1016/j.spasta.2013.12.001>.",
    "version": "0.1.0",
    "maintainer": "Lars Vatten <lavatt99@gmail.com>",
    "url": "https://github.com/LarsVatten/MAPCtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3887,
    "package_name": "MAVE",
    "title": "Methods for Dimension Reduction",
    "description": "Functions for dimension reduction, using MAVE (Minimum Average Variance Estimation), OPG (Outer Product of Gradient) and KSIR (sliced inverse regression of kernel version). Methods for selecting the best dimension are also included. Xia (2002) <doi:10.1111/1467-9868.03411>; Xia (2007) <doi:10.1214/009053607000000352>; Wang (2008) <doi:10.1198/016214508000000418>.",
    "version": "1.3.12",
    "maintainer": "Weiqiang Hang <weiqiang.hang@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3891,
    "package_name": "MB",
    "title": "The Use of Marginal Distributions in Conditional Forecasting",
    "description": "A new way to predict time series using the marginal distribution table in the absence of the significance of traditional models.",
    "version": "0.1.1",
    "maintainer": "Bushra Alsaeed <alsaeedbushra41@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3904,
    "package_name": "MBNMAdose",
    "title": "Dose-Response MBNMA Models",
    "description": "Fits Bayesian dose-response model-based network meta-analysis (MBNMA) \n    that incorporate multiple doses within an agent by modelling different dose-response functions, as\n    described by Mawdsley et al. (2016) <doi:10.1002/psp4.12091>. \n    By modelling dose-response relationships this can connect networks of evidence that might\n    otherwise be disconnected, and can improve precision on treatment estimates. Several common \n    dose-response functions are provided; others may be added by the user. Various characteristics\n    and assumptions can be flexibly added to the models, such as shared class effects. The consistency \n    of direct and indirect evidence in the network can be assessed using unrelated mean effects models \n    and/or by node-splitting at the treatment level.",
    "version": "0.5.0",
    "maintainer": "Hugo Pedder <hugopedder@gmail.com>",
    "url": "https://hugaped.github.io/MBNMAdose/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3905,
    "package_name": "MBNMAtime",
    "title": "Run Time-Course Model-Based Network Meta-Analysis (MBNMA) Models",
    "description": "Fits Bayesian time-course models for model-based network meta-analysis (MBNMA) that allows inclusion of multiple\n  time-points from studies. Repeated measures over time are accounted for within studies by applying different time-course functions,\n  following the method of Pedder et al. (2019) <doi:10.1002/jrsm.1351>. \n  The method allows synthesis of studies with multiple follow-up measurements that can account for time-course for a single or multiple \n  treatment comparisons. Several general time-course functions are provided; others may be added \n  by the user. Various characteristics can be flexibly added to the models, such as correlation between time points and shared \n  class effects. The consistency of direct and indirect evidence in the network can be assessed using unrelated mean effects \n  models and/or by node-splitting.",
    "version": "0.2.6",
    "maintainer": "Hugo Pedder <hugopedder@gmail.com>",
    "url": "https://hugaped.github.io/MBNMAtime/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3907,
    "package_name": "MBRM",
    "title": "Mixed Regression Models with Generalized Log-Gamma Random\nEffects",
    "description": "Multivariate distribution derived from a Bernoulli mixed model under a marginal approach, incorporating a non-normal random intercept whose distribution is assumed to follow a generalized log-gamma (GLG) specification under a particular parameter setting. Estimation is performed by maximizing the log-likelihood using numerical optimization techniques (Lizandra C. Fabio, Vanessa Barros, Cristian Lobos, Jalmar M. F. Carrasco, Marginal multivariate approach: A novel strategy for handling correlated binary outcomes, 2025, under submission).",
    "version": "0.1.1",
    "maintainer": "Jalmar M. F. Carrasco <carrasco.jalmar@ufba.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3908,
    "package_name": "MBSGS",
    "title": "Multivariate Bayesian Sparse Group Selection with Spike and Slab",
    "description": "An implementation of a Bayesian sparse group model using spike and slab priors in a regression context. It is designed for regression with a multivariate response variable, but also provides an implementation for univariate response.",
    "version": "1.2.0",
    "maintainer": "Benoit Liquet <benoit.liquet@univ-pau.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3909,
    "package_name": "MBSP",
    "title": "Multivariate Bayesian Model with Shrinkage Priors",
    "description": "Gibbs sampler for fitting multivariate Bayesian linear regression with shrinkage priors (MBSP), using the three parameter beta normal family. The method is described in Bai and Ghosh (2018) <doi:10.1016/j.jmva.2018.04.010>. ",
    "version": "5.0",
    "maintainer": "Ray Bai <raybaistat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3916,
    "package_name": "MCI",
    "title": "Multiplicative Competitive Interaction (MCI) Model",
    "description": "Market area models are used to analyze and predict store choices and market areas concerning retail and service locations. This package implements two market area models (Huff Model, Multiplicative Competitive Interaction Model) into R, while the emphases lie on 1.) fitting these models based on empirical data via OLS regression and nonlinear techniques and 2.) data preparation and processing (esp. interaction matrices and data preparation for the MCI Model).",
    "version": "1.3.3",
    "maintainer": "Thomas Wieland <thomas.wieland.geo@googlemail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3920,
    "package_name": "MCMC.qpcr",
    "title": "Bayesian Analysis of qRT-PCR Data",
    "description": "Quantitative RT-PCR data are analyzed using generalized linear mixed models based on lognormal-Poisson error distribution, fitted using MCMC. Control genes are not required but can be incorporated as Bayesian priors or, when template abundances correlate with conditions, as trackers of global effects (common to all genes). The package also implements a lognormal model for higher-abundance data and a \"classic\" model involving multi-gene normalization on a by-sample basis. Several plotting functions are included to extract and visualize results. The detailed tutorial is available here: <https://matzlab.weebly.com/uploads/7/6/2/2/76229469/mcmc.qpcr.tutorial.v1.2.4.pdf>.",
    "version": "1.2.4",
    "maintainer": "Mikhail V. Matz <matz@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3921,
    "package_name": "MCMC4Extremes",
    "title": "Posterior Distribution of Extreme Value Models in R",
    "description": "Provides some function to perform posterior estimation for some distribution, with emphasis to extreme value distributions. It contains some extreme datasets, and functions that perform the runs of posterior points of the GPD and GEV distribution. The package calculate some important extreme measures like return level for each t periods of time, and some plots as the predictive distribution, and return level plots. ",
    "version": "1.1",
    "maintainer": "Fernando Ferraz do Nascimento <fernandofn@ufpi.edu.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3922,
    "package_name": "MCMCglmm",
    "title": "MCMC Generalised Linear Mixed Models",
    "description": "Fits Multivariate Generalised Linear Mixed Models (and related models) using Markov chain Monte Carlo techniques (Hadfield 2010 J. Stat. Soft.). ",
    "version": "2.36",
    "maintainer": "Jarrod Hadfield <j.hadfield@ed.ac.uk>",
    "url": "https://github.com/jarrodhadfield/MCMCglmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3923,
    "package_name": "MCMCpack",
    "title": "Markov Chain Monte Carlo (MCMC) Package",
    "description": "Contains functions to perform Bayesian\n        inference using posterior simulation for a number of\n        statistical models. Most simulation is done in compiled C++\n        written in the Scythe Statistical Library Version 1.0.3. All\n        models return 'coda' mcmc objects that can then be summarized\n        using the 'coda' package. Some useful\n        utility functions such as density functions,\n\tpseudo-random number generators for statistical\n        distributions, a general purpose Metropolis sampling algorithm,\n        and tools for visualization are provided.",
    "version": "1.7-1",
    "maintainer": "Jong Hee Park <jongheepark@snu.ac.kr>",
    "url": "https://CRAN.R-project.org/package=MCMCpack",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3924,
    "package_name": "MCMCprecision",
    "title": "Precision of Discrete Parameters in Transdimensional MCMC",
    "description": "Estimates the precision of transdimensional Markov chain Monte Carlo \n    (MCMC) output, which is often used for Bayesian analysis of models with different \n    dimensionality (e.g., model selection). Transdimensional MCMC (e.g., reversible \n    jump MCMC) relies on sampling a discrete model-indicator variable to estimate \n    the posterior model probabilities. If only few switches occur between the models, \n    precision may be low and assessment based on the assumption of independent \n    samples misleading. Based on the observed transition matrix of the indicator \n    variable, the method of Heck, Overstall, Gronau, & Wagenmakers (2019, \n    Statistics & Computing, 29, 631-643) <doi:10.1007/s11222-018-9828-0> draws \n    posterior samples of the stationary distribution to (a) assess the uncertainty \n    in the estimated posterior model probabilities and (b) estimate the effective \n    sample size of the MCMC output.",
    "version": "0.4.2",
    "maintainer": "Daniel W. Heck <daniel.heck@uni-marburg.de>",
    "url": "https://github.com/danheck/MCMCprecision",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3925,
    "package_name": "MCMCtreeR",
    "title": "Prepare MCMCtree Analyses and Plot Bayesian Divergence Time\nAnalyses Estimates on Trees",
    "description": "Provides functions to prepare time priors for 'MCMCtree' analyses in the 'PAML' software from Yang (2007)<doi:10.1093/molbev/msm088> and plot time-scaled phylogenies from any Bayesian divergence time analysis. Most time-calibrated node prior distributions require user-specified parameters. The package provides functions to refine these parameters, so that the resulting prior distributions accurately reflect confidence in known, usually fossil, time information. These functions also enable users to visualise distributions and write 'MCMCtree' ready input files. Additionally, the package supplies flexible functions to visualise age uncertainty on a plotted tree with using node bars, using branch widths proportional to the age uncertainty, or by plotting the full posterior distributions on nodes. Time-scaled phylogenetic plots can be visualised with absolute and geological timescales . All plotting functions are applicable with output from any Bayesian software, not just 'MCMCtree'.",
    "version": "1.1",
    "maintainer": "Mark Puttick <mark.puttick@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3926,
    "package_name": "MCMCvis",
    "title": "Tools to Visualize, Manipulate, and Summarize MCMC Output",
    "description": "Performs key functions for MCMC analysis using minimal code - visualizes, manipulates, and summarizes MCMC output. Functions support simple and straightforward subsetting of model parameters within the calls, and produce presentable and 'publication-ready' output. MCMC output may be derived from Bayesian model output fit with Stan, NIMBLE, JAGS, and other software.",
    "version": "0.16.5",
    "maintainer": "Casey Youngflesh <caseyyoungflesh@gmail.com>",
    "url": "https://github.com/caseyyoungflesh/MCMCvis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3931,
    "package_name": "MCPModGeneral",
    "title": "A Supplement to the 'DoseFinding' Package for the General Case",
    "description": "Analyzes non-normal data via the Multiple Comparison Procedures and Modeling approach (MCP-Mod). Many functions rely on the 'DoseFinding' package. This package makes it so the user does not need to provide or calculate the mu vector and S matrix. Instead, the user typically supplies the data in its raw form, and this package will calculate the needed objects and passes them into the 'DoseFinding' functions. If the user wishes to primarily use the functions provided in the 'DoseFinding' package, a singular function (prepareGen()) will provide mu and S. The package currently handles power analysis and the MCP-Mod procedure for negative binomial, Poisson, and binomial data. The MCP-Mod procedure can also be applied to survival data, but power analysis is not available.\n\tBretz, F., Pinheiro, J. C., and Branson, M. (2005) <doi:10.1111/j.1541-0420.2005.00344.x>.\n\tBuckland, S. T., Burnham, K. P. and Augustin, N. H. (1997) <doi:10.2307/2533961>.\n\tPinheiro, J. C., Bornkamp, B., Glimm, E. and Bretz, F. (2014) <doi:10.1002/sim.6052>.",
    "version": "0.1-3",
    "maintainer": "Ian Laga <ilaga25@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3946,
    "package_name": "MDMR",
    "title": "Multivariate Distance Matrix Regression",
    "description": "Allows users to conduct multivariate distance matrix regression using analytic p-values and compute measures of effect size. For details on the method, see McArtor, Lubke, & Bergeman (2017) <doi:10.1007/s11336-016-9527-8>.",
    "version": "0.5.2",
    "maintainer": "Dan McArtor <dmcartor@gmail.com>",
    "url": "https://github.com/dmcartor/mdmr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3960,
    "package_name": "MECfda",
    "title": "Scalar-on-Function Regression with Measurement Error Correction",
    "description": "Solve scalar-on-function linear models,\n    including generalized linear mixed effect model and quantile linear regression model,\n    and bias correction estimation methods due to measurement error. \n    Details about the measurement error bias correction methods, see\n    Luan  et al. (2023) <doi:10.48550/arXiv.2305.12624>, \n    Tekwe et al. (2022) <doi:10.1093/biostatistics/kxac017>, \n    Zhang et al. (2023) <doi:10.5705/ss.202021.0246>, \n    Tekwe et al. (2019) <doi:10.1002/sim.8179>. ",
    "version": "0.2.1",
    "maintainer": "Heyang Ji <jihx1015@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3966,
    "package_name": "MEFM",
    "title": "Perform MEFM Estimation on Matrix Time Series",
    "description": "To perform main effect matrix factor model (MEFM) estimation for a given matrix time series as described in Lam and Cen (2024) <doi:10.48550/arXiv.2406.00128>. Estimation of traditional matrix factor models is also supported. Supplementary functions for testing MEFM over factor models are included.",
    "version": "0.1.1",
    "maintainer": "Zetai Cen <z.cen@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3982,
    "package_name": "MFT",
    "title": "The Multiple Filter Test for Change Point Detection",
    "description": "Provides statistical tests and algorithms for the detection of change points in time series and point processes - particularly for changes in the mean in time series and for changes in the rate and in the variance in point processes. References - Michael Messer, Marietta Kirchner, Julia Schiemann, Jochen Roeper, Ralph Neininger and Gaby Schneider (2014), A multiple filter test for the detection of rate changes in renewal processes with varying variance <doi:10.1214/14-AOAS782>. Stefan Albert, Michael Messer, Julia Schiemann, Jochen Roeper, Gaby Schneider (2017), Multi-scale detection of variance changes in renewal processes in the presence of rate change points <doi:10.1111/jtsa.12254>. Michael Messer, Kaue M. Costa, Jochen Roeper and Gaby Schneider (2017), Multi-scale detection of rate changes in spike trains with weak dependencies <doi:10.1007/s10827-016-0635-3>. Michael Messer, Stefan Albert and Gaby Schneider (2018), The multiple filter test for change point detection in time series <doi:10.1007/s00184-018-0672-1>. Michael Messer, Hendrik Backhaus, Albrecht Stroh and Gaby Schneider (2019+) Peak detection in time series.  ",
    "version": "2.0",
    "maintainer": "Michael Messer <messer@math.uni-frankfurt.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 3989,
    "package_name": "MGLM",
    "title": "Multivariate Response Generalized Linear Models",
    "description": "Provides functions that (1) fit multivariate discrete distributions, (2) generate random numbers from multivariate discrete distributions, and (3) run regression and penalized regression on the multivariate categorical response data.  Implemented models include: multinomial logit model, Dirichlet multinomial model, generalized Dirichlet multinomial model, and negative multinomial model. Making the best of the minorization-maximization (MM) algorithm and Newton-Raphson method, we derive and implement stable and efficient algorithms to find the maximum likelihood estimates. On a multi-core machine, multi-threading is supported.",
    "version": "0.2.1",
    "maintainer": "Juhyun Kim <juhkim111@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4005,
    "package_name": "MICsplines",
    "title": "The Computing of Monotonic Spline Bases and Constrained\nLeast-Squares Estimates",
    "description": "Providing C implementation for the computing of monotonic spline bases, including M-splines, I-splines, and C-splines, denoted by MIC splines. The definitions of the spline bases are described in Meyer (2008) <doi: 10.1214/08-AOAS167>. The package also provides the computing of constrained least-squares estimates when a subset of or all of the regression coefficients are constrained to be non-negative.",
    "version": "1.0",
    "maintainer": "Yili Hong <yilihong@vt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4018,
    "package_name": "MIXFIM",
    "title": "Evaluation of the FIM in NLMEMs using MCMC",
    "description": "Evaluation and optimization of the Fisher Information Matrix in NonLinear Mixed Effect Models using Markov Chains Monte Carlo for continuous and discrete data.",
    "version": "1.1",
    "maintainer": "Marie-Karelle Riviere-Jourdan <eldamjh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4039,
    "package_name": "MLEcens",
    "title": "Computation of the MLE for Bivariate Interval Censored Data",
    "description": "We provide functions to compute the nonparametric \n  maximum likelihood estimator (MLE) for \n  the bivariate distribution of (X,Y), when \n  realizations of (X,Y) cannot be observed directly. \n  To be more precise, we consider the situation \n  where we observe a set of rectangles in R^2 that are known \n  to contain the unobservable realizations of (X,Y). We\n  compute the MLE based on such a set of rectangles. \n  The methods can also be used for univariate censored data (see data set\n  'cosmesis'), and for \n  censored data with competing risks (see data set 'menopause'). \n  We also provide functions to visualize the observed data and the MLE. ",
    "version": "0.1-7.1",
    "maintainer": "Marloes Maathuis <marloesmaathuis@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4048,
    "package_name": "MLModelSelection",
    "title": "Model Selection in Multivariate Longitudinal Data Analysis",
    "description": "An efficient Gibbs sampling algorithm is developed for Bayesian multivariate longitudinal data analysis with the focus on selection of important elements in the generalized autoregressive matrix. It provides posterior samples and estimates of parameters. In addition, estimates of several information criteria such as Akaike information criterion (AIC), Bayesian information criterion (BIC), deviance information criterion (DIC) and prediction accuracy such as the marginal predictive likelihood (MPL) and the mean squared prediction error (MSPE) are provided for model selection. ",
    "version": "1.0",
    "maintainer": "Kuo-Jung Lee <kuojunglee@mail.ncku.edu.tw>",
    "url": "https://github.com/kuojunglee/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4049,
    "package_name": "MLMusingR",
    "title": "Practical Multilevel Modeling",
    "description": "Convenience functions and datasets to be used with Practical Multilevel Modeling using R. The package includes functions for calculating group means, group mean centered variables, and displaying some basic missing data information. A function for computing robust standard errors for linear mixed models based on Liang and Zeger (1986) <doi:10.1093/biomet/73.1.13> and Bell and 'McCaffrey' (2002) <https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2002002/article/9058-eng.pdf?st=NxMjN1YZ> is included as well as a function for checking for level-one homoskedasticity (Raudenbush & Bryk, 2002, ISBN:076191904X).  ",
    "version": "0.4.0",
    "maintainer": "Francis Huang <flhuang2000@yahoo.com>",
    "url": "https://github.com/flh3/MLMusingR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4079,
    "package_name": "MNB",
    "title": "Diagnostic Tools for a Multivariate Negative Binomial Regression\nModel",
    "description": "Diagnostic tools as residual analysis, global, \n    local and total-local influence for the multivariate model \n    from the random intercept Poisson generalized log gamma model \n    are available in this package. Including also, the estimation \n    process by maximum likelihood method, for details see \n    Fabio, L. C; Villegas, C. L.; Carrasco, J.M.F and de Castro, M. (2023) \n    <doi:10.1080/03610926.2021.1939380> and Fábio, L. C.; Villegas, C.; \n    Mamun, A. S. M. A. and Carrasco, J. M. F. (2025) <doi:10.28951/bjb.v43i1.728>.",
    "version": "1.2.0",
    "maintainer": "Jalmar Carrasco <carrascojalmar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4082,
    "package_name": "MNP",
    "title": "Fitting the Multinomial Probit Model",
    "description": "Fits the Bayesian multinomial probit model via Markov chain\n Monte Carlo.  The multinomial probit model is often used to analyze \n the discrete choices made by individuals recorded in survey data. \n Examples where the multinomial probit model may be useful include the \n analysis of product choice by consumers in market research and the \n analysis of candidate or party choice by voters in electoral studies.  \n The MNP package can also fit the model with different choice sets for \n each individual, and complete or partial individual choice orderings \n of the available alternatives from the choice set. The estimation is\n based on the efficient marginal data augmentation algorithm that is \n developed by Imai and van Dyk (2005). \"A Bayesian Analysis of the \n Multinomial Probit Model Using the Data Augmentation.\" Journal of \n Econometrics, Vol. 124, No. 2 (February), pp. 311-334. \n <doi:10.1016/j.jeconom.2004.02.002>  Detailed examples are given in \n Imai and van Dyk (2005). \"MNP: R Package for Fitting the Multinomial \n Probit Model.\"  Journal of Statistical Software, Vol. 14, No. 3 (May), \n pp. 1-32. <doi:10.18637/jss.v014.i03>.",
    "version": "3.1-5",
    "maintainer": "Kosuke Imai <imai@harvard.edu>",
    "url": "https://github.com/kosukeimai/MNP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4116,
    "package_name": "MPV",
    "title": "Data Sets from Montgomery, Peck and Vining",
    "description": "Most of this package consists of data sets from the \n             textbook Introduction\n             to Linear Regression Analysis (3rd ed), by Montgomery, Peck\n             and Vining.\n             Some additional data sets and functions are also included.",
    "version": "2.0",
    "maintainer": "W.J. Braun <john.braun@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4122,
    "package_name": "MRCE",
    "title": "Multivariate Regression with Covariance Estimation",
    "description": "Compute and select tuning parameters for the MRCE estimator proposed by Rothman, Levina, and Zhu (2010) <doi:10.1198/jcgs.2010.09188>.  This estimator fits the multiple output linear regression model with a sparse estimator of the error precision matrix and a sparse estimator of the regression coefficient matrix.",
    "version": "2.4",
    "maintainer": "Adam J. Rothman <arothman@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4127,
    "package_name": "MRHawkes",
    "title": "Multivariate Renewal Hawkes Process",
    "description": "Simulate a (bivariate) multivariate renewal Hawkes (MRHawkes) \n    self-exciting process, with given immigrant hazard rate functions and \n    offspring density function. Calculate the likelihood of a MRHawkes process \n    with given hazard rate functions and offspring density function for an \n    (increasing) sequence of event times. Calculate the Rosenblatt residuals of the \n    event times. Predict future event times based on observed event times up to a \n    given time. For details see Stindl and Chen (2018) <doi:10.1016/j.csda.2018.01.021>.",
    "version": "1.0",
    "maintainer": "Tom Stindl <t.stindl@unsw.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4131,
    "package_name": "MRPC",
    "title": "PC Algorithm with the Principle of Mendelian Randomization",
    "description": "A PC Algorithm with the Principle of Mendelian Randomization. This package implements the MRPC \n            (PC with the principle of Mendelian randomization) algorithm to infer causal graphs. It also \n            contains functions to simulate data under a certain topology, to visualize a graph in different \n            ways, and to compare graphs and quantify the differences. \n            See Badsha and Fu (2019) <doi:10.3389/fgene.2019.00460>,\n            Badsha, Martin and Fu (2021) <doi:10.3389/fgene.2021.651812>,\n            Kvamme and Badsha, et al. (2025) <doi:10.1093/genetics/iyaf064>.",
    "version": "3.2.0",
    "maintainer": "Audrey Fu <audreyqyfu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4133,
    "package_name": "MRReg",
    "title": "MDL Multiresolution Linear Regression Framework",
    "description": "We provide the framework to analyze multiresolution partitions (e.g. country, provinces, subdistrict) where each individual data point belongs to only one partition in each layer (e.g. i belongs to subdistrict A, province P, and country Q).   We assume that a partition in a higher layer subsumes lower-layer partitions (e.g. a nation is at the 1st layer subsumes all provinces at the 2nd layer). Given N individuals that have a pair of real values (x,y) that generated from  independent variable X and dependent variable Y. Each individual i belongs to one partition per layer. Our goal is to find which partitions at which highest level that all individuals  in the these partitions share the same linear model Y=f(X) where f is a linear function. The framework deploys the Minimum Description Length principle (MDL) to infer solutions. The publication of this package is at Chainarong Amornbunchornvej, Navaporn Surasvadi, Anon Plangprasopchok, and Suttipong Thajchayapong (2021) <doi:10.1145/3424670>.",
    "version": "0.1.6",
    "maintainer": "Chainarong Amornbunchornvej <grandca@gmail.com>",
    "url": "https://github.com/DarkEyes/MRReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4134,
    "package_name": "MRStdCRT",
    "title": "Model-Robust Standardization in Cluster-Randomized Trials",
    "description": "Implements model-robust standardization for cluster-randomized trials (CRTs). Provides functions that standardize user-specified regression models to estimate marginal treatment effects. The targets include the cluster-average and individual-average treatment effects, with utilities for variance estimation and example simulation datasets. Methods are described in Li, Tong, Fang, Cheng, Kahan, and Wang (2025) <doi:10.1002/sim.70270>.",
    "version": "0.1.1",
    "maintainer": "Changjun Li <changjun.li@yale.edu>",
    "url": "https://github.com/deckardt98/MRStdCRT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4143,
    "package_name": "MSCCT",
    "title": "Multiple Survival Crossing Curves Tests",
    "description": "Tests of comparison of two or more survival curves. Allows for \n    comparison of more than two survival curves whether the proportional\n    hazards hypothesis is verified or not.",
    "version": "1.0.2",
    "maintainer": "Hugo MINA PASSI <hugo.minapassi@gmail.com>",
    "url": "https://github.com/HMinP/MSCCT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4144,
    "package_name": "MSCMT",
    "title": "Multivariate Synthetic Control Method Using Time Series",
    "description": "Three generalizations of the synthetic control method (which has \n    already an implementation in package 'Synth') are implemented: first, \n    'MSCMT' allows for using multiple outcome variables, second, time series \n    can be supplied as economic predictors, and third, a well-defined \n    cross-validation approach can be used.\n    Much effort has been taken to make the implementation as stable as possible \n    (including edge cases) without losing computational efficiency.\n    A detailed description of the main algorithms is given in \n    Becker and Klößner (2018) <doi:10.1016/j.ecosta.2017.08.002>.",
    "version": "1.4.1",
    "maintainer": "Martin Becker <martin.becker@mx.uni-saarland.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4149,
    "package_name": "MSGARCH",
    "title": "Markov-Switching GARCH Models",
    "description": "Fit (by Maximum Likelihood or MCMC/Bayesian), simulate, and forecast various Markov-Switching GARCH models as described in Ardia et al. (2019) <doi:10.18637/jss.v091.i04>.",
    "version": "2.51",
    "maintainer": "Keven Bluteau <Keven.Bluteau@usherbrooke.ca>",
    "url": "https://github.com/keblu/MSGARCH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4150,
    "package_name": "MSGARCHelm",
    "title": "Hybridization of MS-GARCH and ELM Model",
    "description": "Implements the three parallel forecast combinations of Markov Switching GARCH and extreme learning machine model along with the selection of appropriate model for volatility forecasting. For method details see Hsiao C, Wan SK (2014). <doi:10.1016/j.jeconom.2013.11.003>, Hansen BE (2007). <doi:10.1111/j.1468-0262.2007.00785.x>, Elliott G, Gargano A, Timmermann A (2013). <doi:10.1016/j.jeconom.2013.04.017>. ",
    "version": "0.1.0",
    "maintainer": "Rajeev Ranjan Kumar <rrk.uasd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4151,
    "package_name": "MSIMST",
    "title": "Bayesian Monotonic Single-Index Regression Model with the Skew-T\nLikelihood",
    "description": "Incorporates a Bayesian monotonic single-index mixed-effect model with a multivariate skew-t likelihood, specifically designed to handle survey weights adjustments. Features include a simulation program and an associated Gibbs sampler for model estimation. The single-index function is constrained to be monotonic increasing, utilizing a customized Gaussian process prior for precise estimation. The model assumes random effects follow a canonical skew-t distribution, while residuals are represented by a multivariate Student-t distribution. Offers robust Bayesian adjustments to integrate survey weight information effectively.",
    "version": "1.1",
    "maintainer": "Qingyang Liu <rh8liuqy@gmail.com>",
    "url": "https://github.com/rh8liuqy/MSIMST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4159,
    "package_name": "MST",
    "title": "Multivariate Survival Trees",
    "description": "Constructs trees for multivariate survival data using marginal and frailty models.\n    Grows, prunes, and selects the best-sized tree.",
    "version": "2.2",
    "maintainer": "Peter Calhoun <calhoun.peter@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4168,
    "package_name": "MSinference",
    "title": "Multiscale Inference for Nonparametric Time Trend(s)",
    "description": "Performs a multiscale analysis of a nonparametric\n  regression or nonparametric regressions with time series errors. In case\n  of one regression, with the help of this package it is possible to detect\n  the regions where the trend function is increasing or decreasing.\n  In case of multiple regressions, the test identifies regions where\n  the trend functions are different from each other. See\n  Khismatullina and Vogt (2020) <doi:10.1111/rssb.12347>,\n  Khismatullina and Vogt (2022) <doi:10.48550/arXiv.2209.10841> and\n  Khismatullina and Vogt (2023) <doi:10.1016/j.jeconom.2021.04.010>\n  for more details on theory and applications.",
    "version": "0.2.1",
    "maintainer": "Marina Khismatullina <khismatullina@ese.eur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4184,
    "package_name": "MSwM",
    "title": "Fitting Markov Switching Models",
    "description": "Estimation, inference and diagnostics for Univariate Autoregressive Markov Switching Models for Linear and Generalized Models. Distributions for the series include gaussian, Poisson, binomial and gamma cases. The EM algorithm is used for estimation (see Perlin (2012) <doi:10.2139/ssrn.1714016>).",
    "version": "1.5",
    "maintainer": "Josep A. Sanchez-Espigares <josep.a.sanchez@upc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4188,
    "package_name": "MTE",
    "title": "Maximum Tangent Likelihood Estimation for Robust Linear\nRegression and Variable Selection",
    "description": "Several robust estimators for linear regression and variable selection are provided. \n              Included are Maximum tangent likelihood estimator by Qin, et al., (2017), arXiv preprint <doi:10.48550/arXiv.1708.05439>, \n              least absolute deviance estimator and Huber regression. The penalized version of each of these \n              estimator incorporates L1 penalty function, i.e., LASSO and Adaptive Lasso. They are able to \n              produce consistent estimates for both fixed and high-dimensional settings. ",
    "version": "1.2.1",
    "maintainer": "Shaobo Li <shaobo.li@ku.edu>",
    "url": "https://github.com/shaobo-li/MTE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4191,
    "package_name": "MTS",
    "title": "All-Purpose Toolkit for Analyzing Multivariate Time Series (MTS)\nand Estimating Multivariate Volatility Models",
    "description": "Multivariate Time Series (MTS) is a general package for analyzing multivariate linear time series and estimating multivariate volatility models. It also handles factor models, constrained factor models, asymptotic principal component analysis commonly used in finance and econometrics, and principal volatility component analysis.  (a) For the multivariate linear time series analysis, the package performs model specification, estimation, model checking, and prediction for many widely used models, including vector AR models, vector MA models, vector ARMA models, seasonal vector ARMA models, VAR models with exogenous variables, multivariate regression models with time series errors, augmented VAR models, and Error-correction VAR models for co-integrated time series. For model specification, the package performs structural specification to overcome the difficulties of identifiability of VARMA models. The methods used for structural specification include Kronecker indices and Scalar Component Models.  (b) For multivariate volatility modeling, the MTS package handles several commonly used models, including multivariate exponentially weighted moving-average volatility, Cholesky decomposition volatility models, dynamic conditional correlation (DCC) models, copula-based volatility models, and low-dimensional BEKK models. The package also considers multiple tests for conditional heteroscedasticity, including rank-based statistics.  (c) Finally, the MTS package also performs forecasting using diffusion index , transfer function analysis, Bayesian estimation of VAR models, and multivariate time series analysis with missing values.Users can also use the package to simulate VARMA models, to compute impulse response functions of a fitted VARMA model, and to calculate theoretical cross-covariance matrices of a given VARMA model. ",
    "version": "1.2.1",
    "maintainer": "Ruey S. Tsay <ruey.tsay@chicagobooth.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4192,
    "package_name": "MTSYS",
    "title": "Methods in Mahalanobis-Taguchi (MT) System",
    "description": "Mahalanobis-Taguchi (MT) system is a collection of multivariate\n    analysis methods developed for the field of quality engineering. MT system\n    consists of two families depending on their purpose. One is a family of\n    Mahalanobis-Taguchi (MT) methods (in the broad sense) for diagnosis (see\n    Woodall, W. H., Koudelik, R., Tsui, K. L., Kim, S. B., Stoumbos, Z. G., and\n    Carvounis, C. P. (2003) <doi:10.1198/004017002188618626>) and the other is a\n    family of Taguchi (T) methods for forecasting (see Kawada, H., and Nagata, Y.\n    (2015) <doi:10.17929/tqs.1.12>). The MT package contains three basic methods\n    for the family of MT methods and one basic method for the family of T\n    methods. The MT method (in the narrow sense), the Mahalanobis-Taguchi\n    Adjoint (MTA) methods, and the Recognition-Taguchi (RT) method are for the\n    MT method and the two-sided Taguchi (T1) method is for the family of T\n    methods. In addition, the Ta and Tb methods, which are the improved versions\n    of the T1 method, are included.",
    "version": "1.2.0",
    "maintainer": "Akifumi Okayama <akifumi.okayama@akane.waseda.jp>",
    "url": "https://github.com/okayaa/MTSYS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4202,
    "package_name": "MVNBayesian",
    "title": "Bayesian Analysis Framework for MVN (Mixture) Distribution",
    "description": "Tools of Bayesian analysis framework using the method\n  suggested by Berger (1985) <doi:10.1007/978-1-4757-4286-2> for\n  multivariate normal (MVN) distribution and multivariate normal\n  mixture (MixMVN) distribution:\n  a) calculating Bayesian posteriori of (Mix)MVN distribution;\n  b) generating random vectors of (Mix)MVN distribution;\n  c) Markov chain Monte Carlo (MCMC) for (Mix)MVN distribution.",
    "version": "0.0.8-11",
    "maintainer": "ZHANG Chen <447974102@qq.com>",
    "url": "https://github.com/CubicZebra/MVNBayesian",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4204,
    "package_name": "MVOPR",
    "title": "Multi-View Orthogonal Projection Regression for Multi-Modality\nIntegration",
    "description": "Implements the 'MVOPR' (Multi-View Orthogonal Projection Regression) method for robust \n    variable selection and integration of multi-modality data.",
    "version": "2.0.0",
    "maintainer": "Zongrui Dai <daizr@umich.edu>",
    "url": "https://arxiv.org/abs/2503.16807",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4211,
    "package_name": "MVar",
    "title": "Multivariate Analysis",
    "description": "Multivariate analysis, having functions that perform simple correspondence analysis (CA) and multiple correspondence analysis (MCA), principal components analysis (PCA), canonical correlation analysis (CCA), factorial analysis (FA), multidimensional scaling (MDS), linear (LDA) and quadratic discriminant analysis (QDA), hierarchical and non-hierarchical cluster analysis, simple and multiple linear regression, multiple factor analysis (MFA) for quantitative, qualitative, frequency (MFACT) and mixed data, biplot, scatter plot, projection pursuit (PP), grant tour method and other useful functions for the multivariate analysis.",
    "version": "2.2.7",
    "maintainer": "Paulo Cesar Ossani <ossanipc@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4236,
    "package_name": "MannWhitneyCopula",
    "title": "Computing Mann-Whitney Effect Based on Copulas",
    "description": "Computing the Mann-Whitney effect based on copula models. \n     Estimation of the association parameter in survival copula models.\n     A description of the underlying methods is described in Nakazono et al. (2024) <doi:10.3390/math12101453> and Nakazono et al. (accepted for publication in Statistical Papers).",
    "version": "0.1.1",
    "maintainer": "Kosuke Nakazono <nakazono@ism.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4250,
    "package_name": "MariNET",
    "title": "Build Network Based on Linear Mixed Models from EHRs",
    "description": "Analyzing longitudinal clinical data from Electronic Health Records (EHRs) using linear mixed models (LMM) and visualizing the results as networks. It includes functions for fitting LMM, normalizing adjacency matrices, and comparing networks. The package is designed for researchers in clinical and biomedical fields who need to model longitudinal data and explore relationships between variables For more details see Bates et al. (2015) <doi:10.18637/jss.v067.i01>.",
    "version": "1.0.0",
    "maintainer": "Vargas-Fernández Marina <marina.vargas@genyo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4251,
    "package_name": "MarketMatching",
    "title": "Market Matching and Causal Impact Inference",
    "description": "For a given test market find the best control markets using time series matching and analyze the impact of an intervention. The intervention could be a marketing event or some other local business tactic that is being tested. The workflow implemented in the Market Matching package utilizes dynamic time warping (the 'dtw' package) to do the matching and the 'CausalImpact' package to analyze the causal impact. In fact, this package can be considered a \"workflow wrapper\" for those two packages. In addition, if you don't have a chosen set of test markets to match, the Market Matching package can provide suggested test/control market pairs and pseudo prospective power analysis (measuring causal impact at fake interventions). ",
    "version": "1.2.1",
    "maintainer": "Larsen Kim <kblarsen4@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4261,
    "package_name": "MatchIt",
    "title": "Nonparametric Preprocessing for Parametric Causal Inference",
    "description": "Selects matched samples of the original treated and\n    control groups with similar covariate distributions -- can be\n    used to match exactly on covariates, to match on propensity\n    scores, or perform a variety of other matching procedures.  The\n    package also implements a series of recommendations offered in\n    Ho, Imai, King, and Stuart (2007) <DOI:10.1093/pan/mpl013>. (The \n    'gurobi' package, which is not on CRAN, is optional and comes with \n    an installation of the Gurobi Optimizer, available at \n    <https://www.gurobi.com>.)",
    "version": "4.7.2",
    "maintainer": "Noah Greifer <noah.greifer@gmail.com>",
    "url": "https://kosukeimai.github.io/MatchIt/,\nhttps://github.com/kosukeimai/MatchIt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4262,
    "package_name": "MatchLinReg",
    "title": "Combining Matching and Linear Regression for Causal Inference",
    "description": "Core functions as well as diagnostic and calibration tools for combining matching and linear regression for causal inference in observational studies.",
    "version": "0.8.1",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4263,
    "package_name": "MatchThem",
    "title": "Matching and Weighting Multiply Imputed Datasets",
    "description": "Provides essential tools for the pre-processing techniques of matching and weighting multiply imputed datasets. The package includes functions for matching within and across multiply imputed datasets using various methods, estimating weights for units in the imputed datasets using multiple weighting methods, calculating causal effect estimates in each matched or weighted dataset using parametric or non-parametric statistical models, and pooling the resulting estimates according to Rubin's rules (please see <https://journal.r-project.org/archive/2021/RJ-2021-073/> for more details).",
    "version": "1.2.1",
    "maintainer": "Farhad Pishgar <Farhad.Pishgar@Gmail.com>",
    "url": "https://github.com/FarhadPishgar/MatchThem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4264,
    "package_name": "Matching",
    "title": "Multivariate and Propensity Score Matching with Balance\nOptimization",
    "description": "Provides functions for multivariate and propensity score matching \n             and for finding optimal balance based on a genetic search algorithm. \n             A variety of univariate and multivariate metrics to\n             determine if balance has been obtained are also provided. For\n             details, see the paper by Jasjeet Sekhon \n             (2007, <doi:10.18637/jss.v042.i07>).",
    "version": "4.10-15",
    "maintainer": "Jasjeet Singh Sekhon <jas.sekhon@yale.edu>",
    "url": "https://github.com/JasjeetSekhon/Matching",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4281,
    "package_name": "MazamaCoreUtils",
    "title": "Utility Functions for Production R Code",
    "description": "A suite of utility functions providing functionality commonly\n    needed for production level projects such as logging, error handling,\n    cache management and date-time parsing. Functions for date-time parsing and \n    formatting require that time zones be specified explicitly, avoiding a common \n    source of error when working with environmental time series.",
    "version": "0.5.3",
    "maintainer": "Jonathan Callahan <jonathan.s.callahan@gmail.com>",
    "url": "https://github.com/MazamaScience/MazamaCoreUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4283,
    "package_name": "MazamaRollUtils",
    "title": "Efficient Rolling Functions",
    "description": "A suite of compiled functions calculating rolling mins, means, \n    maxes and other statistics. This package is designed to meet the needs of\n    data processing systems for environmental time series.",
    "version": "0.1.4",
    "maintainer": "Jonathan Callahan <jonathan.s.callahan@gmail.com>",
    "url": "https://github.com/MazamaScience/MazamaRollUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4287,
    "package_name": "McMiso",
    "title": "Multicore Multivariable Isotonic Regression",
    "description": "The goal of 'McMiso' is to provide functions for isotonic regression when there are multiple independent \n\tvariables.  The functions solve the optimization problem using recursion and leverage parallel computing to improve\n\tspeed, and are useful for situations with relatively large number of covariates.  The estimation method follows the\n\tprojective Bayes solution described in Cheung and Diaz (2023) <doi:10.1093/jrsssb/qkad014>.  ",
    "version": "0.1.2",
    "maintainer": "Cheung Ken <yc632@cumc.columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4296,
    "package_name": "MedZIsc",
    "title": "Statistical Framework for Co-Mediators of Zero-Inflated\nSingle-Cell Data",
    "description": "A causal mediation framework for single-cell data that incorporates two key features ('MedZIsc', pronounced Magics): (1) zero-inflation using beta regression and (2) overdispersed expression counts using negative binomial regression. This approach also includes a screening step based on penalized and marginal models to handle high-dimensionality. Full methodological details are available in our recent preprint by Ahn S and Li Z (2025) <doi:10.48550/arXiv.2505.22986>.",
    "version": "0.0.4",
    "maintainer": "Seungjun Ahn <seungjun.ahn@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4303,
    "package_name": "MendelianRandomization",
    "title": "Mendelian Randomization Package",
    "description": "Encodes several methods for performing Mendelian randomization\n    analyses with summarized data. Summarized data on genetic associations with the\n    exposure and with the outcome can be obtained from large consortia. These data\n    can be used for obtaining causal estimates using instrumental variable methods.",
    "version": "0.10.0",
    "maintainer": "Stephen Burgess <sb452@medschl.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4329,
    "package_name": "MetaStan",
    "title": "Bayesian Meta-Analysis via 'Stan'",
    "description": "Performs Bayesian meta-analysis, meta-regression and model-based meta-analysis \n             using 'Stan'. Includes binomial-normal hierarchical models and option to use \n             weakly informative priors for the heterogeneity parameter and the treatment effect \n             parameter which are described in Guenhan, Roever, and Friede (2020) <doi:10.1002/jrsm.1370>.",
    "version": "1.0.0",
    "maintainer": "Burak Kuersad Guenhan <burakgunhan@gmail.com>",
    "url": "https://github.com/gunhanb/MetaStan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4342,
    "package_name": "MethComp",
    "title": "Analysis of Agreement in Method Comparison Studies",
    "description": "Methods (standard and advanced) for analysis of agreement between measurement methods. These cover Bland-Altman plots, Deming regression, Lin's Total deviation index, and difference-on-average regression. See Carstensen B. (2010) \"Comparing Clinical Measurement Methods: A Practical Guide (Statistics in Practice)\" <doi:10.1002/9780470683019> for more information.",
    "version": "1.30.2",
    "maintainer": "Claus Thorn Ekstrøm <ekstrom@sund.ku.dk>",
    "url": "http://BendixCarstensen.com/MethComp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4353,
    "package_name": "MetricGraph",
    "title": "Random Fields on Metric Graphs",
    "description": "Facilitates creation and manipulation of metric graphs, such as street or river networks. Further facilitates operations and visualizations of data on metric graphs, and the creation of a large class of random fields and stochastic partial differential equations on such spaces. These random fields can be used for simulation, prediction and inference. In particular, linear mixed effects models including random field components can be fitted to data based on computationally efficient sparse matrix representations. Interfaces to the R packages 'INLA' and 'inlabru' are also provided, which facilitate working with Bayesian statistical models on metric graphs. The main references for the methods are Bolin, Simas and Wallin (2024) <doi:10.3150/23-BEJ1647>, Bolin, Kovacs, Kumar and Simas (2023) <doi:10.1090/mcom/3929> and Bolin, Simas and Wallin (2023) <doi:10.48550/arXiv.2304.03190> and <doi:10.48550/arXiv.2304.10372>.",
    "version": "1.5.0",
    "maintainer": "David Bolin <davidbolin@gmail.com>",
    "url": "https://davidbolin.github.io/MetricGraph/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4358,
    "package_name": "MfUSampler",
    "title": "Multivariate-from-Univariate (MfU) MCMC Sampler",
    "description": "Convenience functions for multivariate MCMC using univariate samplers including:\n  slice sampler with stepout and shrinkage (Neal (2003) <DOI:10.1214/aos/1056562461>),\n  adaptive rejection sampler (Gilks and Wild (1992) <DOI:10.2307/2347565>),\n  adaptive rejection Metropolis (Gilks et al (1995) <DOI:10.2307/2986138>), and\n  univariate Metropolis with Gaussian proposal.",
    "version": "1.1.0",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4360,
    "package_name": "Mhorseshoe",
    "title": "Approximate Algorithm for Horseshoe Prior",
    "description": "Provides exact and approximate algorithms for the horseshoe prior\n    in linear regression models, which were proposed by Johndrow et al. (2020)\n    <https://www.jmlr.org/papers/v21/19-536.html>.",
    "version": "0.1.5",
    "maintainer": "Kang Mingi <leehuimin115@g.skku.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4363,
    "package_name": "MiRKAT",
    "title": "Microbiome Regression-Based Kernel Association Tests",
    "description": "Test for overall association between microbiome composition data \n  and phenotypes via phylogenetic kernels. The phenotype can be univariate \n  continuous or binary (Zhao et al. (2015) <doi:10.1016/j.ajhg.2015.04.003>), \n  survival outcomes (Plantinga et al. (2017) <doi:10.1186/s40168-017-0239-9>), \n  multivariate (Zhan et al. (2017) <doi:10.1002/gepi.22030>) and \n  structured phenotypes (Zhan et al. (2017) <doi:10.1111/biom.12684>). \n  The package can also use robust regression (unpublished work) and \n  integrated quantile regression (Wang et al. (2021) <doi:10.1093/bioinformatics/btab668>). \n  In each case, the microbiome community effect is modeled nonparametrically \n  through a kernel function, which can incorporate phylogenetic tree information. ",
    "version": "1.2.3",
    "maintainer": "Anna Plantinga <amp9@williams.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4368,
    "package_name": "MicSim",
    "title": "Performing Continuous-Time Microsimulation",
    "description": "This toolkit allows performing continuous-time microsimulation for a wide range of life science (demography, social sciences, epidemiology) applications. Individual life-courses are specified by a continuous-time multi-state model as described in Zinn (2014) <doi:10.34196/IJM.00105>. ",
    "version": "3.0.0",
    "maintainer": "Sabine Zinn <szinn@diw.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4376,
    "package_name": "MicrobiomeStat",
    "title": "Comprehensive Statistical and Visualization Methods for Microbiome and Multi-Omics Data",
    "description": "A comprehensive toolkit for advanced, longitudinal microbiome and multi-omics data analysis. Specializing in in-depth longitudinal microbiome analysis for precise data interpretation across time. Generate professional analysis reports with just a click through the One Click feature. Delivers robust solutions for diverse analytic scenarios and effortless handling of multi-omics data, amplifying research efficiency. The user-friendly design, diverse features, and practical use cases assist researchers in navigating microbiome data complexities, delivering meaningful insights for longitudinal studies.",
    "version": "1.4.3",
    "maintainer": "Jun Chen <chen.jun2@mayo.edu>",
    "url": "https://github.com/cafferychen777/MicrobiomeStat",
    "exports": [],
    "topics": ["16s-rrna", "amplicon-sequencing", "dada2", "longitudinal-data", "metagenomes", "metagenomic-analysis", "microbiome", "microbiome-analysis", "microbiome-data", "microbiome-workflow", "omics", "omics-data-integration", "phyloseq", "qiime2", "statistics", "visualization"],
    "score": "NA",
    "stars": 46
  },
  {
    "id": 4388,
    "package_name": "MisRepARMA",
    "title": "Misreported Time Series Analysis",
    "description": "Provides a simple and trustworthy methodology for the analysis of misreported continuous time series. See Moriña, D, Fernández-Fontelo, A, Cabaña, A, Puig P. (2021) <arXiv:2003.09202v2>.",
    "version": "0.0.2",
    "maintainer": "David Moriña Soler <dmorina@ub.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4406,
    "package_name": "MixSIAR",
    "title": "Bayesian Mixing Models in R",
    "description": "Creates and runs Bayesian mixing models to analyze\n    biological tracer data (i.e. stable isotopes, fatty acids), which estimate the\n    proportions of source (prey) contributions to a mixture (consumer). 'MixSIAR'\n    is not one model, but a framework that allows a user to create a mixing model\n    based on their data structure and research questions, via options for fixed/\n    random effects, source data types, priors, and error terms. 'MixSIAR' incorporates\n    several years of advances since 'MixSIR' and 'SIAR'.",
    "version": "3.1.12",
    "maintainer": "Brian Stock <bstock09@gmail.com>",
    "url": "https://github.com/brianstock/MixSIAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4408,
    "package_name": "MixStable",
    "title": "Parameter Estimation for Stable Distributions and Their Mixtures",
    "description": "Provides various functions for parameter estimation of one-dimensional\n    stable distributions and their mixtures. It implements a diverse set of\n    estimation methods, including quantile-based approaches, regression methods\n    based on the empirical characteristic function (empirical, kernel, and\n    recursive), and maximum likelihood estimation. For mixture models, it provides\n    stochastic expectation–maximization (SEM) algorithms and Bayesian estimation\n    methods using sampling and importance sampling to overcome the long burn-in\n    period of Markov Chain Monte Carlo (MCMC) strategies. The package also includes\n    tools and statistical tests for analyzing whether a dataset follows a stable\n    distribution. Some of the implemented methods are described in\n    Hajjaji, O., Manou-Abi, S. M., and Slaoui, Y. (2024) <doi:10.1080/02664763.2024.2434627>.",
    "version": "0.1.0",
    "maintainer": "Solym Manou-Abi <solym.manou.abi@univ-poitiers.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4410,
    "package_name": "MixedIndTests",
    "title": "Tests of Randomness and Tests of Independence",
    "description": "Functions for testing randomness for a univariate time series with arbitrary distribution  (discrete, continuous, mixture of both types) and for testing  independence between random variables with arbitrary distributions. The test statistics are based on the multilinear empirical copula and multipliers are used to compute P-values. The test of independence between random variables appeared in  Genest, Nešlehová, Rémillard & Murphy (2019) and the test of randomness appeared in Nasri (2022).",
    "version": "1.2.0",
    "maintainer": "Bouchra R. Nasri <bouchra.nasri@umontreal.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4413,
    "package_name": "MixedPsy",
    "title": "Statistical Tools for the Analysis of Psychophysical Data",
    "description": "Tools for the analysis of psychophysical data in R. This package allows to estimate the Point of Subjective Equivalence (PSE) \n    and the Just Noticeable Difference (JND), either from a psychometric function or from a Generalized Linear Mixed Model (GLMM). \n    Additionally, the package allows plotting the fitted models and the response data, simulating psychometric functions of different shapes, and simulating data sets.\n    For a description of the use of GLMMs applied to psychophysical data, refer to Moscatelli et al. (2012).",
    "version": "1.3.0",
    "maintainer": "Alessandro Moscatelli <moskante@gmail.com>",
    "url": "https://mixedpsychophysics.wordpress.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4444,
    "package_name": "MonotoneHazardRatio",
    "title": "Nonparametric Estimation and Inference of a Monotone Hazard\nRatio Function",
    "description": "Nonparametric estimation and inference of a non-decreasing monotone hazard\n    ratio from a right censored survival dataset.  The\n    estimator is based on a generalized Grenander typed estimator, and the\n    inference procedure relies on direct plugin estimation of a first order derivative.  More\n    details please refer to the paper \"Nonparametric inference under a monotone\n    hazard ratio order\" by Y. Wu and T. Westling (2023) <doi:10.1214/23-EJS2173>.",
    "version": "0.2.0",
    "maintainer": "Yujian Wu <yujianwu@umass.edu>",
    "url": "https://github.com/Yujian-Wu/MonotoneHazardRatio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4445,
    "package_name": "MonotonicityTest",
    "title": "Nonparametric Bootstrap Test for Regression Monotonicity",
    "description": "Implements nonparametric bootstrap tests for detecting monotonicity\n  in regression functions from Hall, P. and Heckman, N. (2000) <doi:10.1214/aos/1016120363>\n  Includes tools for visualizing results using Nadaraya-Watson kernel regression and supports \n  efficient computation with 'C++'. Tutorials and shiny application demo are \n  available at <https://www.laylaparast.com/monotonicitytest> and <https://parastlab.shinyapps.io/MonotonicityTest>.",
    "version": "1.3",
    "maintainer": "Dylan Huynh <dylanhuynh@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4452,
    "package_name": "MorphoRegions",
    "title": "Analysis of Regionalization Patterns in Serially Homologous\nStructures",
    "description": "Computes the optimal number of regions (or subdivisions) and \n    their position in serial structures without a priori assumptions and to visualize \n    the results. After reducing data dimensionality with the built-in function for\n    data ordination, regions are fitted as segmented linear regressions\n    along the serial structure. Every region boundary\n    position and increasing number of regions are iteratively fitted and\n    the best model (number of regions and boundary positions) is selected\n    with an information criterion. This package expands on the previous\n    'regions' package (Jones et al., Science 2018) with improved\n    computation and more fitting and plotting options.",
    "version": "0.1.0",
    "maintainer": "Amandine Gillet <gillet.aman@gmail.com>",
    "url": "https://aagillet.github.io/MorphoRegions/,\nhttps://github.com/aagillet/MorphoRegions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4456,
    "package_name": "MortalityGaps",
    "title": "The Double-Gap Life Expectancy Forecasting Model",
    "description": "Life expectancy is highly correlated over time among countries and \n  between males and females. These associations can be used to improve forecasts. \n  Here we have implemented a method for forecasting female life expectancy based on \n  analysis of the gap between female life expectancy in a country compared with\n  the record level of female life expectancy in the world. Second, to forecast \n  male life expectancy, the gap between male life expectancy and female life \n  expectancy in a country is analysed. We named this method the Double-Gap model.\n  For a detailed description of the method see Pascariu et al. (2018). \n  <doi:10.1016/j.insmatheco.2017.09.011>.",
    "version": "1.0.7",
    "maintainer": "Marius D. Pascariu <rpascariu@outlook.com>",
    "url": "https://github.com/mpascariu/MortalityGaps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4465,
    "package_name": "Mqrcm",
    "title": "M-Quantile Regression Coefficients Modeling",
    "description": "Parametric modeling of M-quantile regression coefficient functions.",
    "version": "1.3",
    "maintainer": "Paolo Frumento <paolo.frumento@unipi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4493,
    "package_name": "MultRegCMP",
    "title": "Bayesian Multivariate Conway-Maxwell-Poisson Regression Model\nfor Correlated Count Data",
    "description": "Fits a Bayesian Regression Model for multivariate count data. This model assumes that the data is distributed according to the Conway-Maxwell-Poisson distribution, and for each response variable it is associate different covariates. This model allows to account for correlations between the counts by using latent effects based on the Chib and Winkelmann (2001) <http://www.jstor.org/stable/1392277> proposal.",
    "version": "0.1.0",
    "maintainer": "Mauro Florez <mf53@rice.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4494,
    "package_name": "MultSurvTests",
    "title": "Permutation Tests for Multivariate Survival Analysis",
    "description": "Multivariate version of the two-sample Gehan and logrank tests, as described in L.J Wei & J.M Lachin (1984) and Persson et al. (2019).",
    "version": "0.2",
    "maintainer": "Lukas Arnroth <lukas.arnroth@gmail.com>",
    "url": "https://github.com/lukketotte/MultSurvTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4495,
    "package_name": "MultiATSM",
    "title": "Multicountry Term Structure of Interest Rates Models",
    "description": "Package for estimating, analyzing, and forecasting multi-country macro-finance affine term structure models (ATSMs). All setups build on the single-country unspanned macroeconomic risk framework from Joslin, Priebsch, and Singleton (2014, JF) <doi:10.1111/jofi.12131>. Multicountry extensions by Jotikasthira, Le, and Lundblad (2015, JFE) <doi:10.1016/j.jfineco.2014.09.004>, Candelon and Moura (2023, EM) <doi:10.1016/j.econmod.2023.106453>, and Candelon and Moura (2024, JFEC) <doi:10.1093/jjfinec/nbae008> are also available. The package also provides tools for bias correction as in Bauer Rudebusch and Wu (2012, JBES) <doi:10.1080/07350015.2012.693855>, bootstrap analysis, and several graphical/numerical outputs. ",
    "version": "1.5.1",
    "maintainer": "Rubens Moura <rubens.gtmoura@gmail.com>",
    "url": "https://github.com/rubensmoura87/MultiATSM,\nhttps://rubensmoura87.github.io/MultiATSM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4499,
    "package_name": "MultiCOAP",
    "title": "High-Dimensional Covariate-Augmented Overdispersed Multi-Study\nPoisson Factor Model",
    "description": "We introduce factor models designed to jointly analyze high-dimensional count data from multiple studies by extracting study-shared and specified factors. Our factor models account for heterogeneous noises and overdispersion among counts with augmented covariates. We propose an efficient and speedy variational estimation procedure for estimating model parameters, along with a novel criterion for selecting the optimal number of factors and the rank of regression coefficient matrix. More details can be referred to Liu et al. (2024) <doi:10.48550/arXiv.2402.15071>.",
    "version": "1.1",
    "maintainer": "Wei Liu <liuweideng@gmail.com>",
    "url": "https://github.com/feiyoung/MultiCOAP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4503,
    "package_name": "MultiGlarmaVarSel",
    "title": "Variable Selection in Sparse Multivariate GLARMA Models",
    "description": "Performs variable selection in high-dimensional sparse GLARMA models. For further details we refer the reader to the paper Gomtsyan et al. (2022), <arXiv:2208.14721>.",
    "version": "1.0",
    "maintainer": "Marina Gomtsyan <marina.gomtsyan@agroparistech.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4504,
    "package_name": "MultiGrey",
    "title": "Fitting and Forecasting of Grey Model for Multivariate Time\nSeries Data",
    "description": "Grey model is commonly used in time series forecasting when statistical assumptions are violated with a limited number of data points. The minimum number of data points\n             required to fit a grey model is four observations. This package fits Grey model of First order and One Variable, i.e., GM (1,1) for multivariate time series data and returns\n             the parameters of the model, model evaluation criteria and h-step ahead forecast values for each of the time series variables. For method details see, Akay, D. and Atak, M. (2007) <DOI:10.1016/j.energy.2006.11.014>,\n             Hsu, L. and Wang, C. (2007).<DOI:10.1016/j.techfore.2006.02.005>.",
    "version": "0.1.0",
    "maintainer": "Pradip Basak <pradip@ubkv.ac.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4508,
    "package_name": "MultiKink",
    "title": "Estimation and Inference for Multi-Kink Quantile Regression",
    "description": "Estimation and inference for multiple kink quantile regression for longitudinal data and the i.i.d data. A bootstrap restarting iterative segmented quantile algorithm is proposed to estimate the multiple kink quantile regression model conditional on a given number of change points. The number of kinks is also allowed to be unknown. In such case, the backward elimination algorithm and the bootstrap restarting iterative segmented quantile algorithm are combined to select the number of change points based on a quantile BIC. For longitudinal data, we also develop the GEE estimator to incorporate the within-subject correlations.  A score-type based test statistic is also developed for testing the existence of kink effect. The package is based on the paper, ``Wei Zhong, Chuang Wan and Wenyang Zhang (2022). Estimation and inference for multikink quantile regression, JBES'' and ``Chuang Wan, Wei Zhong, Wenyang Zhang and Changliang Zou (2022). Multi-kink quantile regression for longitudinal data with application to progesterone data analysis, Biometrics\". ",
    "version": "0.2.0",
    "maintainer": "Chuang Wan <wanchuanghnu@126.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4510,
    "package_name": "MultiLevelOptimalBayes",
    "title": "Regularized Bayesian Estimator for Two-Level Latent Variable\nModels",
    "description": "Implements a regularized Bayesian estimator that optimizes the estimation\n of between-group coefficients for multilevel latent variable models by minimizing\n mean squared error (MSE) and balancing variance and bias. The package provides more reliable\n estimates in scenarios with limited data, offering a robust solution for accurate\n parameter estimation in two-level latent variable models. It is designed for\n researchers in psychology, education, and related fields who face challenges in\n estimating between-group effects under small sample sizes and low intraclass\n correlation coefficients. The package includes comprehensive S3 methods for result\n objects: print(), summary(), coef(), se(), vcov(), confint(), as.data.frame(),\n dim(), length(), names(), and update() for enhanced usability and integration\n with standard R workflows. Dashuk et al. (2025a) <doi:10.1017/psy.2025.10045>\n derived the optimal regularized Bayesian estimator;\n Dashuk et al. (2025b) <doi:10.1007/s41237-025-00264-7> extended it to \n the multivariate case; and Luedtke et al. (2008) <doi:10.1037/a0012869>\n formalized the two-level latent variable framework.",
    "version": "0.0.4.0",
    "maintainer": "Valerii Dashuk <vadashuk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4512,
    "package_name": "MultiNMix",
    "title": "Multi-Species N-Mixture (MNM) Models with 'nimble'",
    "description": "Simulating data and fitting multi-species N-mixture models using 'nimble'. Includes features for handling zero-inflation and temporal correlation, Bayesian inference, model diagnostics, parameter estimation, and predictive checks. Designed for ecological studies with zero-altered or time-series data. Mimnagh, N., Parnell, A., Prado, E., & Moral, R. A. (2022) <doi:10.1007/s10651-022-00542-7>. Royle, J. A. (2004) <doi:10.1111/j.0006-341X.2004.00142.x>.",
    "version": "0.1.0",
    "maintainer": "Niamh Mimnagh <niamhmimnagh@gmail.com>",
    "url": "https://github.com/niamhmimnagh/MultiNMix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4519,
    "package_name": "MultiRobust",
    "title": "Multiply Robust Methods for Missing Data Problems",
    "description": "Multiply robust estimation for population mean (Han and Wang 2013) <doi:10.1093/biomet/ass087>, regression analysis (Han 2014) <doi:10.1080/01621459.2014.880058> (Han 2016) <doi:10.1111/sjos.12177> and quantile regression (Han et al. 2019) <doi:10.1111/rssb.12309>.",
    "version": "1.0.5",
    "maintainer": "Shixiao Zhang <praetere@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4523,
    "package_name": "MultiVarMI",
    "title": "Multiple Imputation for Multivariate Data",
    "description": "Fully parametric Bayesian multiple imputation framework for massive multivariate data of different variable types as seen in Demirtas, H. (2017) <doi:10.1007/978-981-10-3307-0_8>.",
    "version": "1.0",
    "maintainer": "Rawan Allozi <ralloz2@uic.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4527,
    "package_name": "MultipleBubbles",
    "title": "Test and Detection of Explosive Behaviors for Time Series",
    "description": "Provides the Augmented Dickey-Fuller test and its variations to check the existence of bubbles (explosive behavior) for time series, based on the article by Peter C. B. Phillips, Shuping Shi and Jun Yu (2015a) <doi:10.1111/iere.12131>. Some functions may take a while depending on the size of the data used, or the number of Monte Carlo replications applied.",
    "version": "0.2.0",
    "maintainer": "Pedro Araujo <pharaujo1094@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4528,
    "package_name": "MultipleRegression",
    "title": "Multiple Regression Analysis",
    "description": "Tools to analysis of experiments having two or more quantitative explanatory variables and one quantitative dependent variable. Experiments can be without repetitions or with a statistical design (Hair JF, 2016) <ISBN: 13: 978-0138132637>. Pacote para uma analise de experimentos havendo duas ou mais variaveis explicativas quantitativas e uma variavel dependente quantitativa. Os experimentos podem ser sem repeticoes ou com delineamento estatistico (Hair JF, 2016) <ISBN: 13: 978-0138132637>. ",
    "version": "0.1.0",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4531,
    "package_name": "MultisiteMediation",
    "title": "Causal Mediation Analysis in Multisite Trials",
    "description": "Multisite causal mediation analysis using the methods proposed by Qin and Hong (2017) <doi:10.3102/1076998617694879>, Qin, Hong, Deutsch, and Bein (2019) <doi:10.1111/rssa.12446>, and Qin, Deutsch, and Hong (2021) <doi:10.1002/pam.22268>. It enables causal mediation analysis in multisite trials, in which individuals are assigned to a treatment or a control group at each site. It allows for estimation and hypothesis testing for not only the population average but also the between-site variance of direct and indirect effects transmitted through one single mediator or two concurrent (conditionally independent) mediators. This strategy conveniently relaxes the assumption of no treatment-by-mediator interaction while greatly simplifying the outcome model specification without invoking strong distributional assumptions. This package also provides a function that can further incorporate a sample weight and a nonresponse weight for multisite causal mediation analysis in the presence of complex sample and survey designs and non-random nonresponse, to enhance both the internal validity and external validity. The package also provides a weighting-based balance checking function for assessing the remaining overt bias.",
    "version": "0.0.4",
    "maintainer": "Xu Qin <xuqin@pitt.edu>",
    "url": "https://github.com/Xu-Qin/MultisiteMediation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4535,
    "package_name": "MultiwayRegression",
    "title": "Perform Tensor-on-Tensor Regression",
    "description": "Functions to predict one multi-way array (i.e., a tensor) from another multi-way array, using a low-rank CANDECOMP/PARAFAC (CP) factorization and a ridge (L_2) penalty [Lock, EF (2018) <doi:10.1080/10618600.2017.1401544>].  Also includes functions to sample from the Bayesian posterior of a tensor-on-tensor model.      ",
    "version": "1.2",
    "maintainer": "Eric F. Lock <elock@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4540,
    "package_name": "My.stepwise",
    "title": "Stepwise Variable Selection Procedures for Regression Analysis",
    "description": "The stepwise variable selection procedure (with iterations\n between the 'forward' and 'backward' steps) can be used to obtain\n the best candidate final regression model in regression analysis.\n All the relevant covariates are put on the 'variable list' to be\n selected. The significance levels for entry (SLE) and for stay\n (SLS) are usually set to 0.15 (or larger) for being conservative.\n Then, with the aid of substantive knowledge, the best candidate\n final regression model is identified manually by dropping the\n covariates with p value > 0.05 one at a time until all regression\n coefficients are significantly different from 0 at the chosen alpha\n level of 0.05.",
    "version": "0.1.0",
    "maintainer": "Fu-Chang Hu <fuchang.hu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4566,
    "package_name": "NBtsVarSel",
    "title": "Variable Selection in a Specific Regression Time Series of\nCounts",
    "description": "Performs variable selection in sparse negative binomial GLARMA (Generalised Linear Autoregressive Moving Average) models. For further details we refer the reader to the paper Gomtsyan (2023), <arXiv:2307.00929>.",
    "version": "1.0",
    "maintainer": "Marina Gomtsyan <mgomtsian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4567,
    "package_name": "NCA",
    "title": "Necessary Condition Analysis",
    "description": "Performs a Necessary Condition Analysis (NCA). (Dul, J. 2016. Necessary Condition Analysis (NCA). ''Logic and Methodology of 'Necessary but not Sufficient' causality.\" Organizational Research Methods 19(1), 10-52) <doi:10.1177/1094428115584005>.\n  NCA identifies necessary (but not sufficient) conditions in datasets, where x causes (e.g. precedes) y. Instead of drawing a regression line ''through the middle of the data'' in an xy-plot, NCA draws the ceiling line. The ceiling line y = f(x) separates the area with observations from the area without observations.\n  (Nearly) all observations are below the ceiling line: y <= f(x). The empty zone is in the upper left hand corner of the xy-plot (with the convention that the x-axis is ''horizontal'' and the y-axis is ''vertical'' and that values increase ''upwards'' and ''to the right''). The ceiling line is a (piecewise) linear non-decreasing line: a linear step function or a straight line. It indicates which level of x (e.g. an effort or input) is necessary but not sufficient for a (desired) level of y (e.g. good performance or output). A quick start guide for using this package can be found here: <https://repub.eur.nl/pub/78323/> or <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=2624981>.",
    "version": "4.0.5",
    "maintainer": "Govert Buijs <buijs@rsm.nl>",
    "url": "https://www.eur.nl/en/erim/erim/research-initiatives/necessary-condition-analysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4577,
    "package_name": "NFCP",
    "title": "N-Factor Commodity Pricing Through Term Structure Estimation",
    "description": "Commodity pricing models are (systems of) stochastic differential equations that are utilized for the valuation and hedging of commodity contingent claims (i.e. derivative products on the commodity) and other commodity related investments. Commodity pricing models that capture market dynamics are of great importance to commodity market participants in order to exercise sound investment and risk-management strategies. Parameters of commodity pricing models are estimated through maximum likelihood estimation, using available term structure futures data of a commodity. 'NFCP' (n-factor commodity pricing) provides a framework for the modeling, parameter estimation, probabilistic forecasting, option valuation and simulation of commodity prices through state space and Monte Carlo methods, risk-neutral valuation and Kalman filtering. 'NFCP' allows the commodity pricing model to consist of n correlated factors, with both random walk and mean-reverting elements. The n-factor commodity pricing model framework was first presented in the work of Cortazar and Naranjo (2006) <doi:10.1002/fut.20198>. Examples presented in 'NFCP' replicate the two-factor crude oil commodity pricing model presented in the prolific work of Schwartz and Smith (2000) <doi:10.1287/mnsc.46.7.893.12034> with the approximate term structure futures data applied within this study provided in the 'NFCP' package.",
    "version": "1.2.2",
    "maintainer": "Thomas Aspinall <tomaspinall2512@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4592,
    "package_name": "NIRStat",
    "title": "Novel Statistical Methods for Studying Near-Infrared\nSpectroscopy (NIRS) Time Series Data",
    "description": "Provides transfusion-related differential tests on Near-infrared spectroscopy (NIRS) time series with detection limit, which contains two testing statistics: Mean Area Under the Curve (MAUC) and slope statistic. This package applied a penalized spline method within imputation setting. Testing is conducted by a nested permutation approach within imputation. Refer to Guo et al (2018) <doi:10.1177/0962280218786302> for further details.",
    "version": "1.1",
    "maintainer": "Yikai Wang <johnzon.wyk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4593,
    "package_name": "NISTnls",
    "title": "Nonlinear least squares examples from NIST",
    "description": "Datasets for testing nonlinear regression routines.",
    "version": "0.9-13",
    "maintainer": "Douglas Bates <bates@stat.wisc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4599,
    "package_name": "NLPwavelet",
    "title": "Bayesian Wavelet Analysis Using Non-Local Priors",
    "description": "Performs Bayesian wavelet analysis using individual non-local priors as described in Sanyal & Ferreira (2017) <DOI:10.1007/s13571-016-0129-3> and non-local prior mixtures as described in Sanyal (2025) <DOI:10.48550/arXiv.2501.18134>.",
    "version": "1.1",
    "maintainer": "Nilotpal Sanyal <nsanyal@utep.edu>",
    "url": "https://nilotpalsanyal.github.io/NLPwavelet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4600,
    "package_name": "NMA",
    "title": "Network Meta-Analysis Based on Multivariate Meta-Analysis and\nMeta-Regression Models",
    "description": "Network meta-analysis tools based on contrast-based approach using the multivariate meta-analysis and meta-regression models (Noma et al. (2025) <doi:10.1101/2025.09.15.25335823>). Comprehensive analysis tools for network meta-analysis and meta-regression (e.g., synthesis analysis, ranking analysis, and creating league table) are available through simple commands. For inconsistency assessment, the local and global inconsistency tests based on the Higgins' design-by-treatment interaction model are available. In addition, the side-splitting methods and Jackson's random inconsistency model can be applied. Standard graphical tools for network meta-analysis, including network plots, ranked forest plots, and transitivity analyses, are also provided. For the synthesis analyses, the Noma-Hamura's improved REML (restricted maximum likelihood)-based methods (Noma et al. (2023) <doi:10.1002/jrsm.1652> <doi:10.1002/jrsm.1651>) are adopted as the default methods. ",
    "version": "2.1-1",
    "maintainer": "Hisashi Noma <noma@ism.ac.jp>",
    "url": "https://doi.org/10.1101/2025.09.15.25335823",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4601,
    "package_name": "NMADTA",
    "title": "Network Meta-Analysis of Multiple Diagnostic Tests",
    "description": "Provides statistical methods for network meta-analysis \n    of 1–5 diagnostic tests to simultaneously compare multiple tests within a \n    missing data framework, including:\n    - Bayesian hierarchical model for network meta-analysis of multiple \n      diagnostic tests \n      (Ma, Lian, Chu, Ibrahim, and Chen (2018) <doi:10.1093/biostatistics/kxx025>)\n    - Bayesian Hierarchical Summary Receiver Operating Characteristic Model \n      for Network Meta-Analysis of Diagnostic Tests \n      (Lian, Hodges, and Chu (2019) <doi:10.1080/01621459.2018.1476239>).",
    "version": "0.1.1",
    "maintainer": "Xing Xing <xxing8@jh.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4615,
    "package_name": "NNMIS",
    "title": "Nearest Neighbor Based Multiple Imputation for Survival Data\nwith Missing Covariates",
    "description": "Imputation for both missing covariates and censored observations (optional) for survival data with missing covariates by the nearest neighbor based multiple imputation algorithm as described in Hsu et al. (2006) <doi:10.1002/sim.2452>, and Hsu and Yu (2018) <doi: 10.1177/0962280218772592>. Note that the current version can only impute for a situation with one missing covariate.",
    "version": "1.0.1",
    "maintainer": "Chiu-Hsieh Hsu <pablo1639@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4619,
    "package_name": "NO.PING.PONG",
    "title": "Incorporating Previous Findings When Evaluating New Data",
    "description": "Functions for revealing what happens when effect size estimates \n    from previous studies are taken into account when evaluating each new dataset \n    in a study sequence. The analyses can be conducted for cumulative\n    meta-analyses and for Bayesian data analyses. The package contains sample \n    data for a wide selection of research topics. Jointly considering \n    previous findings along with new data is more likely to result in correct \n    conclusions than does the traditional practice of not incorporating previous\n    findings, which often results in a back and forth ping-pong of conclusions \n    when evaluating a sequence of studies.\n    O'Connor & Ermacora (2021, <doi:10.1037/cbs0000259>).",
    "version": "0.1.8.7",
    "maintainer": "Brian P. O'Connor  <brian.oconnor@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4624,
    "package_name": "NPBayesImputeCat",
    "title": "Non-Parametric Bayesian Multiple Imputation for Categorical Data",
    "description": "These routines create multiple imputations of missing at random categorical data, and create multiply imputed synthesis of categorical data, with or without structural zeros. Imputations and syntheses are based on Dirichlet process mixtures of multinomial distributions, which is a non-parametric Bayesian modeling approach that allows for flexible joint modeling, described in Manrique-Vallier and Reiter (2014) <doi:10.1080/10618600.2013.844700>.",
    "version": "0.6",
    "maintainer": "Jingchen Hu <jingchen.monika.hu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4627,
    "package_name": "NPCirc",
    "title": "Nonparametric Circular Methods",
    "description": "Nonparametric smoothing methods for density and regression estimation involving circular data, \n\t\t\t\tincluding the estimation of the mean regression function and other conditional characteristics.",
    "version": "3.1.2",
    "maintainer": "Maria Alonso-Pena <mariaalonso.pena@usc.es>",
    "url": "https://www.jstatsoft.org/v61/i09/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4628,
    "package_name": "NPCox",
    "title": "Nonparametric and Semiparametric Proportional Hazards Model",
    "description": "An estimation procedure for the analysis of nonparametric proportional hazards model (e.g. h(t) = h0(t)exp(b(t)'Z)), providing estimation of b(t) and its pointwise standard errors, and semiparametric proportional hazards model (e.g. h(t) = h0(t)exp(b(t)'Z1 + c*Z2)), providing estimation of b(t), c and their standard errors. More details can be found in Lu Tian et al. (2005) <doi:10.1198/016214504000000845>.",
    "version": "1.3",
    "maintainer": "Qi Yang <qiyang-sdu@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4630,
    "package_name": "NPHMC",
    "title": "Sample Size Calculation for the Proportional Hazards Mixture\nCure Model",
    "description": "An R-package for calculating sample size of a survival trial with or without cure fractions.",
    "version": "2.4.2",
    "maintainer": "Chao Cai <caic@mailbox.sc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4631,
    "package_name": "NPHazardRate",
    "title": "Nonparametric Hazard Rate Estimation",
    "description": "Provides functions and examples for histogram, kernel (classical, variable bandwidth and transformations based), discrete and semiparametric hazard rate estimators.",
    "version": "0.1",
    "maintainer": "Dimitrios Bagkavos <dimitrios.bagkavos@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4634,
    "package_name": "NPMLEcmprsk",
    "title": "Type-Specific Failure Rate and Hazard Rate on Competing Risks\nData",
    "description": "Given a failure type, the function computes covariate-specific probability of failure over time and covariate-specific conditional hazard rate based on possibly right-censored competing risk data. Specifically, it computes the non-parametric maximum-likelihood estimates of these quantities and their asymptotic variances in a semi-parametric mixture model for competing-risks data, as described in Chang et al. (2007a).",
    "version": "3.0",
    "maintainer": "Chung-Hsing Chen <chchen@nhri.org.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4635,
    "package_name": "NPP",
    "title": "Normalized Power Prior Bayesian Analysis",
    "description": "Posterior sampling in several commonly used distributions using\n    normalized power prior as described in \n    Duan, Ye and Smith (2006) <doi:10.1002/env.752> and \n    Ibrahim et.al. (2015) <doi:10.1002/sim.6728>. \n    Sampling of the power parameter is achieved via \n    either independence Metropolis-Hastings or random walk Metropolis-Hastings \n    based on transformation. ",
    "version": "0.7.0",
    "maintainer": "Zifei Han <hanzifei1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4637,
    "package_name": "NPflow",
    "title": "Bayesian Nonparametrics for Automatic Gating of Flow-Cytometry\nData",
    "description": "Dirichlet process mixture of multivariate normal, skew normal or skew t-distributions\n             modeling oriented towards flow-cytometry data preprocessing applications. Method is \n             detailed in: Hejblum, Alkhassimn, Gottardo, Caron & Thiebaut (2019) <doi: 10.1214/18-AOAS1209>.",
    "version": "0.13.6",
    "maintainer": "Boris P Hejblum <boris.hejblum@u-bordeaux.fr>",
    "url": "http://sistm.github.io/NPflow/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4645,
    "package_name": "NTS",
    "title": "Nonlinear Time Series Analysis",
    "description": "Simulation, estimation, prediction procedure, and model identification methods for nonlinear time series analysis, including threshold autoregressive models, Markov-switching models, convolutional functional autoregressive models, nonlinearity tests, Kalman filters and various sequential Monte Carlo methods. More examples and details about this package can be found in the book \"Nonlinear Time Series Analysis\" by Ruey S. Tsay and Rong Chen, John Wiley & Sons, 2018 (ISBN: 978-1-119-26407-1).",
    "version": "1.1.3",
    "maintainer": "Xialu Liu <xialu.liu@sdsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4652,
    "package_name": "NVAR",
    "title": "Nonlinear Vector Autoregression Models",
    "description": "Estimate nonlinear vector autoregression models (also known as the \n    next generation reservoir computing) for nonlinear dynamic systems. The \n    algorithm was described by Gauthier et al. (2021) <doi:10.1038/s41467-021-25801-2>.",
    "version": "0.1.0",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "url": "https://github.com/Sciurus365/NVAR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4653,
    "package_name": "NVCSSL",
    "title": "Nonparametric Varying Coefficient Spike-and-Slab Lasso",
    "description": "Fits Bayesian regularized varying coefficient models with the Nonparametric Varying Coefficient Spike-and-Slab Lasso (NVC-SSL) introduced by Bai et al. (2023) <https://jmlr.org/papers/volume24/20-1437/20-1437.pdf>. Functions to fit frequentist penalized varying coefficients are also provided, with the option of employing the group lasso penalty of Yuan and Lin (2006) <doi:10.1111/j.1467-9868.2005.00532.x>, the group minimax concave penalty (MCP) of Breheny and Huang <doi:10.1007/s11222-013-9424-2>, or the group smoothly clipped absolute deviation (SCAD) penalty of Breheny and Huang (2015) <doi:10.1007/s11222-013-9424-2>.",
    "version": "3.0",
    "maintainer": "Ray Bai <raybaistat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4663,
    "package_name": "NeEDS4BigData",
    "title": "New Experimental Design Based Subsampling Methods for Big Data",
    "description": "Subsampling methods for big data under different models and assumptions.\n    Starting with linear regression and leading to Generalised Linear Models, softmax\n    regression, and quantile regression. Specifically, the model-robust subsampling method \n    proposed in Mahendran, A., Thompson, H., and McGree, J. M. (2023) <doi:10.1007/s00362-023-01446-9>, \n    where multiple models can describe the big data, and the subsampling framework for potentially \n    misspecified Generalised Linear Models in Mahendran, A., Thompson, H., and McGree, J. M. (2025)\n    <doi:10.48550/arXiv.2510.05902>.",
    "version": "1.0.1",
    "maintainer": "Amalan Mahendran <amalan0595@gmail.com>",
    "url": "https://github.com/Amalan-ConStat/NeEDS4BigData,https://amalan-constat.github.io/NeEDS4BigData/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4666,
    "package_name": "NegBinBetaBinreg",
    "title": "Negative Binomial and Beta Binomial Bayesian Regression Models",
    "description": "The Negative Binomial regression with mean and shape modeling and mean and variance modeling and Beta Binomial regression with mean and dispersion modeling.",
    "version": "1.0",
    "maintainer": "Edilberto Cepeda <ecepedac@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4677,
    "package_name": "NetGreg",
    "title": "Network-Guided Penalized Regression (NetGreg)",
    "description": "A network-guided penalized regression framework that integrates network characteristics from Gaussian graphical models with partial penalization, accounting for both network structure (hubs and non-hubs) and clinical covariates in high-dimensional omics data, including transcriptomics and proteomics. The full methodological details can be found in our recent preprint by Ahn S and Oh EJ (2025) <doi:10.48550/arXiv.2505.22986>.",
    "version": "0.0.2",
    "maintainer": "Seungjun Ahn <seungjun.ahn@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4689,
    "package_name": "NetVAR",
    "title": "Network Structures in VAR Models",
    "description": "Vector AutoRegressive (VAR) type models with tailored regularisation structures are provided to uncover network type structures in the data, such as influential time series (influencers). Currently the package implements the LISAR model from Zhang and Trimborn (2023) <doi:10.2139/ssrn.4619531>. The package automatically derives the required regularisation sequences and refines it during the estimation to provide the optimal model. The package allows for model optimisation under various loss functions such as Mean Squared Forecasting Error (MSFE), Akaike Information Criterion (AIC), and Bayesian Information Criterion (BIC). It provides a dedicated class, allowing for summary prints of the optimal model and a plotting function to conveniently analyse the optimal model via heatmaps.",
    "version": "0.1-2",
    "maintainer": "Simon Trimborn <trimborn.econometrics@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4697,
    "package_name": "NetworkReg",
    "title": "Generalized Linear Regression Models on Network-Linked Data with\nStatistical Inference",
    "description": "Linear regression model and generalized linear models with nonparametric network effects on network-linked observations. The model is originally proposed by Le and Li (2022) <doi:10.48550/arXiv.2007.00803> and is assumed on observations that are connected by a network or similar relational data structure. A more recent work by Wang, Le and Li (2024) <doi:10.48550/arXiv.2410.01163> further extends the framework to generalized linear models. All these models are implemented in the current package. The model does not assume that the relational data or network structure to be precisely observed; thus, the method is provably robust to a certain level of perturbation of the network structure. The package contains the estimation and inference function for the model.",
    "version": "2.0",
    "maintainer": "Jianxiang Wang <jw1881@scarletmail.rutgers.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4715,
    "package_name": "NlinTS",
    "title": "Models for Non Linear Causality Detection in Time Series",
    "description": "Models for non-linear time series analysis and causality detection. The main functionalities of this package consist of an implementation of the classical causality test (C.W.J.Granger 1980) <doi:10.1016/0165-1889(80)90069-X>,  and a non-linear version of it based on feed-forward neural networks. This package contains also an implementation of the Transfer Entropy <doi:10.1103/PhysRevLett.85.461>, and the continuous Transfer Entropy using an approximation based on the k-nearest neighbors <doi:10.1103/PhysRevE.69.066138>. There are also some other useful tools, like the VARNN (Vector Auto-Regressive Neural Network) prediction model, the Augmented test of stationarity, and the discrete and continuous entropy and mutual information.",
    "version": "1.4.6",
    "maintainer": "Youssef Hmamouche <hmamoucheyussef@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4718,
    "package_name": "Nmix",
    "title": "Bayesian Inference on Univariate Normal Mixtures",
    "description": "A program for Bayesian analysis of univariate normal mixtures with an unknown number\n  of components, following the approach of Richardson and Green (1997) <doi:10.1111/1467-9868.00095>.\n  This makes use of reversible jump Markov chain Monte Carlo methods that are capable of jumping\n  between the parameter sub-spaces corresponding to different numbers of components in the mixture.\n  A sample from the full joint distribution of all unknown variables is thereby generated,\n  and this can be used as a basis for a thorough presentation of many aspects of the posterior\n  distribution. ",
    "version": "2.0.5",
    "maintainer": "Peter Green <P.J.Green@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4724,
    "package_name": "NonParRolCor",
    "title": "a Non-Parametric Statistical Significance Test for Rolling\nWindow Correlation",
    "description": "Estimates and plots (as a single plot and as a heat map) the rolling window correlation coefficients between two time series and computes their statistical significance, which is carried out through a non-parametric computing-intensive method. This method addresses the effects due to the multiple testing (inflation of the Type I error) when the statistical significance is estimated for the rolling window correlation coefficients. The method is based on Monte Carlo simulations by permuting one of the variables (e.g., the dependent) under analysis and keeping fixed the other variable (e.g., the independent). We improve the computational efficiency of this method to reduce the computation time through parallel computing. The 'NonParRolCor' package also provides examples with synthetic and real-life environmental time series to exemplify its use. Methods derived from R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/> and J.M. Polanco-Martinez and J.L. Lopez-Martinez (2021) <doi:10.1016/j.ecoinf.2021.101379>.",
    "version": "0.8.0",
    "maintainer": "Josue M. Polanco-Martinez <josue.m.polanco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4726,
    "package_name": "NonlinearTSA",
    "title": "Nonlinear Time Series Analysis",
    "description": "Function and data sets in the book entitled \"Nonlinear Time Series Analysis with R Applications\" B.Guris (2020). The book will be published in Turkish and the original name of this book will be \"R Uygulamali Dogrusal Olmayan Zaman Serileri Analizi\". It is possible to perform nonlinearity tests, nonlinear unit root tests, nonlinear cointegration tests and estimate nonlinear error correction models by using the functions written in this package. The Momentum Threshold Autoregressive (MTAR), the Smooth Threshold Autoregressive (STAR) and the Self Exciting Threshold Autoregressive (SETAR) type unit root tests can be performed using the functions written. In addition, cointegration tests using the Momentum Threshold Autoregressive (MTAR), the Smooth Threshold Autoregressive (STAR) and the Self Exciting Threshold Autoregressive (SETAR) models can be applied. It is possible to estimate nonlinear error correction models. The Granger causality test performed using nonlinear models can also be applied.",
    "version": "0.5.0",
    "maintainer": "Burak Guris <bguris@istanbul.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4728,
    "package_name": "NormData",
    "title": "Derivation of Regression-Based Normative Data",
    "description": "Normative data are often used to estimate the relative position of a raw test score in the population. This package allows for deriving regression-based normative data. It includes functions that enable the fitting of regression models for the mean and residual (or variance) structures, test the model assumptions, derive the normative data in the form of normative tables or automatic scoring sheets, and estimate confidence intervals for the norms. This package accompanies the book Van der Elst, W. (2024). Regression-based normative data for psychological assessment. A hands-on approach using R. Springer Nature.  ",
    "version": "1.1",
    "maintainer": "Wim Van der Elst <Wim.vanderelst@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4730,
    "package_name": "NormPsy",
    "title": "Normalisation of Psychometric Tests",
    "description": "Functions for normalizing psychometric test scores. The normalization aims at correcting the metrological properties of the psychometric tests such as the ceiling and floor effects and the curvilinearity (unequal interval scaling). Functions to compute and plot predictions in the natural scale of the psychometric test from the estimates of a linear mixed model estimated on the normalized scores are also provided. See Philipps et al (2014) <doi:10.1159/000365637> for details.",
    "version": "1.0.8",
    "maintainer": "Cecile Proust-Lima <cecile.proust-lima@inserm.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4735,
    "package_name": "NostalgiR",
    "title": "Advanced Text-Based Plots",
    "description": "Provides functions to produce advanced ascii graphics, directly to the terminal window. This package utilizes the txtplot() function from the 'txtplot' package, to produce text-based histograms, empirical cumulative distribution function plots, scatterplots with fitted and regression lines, quantile plots, density plots, image plots, and contour plots.",
    "version": "1.0.2",
    "maintainer": "Hien D. Nguyen <h.nguyen7@uq.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4742,
    "package_name": "NutrienTrackeR",
    "title": "Food Composition Information and Dietary Assessment",
    "description": "Provides a tool set for food information and dietary assessment. It \n    uses food composition data from several reference databases, including: 'USDA' (United States), \n    'CIQUAL' (France), 'BEDCA' (Spain), 'CNF' (Canada) and 'STFCJ' (Japan). 'NutrienTrackeR' calculates \n    the intake levels for both macronutrient and micronutrients, and compares them with the recommended \n    dietary allowances (RDA). It includes a number of visualization tools, such as time series \n    plots of nutrient intake, and pie-charts showing the main foods contributing to the intake \n    level of a given nutrient. A shiny app exposing the main functionalities of the package is also \n    provided.",
    "version": "1.4.0",
    "maintainer": "Rafael Ayala <rafaelayalahernandez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4747,
    "package_name": "OBMbpkg",
    "title": "Estimate the Population Size for the Mb Capture-Recapture Model",
    "description": "Applies an objective Bayesian method to the Mb capture-recapture model to estimate the population size N.  The Mb model is a class of capture-recapture methods used to account for variations in capture probability due to animal behavior.  Under the Mb formulation,  the initial capture of an animal may effect the probability of subsequent captures due to their becoming \"trap happy\" or \"trap shy.\"",
    "version": "1.0.0",
    "maintainer": "John Snyder <jcs8v6@mail.missouri.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4749,
    "package_name": "OBsMD",
    "title": "Objective Bayesian Model Discrimination in Follow-Up Designs",
    "description": "Implements the objective Bayesian methodology proposed in Consonni and Deldossi in order to choose the optimal experiment that better discriminate between competing models, see Deldossi and Nai Ruscone (2020) <doi:10.18637/jss.v094.i02>.",
    "version": "12.0",
    "maintainer": "Marta Nai Ruscone <marta.nairuscone@unige.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4756,
    "package_name": "ODS",
    "title": "Statistical Methods for Outcome-Dependent Sampling Designs",
    "description": "Outcome-dependent sampling (ODS) schemes are cost-effective ways to enhance study efficiency. \n    In ODS designs, one observes the exposure/covariates with a probability that depends on the outcome \n    variable. Popular ODS designs include case-control for binary outcome, case-cohort for time-to-event \n    outcome, and continuous outcome ODS design (Zhou et al. 2002) <doi: 10.1111/j.0006-341X.2002.00413.x>. \n    Because ODS data has biased sampling nature, standard statistical analysis such as linear regression \n    will lead to biases estimates of the population parameters. This package implements four statistical \n    methods related to ODS designs: (1) An empirical likelihood method analyzing the primary continuous \n    outcome with respect to exposure variables in continuous ODS design (Zhou et al., 2002). (2) A partial \n    linear model analyzing the primary outcome in continuous ODS design (Zhou, Qin and Longnecker, 2011) \n    <doi: 10.1111/j.1541-0420.2010.01500.x>. (3) Analyze a secondary outcome in continuous ODS design \n    (Pan et al. 2018) <doi: 10.1002/sim.7672>. (4) An estimated likelihood method analyzing a secondary \n    outcome in case-cohort data (Pan et al. 2017) <doi: 10.1111/biom.12838>.",
    "version": "0.2.0",
    "maintainer": "Yinghao Pan <ypan8@uncc.edu>",
    "url": "https://github.com/Yinghao-Pan/ODS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4764,
    "package_name": "OLCPM",
    "title": "Online Change Point Detection for Matrix-Valued Time Series",
    "description": "We provide two algorithms for monitoring change points with online matrix-valued time series, under the assumption of a two-way factor structure. The algorithms are based on different calculations of the second moment matrices. One is based on stacking the columns of matrix observations, while another is by a more delicate projected approach. A well-known fact is that, in the presence of a change point, a factor model can be rewritten as a model with a larger number of common factors. In turn, this entails that, in the presence of a change point, the number of spiked eigenvalues in the second moment matrix of the data increases. Based on this, we propose two families of procedures - one based on the fluctuations of partial sums, and one based on extreme value theory - to monitor whether the first non-spiked eigenvalue diverges after a point in time in the monitoring horizon, thereby indicating the presence of a change point. This package also provides some simple functions for detecting and removing outliers, imputing missing entries and testing moments. See more details in He et al. (2021)<doi:10.48550/arXiv.2112.13479>.",
    "version": "0.1.2",
    "maintainer": "Long Yu <fduyulong@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4767,
    "package_name": "OLStrajr",
    "title": "Ordinary Least Squares Trajectory Analysis",
    "description": "The 'OLStrajr' package provides comprehensive functions for ordinary\n    least squares (OLS) trajectory analysis and case-by-case OLS regression as\n    outlined in Carrig, Wirth, and Curran (2004) <doi:10.1207/S15328007SEM1101_9>\n    and Rogosa and Saner (1995) <doi:10.3102/10769986020002149>. It encompasses two\n    primary functions, OLStraj() and cbc_lm(). The OLStraj() function simplifies\n    the estimation of individual growth curves over time via OLS regression, with\n    options for visualizing both group-level and individual-level growth trajectories\n    and support for linear and quadratic models. The cbc_lm() function facilitates\n    case-by-case OLS estimates and provides unbiased mean population intercept and\n    slope estimators by averaging OLS intercepts and slopes across cases. It further\n    offers standard error calculations across bootstrap replicates and computation\n    of 95% confidence intervals based on empirical distributions from the resampling\n    processes.",
    "version": "0.1.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "url": "https://github.com/mightymetrika/OLStrajr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4774,
    "package_name": "OPCreg",
    "title": "Online Principal Component Regression for Online Datasets",
    "description": "The online principal component regression method can process the online data set. 'OPCreg' implements the online principal component regression method, which is specifically designed to process online datasets efficiently. This method is particularly useful for handling large-scale, streaming data where traditional batch processing methods may be computationally infeasible.The philosophy of the package is described in 'Guo' (2025) <doi:10.1016/j.physa.2024.130308>.",
    "version": "3.0.0",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4778,
    "package_name": "OPSR",
    "title": "Ordered Probit Switching Regression",
    "description": "Estimates ordered probit switching regression models - a Heckman\n    type selection model with an ordinal selection and continuous outcomes.\n    Different model specifications are allowed for each treatment/regime. For\n    more details on the method, see Wang & Mokhtarian (2024) <doi:10.1016/j.tra.2024.104072>\n    or Chiburis & Lokshin (2007) <doi:10.1177/1536867X0700700202>.",
    "version": "1.0.1",
    "maintainer": "Daniel Heimgartner <d.heimgartners@gmail.com>",
    "url": "https://github.com/dheimgartner/OPSR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4781,
    "package_name": "OPTtesting",
    "title": "Optimal Testing",
    "description": "Optimal testing under general dependence. The R package implements procedures proposed in Wang, Han, and Tong (2022). The package includes parameter estimation procedures, the computation for the posterior probabilities, and the testing procedure.",
    "version": "1.0.0",
    "maintainer": "Lijia Wang <lijiawan@usc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4789,
    "package_name": "ORIONZ.G",
    "title": "EAP Scoring in Exploratory FA Solutions with Correlated\nResiduals",
    "description": "Obtaining Bayes Expected A Posteriori (EAP) individual score \n    estimates based on linear and non-linear extended Exploratoy Factor Analysis\n    solutions that include a correlated-residual structure.",
    "version": "1.0.1",
    "maintainer": "David Navarro-Gonzalez <david.navarro@udl.cat>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4794,
    "package_name": "OSCV",
    "title": "One-Sided Cross-Validation",
    "description": "Functions for implementing different versions of the OSCV method in the kernel regression and density estimation frameworks. The package mainly supports the following articles: (1) Savchuk, O.Y., Hart, J.D. (2017). Fully robust one-sided cross-validation for regression functions. Computational Statistics, <doi:10.1007/s00180-017-0713-7> and (2) Savchuk, O.Y. (2017). One-sided cross-validation for nonsmooth density functions, <arXiv:1703.05157>.",
    "version": "1.0",
    "maintainer": "Olga Savchuk <olga.y.savchuk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4804,
    "package_name": "OTRselect",
    "title": "Variable Selection for Optimal Treatment Decision",
    "description": "A penalized regression framework that can simultaneously estimate \n    the optimal treatment strategy and identify important variables. \n    Appropriate for either censored or uncensored continuous response.",
    "version": "1.3",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4813,
    "package_name": "OVtool",
    "title": "Omitted Variable Tool",
    "description": "This tool was designed to assess the sensitivity of research findings to omitted variables when estimating causal effects using propensity score (PS) weighting. This tool produces graphics and summary results that will enable a researcher to quantify the impact an omitted variable would have on their results. Burgette et al. (2021) describe the methodology behind the primary function in this package, ov_sim. The method is demonstrated in Griffin et al. (2020) <doi:10.1016/j.jsat.2020.108075>.",
    "version": "1.0.3",
    "maintainer": "Lane Burgette <burgette@rand.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4823,
    "package_name": "Ohit",
    "title": "OGA+HDIC+Trim and High-Dimensional Linear Regression Models",
    "description": "Ing and Lai (2011) <doi:10.5705/ss.2010.081> proposed a high-dimensional model selection procedure that comprises three steps: orthogonal greedy algorithm (OGA), high-dimensional information criterion (HDIC), and Trim. The first two steps, OGA and HDIC, are used to sequentially select input variables and determine stopping rules, respectively. The third step, Trim, is used to delete irrelevant variables remaining in the second step. This package aims at fitting a high-dimensional linear regression model via OGA+HDIC+Trim.",
    "version": "1.0.0",
    "maintainer": "Hai-Tang Chiou <htchiou1@gmail.com>",
    "url": "http://mx.nthu.edu.tw/~cking/pdf/IngLai2011.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4824,
    "package_name": "OkNNE",
    "title": "A k-Nearest Neighbours Ensemble via Optimal Model Selection for\nRegression",
    "description": "Optimal k Nearest Neighbours Ensemble is an ensemble of base k nearest neighbour models each constructed on a bootstrap sample with a random subset of features. k closest observations are identified for a test point \"x\" (say), in each base k nearest neighbour model to fit a stepwise regression to predict the output value of \"x\". The final predicted value of \"x\" is the mean of estimates given by all the models. The implemented model takes training and test datasets and trains the model on training data to predict the test data. Ali, A., Hamraz, M., Kumam, P., Khan, D.M., Khalil, U., Sulaiman, M. and Khan, Z. (2020) <DOI:10.1109/ACCESS.2020.3010099>.",
    "version": "1.0.1",
    "maintainer": "Amjad Ali <aalistat1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4828,
    "package_name": "OmegaG",
    "title": "Omega-Generic: Composite Reliability of Multidimensional\nMeasures",
    "description": "\n    It is a computer tool to estimate the item-sum score's reliability (composite reliability, CR) in multidimensional scales with overlapping items. An item that measures more than one domain construct is called an overlapping item. \n    The estimation is based on factor models allowing unlimited cross-factor loadings such as exploratory structural equation modeling (ESEM) and Bayesian structural equation modeling (BSEM). The factor models include correlated-factor models and bi-factor models. Specifically for bi-factor models, a type of hierarchical factor model, the package estimates the CR hierarchical subscale/hierarchy and CR subscale/scale total. The CR estimator 'Omega-generic' was proposed by Mai, Srivastava, and Krull (2021) <https://whova.com/embedded/subsession/enars_202103/1450751/1452993/>. The current version can only handle continuous data. \n    Yujiao Mai contributes to the algorithms, R programming, and application example. Deo Kumar Srivastava contributes to the algorithms and the application example. Kevin R. Krull contributes to the application example. The package 'OmegaG' was sponsored by American Lebanese Syrian Associated Charities (ALSAC). However, the contents of 'OmegaG' do not necessarily represent the policy of the ALSAC.",
    "version": "1.0.1",
    "maintainer": "Yujiao Mai <yujiaomai@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4831,
    "package_name": "OmicKriging",
    "title": "Poly-Omic Prediction of Complex TRaits",
    "description": "It provides functions to generate a correlation matrix\n    from a genetic dataset and to use this matrix to predict the phenotype of an\n    individual by using the phenotypes of the remaining individuals through\n    kriging. Kriging is a geostatistical method for optimal prediction or best\n    unbiased linear prediction. It consists of predicting the value of a\n    variable at an unobserved location as a weighted sum of the variable at\n    observed locations. Intuitively, it works as a reverse linear regression:\n    instead of computing correlation (univariate regression coefficients are\n    simply scaled correlation) between a dependent variable Y and independent\n    variables X, it uses known correlation between X and Y to predict Y.",
    "version": "1.4.0",
    "maintainer": "Hae Kyung Im <haky@uchicago.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4840,
    "package_name": "Omisc",
    "title": "Univariate Bootstrapping and Other Things",
    "description": "Primarily devoted to implementing the Univariate Bootstrap (as well as the Traditional Bootstrap). In addition there are multiple functions for DeFries-Fulker behavioral genetics models. The univariate bootstrapping functions, DeFries-Fulker functions, regression and traditional bootstrapping functions form the original core. Additional features may come online later, however this software is a work in progress. For more information about univariate bootstrapping see: Lee and Rodgers (1998) and Beasley et al (2007) <doi:10.1037/1082-989X.12.4.414>.",
    "version": "0.1.5",
    "maintainer": "Patrick O'Keefe <okeefep@ohsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4849,
    "package_name": "OnboardClient",
    "title": "Bindings for Onboard Data's Building Data API",
    "description": "Provides a wrapper for the Onboard Data building data API <https://api.onboarddata.io/swagger>. Along with streamlining access to the API, this package simplifies access to sensor time series data, metadata (sensors, equipment, and buildings), and details about the Onboard data model/ontology.",
    "version": "1.0.0",
    "maintainer": "Christopher Dudas-Thomas <christopher@onboarddata.io>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4851,
    "package_name": "OncoDataSets",
    "title": "A Comprehensive Collection of Cancer Types and Cancer-Related\nDatasets",
    "description": "Offers a rich collection of data focused on cancer research, covering survival rates, genetic studies, biomarkers, and epidemiological insights. \n    Designed for researchers, analysts, and bioinformatics practitioners, the package includes datasets on various cancer types such as melanoma, leukemia, breast, ovarian, and lung cancer, among others. \n    It aims to facilitate advanced research, analysis, and understanding of cancer epidemiology, genetics, and treatment outcomes.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "url": "https://github.com/lightbluetitan/oncodatasets,\nhttps://lightbluetitan.github.io/oncodatasets/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4855,
    "package_name": "Oncofilterfast",
    "title": "Aids in the Analysis of Genes Influencing Cancer Survival",
    "description": "Aids in the analysis of genes influencing cancer survival by including a principal function, calculator(), which calculates the P-value for each provided gene under the optimal cutoff in cancer survival studies. Grounded in methodologies from significant works, this package references Therneau's 'survival' package (Therneau, 2024; <https://CRAN.R-project.org/package=survival>) and the survival analysis extensions by Therneau and Grambsch (2000, ISBN 0-387-98784-3). It also integrates the 'survminer' package by Kassambara et al. (2021; <https://CRAN.R-project.org/package=survminer>), enhancing survival curve visualizations with 'ggplot2'.",
    "version": "1.0.0",
    "maintainer": "Pheonix Chen <shuaiyuchen4@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4862,
    "package_name": "OneSampleLogRankTest",
    "title": "One-Sample Log-Rank Test",
    "description": "The log-rank test is performed to assess the survival outcomes between two group. \n             When there is no proper control group or obtaining such data is cumbersome, one sample\n             log-rank test can be applied. This package performs one sample log-rank test as described in Finkelstein et al. (2003)<doi:10.1093/jnci/djt227> and variation of the \n             test for small sample sizes which is detailed in FD Liddell (1984)<doi:10.1136/jech.38.1.85> paper. Visualization function in the package\n             generates Kaplan-Meier Curve comparing survival curve of the general population against that of the population of interest.",
    "version": "0.9.2",
    "maintainer": "Divy Kangeyan <dkangeyan@kitepharma.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4869,
    "package_name": "OpeNoise",
    "title": "Environmental Noise Pollution Data Analysis",
    "description": "Provides analyse, interpret and understand noise pollution data. Data are typically regular time series measured with sound meter. The package is partially described in Fogola, Grasso, Masera and Scordino (2023, <DOI:10.61782/fa.2023.0063>).",
    "version": "0.2-18",
    "maintainer": "Pasquale Scordino <scordino.pasquale@gmail.com>",
    "url": "https://arpapiemonte.github.io/openoise-analysis/,\nhttps://github.com/Arpapiemonte/openoise-analysis/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4876,
    "package_name": "OpenMx",
    "title": "Extended Structural Equation Modelling",
    "description": "Create structural equation models that can be manipulated programmatically.",
    "version": "2.22.7-19",
    "maintainer": "",
    "url": "https://github.com/OpenMx/OpenMx",
    "exports": [],
    "topics": ["behavior-genetics", "c-plus-plus", "estimation", "graphical-models", "growth-curves", "item-response-theory", "multilevel-models", "openmx", "psychology", "r", "statistics", "structural-equation-modeling"],
    "score": "NA",
    "stars": 96
  },
  {
    "id": 4891,
    "package_name": "OptSig",
    "title": "Optimal Level of Significance for Regression and Other\nStatistical Tests",
    "description": "The optimal level of significance is calculated based on a decision-theoretic approach. The optimal level is chosen so that the expected loss from hypothesis testing is minimized. A range of statistical tests are covered, including the test for the population mean, population proportion, and a linear restriction in a multiple regression model. \n             The details are covered in Kim and Choi (2020) <doi:10.1111/abac.12172>, and Kim (2021) <doi:10.1080/00031305.2020.1750484>. ",
    "version": "2.2",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4892,
    "package_name": "OptimModel",
    "title": "Perform Nonlinear Regression Using 'optim' as the Optimization\nEngine",
    "description": "A wrapper for 'optim' for nonlinear regression problems; see Nocedal J and Write S (2006, ISBN: 978-0387-30303-1).  Performs ordinary least squares (OLS), iterative re-weighted least squares (IRWLS), and maximum likelihood (MLE). Also includes the robust outlier detection (ROUT) algorithm; see Motulsky, H and Brown, R (2006) <doi:10.1186/1471-2105-7-123>.",
    "version": "2.0-3",
    "maintainer": "Steven Novick <Steven.Novick@takeda.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4903,
    "package_name": "OrdFacReg",
    "title": "Least Squares, Logistic, and Cox-Regression with Ordered\nPredictors",
    "description": "In biomedical studies, researchers are often interested in assessing the association between one or more ordinal explanatory variables and an outcome variable, at the same time adjusting for covariates of any type. The outcome variable may be continuous, binary, or represent censored survival times. In the absence of a precise knowledge of the response function, using monotonicity constraints on the ordinal variables improves efficiency in estimating parameters, especially when sample sizes are small. This package implements an active set algorithm that efficiently computes such estimators.",
    "version": "1.0.6",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "url": "http://www.kasparrufibach.ch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4904,
    "package_name": "OrdMonReg",
    "title": "Compute least squares estimates of one bounded or two ordered\nisotonic regression curves",
    "description": "We consider the problem of estimating two isotonic regression curves g1* and g2* under the constraint that they are ordered, i.e. g1* <= g2*. Given two sets of n data points y_1, ..., y_n and z_1, ..., z_n that are observed at (the same) deterministic design points x_1, ..., x_n, the estimates are obtained by minimizing the Least Squares criterion L(a, b) = sum_{i=1}^n (y_i - a_i)^2 w1(x_i) + sum_{i=1}^n (z_i - b_i)^2 w2(x_i) over the class of pairs of vectors (a, b) such that a and b are isotonic and a_i <= b_i for all i = 1, ..., n. We offer two different approaches to compute the estimates: a projected subgradient algorithm where the projection is calculated using a PAVA as well as Dykstra's cyclical projection algorithm.",
    "version": "1.0.3",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "url": "http://www.ceremade.dauphine.fr/~fadoua,\nhttp://www.kasparrufibach.ch,\nhttp://www.math.u-psud.fr/~santambr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4914,
    "package_name": "OscillatorGenerator",
    "title": "Generation of Customizable, Discretized Time Series of\nOscillating Species",
    "description": "The supplied code allows for the generation of discrete time series of oscillating species. General shapes can be selected by means of individual functions, which are widely customizable by means of function arguments. All code was developed in the Biological Information Processing Group at the BioQuant Center at Heidelberg University, Germany.",
    "version": "0.1.0",
    "maintainer": "Arne Schoch <arne_schoch@gmx.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4934,
    "package_name": "PAGE",
    "title": "Predictor-Assisted Graphical Models under Error-in-Variables",
    "description": "We consider the network structure detection for variables Y with auxiliary variables X accommodated, which are possibly subject to measurement error. The following three functions are designed to address various structures by different methods : one is NP_Graph() that is used for handling the nonlinear relationship between the responses and the covariates,  another is Joint_Gaussian() that is used for correction in linear regression models via the Gaussian maximum likelihood, and the other Cond_Gaussian() is for linear regression models via conditional likelihood function.",
    "version": "0.4.0",
    "maintainer": "Wan-Yi Chang <jessica306a@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4952,
    "package_name": "PAmeasures",
    "title": "Prediction and Accuracy Measures for Nonlinear Models and for\nRight-Censored Time-to-Event Data",
    "description": "We propose a pair of summary measures for the predictive power of a prediction\n    function based on a regression model. The regression model can be linear\n    or nonlinear, parametric, semi-parametric, or nonparametric, and correctly\n    specified or mis-specified. The first measure, R-squared, is an extension of\n    the classical R-squared statistic for a linear model, quantifying the prediction\n    function's ability to capture the variability of the response. The second\n    measure, L-squared, quantifies the prediction function's bias for predicting the\n    mean regression function. When used together, they give a complete summary of\n    the predictive power of a prediction function. Please refer to Gang Li and Xiaoyan Wang (2016) <arXiv:1611.03063> for more details.",
    "version": "0.1.0",
    "maintainer": "Xiaoyan Wang<xywang@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4970,
    "package_name": "PCBN",
    "title": "Inference of Pair-Copula Bayesian Networks",
    "description": "Creates, fits and samples Pair-Copula Bayesian networks (PCBN)\n    under some restrictions on the underlying Directed Acyclic Graph (DAG),\n    that is, no active cycles nor interfering v-structures, following \n    Derumigny, Horsman and Kurowicka (2025) <doi:10.48550/arXiv.2510.03518>.",
    "version": "0.1.1",
    "maintainer": "Alexis Derumigny <a.f.f.derumigny@tudelft.nl>",
    "url": "https://github.com/AlexisDerumigny/PCBN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4987,
    "package_name": "PCovR",
    "title": "Principal Covariates Regression",
    "description": "Analyzing regression data with many and/or highly collinear predictor variables, by simultaneously reducing the predictor variables to a limited number of components and regressing the criterion variables on these components (de Jong S. & Kiers H. A. L. (1992) <doi:10.1016/0169-7439(92)80100-I>). Several rotation and model selection options are provided.",
    "version": "2.7.2",
    "maintainer": "Kristof Meers <kristof.meers+cran@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 4988,
    "package_name": "PCpluS",
    "title": "Piecewise Constant Plus Smooth Regression",
    "description": "Allows for nonparametric regression where one assumes that the signal is given by the sum of a piecewise constant function and a smooth function. More precisely, it implements the estimator PCpluS (piecewise constant plus smooth regression estimator) from Pein and Shah (2025) <doi:10.48550/arXiv.2112.03878>.",
    "version": "1.0.1",
    "maintainer": "Pein Florian <f.pein@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5007,
    "package_name": "PEPBVS",
    "title": "Bayesian Variable Selection using Power-Expected-Posterior Prior",
    "description": "Performs Bayesian variable selection under normal linear\n          models for the data with the model parameters following as prior distributions either \n          the power-expected-posterior (PEP) or the intrinsic (a special case of the former)\n          (Fouskakis and Ntzoufras (2022) <doi: 10.1214/21-BA1288>,\n          Fouskakis and Ntzoufras (2020) <doi: 10.3390/econometrics8020017>).          \n          The prior distribution on model space is the uniform over all models\n          or the uniform on model dimension (a special case of the beta-binomial prior). \n          The selection is performed by either implementing a full enumeration \n          and evaluation of all possible models or using the Markov Chain \n          Monte Carlo Model Composition (MC3) algorithm (Madigan and York (1995) <doi: 10.2307/1403615>). \n          Complementary functions for hypothesis testing, estimation and \n          predictions under Bayesian model averaging, as well as, plotting and \n          printing the results are also provided. The results can be compared to the\n          ones obtained under other well-known priors on model parameters and model spaces.",
    "version": "2.2",
    "maintainer": "Konstantina Charmpi <xarmpi.kon@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5010,
    "package_name": "PERSUADE",
    "title": "Parametric Survival Model Selection for Decision-Analytic Models",
    "description": "Provides a standardized framework to support the selection\n    and evaluation of parametric survival models for time-to-event data.\n    Includes tools for visualizing survival data, checking proportional\n    hazards assumptions (Grambsch and Therneau, 1994,\n    <doi:10.1093/biomet/81.3.515>), comparing parametric (Ishak and\n    colleagues, 2013, <doi:10.1007/s40273-013-0064-3>), spline (Royston\n    and Parmar, 2002, <doi:10.1002/sim.1203>) and cure models, examining\n    hazard functions, and evaluating model extrapolation. Methods are\n    consistent with recommendations in the NICE Decision Support Unit\n    Technical Support Documents (14 and 21\n    <https://sheffield.ac.uk/nice-dsu/tsds/survival-analysis>). Results\n    are structured to facilitate integration into decision-analytic\n    models, and reports can be generated with 'rmarkdown'. The package\n    builds on existing tools including 'flexsurv' (Jackson, 2016,\n    <doi:10.18637/jss.v070.i08>)) and 'flexsurvcure' for estimating cure\n    models.",
    "version": "0.1.2",
    "maintainer": "Bram Ramaekers <bram.ramaekers@mumc.nl>",
    "url": "https://github.com/Bram-R/PERSUADE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5057,
    "package_name": "PFLR",
    "title": "Estimating Penalized Functional Linear Regression",
    "description": "Implementation of commonly used penalized functional linear regression models, including the Smooth and Locally Sparse (SLoS) method by Lin et al. (2016) <doi:10.1080/10618600.2016.1195273>, Nested Group bridge Regression (NGR) method by Guan et al. (2020) <doi:10.1080/10618600.2020.1713797>, Functional Linear Regression That's interpretable (FLIRTI) by James et al. (2009) <doi:10.1214/08-AOS641>, and the Penalized B-spline regression method.",
    "version": "1.1.0",
    "maintainer": "Haolun Shi <shl2003@connect.hku.hk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5063,
    "package_name": "PGaGEV",
    "title": "Power Garima-Generalized Extreme Value Distribution",
    "description": "Density, distribution function, quantile function, \n    and random generation function based on Kittipong Klinjan,Tipat Sottiwan and Sirinapa Aryuyuen (2024)<DOI:10.28919/cmbn/8833>.",
    "version": "0.1.0",
    "maintainer": "Kittipong Klinjan <kittipong_k@rmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5069,
    "package_name": "PHSMM",
    "title": "Penalised Maximum Likelihood Estimation for Hidden Semi-Markov\nModels",
    "description": "Provides tools for penalised maximum likelihood estimation of hidden semi-Markov models (HSMMs) with flexible state dwell-time distributions. These include functions for model fitting, model checking and state-decoding. The package considers HSMMs for univariate time series with state-dependent gamma, normal, Poisson or Bernoulli distributions. For details, see Pohle, J., Adam, T. and Beumer, L.T. (2021): Flexible estimation of the state dwell-time distribution in hidden semi-Markov models. <arXiv:2101.09197>.",
    "version": "1.0",
    "maintainer": "Jennifer Pohle <jennifer.pohle@uni-bielefeld.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5072,
    "package_name": "PHeval",
    "title": "Evaluation of the Proportional Hazards Assumption and Test for\nComparing Survival Curves with a Standardized Score Process",
    "description": "Provides tools for the test for the comparison of survival curves, the evaluation of the goodness-of-fit and the predictive capacity of the proportional hazards model.",
    "version": "1.1",
    "maintainer": "Cecile Chauvel <cecile.chauvel@univ-lyon1.fr>",
    "url": "https://cran.r-project.org/package=PHeval",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5083,
    "package_name": "PJFM",
    "title": "Variational Inference for High-Dimensional Joint Frailty Model",
    "description": "Joint frailty models have been widely used to study the associations between recurrent events and a survival outcome. However, existing joint frailty models only consider one or a few recurrent events\n    and cannot deal with high-dimensional recurrent events. This package can be used to fit our recently developed penalized joint frailty model that can handle high-dimensional recurrent events. \n    Specifically, an adaptive lasso penalty is imposed on the parameters for the effects of the recurrent events on the survival outcome, which allows for variable selection. \n    Also, our algorithm is computationally efficient, which is based on the Gaussian variational approximation method.",
    "version": "0.1.0",
    "maintainer": "Jiehuan Sun <jiehuan.sun@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5096,
    "package_name": "PLMIX",
    "title": "Bayesian Analysis of Finite Mixture of Plackett-Luce Models",
    "description": "Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi:10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi:10.1002/sim.6224>.",
    "version": "2.2.0",
    "maintainer": "Cristina Mollica <cristina.mollica@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5100,
    "package_name": "PLRModels",
    "title": "Statistical Inference in Partial Linear Regression Models",
    "description": "Contains statistical inference tools applied to Partial Linear Regression (PLR) models. Specifically, point estimation, confidence intervals estimation, bandwidth selection, goodness-of-fit tests and analysis of covariance are considered. Kernel-based methods, combined with ordinary least squares estimation, are used and time series errors are allowed. In addition, these techniques are also implemented for both parametric (linear) and nonparametric regression models.",
    "version": "1.4",
    "maintainer": "Ana Lopez-Cheda <ana.lopez.cheda@udc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5103,
    "package_name": "PLmixed",
    "title": "Estimate (Generalized) Linear Mixed Models with Factor\nStructures",
    "description": "Utilizes the 'lme4' and 'optimx' packages (previously the optim()\n    function from 'stats') to estimate (generalized) linear mixed models (GLMM)\n    with factor structures using a profile likelihood approach, as outlined in\n    Jeon and Rabe-Hesketh (2012) <doi:10.3102/1076998611417628> and\n    Rockwood and Jeon (2019) <doi:10.1080/00273171.2018.1516541>.\n    Factor analysis and item response models can be extended to allow\n    for an arbitrary number of nested and crossed random effects,\n    making it useful for multilevel and cross-classified models.",
    "version": "0.1.8",
    "maintainer": "Nicholas Rockwood <njrockwood@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5119,
    "package_name": "POCRE",
    "title": "Penalized Orthogonal-Components Regression",
    "description": "Penalized orthogonal-components regression (POCRE) is a supervised dimension reduction method for high-dimensional data. It sequentially constructs orthogonal components (with selected features) which are maximally correlated to the response residuals. POCRE can also construct common components for multiple responses and thus build up latent-variable models.",
    "version": "0.6.0",
    "maintainer": "Dabao Zhang <zhangdb@purdue.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5130,
    "package_name": "POSTm",
    "title": "Phylogeny-Guided OTU-Specific Association Test for Microbiome\nData",
    "description": "Implements the Phylogeny-Guided Microbiome OTU-Specific Association \n    Test method, which boosts the testing power by adaptively borrowing \n    information from phylogenetically close OTUs (operational taxonomic units)\n    of the target OTU. This method\n    is built on a kernel machine regression framework and allows for flexible \n    modeling of complex microbiome effects, adjustments for covariates, and \n    can accommodate both continuous and binary outcomes. ",
    "version": "1.4",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5133,
    "package_name": "POUMM",
    "title": "The Phylogenetic Ornstein-Uhlenbeck Mixed Model",
    "description": "The Phylogenetic Ornstein-Uhlenbeck Mixed Model (POUMM) allows to \n    estimate the phylogenetic heritability of continuous traits, to test \n    hypotheses of neutral evolution versus stabilizing selection, to quantify \n    the strength of stabilizing selection, to estimate measurement error and to\n    make predictions about the evolution of a phenotype and phenotypic variation \n    in a population. The package implements combined maximum likelihood and \n    Bayesian inference of the univariate Phylogenetic Ornstein-Uhlenbeck Mixed \n    Model, fast parallel likelihood calculation, maximum likelihood \n    inference of the genotypic values at the tips, functions for summarizing and\n    plotting traces and posterior samples, functions for simulation of a univariate \n    continuous trait evolution model along a phylogenetic tree. So far, the \n    package has been used for estimating the heritability of quantitative traits\n    in macroevolutionary and epidemiological studies, see e.g. \n    Bertels et al. (2017) <doi:10.1093/molbev/msx246> and \n    Mitov and Stadler (2018) <doi:10.1093/molbev/msx328>. The algorithm for \n    parallel POUMM likelihood calculation has been published in \n    Mitov and Stadler (2019) <doi:10.1111/2041-210X.13136>.",
    "version": "2.1.8",
    "maintainer": "Venelin Mitov <vmitov@gmail.com>",
    "url": "https://venelin.github.io/POUMM/index.html,\nhttps://github.com/venelin/POUMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5145,
    "package_name": "PPTcirc",
    "title": "Projected Polya Tree for Circular Data",
    "description": "Provides functionality for the prior and posterior projected Polya tree for the analysis of circular data \n  (Nieto-Barajas and Nunez-Antonio (2019) <arXiv:1902.06020>).",
    "version": "0.2.3",
    "maintainer": "Karla Mayra Perez <karla.mayra25@gmail.com>",
    "url": "https://github.com/Karlampm/PPTcirc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5150,
    "package_name": "PRANA",
    "title": "Pseudo-Value Regression Approach for Network Analysis (PRANA)",
    "description": "A novel pseudo-value regression approach for the differential co-expression network analysis in expression data, which can incorporate additional clinical variables in the model. This is a direct regression modeling for the differential network analysis, and it is therefore computationally amenable for the most users. The full methodological details can be found in Ahn S et al (2023) <doi:10.1186/s12859-022-05123-w>.",
    "version": "1.0.6",
    "maintainer": "Seungjun Ahn <seungjun.ahn@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5155,
    "package_name": "PRIMAL",
    "title": "Parametric Simplex Method for Sparse Learning",
    "description": "Implements a unified framework of parametric simplex method for a variety of sparse learning problems (e.g., Dantzig selector (for linear regression), sparse quantile regression, sparse support vector machines, and compressive sensing) combined with efficient hyper-parameter selection strategies. The core algorithm is implemented in C++ with Eigen3 support for portable high performance linear algebra. For more details about parametric simplex method, see Haotian Pang (2017) <https://papers.nips.cc/paper/6623-parametric-simplex-method-for-sparse-learning.pdf>.",
    "version": "1.0.3",
    "maintainer": "Zichong Li <zichongli5@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5158,
    "package_name": "PRISM.forecast",
    "title": "Penalized Regression with Inferred Seasonality Module -\nForecasting Unemployment Initial Claims using 'Google Trends'\nData",
    "description": "Implements Penalized Regression with Inferred Seasonality Module (PRISM) to generate forecast estimation of weekly unemployment initial claims using 'Google Trends' data. It includes required data and tools for backtesting the performance in 2007-2020.",
    "version": "0.2.1",
    "maintainer": "Dingdong Yi <ryan.ddyi@gmail.com>",
    "url": "https://github.com/ryanddyi/prism",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5172,
    "package_name": "PROreg",
    "title": "Patient Reported Outcomes Regression Analysis",
    "description": "It offers a wide variety of techniques, such as graphics, recoding, or regression models, for a comprehensive analysis of patient-reported outcomes (PRO). Especially novel is the broad range of regression models based on the beta-binomial distribution useful for analyzing binomial data with over-dispersion in cross-sectional, longitudinal, or multidimensional response studies (see Najera-Zuloaga J., Lee D.-J. and Arostegui I. (2019) <doi:10.1002/bimj.201700251>).",
    "version": "1.3.2",
    "maintainer": "Josu Najera-Zuloaga <josu.najera@ehu.eus>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5176,
    "package_name": "PRP",
    "title": "Bayesian Prior and Posterior Predictive Replication Assessment",
    "description": "Utilize the Bayesian prior and posterior predictive checking\n   approach to provide a statistical assessment of replication success \n   and failure. The package is based on the methods proposed in \n   Zhao,Y., Wen X.(2021) <arXiv:2105.03993>. ",
    "version": "0.1.1",
    "maintainer": "Yi Zhao <zhayi@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5178,
    "package_name": "PRTree",
    "title": "Probabilistic Regression Trees",
    "description": "Implementation of Probabilistic Regression Trees (PRTree),\n  providing functions for model fitting and prediction, with specific adaptations\n  to handle missing values. The main computations are implemented in 'Fortran'\n  for high efficiency. The package is based on the PRTree methodology described in\n  Alkhoury et al. (2020), \"Smooth and Consistent Probabilistic Regression Trees\"\n  <https://proceedings.neurips.cc/paper_files/paper/2020/file/8289889263db4a40463e3f358bb7c7a1-Paper.pdf>.\n  Details on the treatment of missing data and implementation aspects are presented in\n  Prass, T.S.; Neimaier, A.S.; Pumi, G. (2025), \"Handling Missing Data in Probabilistic Regression Trees:\n  Methods and Implementation in R\" <doi:10.48550/arXiv.2510.03634>.",
    "version": "1.0.2",
    "maintainer": "Taiane Schaedler Prass <taianeprass@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5186,
    "package_name": "PSF",
    "title": "Forecasting of Univariate Time Series Using the Pattern\nSequence-Based Forecasting (PSF) Algorithm",
    "description": "Pattern Sequence Based Forecasting (PSF) takes univariate\n    time series data as input and assist to forecast its future values.\n    This algorithm forecasts the behavior of time series\n    based on similarity of pattern sequences. Initially, clustering is done with the\n    labeling of samples from database. The labels associated with samples are then\n    used for forecasting the future behaviour of time series data. The further\n    technical details and references regarding PSF are discussed in Vignette.",
    "version": "0.5",
    "maintainer": "Neeraj Bokde <neerajdhanraj@gmail.com>",
    "url": "https://www.neerajbokde.in/viggnette/2021-10-13-PSF/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5193,
    "package_name": "PSPI",
    "title": "Propensity Score Predictive Inference for Generalizability",
    "description": "Provides a suite of Propensity Score Predictive Inference (PSPI) methods to generalize treatment effects in trials to target populations. The package includes an existing model Bayesian Causal Forest (BCF) and four PSPI models (BCF-PS, FullBART, SplineBART, DSplineBART). These methods leverage Bayesian Additive Regression Trees (BART) to adjust for high-dimensional covariates and nonlinear associations, while SplineBART and DSplineBART further use propensity score based splines to address covariate shift between trial data and target population. ",
    "version": "1.2",
    "maintainer": "Jungang Zou <jungang.zou@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5200,
    "package_name": "PSW",
    "title": "Propensity Score Weighting Methods for Dichotomous Treatments",
    "description": "Provides propensity score weighting methods to control for confounding in causal inference with dichotomous treatments and continuous/binary outcomes. It includes the following functional modules: (1) visualization of the propensity score distribution in both treatment groups with mirror histogram, (2) covariate balance diagnosis, (3) propensity score model specification test, (4) weighted estimation of treatment effect, and (5) augmented estimation of treatment effect with outcome regression. The weighting methods include the inverse probability weight (IPW) for estimating the average treatment effect (ATE), the IPW for average treatment effect of the treated (ATT), the IPW for the average treatment effect of the controls (ATC), the matching weight (MW), the overlap weight (OVERLAP), and the trapezoidal weight (TRAPEZOIDAL). Sandwich variance estimation is provided to adjust for the sampling variability of the estimated propensity score. These methods are discussed by Hirano et al (2003) <DOI:10.1111/1468-0262.00442>, Lunceford and Davidian (2004) <DOI:10.1002/sim.1903>, Li and Greene (2013) <DOI:10.1515/ijb-2012-0030>, and Li et al (2016) <DOI:10.1080/01621459.2016.1260466>.",
    "version": "1.1-3",
    "maintainer": "Huzhang Mao <huzhangmao@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5202,
    "package_name": "PSinference",
    "title": "Inference for Released Plug-in Sampling Single Synthetic Dataset",
    "description": "Considering the singly imputed synthetic data generated via plug-in sampling under the multivariate normal model, draws inference procedures including the generalized variance, the sphericity test, the test for independence between two subsets of variables, and the test for the regression of one set of variables on the other. For more details see Klein et al. (2021) <doi:10.1007/s13571-019-00215-9>.",
    "version": "0.2.2",
    "maintainer": "Ricardo Moura <rp.moura@fct.unl.pt>",
    "url": "https://github.com/ricardomourarpm/PSinference",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5204,
    "package_name": "PSsurvival",
    "title": "Propensity Score Methods for Survival Analysis",
    "description": "Implements propensity score weighting methods for estimating\n    counterfactual survival functions, marginal hazard ratios, and\n    weighted Kaplan-Meier and cumulative risk curves in observational\n    studies with time-to-event outcomes. Supports binary and multiple\n    treatment groups with inverse probability of treatment weighting (IPW),\n    overlap weighting (OW), and average treatment effect on the treated\n    (ATT). Includes symmetric trimming (Crump extension) for extreme\n    propensity scores. Variance estimation via analytical M-estimation or\n    bootstrap. Methods based on Li et al. (2018) <doi:10.1080/01621459.2016.1260466>,\n    Li & Li (2019) <doi:10.1214/19-AOAS1282>, and Cheng et al. (2022)\n    <doi:10.1093/aje/kwac043>.",
    "version": "0.2.0",
    "maintainer": "Chengxin Yang <chengxin.yang@duke.edu>",
    "url": "https://github.com/cxinyang/PSsurvival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5205,
    "package_name": "PSweight",
    "title": "Propensity Score Weighting for Causal Inference with\nObservational Studies and Randomized Trials",
    "description": "Supports propensity score weighting analysis of observational studies and randomized trials. Enables the estimation and inference of average causal effects with binary and multiple treatments using overlap weights (ATO), inverse probability of treatment weights (ATE), average treatment effect among the treated weights (ATT), matching weights (ATM) and entropy weights (ATEN), with and without propensity score trimming. These weights are members of the family of balancing weights introduced in Li, Morgan and Zaslavsky (2018) <doi:10.1080/01621459.2016.1260466> and Li and Li (2019) <doi:10.1214/19-AOAS1282>.",
    "version": "2.1.2",
    "maintainer": "Yukang Zeng <yukang.zeng@yale.edu>",
    "url": "https://github.com/thuizhou/PSweight",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5209,
    "package_name": "PTSR",
    "title": "Positive Time Series Regression",
    "description": "A collection of functions to simulate, estimate and forecast a wide range of regression based dynamic models for positive time series. This package implements the results presented in Prass, T.S.; Pumi, G.; Taufemback, C.G. and Carlos, J.H. (2025). \"Positive time series regression models: theoretical and computational aspects\". Computational Statistics 40, 1185–1215. <doi:10.1007/s00180-024-01531-z>.",
    "version": "0.1.3",
    "maintainer": "Taiane Schaedler Prass <taianeprass@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5218,
    "package_name": "PVAClone",
    "title": "Population Viability Analysis with Data Cloning",
    "description": "Likelihood based population viability analysis in the\n  presence of observation error and missing data.\n  The package can be used to fit, compare, predict,\n  and forecast various growth model types using data cloning.",
    "version": "0.1-8",
    "maintainer": "Peter Solymos <psolymos@gmail.com>",
    "url": "https://github.com/psolymos/PVAClone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5220,
    "package_name": "PVR",
    "title": "Phylogenetic Eigenvectors Regression and Phylogentic\nSignal-Representation Curve",
    "description": "Estimates (and controls for) phylogenetic signal through phylogenetic eigenvectors regression (PVR) and phylogenetic signal-representation (PSR) curve, along with some plot utilities.",
    "version": "0.3",
    "maintainer": "Thiago Santos <thiago.santos@ufvjm.edu.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5221,
    "package_name": "PWEALL",
    "title": "Design and Monitoring of Survival Trials Accounting for Complex\nSituations",
    "description": "Calculates various functions needed for design and monitoring survival trials\n    accounting for complex situations such as delayed treatment effect, treatment crossover, non-uniform accrual,\n    and different censoring distributions between groups. The event time distribution is assumed to be\n    piecewise exponential (PWE) distribution and the entry time is assumed to be piecewise uniform distribution.\n    As compared with Version 1.2.1, two more types of hybrid crossover are added. \n    A bug is corrected in the function \"pwecx\" that calculates the crossover-adjusted survival, distribution, \n    density, hazard and cumulative hazard functions. \n    Also, to generate the crossover-adjusted event time random variable,  a more efficient \n    algorithm is used and the output includes crossover indicators. ",
    "version": "1.3.0.1",
    "maintainer": "Xiaodong Luo <Xiaodong.Luo@sanofi.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5222,
    "package_name": "PWEV",
    "title": "PSO Based Weighted Ensemble Algorithm for Volatility Modelling",
    "description": "Price volatility refers to the degree of variation in series over a certain period of time. This volatility is especially noticeable in agricultural commodities, adding uncertainty for farmers, traders, and others in the agricultural supply chain. Commonly and popularly used four volatility models viz, GARCH, Glosten Jagannatan Runkle-GARCH (GJR-GARCH) model, exponentially weighted moving average (EWMA) model and Multiplicative Error Model (MEM) are selected and implemented. PWAVE, weighted ensemble model based on particle swarm optimization (PSO) is proposed to combine the forecast obtained from all the candidate models. This package has been developed using algorithm of Paul et al. <doi:10.1007/s40009-023-01218-x> and Yeasin and Paul (2024) <doi:10.1007/s11227-023-05542-3>.",
    "version": "0.1.0",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5223,
    "package_name": "PWEXP",
    "title": "Piecewise Exponential Distribution Prediction Model",
    "description": "Build piecewise exponential survival model for study design (planning) and event/timeline prediction. ",
    "version": "0.5.0",
    "maintainer": "Tianchen Xu <zjph602xutianchen@gmail.com>",
    "url": "https://github.com/zjph602xtc/PWEXP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5248,
    "package_name": "PanCanVarSel",
    "title": "Pan-Cancer Variable Selection",
    "description": "Provides function for performing Bayesian survival regression using \n    Horseshoe prior in the accelerated failure time model with log normal assumption \n    in order to achieve high dimensional pan-cancer variable selection as developed in\n    Maity et. al. (2019) <doi:10.1111/biom.13132>.",
    "version": "0.0.3",
    "maintainer": "Arnab Maity <arnab.maity@pfizer.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5249,
    "package_name": "PanJen",
    "title": "A Semi-Parametric Test for Specifying Functional Form",
    "description": "A central decision in a parametric regression is how to specify the relation between an dependent variable and each explanatory variable. This package provides a semi-parametric tool for comparing different transformations of an  explanatory variables in a parametric regression. The functions is relevant in a situation, where you would use a box-cox or Box-Tidwell transformations.  In contrast to the classic power-transformations, the methods in this package allows for  theoretical driven user input and the possibility to compare with a non-parametric transformation.",
    "version": "1.6",
    "maintainer": "Cathrine Ulla Jensen <cuj@ifro.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5252,
    "package_name": "PanelMatch",
    "title": "Matching Methods for Causal Inference with Time-Series\nCross-Sectional Data",
    "description": "Implements a set of methodological tools\n\t     that enable researchers to apply matching methods to\n\t     time-series cross-sectional data. Imai, Kim, and Wang\n\t     (2023) <http://web.mit.edu/insong/www/pdf/tscs.pdf> \n\t     proposes a nonparametric generalization of the\n\t     difference-in-differences estimator, which does not rely\n\t     on the linearity assumption as often done in\n\t     practice. Researchers first select a method of matching\n\t     each treated observation for a given unit in a\n\t     particular time period with control observations from\n\t     other units in the same time period that have a similar\n\t     treatment and covariate history. These methods include\n\t     standard matching methods based on propensity score and\n\t     Mahalanobis distance, as well as weighting methods. Once \n\t     matching and refinement is done,  \n\t     treatment effects can be estimated with \n\t     standard errors. The package also offers diagnostics for researchers to assess the quality \n\t     of their results.",
    "version": "3.1.3",
    "maintainer": "In Song Kim <insong@mit.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5254,
    "package_name": "PanelTM",
    "title": "Two- And Three-Way Dynamic Panel Threshold Regression Model for\nChange Point Detection",
    "description": "Estimation of two- and three-way dynamic panel threshold regression models (Di Lascio and Perazzini (2024) <https://repec.unibz.it/bemps104.pdf>; Di Lascio and Perazzini (2022, ISBN:978-88-9193-231-0); Seo and Shin (2016) <doi:10.1016/j.jeconom.2016.03.005>) through the generalized method of moments based on the first difference transformation and the use of instrumental variables. The models can be used to find a change point detection in the time series. In addition, random number generation is also implemented.",
    "version": "1.0",
    "maintainer": "F. Marta L. Di Lascio <marta.dilascio@unibz.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5265,
    "package_name": "PartCensReg",
    "title": "Estimation and Diagnostics for Partially Linear Censored\nRegression Models Based on Heavy-Tailed Distributions",
    "description": "It estimates the parameters of a partially linear regression censored model via maximum penalized likelihood through of ECME algorithm. The model belong to the semiparametric class, that including a parametric and nonparametric component. The error term considered belongs to the scale-mixture of normal (SMN) distribution, that includes well-known heavy tails distributions as the Student-t distribution, among others. To examine the performance of the fitted model, case-deletion and local influence techniques are provided to show its robust aspect against outlying and influential observations. This work is based in Ferreira, C. S., & Paula, G. A. (2017) <doi:10.1080/02664763.2016.1267124> but considering the SMN family.",
    "version": "1.39",
    "maintainer": "Marcela Nunez Lemus <marcela.nunez.lemus@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5289,
    "package_name": "PenCoxFrail",
    "title": "Regularization in Cox Frailty Models",
    "description": "Different regularization approaches for Cox Frailty Models by penalization methods are provided.\n\tsee Groll et al. (2017) <doi:10.1111/biom.12637> for effects selection.\n\tSee also Groll and Hohberg (2024) <doi:10.1002/bimj.202300020> for classical LASSO approach.",
    "version": "2.0.1",
    "maintainer": "Andreas Groll <groll@statistik.tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5290,
    "package_name": "PenIC",
    "title": "Semiparametric Regression Analysis of Interval-Censored Data\nusing Penalized Splines",
    "description": "Currently incorporate the generalized odds-rate model (a type of linear \n    transformation model) for interval-censored data based on penalized monotonic B-Spline. \n    More methods under other semiparametric models such as cure model or additive model will\n    be included in future versions. For more details see Lu, M., Liu, Y., Li, C. and Sun, J. \n    (2019) <arXiv:1912.11703>.",
    "version": "1.0.0",
    "maintainer": "Yan Liu <yanliuresearch@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5291,
    "package_name": "PenguinR",
    "title": "A Comprehensive Collection of Penguin Datasets for Statistical\nAnalysis and Experimental Design",
    "description": "Offers a comprehensive collection of penguin-related datasets suitable for descriptive statistics, hypothesis testing, and experimental design.\n    Derived from open ecological and biological sources such as Palmer Station studies, the package integrates datasets covering adult morphology, clutch size, blood isotope composition, and heart rate.\n    It is designed for researchers, students, and educators to explore statistical methods including ANOVA, regression, multivariate analysis, and design of experiments in an accessible and reproducible context.",
    "version": "0.1.0",
    "maintainer": "Juan Pablo Vargas Perez <j.pablovargas340@gmail.com>",
    "url": "https://github.com/jpablovargas340/PenguinR,\nhttps://jpablovargas340.github.io/PenguinR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5298,
    "package_name": "PerMat",
    "title": "Performance Metrics in Predictive Modeling",
    "description": "Performance metric provides different performance measures like mean squared error, root mean square error, mean absolute deviation, mean absolute percentage error etc. of a fitted model. These can provide a way for forecasters to quantitatively compare the performance of competing models. For method details see (i) Pankaj Das (2020) <http://krishi.icar.gov.in/jspui/handle/123456789/44138>.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.das2@icar.gov.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5299,
    "package_name": "PerRegMod",
    "title": "Fitting Periodic Coefficients Linear Regression Models",
    "description": "Provides tools for fitting periodic coefficients regression models to data where periodicity plays a crucial role. It allows users to model and analyze relationships between variables that exhibit cyclical or seasonal patterns, offering functions for estimating parameters and testing the periodicity of coefficients in linear regression models. For simple periodic coefficient regression model see Regui et al. (2024) <doi:10.1080/03610918.2024.2314662>.",
    "version": "4.4.3",
    "maintainer": "Slimane Regui <slimaneregui111997@gmail.com>",
    "url": "https://doi.org/10.1080/03610918.2024.2314662",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5305,
    "package_name": "PermAlgo",
    "title": "Permutational Algorithm to Simulate Survival Data",
    "description": "This version of the permutational algorithm generates a\n        dataset in which event and censoring times are conditional on\n        an user-specified list of covariates, some or all of which are\n        time-dependent.",
    "version": "1.2",
    "maintainer": "Marie-Pierre Sylvestre\n<marie-pierre.sylvestre@umontreal.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5317,
    "package_name": "Phase123",
    "title": "Simulating and Conducting Phase 123 Trials",
    "description": "Contains three simulation functions for implementing the entire Phase 123 trial and the separate Eff-Tox and Phase 3 portions of the trial, which may be beneficial for use on clusters. The functions AssignEffTox() and RandomizeEffTox() assign doses to patient cohorts during phase 12 and Reoptimize() determines the optimal dose to continue with during Phase 3. The functions ReturnMeansAgent() and ReturnMeanControl() gives the true mean survival for the agent doses and control and ReturnOCS() gives the operating characteristics of the design.",
    "version": "2.1",
    "maintainer": "Andrew G Chapple <achapp@lsuhsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5319,
    "package_name": "PhaseType",
    "title": "Inference for Phase-Type Distributions",
    "description": "Functions to perform Bayesian inference on absorption time data for\n             Phase-type distributions. The methods of Bladt et al (2003)\n             <doi:10.1080/03461230110106435> and Aslett (2012)\n             <https://www.louisaslett.com/PhD_Thesis.pdf> are provided.",
    "version": "0.3.0",
    "maintainer": "Louis Aslett <louis.aslett@durham.ac.uk>",
    "url": "https://www.louisaslett.com/PhaseType/,\nhttps://github.com/louisaslett/PhaseType",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5327,
    "package_name": "Phenotype",
    "title": "A Tool for Phenotypic Data Processing",
    "description": "Large-scale phenotypic data processing is essential in research. Researchers need to eliminate outliers from the data in order to obtain true and reliable results. Best linear unbiased prediction (BLUP) is a standard method for estimating random effects of a mixed model. This method can be used to process phenotypic data under different conditions and is widely used in animal and plant breeding. The 'Phenotype' can remove outliers from phenotypic data and performs the best linear unbiased prediction (BLUP), help researchers quickly complete phenotypic data analysis. H.P.Piepho. (2008) <doi:10.1007/s10681-007-9449-8>.",
    "version": "0.1.0",
    "maintainer": "Peng Zhao <pengzhao@nwafu.edu.cn>",
    "url": "https://github.com/biozhp/Phenotype",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5347,
    "package_name": "PieceExpIntensity",
    "title": "Bayesian Model to Find Changepoints Based on Rates and Count\nData",
    "description": "This function fits a reversible jump Bayesian piecewise exponential model that also includes the intensity of each event considered along with the rate of events.",
    "version": "1.0.4",
    "maintainer": "Andrew G. Chapple <agc6@rice.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5353,
    "package_name": "PlackettLuce",
    "title": "Plackett-Luce Models for Rankings",
    "description": "Functions to prepare rankings data and fit the Plackett-Luce model\n    jointly attributed to Plackett (1975) <doi:10.2307/2346567> and Luce\n    (1959, ISBN:0486441369). The standard Plackett-Luce model is generalized\n    to accommodate ties of any order in the ranking. Partial rankings, in which\n    only a subset of items are ranked in each ranking, are also accommodated in\n    the implementation. Disconnected/weakly connected networks implied by the\n    rankings may be handled by adding pseudo-rankings with a hypothetical item.\n    Optionally, a multivariate normal prior may be set on the log-worth\n    parameters and ranker reliabilities may be incorporated as proposed by\n    Raman and Joachims (2014) <doi:10.1145/2623330.2623654>. Maximum a\n    posteriori estimation is used when priors are set. Methods are provided to\n    estimate standard errors or quasi-standard errors for inference as well as\n    to fit Plackett-Luce trees. See the package website or vignette for further\n    details.",
    "version": "0.4.4",
    "maintainer": "Heather Turner <ht@heatherturner.net>",
    "url": "https://hturner.github.io/PlackettLuce/,\nhttps://github.com/hturner/PlackettLuce",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5373,
    "package_name": "PoPdesign",
    "title": "Posterior Predictive (PoP) Design for Phase I Clinical Trials",
    "description": "The primary goal of phase I clinical trials is to find the maximum tolerated dose (MTD). To reach this objective, we introduce a new design for phase I clinical trials, the posterior predictive (PoP) design. The PoP design is an innovative model-assisted design that is as simply as the conventional algorithmic designs as its decision rules can be pre-tabulated prior to the onset of trial, but is of more flexibility of selecting diverse target toxicity rates and cohort sizes. The PoP design has desirable properties, such as coherence and consistency. Moreover, the PoP design provides better empirical performance than the BOIN and Keyboard design with respect to high average probabilities of choosing the MTD and slightly lower risk of treating patients at subtherapeutic or overly toxic doses. ",
    "version": "1.1.0",
    "maintainer": "Xinying Fang <xf.research@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5374,
    "package_name": "PoSI",
    "title": "Valid Post-Selection Inference for Linear LS Regression",
    "description": "\n  In linear LS regression, calculate for a given design matrix\n  the multiplier K of coefficient standard errors such that the\n  confidence intervals [b - K*SE(b), b + K*SE(b)] have a\n  guaranteed coverage probability for all coefficient estimates\n  b in any submodels after performing arbitrary model selection.",
    "version": "1.1",
    "maintainer": "Wan Zhang <wanz63@live.unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5375,
    "package_name": "PoSIAdjRSquared",
    "title": "Post-Selection Inference for Adjusted R Squared",
    "description": "Conduct post-selection inference for regression coefficients in linear models after they have been selected by adjusted R squared. The p-values and confidence intervals are valid after model selection with the same data. This allows the user to use all data for both model selection and inference without losing control over the type I error rate. The provided tests are more powerful than data splitting, which bases inference on less data since it discards all information used for selection.",
    "version": "0.1.0",
    "maintainer": "Sarah Pirenne <sarah.pirenne@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5378,
    "package_name": "PointFore",
    "title": "Interpretation of Point Forecasts as State-Dependent Quantiles\nand Expectiles",
    "description": "Estimate specification models for the state-dependent level of an optimal quantile/expectile forecast.\n  Wald Tests and the test of overidentifying restrictions are implemented. Plotting of the estimated specification model is possible.\n  The package contains two data sets with forecasts and realizations: the daily accumulated precipitation at London, UK from the high-resolution model of the\n  European Centre for Medium-Range Weather Forecasts (ECMWF, <https://www.ecmwf.int/>) and GDP growth Greenbook data by the US Federal Reserve.\n  See Schmidt, Katzfuss and Gneiting (2015) <arXiv:1506.01917> for more details on the identification and estimation of a directive behind a point forecast.",
    "version": "0.2.0",
    "maintainer": "Patrick Schmidt <pschmidte@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5402,
    "package_name": "PoolDilutionR",
    "title": "Calculate Gross Biogeochemical Flux Rates from Isotope Pool\nDilution Data",
    "description": "Pool dilution is a isotope tracer technique wherein a \n    biogeochemical pool is artifically enriched with its heavy isotopologue \n    and the gross productive and consumptive fluxes of that pool are \n    quantified by the change in pool size and isotopic composition over time. \n    This package calculates gross production and consumption rates from\n    closed-system isotopic pool dilution time series data. Pool size \n    concentrations and heavy isotope (e.g., 15N) content are measured over time \n    and the model optimizes production rate (P) and the first order rate \n    constant (k) by minimizing error in the model-predicted total pool size, \n    as well as the isotopic signature. The model optimizes rates by weighting \n    information against the signal:noise ratio of concentration and heavy-\n    isotope signatures using measurement precision as well as the magnitude of \n    change over time. The calculations used here are based on von Fischer and \n    Hedin (2002) <doi:10.1029/2001GB001448> with some modifications.",
    "version": "1.0.0",
    "maintainer": "Kendalynn A. Morris <kendalynn.morris@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5406,
    "package_name": "PopComm",
    "title": "Population-Level Cell-Cell Communication Analysis Tools",
    "description": "Facilitates population-level analysis of ligand-receptor (LR) interactions \n    using large-scale single-cell transcriptomic data. Identifies significant LR pairs \n    and quantifies their interactions through correlation-based filtering and projection score \n    computations. Designed for large-sample single-cell studies, the package employs \n    statistical modeling, including linear regression, to investigate LR relationships \n    between cell types. It provides a systematic framework for understanding cell-cell \n    communication, uncovering regulatory interactions and signaling mechanisms. \n    Offers tools for LR pair-level, sample-level, and differential interaction analyses, \n    with comprehensive visualization support to aid biological interpretation. \n    The methodology is described in a manuscript currently under review and will be \n    referenced here once published or publicly available.",
    "version": "1.0.0",
    "maintainer": "Zheng Gong <gongzheng131@hotmail.com>",
    "url": "https://github.com/JusticeGO/PopComm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5412,
    "package_name": "PopulationGrowthR",
    "title": "Linear Population Growth Scenarios",
    "description": "Fit linear splines to species time series to detect population growth scenarios based on Hyndman, R J and Mesgaran, M B and Cousens, R D (2015) <doi:10.1007/s10530-015-0962-8>.",
    "version": "0.1.1",
    "maintainer": "Biman Chakraborty <biman_c@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5413,
    "package_name": "PortalHacienda",
    "title": "Acceder Con R a Los Datos Del Portal De Hacienda",
    "description": "Obtener listado de datos, acceder y extender series del Portal de \n    Datos de Hacienda.Las proyecciones se realizan con 'forecast', \n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>. \n    Search, download and forecast time-series from the Ministry of Economy \n    of Argentina. Forecasts are built with the 'forecast' package, \n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>. ",
    "version": "0.1.7",
    "maintainer": "Fernando Garcia Diaz <fmgarciadiaz78@gmail.com>",
    "url": "https://github.com/fmgarciadiaz/PortalHacienda-CRAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5418,
    "package_name": "PosiR",
    "title": "Post-Selection Inference via Simultaneous Confidence Intervals",
    "description": "Post-selection inference in linear regression models,\n    constructing simultaneous confidence intervals across a user-specified universe of models.\n    Implements the methodology described in Kuchibhotla, Kolassa, and Kuffner (2022) \"Post-Selection Inference\"\n    <doi:10.1146/annurev-statistics-100421-044639> to ensure valid inference after model\n    selection, with applications in high-dimensional settings like Lasso selection.",
    "version": "0.1.2",
    "maintainer": "Henry Chukwuma <chukyhenry55@gmail.com>",
    "url": "https://github.com/Chukyhenry/PosiR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5422,
    "package_name": "PowRPriori",
    "title": "Power Analysis via Data Simulation for (Generalized) Linear\nMixed Effects Models",
    "description": "Conduct a priori power analyses via Monte-Carlo style data simulation for linear and generalized linear mixed-effects models (LMMs/GLMMs). Provides a user-friendly workflow with helper functions to easily define fixed and random effects as well as diagnostic functions to evaluate the adequacy of the results of the power analysis.",
    "version": "0.1.1",
    "maintainer": "Markus Grill <markus.grill@uni-wh.de>",
    "url": "https://github.com/mirgll/PowRPriori",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5442,
    "package_name": "PredictorSelect",
    "title": "Out-of-Sample Predictability in Predictive Regressions with Many\nPredictor Candidates",
    "description": "Consider a linear predictive regression setting with a potentially large set of candidate predictors. This work is concerned with detecting the presence of out of sample predictability based on out of sample mean squared error comparisons given in Gonzalo and Pitarakis (2023) <doi:10.1016/j.ijforecast.2023.10.005>.",
    "version": "0.1.0",
    "maintainer": "Rong Peng <r.peng@soton.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5454,
    "package_name": "ProSGPV",
    "title": "Penalized Regression with Second-Generation P-Values",
    "description": "Implementation of penalized regression with second-generation p-values for variable\n    selection. The algorithm can handle linear regression, GLM, and Cox regression. S3 methods print(), summary(), coef(), predict(), and plot() are available for the algorithm. Technical details\n    can be found at Zuo et al. (2021) <doi:10.1080/00031305.2021.1946150>. ",
    "version": "1.0.0",
    "maintainer": "Yi Zuo <yi.zuo@vanderbilt.edu>",
    "url": "https://github.com/zuoyi93/ProSGPV",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5458,
    "package_name": "ProbBayes",
    "title": "Probability and Bayesian Modeling",
    "description": "Functions and datasets  to accompany J. Albert and J. Hu, \"Probability and Bayesian Modeling\", CRC Press, (2019, ISBN: 1138492566).",
    "version": "1.1",
    "maintainer": "Jim Albert <albert@bgsu.edu>",
    "url": "https://github.com/bayesball/ProbBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5459,
    "package_name": "ProbBreed",
    "title": "Probability Theory for Selecting Candidates in Plant Breeding",
    "description": "Use probability theory under the Bayesian framework for calculating the risk of selecting candidates in a multi-environment context. Contained are functions used to fit a Bayesian multi-environment model (based on the available presets), extract posterior values and maximum posterior values, compute the variance components, check the model’s convergence, and calculate the probabilities. For both across and within-environments scopes, the package computes the probability of superior performance and the pairwise probability of superior performance. Furthermore, the probability of superior stability and the pairwise probability of superior stability across environments is estimated. A joint probability of superior performance and stability is also provided. ",
    "version": "1.0.4.9",
    "maintainer": "Saulo Chaves <saulochaves@usp.br>",
    "url": "https://github.com/saulo-chaves/ProbBreed,\nhttps://saulo-chaves.github.io/ProbBreed_site/,\nhttps://saulo-chaves.github.io/ProbBreed/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5460,
    "package_name": "ProbMarg",
    "title": "Computing Logit & Probit Predicted Probabilities & Marginal\nEffects",
    "description": "Computes predicted probabilities and marginal effects for \n  binary & ordinal logit and probit, (partial) generalized ordinal & \n  multinomial logit models estimated with the glm(), clm() (in the \n  'ordinal' package), and vglm() (in the 'VGAM' package) functions.",
    "version": "1.0.1",
    "maintainer": "Tim Liao <tfliao@illinois.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5483,
    "package_name": "ProteoBayes",
    "title": "Bayesian Statistical Tools for Quantitative Proteomics",
    "description": "Bayesian toolbox for quantitative proteomics. In particular, this \n    package provides functions to generate synthetic datasets, execute Bayesian\n    differential analysis methods, and display results as, described in the \n    associated article Marie Chion and Arthur Leroy (2023) <arXiv:2307.08975>.",
    "version": "1.0.0",
    "maintainer": "Arthur Leroy <arthur.leroy.pro@gmail.com>",
    "url": "https://mariechion.github.io/ProteoBayes/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5488,
    "package_name": "Pstat",
    "title": "Assessing Pst Statistics",
    "description": "Calculating Pst values to assess differentiation among populations from a set of quantitative traits is the primary purpose of such a package. The bootstrap method provides confidence intervals and distribution histograms of Pst. Variations of Pst in function of the parameter c/h^2 are studied as well. Finally, the package proposes different transformations especially to eliminate any variation resulting from allometric growth (calculation of residuals from linear regressions, Reist standardizations or Aitchison transformation).",
    "version": "1.2",
    "maintainer": "Blondeau Da Silva Stephane <blondeaudasilva@xlim.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5505,
    "package_name": "Pv3Rs",
    "title": "Estimate the Cause of Recurrent Vivax Malaria using Genetic Data",
    "description": "Plot malaria parasite genetic data on two or more episodes. \n    Compute per-person posterior probabilities that each\n    Plasmodium vivax (Pv) recurrence is a recrudescence, relapse,\n    or reinfection (3Rs) using per-person P. vivax genetic data on two or\n    more episodes and a statistical model described in \n    Taylor, Foo and White (2022) <doi:10.1101/2022.11.23.22282669>. \n    Plot per-recurrence posterior probabilities. ",
    "version": "1.0.0",
    "maintainer": "Aimee Taylor <aimee.taylor@pasteur.fr>",
    "url": "https://aimeertaylor.github.io/Pv3Rs/,\nhttps://github.com/aimeertaylor/Pv3Rs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5508,
    "package_name": "PwePred",
    "title": "Event/Timeline Prediction Model Based on Piecewise Exponential",
    "description": "Efficient algorithm for estimating piecewise exponential hazard models for right-censored data, and is useful for reliable power calculation, study design, and event/timeline prediction for study monitoring. ",
    "version": "1.0.0",
    "maintainer": "Tianchen Xu <zjph602xutianchen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5533,
    "package_name": "QGglmm",
    "title": "Estimate Quantitative Genetics Parameters from Generalised\nLinear Mixed Models",
    "description": "Compute various quantitative genetics parameters from a Generalised Linear Mixed Model (GLMM) estimates. Especially, it yields the observed phenotypic mean, phenotypic variance and additive genetic variance.",
    "version": "0.8.0",
    "maintainer": "Pierre de Villemereuil <pierre.de-villemereuil@mnhn.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5543,
    "package_name": "QRIpkg",
    "title": "Quantile Regression Index Score",
    "description": "The QRI_func() function performs quantile regression analysis using age and sex as predictors to calculate the Quantile Regression Index (QRI) score for each individual’s regional brain imaging metrics and then averages across the regional scores to generate an average tissue specific score for each subject. The QRI_plot() is used to plot QRI and generate the normative curves for individual measurements.",
    "version": "0.2.2",
    "maintainer": "Si Gao <sgao@som.umaryland.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5545,
    "package_name": "QRank",
    "title": "A Novel Quantile Regression Approach for eQTL Discovery",
    "description": "A Quantile Rank-score based test for the identification of expression quantitative trait loci.",
    "version": "1.0",
    "maintainer": "Xiaoyu Song <xs2148@cumc.columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5548,
    "package_name": "QTE.RD",
    "title": "Quantile Treatment Effects under the Regression Discontinuity\nDesign",
    "description": "Provides comprehensive methods for testing, estimating, and conducting uniform inference on quantile treatment effects (QTEs) in sharp regression discontinuity (RD) designs, incorporating covariates and implementing robust bias correction methods of Qu, Yoon, Perron (2024) <doi:10.1162/rest_a_01168>.",
    "version": "1.2.0",
    "maintainer": "Jungmo Yoon <jmyoon@hanyang.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5554,
    "package_name": "QTOCen",
    "title": "Quantile-Optimal Treatment Regimes with Censored Data",
    "description": "Provides methods for estimation of mean- and quantile-optimal treatment regimes from censored data. \n    Specifically, we have developed distinct functions for three types of right censoring for static treatment using quantile criterion: (1) independent/random censoring, (2) treatment-dependent random censoring, and (3) covariates-dependent random censoring. It also includes a function to estimate quantile-optimal dynamic treatment regimes for independent censored data. Finally, this package also includes a simulation data generative model of a dynamic treatment experiment proposed in literature.",
    "version": "0.1.1",
    "maintainer": "Yu Zhou <zhou0269@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5555,
    "package_name": "QUALYPSO",
    "title": "Partitioning Uncertainty Components of an Incomplete Ensemble of\nClimate Projections",
    "description": "These functions use data augmentation and Bayesian techniques for the assessment of single-member and incomplete ensembles of climate projections. It provides unbiased estimates of climate change responses of all simulation chains and of all uncertainty variables. It additionally propagates uncertainty due to missing information in the estimates.\n  - Evin, G., B. Hingray, J. Blanchet, N. Eckert, S. Morin, and D. Verfaillie. (2019) <doi:10.1175/JCLI-D-18-0606.1>.",
    "version": "2.3",
    "maintainer": "Guillaume Evin <guillaume.evin@inrae.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5557,
    "package_name": "QWDAP",
    "title": "Quantum Walk-Based Data Analysis and Prediction",
    "description": "The modeling and prediction of graph-associated time series(GATS) based on continuous time quantum walk. This software is mainly used for feature extraction, modeling, prediction and result evaluation of GATS, including continuous time quantum walk simulation, feature selection, regression analysis, time series prediction, and series fit calculation. A paper is attached to the package for reference.",
    "version": "1.1.20",
    "maintainer": "Binghuang Pan <bright1up@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5559,
    "package_name": "Qapprox",
    "title": "Approximation to the Survival Functions of Quadratic Forms of\nGaussian Variables",
    "description": "Calculates the right-tail probability of quadratic forms of Gaussian variables using the skewness-kurtosis ratio matching method, modified Liu-Tang-Zhang method and Satterthwaite-Welch method. The technical details can be found in Hong Zhang, Judong Shen and Zheyang Wu (2020) <arXiv:2005.00905>.",
    "version": "0.2.0",
    "maintainer": "Hong Zhang <hzhang@wpi.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5560,
    "package_name": "Qest",
    "title": "Quantile-Based Estimator",
    "description": "Quantile-based estimators (Q-estimators) can be used to fit any parametric distribution, using its quantile function. Q-estimators are usually more robust than standard maximum likelihood estimators. The method is described in: Sottile G. and Frumento P. (2022). Robust estimation and regression with parametric quantile functions. <doi:10.1016/j.csda.2022.107471>.",
    "version": "1.0.2",
    "maintainer": "Gianluca Sottile <gianluca.sottile@unipa.it>",
    "url": "https://www.sciencedirect.com/science/article/abs/pii/S0167947322000512",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5576,
    "package_name": "Quandl",
    "title": "API Wrapper for Quandl.com",
    "description": "Functions for interacting directly with the Quandl API to offer data in a number of formats usable in R, downloading a zip with all data from a Quandl database, and the ability to search. This R package uses the Quandl API. For more information go to <https://docs.quandl.com>. For more help on the package itself go to <https://www.quandl.com/tools/r>.",
    "version": "2.11.0",
    "maintainer": "Dave Dotson <dave@quandl.com>",
    "url": "https://github.com/quandl/quandl-r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5580,
    "package_name": "QuantRegGLasso",
    "title": "Adaptively Weighted Group Lasso for Semiparametric Quantile\nRegression Models",
    "description": "Implements an adaptively weighted group Lasso procedure for simultaneous variable selection and structure identification in varying\n  coefficient quantile regression models and additive quantile regression models with ultra-high dimensional covariates. The methodology, grounded\n  in a strong sparsity condition, establishes selection consistency under certain weight conditions. To address the challenge of tuning parameter \n  selection in practice, a BIC-type criterion named high-dimensional information criterion (HDIC) is proposed. The Lasso procedure, guided by\n  HDIC-determined tuning parameters, maintains selection consistency. Theoretical findings are strongly supported by simulation studies.\n  (Toshio Honda, Ching-Kang Ing, Wei-Ying Wu, 2019, <DOI:10.3150/18-BEJ1091>).",
    "version": "1.0.1",
    "maintainer": "Wen-Ting Wang <egpivo@gmail.com>",
    "url": "https://github.com/egpivo/QuantRegGLasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5592,
    "package_name": "QurvE",
    "title": "Robust and User-Friendly Analysis of Growth and Fluorescence\nCurves",
    "description": "High-throughput analysis of growth curves and fluorescence\n    data using three methods: linear regression, growth model fitting, and\n    smooth spline fit. Analysis of dose-response relationships via\n    smoothing splines or dose-response models. Complete data analysis\n    workflows can be executed in a single step via user-friendly wrapper\n    functions. The results of these workflows are summarized in detailed\n    reports as well as intuitively navigable 'R' data containers. A 'shiny'\n    application provides access to all features without\n    requiring any programming knowledge. The package is described in further\n    detail in Wirth et al. (2023) <doi:10.1038/s41596-023-00850-7>.",
    "version": "1.1.2",
    "maintainer": "Nicolas T. Wirth <mail.nicowirth@gmail.com>",
    "url": "https://github.com/NicWir/QurvE, https://nicwir.github.io/QurvE/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5606,
    "package_name": "R1magic",
    "title": "Compressive Sampling: Sparse Signal Recovery Utilities",
    "description": "Utilities for sparse signal recovery suitable for compressed sensing. L1, L2 and TV penalties, DFT basis matrix, simple sparse signal generator, mutual cumulative coherence between two matrices and examples, Lp complex norm, scaling back regression coefficients.",
    "version": "0.3.4",
    "maintainer": "Mehmet Suzen <mehmet.suzen@physics.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5607,
    "package_name": "R2Addhaz",
    "title": "R2 Measure of Explained Variation under the Additive Hazards\nModel",
    "description": "R^2 measure of explained variation under the semiparametric additive hazards model is estimated. The measure can be used as a measure of predictive capability and therefore it can be adopted in model selection process. Rava, D. and Xu, R. (2020) <arXiv:2003.09460>.",
    "version": "0.1.0",
    "maintainer": "Denise Rava <drava@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5609,
    "package_name": "R2BayesX",
    "title": "Estimate Structured Additive Regression Models with 'BayesX'",
    "description": "An R interface to estimate structured additive regression (STAR) models with 'BayesX'.",
    "version": "1.1-6",
    "maintainer": "Nikolaus Umlauf <Nikolaus.Umlauf@uibk.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5610,
    "package_name": "R2D2ordinal",
    "title": "Implements Pseudo-R2D2 Prior for Ordinal Regression",
    "description": "Implements the pseudo-R2D2 prior for ordinal regression from the paper \"Pseudo-R2D2 prior for high-dimensional ordinal regression\" by Yanchenko (2025) <doi:10.1007/s11222-025-10667-x>. In particular, it provides code to evaluate the probability distribution function for the cut-points, compute the log-likelihood, calculate the hyper-parameters for the global variance parameter, find the distribution of McFadden's coefficient-of-determination, and fit the model in 'rstan'. Please cite the paper if you use these codes.",
    "version": "1.0.2",
    "maintainer": "Eric Yanchenko <eyanchenko@aiu.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5617,
    "package_name": "R2WinBUGS",
    "title": "Running 'WinBUGS' and 'OpenBUGS' from 'R' / 'S-PLUS'",
    "description": "Invoke a 'BUGS' model in 'OpenBUGS' or 'WinBUGS', a class \"bugs\" for 'BUGS' \n  results and functions to work with that class.\n  Function write.model() allows a 'BUGS' model file to be written.  \n  The class and auxiliary functions could be used with other MCMC programs, including 'JAGS'.\n  The suggested package 'BRugs' (only needed for function openbugs()) is only available from the CRAN archives, \n  see <https://cran.r-project.org/package=BRugs>.",
    "version": "2.1-23",
    "maintainer": "Uwe Ligges <ligges@statistik.tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5619,
    "package_name": "R2jags",
    "title": "Using R to Run 'JAGS'",
    "description": "Providing wrapper functions to implement Bayesian analysis in JAGS.  Some major features include monitoring convergence of a MCMC model using Rubin and Gelman Rhat statistics, automatically running a MCMC model till it converges, and implementing parallel processing of a MCMC model for multiple chains.",
    "version": "0.8-9",
    "maintainer": "Yu-Sung Su <suyusung@tsinghua.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5648,
    "package_name": "RANSAC",
    "title": "Robust Model Fitting Using the RANSAC Algorithm",
    "description": "Provides tools for robust regression model fitting using the RANSAC (Random Sample Consensus) algorithm. RANSAC is an iterative method to estimate parameters of a model from a dataset that contains outliers. This package allows fitting both linear lm and nonlinear nls models using RANSAC, helping users obtain more reliable models in the presence of noisy or corrupted data. The methods are particularly useful in contexts where traditional least squares regression fails due to the influence of outliers. Implementations include support for performance metrics such as RMSE, MAE, and R² based on the inlier subset. For further details, see Fischler and Bolles (1981) <doi:10.1145/358669.358692>.",
    "version": "0.1.0",
    "maintainer": "Jadson Abreu <jadson.ap@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5654,
    "package_name": "RARtrials",
    "title": "Response-Adaptive Randomization in Clinical Trials",
    "description": "Some response-adaptive randomization methods commonly found in literature are included in this package. These methods include the randomized play-the-winner rule for binary endpoint (Wei and Durham (1978) <doi:10.2307/2286290>), the doubly adaptive biased coin design with minimal variance strategy for binary endpoint (Atkinson and Biswas (2013) <doi:10.1201/b16101>, Rosenberger and Lachin (2015) <doi:10.1002/9781118742112>) and maximal power strategy targeting Neyman allocation for binary endpoint (Tymofyeyev, Rosenberger, and Hu (2007) <doi:10.1198/016214506000000906>) and RSIHR allocation with each letter representing the first character of the names of the individuals who first proposed this rule (Youngsook and Hu (2010) <doi:10.1198/sbr.2009.0056>, Bello and Sabo (2016) <doi:10.1080/00949655.2015.1114116>), A-optimal Allocation for continuous endpoint (Sverdlov and Rosenberger (2013) <doi:10.1080/15598608.2013.783726>), Aa-optimal Allocation for continuous endpoint (Sverdlov and Rosenberger (2013) <doi:10.1080/15598608.2013.783726>), generalized RSIHR allocation for continuous endpoint (Atkinson and Biswas (2013) <doi:10.1201/b16101>), Bayesian response-adaptive randomization with a control group using the Thall \\& Wathen method for binary and continuous endpoints (Thall and Wathen (2007) <doi:10.1016/j.ejca.2007.01.006>) and the forward-looking Gittins index rule for binary and continuous endpoints (Villar, Wason, and Bowden (2015) <doi:10.1111/biom.12337>, Williamson and Villar (2019) <doi:10.1111/biom.13119>).",
    "version": "0.0.2",
    "maintainer": "Chuyao Xu  <cxu870@aucklanduni.ac.nz>",
    "url": "https://github.com/yayayaoyaoyao/RARtrials",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5656,
    "package_name": "RATest",
    "title": "Randomization Tests",
    "description": "A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter.",
    "version": "0.1.10",
    "maintainer": "Mauricio Olivares <mau.olivarego@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5664,
    "package_name": "RBE3",
    "title": "Estimation and Additional Tools for Quantile Generalized Beta\nRegression Model",
    "description": "Provide estimation and data generation tools for the quantile generalized beta \n        regression model. For details, see Bourguignon, Gallardo and Saulo <arXiv:2110.04428>\n        The package also provides tools to perform covariates selection.",
    "version": "1.1",
    "maintainer": "Diego Gallardo <dgallardo@ubiobio.cl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5670,
    "package_name": "RBaM",
    "title": "Bayesian Modeling: Estimate a Computer Model and Make Uncertain\nPredictions",
    "description": "An interface to the 'BaM' (Bayesian Modeling) engine, \n    a 'Fortran'-based executable aimed at estimating a model \n    with a Bayesian approach and using it for prediction,\n    with a particular focus on uncertainty quantification.\n    Classes are defined for the various building blocks of \n    'BaM' inference (model, data, error models, Markov Chain Monte Carlo (MCMC) samplers, predictions).\n    The typical usage is as follows:\n    (1) specify the model to be estimated; \n    (2) specify the inference setting (dataset, parameters, error models...);\n    (3) perform Bayesian-MCMC inference;\n    (4) read, analyse and use MCMC samples;\n    (5) perform prediction experiments.\n    Technical details are available (in French) in \n    Renard (2017) <https://hal.science/hal-02606929v1>.\n    Examples of applications include \n    Mansanarez et al. (2019) <doi:10.1029/2018WR023389>,\n    Le Coz et al. (2021) <doi:10.1002/hyp.14169>,\n    Perret et al. (2021) <doi:10.1029/2020WR027745>,\n    Darienzo et al. (2021) <doi:10.1029/2020WR028607> and\n    Perret et al. (2023) <doi:10.1061/JHEND8.HYENG-13101>.",
    "version": "1.1.2",
    "maintainer": "Benjamin Renard <benjamin.renard@inrae.fr>",
    "url": "https://github.com/BaM-tools/RBaM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5672,
    "package_name": "RBesT",
    "title": "R Bayesian Evidence Synthesis Tools",
    "description": "Tool-set to support Bayesian evidence synthesis.  This",
    "version": "1.8-2",
    "maintainer": "",
    "url": "https://github.com/Novartis/RBesT",
    "exports": [],
    "topics": ["bayesian", "clinical", "historical-data", "meta-analysis"],
    "score": "NA",
    "stars": 25
  },
  {
    "id": 5675,
    "package_name": "RBtest",
    "title": "Regression-Based Approach for Testing the Type of Missing Data",
    "description": "The regression-based (RB) approach is a method to test the missing data mechanism.\n\t\t\tThis package contains two functions that test the type of missing data (Missing Completely \n\t\t\tAt Random vs Missing At Random) on the basis of the RB approach. The first function applies \n\t\t\tthe RB approach independently on each variable with missing data, using the completely \n\t\t\tobserved variables only. The second function tests the missing data mechanism globally \n\t\t\t(on all variables with missing data) with the use of all available information. The \n\t\t\talgorithm is adapted both to continuous and categorical data. ",
    "version": "1.1",
    "maintainer": "Serguei Rouzinov <rouzinovs@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5688,
    "package_name": "RCMinification",
    "title": "Random Coefficient Minification Time Series Models",
    "description": "Data sets, and functions for simulating and fitting nonlinear time series with minification and nonparametric models.",
    "version": "1.2",
    "maintainer": "L. Han <lengyi.han@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5696,
    "package_name": "RCT",
    "title": "Assign Treatments, Power Calculations, Balances, Impact\nEvaluation of Experiments",
    "description": "Assists in the whole process of designing and evaluating Randomized Control Trials.\n    Robust treatment assignment by strata/blocks, that handles misfits; \n    Power calculations of the minimum detectable treatment effect or minimum populations;\n    Balance tables of T-test of covariates; \n    Balance Regression: (treatment ~ all x variables) with F-test of null model; \n    Impact_evaluation: Impact evaluation regressions. This function\n    gives you the option to include control_vars, fixed effect variables,\n    cluster variables (for robust SE), multiple endogenous variables and\n    multiple heterogeneous variables (to test treatment effect heterogeneity)\n    summary_statistics: Function that creates a summary statistics table with statistics \n    rank observations in n groups: Creates a factor variable with n groups. Each group has \n    a min and max label attach to each category.\n    Athey, Susan, and Guido W. Imbens (2017) <arXiv:1607.00698>.",
    "version": "1.2",
    "maintainer": "Isidoro Garcia-Urquieta <isidoro.gu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5705,
    "package_name": "RChronoModel",
    "title": "Post-Processing of the Markov Chain Simulated by ChronoModel or\nOxcal",
    "description": "Provides a list of functions for the statistical analysis and the post-processing of the Markov Chains simulated by ChronoModel (see <http://www.chronomodel.fr> for more information).  ChronoModel is a friendly software to construct a chronological model in a Bayesian framework.  Its output is  a sampled Markov chain from the posterior distribution of dates component  the chronology. The functions can also be applied  to the  analyse of mcmc output generated by Oxcal software.",
    "version": "0.4",
    "maintainer": "Anne Philippe <anne.philippe@univ-nantes.fr>",
    "url": "http://www.chronomodel.fr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5708,
    "package_name": "RClimacell",
    "title": "R Wrapper for the 'Climacell' API",
    "description": "'Climacell' is a weather platform that provides hyper-local forecasts and weather \n    data. This package enables the user to query the core layers of the \n    time line interface of the 'Climacell' v4 API <https://www.climacell.co/weather-api/>. \n    This package requires a valid API key. See vignettes for instructions on use.",
    "version": "0.1.4",
    "maintainer": "Nikhil Agarwal <gitnik@niks.me>",
    "url": "https://nikdata.github.io/RClimacell/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5734,
    "package_name": "REBayes",
    "title": "Empirical Bayes Estimation and Inference",
    "description": "Kiefer-Wolfowitz maximum likelihood estimation for mixture models\n    and some other density estimation and regression methods based on convex\n    optimization.  See Koenker and Gu (2017) REBayes: An R Package for Empirical\n    Bayes Mixture Methods, Journal of Statistical Software, 82, 1--26, \n    <DOI:10.18637/jss.v082.i08>.",
    "version": "2.59",
    "maintainer": "Roger Koenker <rkoenker@uiuc.edu>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5743,
    "package_name": "REEMtree",
    "title": "Regression Trees with Random Effects for Longitudinal (Panel)\nData",
    "description": "A data mining approach for longitudinal and clustered data, \n        which combines the structure of mixed effects model with tree-based \n        estimation methods. See Sela, R.J. and Simonoff, J.S. (2012) RE-EM \n        trees: a data mining approach for longitudinal and clustered data \n        <doi:10.1007/s10994-011-5258-3>.",
    "version": "0.90.6",
    "maintainer": "Wenbo Jing <wj2093@stern.nyu.edu>",
    "url": "http://pages.stern.nyu.edu/~jsimonof/REEMtree/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5749,
    "package_name": "REN",
    "title": "Regularization Ensemble for Robust Portfolio Optimization",
    "description": "Portfolio optimization is achieved through a combination of regularization techniques and ensemble methods that are designed to generate stable out-of-sample return predictions, particularly in the presence of strong correlations among assets. The package includes functions for data preparation, parallel processing, and portfolio analysis using methods such as Mean-Variance, James-Stein, LASSO, Ridge Regression, and Equal Weighting. It also provides visualization tools and performance metrics, such as the Sharpe ratio, volatility, and maximum drawdown, to assess the results.",
    "version": "0.1.0",
    "maintainer": "Bonsoo Koo <bonsoo.koo@monash.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5753,
    "package_name": "REPS",
    "title": "Hedonic and Multilateral Index Methods for Real Estate Price\nStatistics",
    "description": "Compute price indices using various Hedonic and\n    multilateral methods, including Laspeyres, Paasche, Fisher, and HMTS (Hedonic\n    Multilateral Time series re-estimation with splicing). The central function\n    calculate_price_index() offers a unified interface for running these methods\n    on structured datasets. This package is designed to support index construction workflows across a wide range of domains \n    — including but not limited to real estate — where quality-adjusted price comparisons over time are essential. \n    The development of this package was funded by Eurostat and Statistics Netherlands (CBS), and carried out by Statistics Netherlands.\n    The HMTS method implemented here is described in Ishaak, Ouwehand and Remøy (2024)\n    <doi:10.1177/0282423X241246617>. For broader methodological context, see Eurostat\n    (2013, ISBN:978-92-79-25984-5, <doi:10.2785/34007>).",
    "version": "1.0.0",
    "maintainer": "Vivek Gajadhar <v.gajadhar@cbs.nl>",
    "url": "https://github.com/vivekag7/REPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5779,
    "package_name": "RGAP",
    "title": "Production Function Output Gap Estimation",
    "description": "The output gap indicates the percentage difference between the actual output of an economy and its potential. Since potential output is a latent process, the estimation of the output gap poses a challenge and numerous filtering techniques have been proposed. 'RGAP' facilitates the estimation of a Cobb-Douglas production function type output gap, as suggested by the European Commission (Havik et al. 2014) <https://ideas.repec.org/p/euf/ecopap/0535.html>. To that end, the non-accelerating wage rate of unemployment (NAWRU) and the trend of total factor productivity (TFP) can be estimated in two bivariate unobserved component models by means of Kalman filtering and smoothing. 'RGAP' features a flexible modeling framework for the appropriate state-space models and offers frequentist as well as Bayesian estimation techniques. Additional functionalities include direct access to the 'AMECO' <https://economy-finance.ec.europa.eu/economic-research-and-databases/economic-databases/ameco-database_en> database and automated model selection procedures. See the paper by Streicher (2022) <http://hdl.handle.net/20.500.11850/552089> for details. ",
    "version": "0.1.1",
    "maintainer": "Sina Streicher <streicher@kof.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5783,
    "package_name": "RGE",
    "title": "Response from Genotype to Environment",
    "description": "Compute yield-stability index based on Bayesian methodology, which is useful for analyze multi-environment trials in plant breeding programs. References: Cotes Torres JM, Gonzalez Jaimes EP, and Cotes Torres A (2016) <https://revistas.unimilitar.edu.co/index.php/rfcb/article/view/2037> Seleccion de Genotipos con Alta Respuesta y Estabilidad Fenotipica en Pruebas Regionales: Recuperando el Concepto Biologico. ",
    "version": "1.0",
    "maintainer": "Jose Miguel Cotes Torres <jmcotes@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5784,
    "package_name": "RGENERATE",
    "title": "Tools to Generate Vector Time Series",
    "description": "A method 'generate()' is implemented in this package for the random\n    generation of vector time series according to models obtained by 'RMAWGEN',\n    'vars' or other packages.  This package was created to generalize the\n    algorithms of the 'RMAWGEN' package for the analysis and generation of any\n    environmental vector time series.",
    "version": "1.3.8",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "url": "https://github.com/ecor/RGENERATE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5794,
    "package_name": "RGeode",
    "title": "Geometric Density Estimation",
    "description": "Provides the hybrid Bayesian method Geometric Density Estimation. On the one hand, it scales the dimension of our data, on the other it performs inference. The method is fully described in the paper \"Scalable Geometric Density Estimation\" by Y. Wang, A. Canale, D. Dunson (2016) <http://proceedings.mlr.press/v51/wang16e.pdf>.                   ",
    "version": "0.1.0",
    "maintainer": "Lorenzo Rimella <lorenzo.rimella@hotmail.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5809,
    "package_name": "RHawkes",
    "title": "Renewal Hawkes Process",
    "description": "The renewal Hawkes (RHawkes) process (Wheatley,\n    Filimonov, and Sornette, 2016 <doi:10.1016/j.csda.2015.08.007>) is\n    an extension to the classical Hawkes self-exciting point process\n    widely used in the modelling of clustered event sequence data.\n    This package provides functions to simulate the RHawkes process\n    with a given immigrant hazard rate function and offspring birth\n    time density function, to compute the exact likelihood of a\n    RHawkes process using the recursive algorithm proposed by Chen and\n    Stindl (2018) <doi:10.1080/10618600.2017.1341324>, to compute the\n    Rosenblatt residuals for goodness-of-fit assessment, and to\n    predict future event times based on observed event times up to a\n    given time. A function implementing the linear time RHawkes\n    process likelihood approximation algorithm proposed in Stindl and\n    Chen (2021) <doi:10.1007/s11222-021-10002-0> is also included.",
    "version": "1.0",
    "maintainer": "Feng Chen <feng.chen@unsw.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5859,
    "package_name": "RMAWGEN",
    "title": "Multi-Site Auto-Regressive Weather GENerator",
    "description": "S3 and S4 functions are implemented for spatial multi-site\n    stochastic generation of daily time series of temperature and\n    precipitation. These tools make use of Vector AutoRegressive models (VARs).\n    The weather generator model is then saved as an object and is calibrated by\n    daily instrumental \"Gaussianized\" time series through the 'vars' package\n    tools. Once obtained this model, it can it can be used for weather\n    generations and be adapted to work with several climatic monthly time\n    series.",
    "version": "1.3.9.3",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "url": "https://ecor.github.io/RMAWGEN/,https://github.com/ecor/RMAWGEN,\nhttps://docs.google.com/file/d/0B66otCUk3Bv6V3RPbm1mUG4zVHc/edit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5919,
    "package_name": "ROCFTP.MMS",
    "title": "Perfect Sampling",
    "description": "The algorithm provided in this package generates perfect sample for unimodal or multimodal posteriors. Read Once Coupling From The Past, with Metropolis-Multishift is used to generate a perfect sample for a given posterior density based on the two extreme starting paths, minimum and maximum of the most interest range of the posterior. It uses the monotone random operation of multishift coupler which allows to sandwich all of the state space in one point. It means both Markov Chains starting from the maximum and minimum will be coalesced. The generated sample is independent from the starting points. It is useful for mixture distributions too. The output of this function is a real value as an exact draw from the posterior distribution.",
    "version": "1.0.0",
    "maintainer": "Majid Nabipoor <nabipoor@ualberta.ca>",
    "url": "https://github.com/nabipoor/ROCFTP.MMS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5927,
    "package_name": "ROCnReg",
    "title": "ROC Curve Inference with and without Covariates",
    "description": "Estimates the pooled (unadjusted) Receiver Operating Characteristic (ROC) curve, the covariate-adjusted ROC (AROC) curve, and the covariate-specific/conditional ROC (cROC) curve by different methods, both Bayesian and frequentist. Also, it provides functions to obtain ROC-based optimal cutpoints utilizing several criteria. Based on Erkanli, A. et al. (2006) <doi:10.1002/sim.2496>; Faraggi, D. (2003) <doi:10.1111/1467-9884.00350>; Gu, J. et al. (2008) <doi:10.1002/sim.3366>; Inacio de Carvalho, V. et al. (2013) <doi:10.1214/13-BA825>; Inacio de Carvalho, V., and Rodriguez-Alvarez, M.X. (2022) <doi:10.1214/21-STS839>; Janes, H., and Pepe, M.S. (2009) <doi:10.1093/biomet/asp002>; Pepe, M.S. (1998) <http://www.jstor.org/stable/2534001?seq=1>; Rodriguez-Alvarez, M.X. et al. (2011a) <doi:10.1016/j.csda.2010.07.018>; Rodriguez-Alvarez, M.X. et al. (2011a) <doi:10.1007/s11222-010-9184-1>. Please see Rodriguez-Alvarez, M.X. and Inacio, V. (2021) <doi:10.32614/RJ-2021-066> for more details.",
    "version": "1.0-9",
    "maintainer": "Maria Xose Rodriguez-Alvarez <mxrodriguez@uvigo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5955,
    "package_name": "ROKET",
    "title": "Optimal Transport-Based Kernel Regression",
    "description": "Perform optimal transport on somatic point mutations and kernel regression hypothesis testing by integrating pathway level similarities at the gene level (Little et al. (2023) <doi:10.1111/biom.13769>). The software implements balanced and unbalanced optimal transport and omnibus tests with 'C++' across a set of tumor samples and allows for multi-threading to decrease computational runtime.",
    "version": "1.0.0",
    "maintainer": "Paul Little <pllittle321@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5966,
    "package_name": "ROpenWeatherMap",
    "title": "R Interface to OpenWeatherMap API",
    "description": "OpenWeatherMap (OWM) <http://openweathermap.org/api> is a service providing weather related data.\n           This package can be used to access current weather data for one location or several locations.\n           It can also be used to forecast weather for 5 days with data for every 3 hours.",
    "version": "1.1",
    "maintainer": "Mukul Chaware <mukul.chaware13@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5974,
    "package_name": "RPEIF",
    "title": "Computation and Plots of Influence Functions for Risk and\nPerformance Measures",
    "description": "Computes the influence functions time series of the returns for the risk and \n             performance measures as mentioned in Chen and Martin (2018) \n             <https://www.ssrn.com/abstract=3085672>, as well as in Zhang et al. (2019)\n             <https://www.ssrn.com/abstract=3415903>. Also evaluates estimators influence\n             functions at a set of parameter values and plots them to display the shapes of \n             the influence functions.",
    "version": "1.2.5",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5976,
    "package_name": "RPEXE.RPEXT",
    "title": "Reduced Piecewise Exponential Estimate/Test Software",
    "description": "This reduced piecewise exponential survival software implements the likelihood ratio test and backward elimination procedure in Han, Schell, and Kim (2012 <doi:10.1080/19466315.2012.698945>, 2014 <doi:10.1002/sim.5915>), and Han et al. (2016 <doi:10.1111/biom.12590>). Inputs to the program can be either times when events/censoring occur or the vectors of total time on test and the number of events. Outputs of the programs are times and the corresponding p-values in the backward elimination. Details about the model and implementation are given in Han et al. 2014. This program can run in R version 3.2.2 and above.",
    "version": "0.0.2",
    "maintainer": "Gang Han <hangang.true@gmail.com>",
    "url": "https://github.com/hangangtrue/RPEXE.RPEXT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5989,
    "package_name": "RPregression",
    "title": "A Simple Regression and Plotting Tool",
    "description": "Perform a regression analysis, generate a regression table, create a scatter plot, and download the results. It uses 'stargazer' for generating regression tables and 'ggplot2' for creating plots. With just two lines of code, you can perform a regression analysis, visualize the results, and save the output. It is part of my make R easy project where one doesn't need to know how to use various packages in order to get results and makes it easily accessible to beginners. This is a part of my make R easy project. Help from 'ChatGPT' was taken. References were Wickham (2016) <doi:10.1007/978-3-319-24277-4>.",
    "version": "0.1.0",
    "maintainer": "Raghav Puri <itsraghavpuri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 5995,
    "package_name": "RPtests",
    "title": "Goodness of Fit Tests for High-Dimensional Linear Regression\nModels",
    "description": "Performs goodness of fits tests for both high and low-dimensional linear models.\n    It can test for a variety of model misspecifications including nonlinearity and heteroscedasticity.\n    In addition one can test the significance of potentially large groups of variables, and also\n    produce p-values for the significance of individual variables in high-dimensional linear\n    regression.",
    "version": "0.1.5",
    "maintainer": "Rajen Shah <r.shah@statslab.cam.ac.uk>",
    "url": "https://rss.onlinelibrary.wiley.com/doi/10.1111/rssb.12234",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6002,
    "package_name": "RRBoost",
    "title": "A Robust Boosting Algorithm",
    "description": "An implementation of robust boosting algorithms for regression in R. This includes the RRBoost method proposed in the paper \"Robust Boosting for Regression Problems\" (Ju X and Salibian-Barrera M. 2020) <doi:10.1016/j.csda.2020.107065>. It also implements previously proposed boosting algorithms in the simulation section of the paper: L2Boost, LADBoost, MBoost (Friedman, J. H. (2001) <doi:10.1214/aos/1013203451>) and Robloss (Lutz et al. (2008) <doi:10.1016/j.csda.2007.11.006>).",
    "version": "0.2",
    "maintainer": "Xiaomeng Ju <xiaomeng.ju@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6005,
    "package_name": "RRI",
    "title": "Residual Randomization Inference for Regression Models",
    "description": "Testing and inference for regression models using residual randomization methods. The basis of inference is an invariance assumption on the regression errors, e.g., clustered errors, or doubly-clustered errors.",
    "version": "1.1",
    "maintainer": "Panos Toulis <panos.toulis@chicagobooth.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6009,
    "package_name": "RRRR",
    "title": "Online Robust Reduced-Rank Regression Estimation",
    "description": "Methods for estimating online robust reduced-rank regression. \n    The Gaussian maximum likelihood estimation method is described in Johansen, S. (1991) <doi:10.2307/2938278>.\n    The majorisation-minimisation estimation method is partly described in Zhao, Z., & Palomar, D. P. (2017) <doi:10.1109/GlobalSIP.2017.8309093>.\n    The description of the generic stochastic successive upper-bound minimisation method\n    and the sample average approximation can be found in Razaviyayn, M., Sanjabi, M., & Luo, Z. Q. (2016) <doi:10.1007/s10107-016-1021-7>.",
    "version": "1.1.1",
    "maintainer": "Yangzhuoran Fin Yang <yangyangzhuoran@gmail.com>",
    "url": "https://pkg.yangzhuoranyang.com/RRRR/,\nhttps://github.com/FinYang/RRRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6015,
    "package_name": "RRphylo",
    "title": "Phylogenetic Ridge Regression Methods for Comparative Studies",
    "description": "Functions for phylogenetic analysis (Castiglione et al., 2018 <doi:10.1111/2041-210X.12954>). The functions perform the estimation of phenotypic evolutionary rates, identification of phenotypic evolutionary rate shifts, quantification of direction and size of evolutionary change in multivariate traits, the computation of ontogenetic shape vectors and test for morphological convergence.",
    "version": "3.0.2",
    "maintainer": "Silvia Castiglione <silvia.castiglione@unina.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6017,
    "package_name": "RSA",
    "title": "Response Surface Analysis",
    "description": "Advanced response surface analysis. The main function RSA computes\n    and compares several nested polynomial regression models (full second- or \n    third-order polynomial, shifted and rotated squared difference model, \n    rising ridge surfaces, basic squared difference model, asymmetric or \n    level-dependent congruence effect models). The package provides plotting \n    functions for 3d wireframe surfaces, interactive 3d plots, and contour plots. \n    Calculates many surface parameters (a1 to a5, principal axes, stationary point,\n    eigenvalues) and provides standard, robust, or bootstrapped standard errors\n    and confidence intervals for them.",
    "version": "0.10.8",
    "maintainer": "Felix Schönbrodt <felix@nicebread.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6026,
    "package_name": "RSDC",
    "title": "Regime-Switching Dynamic Correlation Models",
    "description": "Estimation, forecasting, simulation, and portfolio construction for \n    regime-switching models with exogenous variables as in \n    Pelletier (2006) <doi:10.1016/j.jeconom.2005.01.013>.",
    "version": "1.1-2",
    "maintainer": "David Ardia <david.ardia.ch@gmail.com>",
    "url": "https://github.com/ArdiaD/RSDC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6028,
    "package_name": "RSE",
    "title": "Number of Newly Discovered Rare Species Estimation",
    "description": "A Bayesian-weighted estimator and two unweighted estimators are\n developed to estimate the number of newly found rare species in additional\n ecological samples. Among these methods, the Bayesian-weighted estimator\n and an unweighted (Chao-derived) estimator are of high accuracy and\n recommended for practical applications.\n Technical details of the proposed estimators have been well described\n in the following paper: Shen TJ, Chen YH (2018) A Bayesian\n weighted approach to predicting the number of newly discovered\n rare species. Conservation Biology, In press.",
    "version": "1.3",
    "maintainer": "Youhua Chen <haydi@126.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6029,
    "package_name": "RSEIS",
    "title": "Seismic Time Series Analysis Tools",
    "description": "Multiple interactive codes to view and analyze seismic data, via spectrum analysis, wavelet transforms, particle motion, hodograms.  Includes general time-series tools, plotting, filtering, interactive display.",
    "version": "4.2-4",
    "maintainer": "Jonathan M. Lees <jonathan.lees@unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6030,
    "package_name": "RSGHB",
    "title": "Functions for Hierarchical Bayesian Estimation: A Flexible\nApproach",
    "description": "Functions for estimating models using a Hierarchical Bayesian (HB) framework. The flexibility comes in allowing the user to specify the likelihood function directly instead of assuming predetermined model structures. Types of models that can be estimated with this code include the family of discrete choice models (Multinomial Logit, Mixed Logit, Nested Logit, Error Components Logit and Latent Class) as well ordered response models like ordered probit and ordered logit. In addition, the package allows for flexibility in specifying parameters as either fixed (non-varying across individuals) or random with continuous distributions. Parameter distributions supported include normal, positive/negative log-normal, positive/negative censored normal, and the Johnson SB distribution. Kenneth Train's Matlab and Gauss code for doing Hierarchical Bayesian estimation has served as the basis for a few of the functions included in this package. These Matlab/Gauss functions have been rewritten to be optimized within R. Considerable code has been added to increase the flexibility and usability of the code base. Train's original Gauss and Matlab code can be found here: <http://elsa.berkeley.edu/Software/abstracts/train1006mxlhb.html> See Train's chapter on HB in Discrete Choice with Simulation here: <http://elsa.berkeley.edu/books/choice2.html>; and his paper on using HB with non-normal distributions here: <http://eml.berkeley.edu//~train/trainsonnier.pdf>. The authors would also like to thank the invaluable contributions of Stephane Hess and the Choice Modelling Centre: <https://cmc.leeds.ac.uk/>.",
    "version": "1.2.2",
    "maintainer": "Jeff Dumont <Jeff.Dumont@rsginc.com>",
    "url": "https://github.com/RSGInc/RSGHB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6037,
    "package_name": "RSSampling",
    "title": "Ranked Set Sampling",
    "description": "Ranked set sampling (RSS) is introduced as an advanced method for data collection which is substantial for the statistical and methodological analysis in scientific studies by McIntyre (1952) (reprinted in 2005) <doi:10.1198/000313005X54180>. This package introduces the first package  that implements the RSS and its modified versions for sampling. With 'RSSampling', the researchers can sample with basic RSS and the modified versions, namely, Median RSS, Extreme RSS, Percentile RSS, Balanced groups RSS, Double RSS, L-RSS, Truncation-based RSS, Robust extreme RSS. The 'RSSampling' also allows imperfect ranking using an auxiliary variable (concomitant) which is widely used in the real life applications. Applicants can also use this package for parametric and nonparametric inference such as mean, median and variance estimation, regression analysis and some distribution-free tests where the the samples are obtained via basic RSS.",
    "version": "1.0",
    "maintainer": "Busra Sevinc <busra.sevincc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6064,
    "package_name": "RTFA",
    "title": "Robust Factor Analysis for Tensor Time Series",
    "description": "Tensor Factor Models (TFM) are appealing dimension reduction tools for high-order tensor time series, and have wide applications in economics, finance and medical imaging. We propose an one-step projection estimator by minimizing the least-square loss function, and further propose a robust estimator with an iterative weighted projection technique by utilizing the Huber loss function. The methods are discussed in Barigozzi et al. (2022) <arXiv:2206.09800>, and Barigozzi et al. (2023) <arXiv:2303.18163>.",
    "version": "0.1.0",
    "maintainer": "Lingxiao Li <lilingxiao@mail.sdu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6077,
    "package_name": "RTransferEntropy",
    "title": "Measuring Information Flow Between Time Series with Shannon and\nRenyi Transfer Entropy",
    "description": "Measuring information flow between time series with Shannon and Rényi transfer entropy. See also Dimpfl and Peter (2013) <doi:10.1515/snde-2012-0044> and Dimpfl and Peter (2014) <doi:10.1016/j.intfin.2014.03.004> for theory and applications to financial time series. Additional references can be found in the theory part of the vignette.",
    "version": "0.2.21",
    "maintainer": "David Zimmermann <david_j_zimmermann@hotmail.com>",
    "url": "https://github.com/BZPaper/RTransferEntropy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6148,
    "package_name": "RaschSampler",
    "title": "Rasch Sampler",
    "description": "MCMC based sampling of binary matrices with fixed margins as used in exact Rasch model tests. ",
    "version": "0.8-10",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6156,
    "package_name": "RavenR",
    "title": "Raven Hydrological Modelling Framework R Support and Analysis",
    "description": "Utilities for processing input and output files associated with the Raven Hydrological Modelling Framework. Includes various plotting functions, model diagnostics, reading output files into extensible time series format, and support for writing Raven input files. \n    The 'RavenR' package is also archived at Chlumsky et al. (2020) <doi:10.5281/zenodo.4248183>.\n    The Raven Hydrologic Modelling Framework method can be referenced with Craig et al. (2020) <doi:10.1016/j.envsoft.2020.104728>.",
    "version": "2.2.4",
    "maintainer": "Robert Chlumsky <rchlumsk@uwaterloo.ca>",
    "url": "https://github.com/rchlumsk/RavenR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6161,
    "package_name": "Rbent",
    "title": "Robust Bent Line Regression",
    "description": "An implementation of robust bent line regression. It can fit the bent line regression and test the existence of change point,\n    for the paper, \"Feipeng Zhang and Qunhua Li (2016). Robust bent line regression, submitted.\"",
    "version": "0.1.0",
    "maintainer": "Feipeng Zhang <zhangfp108@gmail.com>",
    "url": "http://arxiv.org/abs/1606.02234",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6197,
    "package_name": "RcmdrPlugin.NMBU",
    "title": "R Commander Plug-in for University Level Applied Statistics",
    "description": "An R Commander \"plug-in\" extending functionality of linear models\n    and providing an interface to Partial Least Squares Regression and Linear and\n    Quadratic Discriminant analysis. Several statistical summaries are extended,\n    predictions are offered for additional types of analyses, and extra plots, tests\n    and mixed models are available.",
    "version": "1.8.15",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://github.com/khliland/RcmdrPlugin.NMBU/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6210,
    "package_name": "RcmdrPlugin.survival",
    "title": "R Commander Plug-in for the 'survival' Package",
    "description": "An R Commander plug-in for the survival\n  package, with dialogs for Cox models, parametric survival regression models,\n  estimation of survival curves, and testing for differences in survival\n  curves, along with data-management facilities and a variety of tests, \n  diagnostics and graphs.",
    "version": "1.3-2",
    "maintainer": "John Fox <jfox@mcmaster.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6236,
    "package_name": "RcppDPR",
    "title": "'Rcpp' Implementation of Dirichlet Process Regression",
    "description": "'Rcpp' reimplementation of the the Bayesian non-parametric Dirichlet Process Regression model for penalized regression first published in Zeng and Zhou (2017) <doi:10.1038/s41467-017-00470-2>. A full Bayesian version is implemented with Gibbs sampling, as well as a faster but less accurate variational Bayes approximation.",
    "version": "0.1.10",
    "maintainer": "Mohammad Abu Gazala <abugazalamohammad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6280,
    "package_name": "Rcriticor",
    "title": "Pierre-Goldwin Correlogram",
    "description": "Goldwin-Pierre correlogram. Research of critical periods in the past. Integrates a time series in a given window.  ",
    "version": "2.0",
    "maintainer": "J.S. Pierre <jean-sebastien.pierre@univ-rennes1.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6295,
    "package_name": "Rdrw",
    "title": "Univariate and Multivariate Damped Random Walk Processes",
    "description": "We provide a toolbox to fit and simulate a univariate or multivariate damped random walk process that is also known as an Ornstein-Uhlenbeck process or a continuous-time autoregressive model of the first order, i.e., CAR(1) or CARMA(1, 0). This process is suitable for analyzing univariate or multivariate time series data with irregularly-spaced observation times and heteroscedastic measurement errors. When it comes to the multivariate case, the number of data points (measurements/observations) available at each observation time does not need to be the same, and the length of each time series can vary. The number of time series data sets that can be modeled simultaneously is limited to ten in this version of the package. We use Kalman-filtering to evaluate the resulting likelihood function, which leads to a scalable and efficient computation in finding maximum likelihood estimates of the model parameters or in drawing their posterior samples. Please pay attention to loading the data if this package is used for astronomical data analyses; see the details in the manual. Also see Hu and Tak (2020) <arXiv:2005.08049>.",
    "version": "1.0.2",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6297,
    "package_name": "Rdta",
    "title": "Data Transforming Augmentation for Linear Mixed Models",
    "description": "We provide a toolbox to fit univariate and multivariate linear mixed models via data transforming augmentation. Users can also fit these models via typical data augmentation for a comparison. It returns either maximum likelihood estimates of unknown model parameters (hyper-parameters) via an EM algorithm or posterior samples of those parameters via MCMC. Also see Tak et al. (2019) <doi:10.1080/10618600.2019.1704295>.",
    "version": "1.0.1",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6318,
    "package_name": "RealSurvSim",
    "title": "Simulate Survival Data",
    "description": "Provides tools for simulating synthetic survival data using a variety of\n    methods, including kernel density estimation, parametric distribution fitting,\n    and bootstrap resampling techniques for a desired sample size.",
    "version": "1.0.0",
    "maintainer": "Maria Thurow <maria.thurow@tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6319,
    "package_name": "RealVAMS",
    "title": "Multivariate VAM Fitting",
    "description": "Fits a multivariate value-added model (VAM), see Broatch, Green, and Karl (2018) <doi:10.32614/RJ-2018-033> and Broatch and Lohr (2012) <doi:10.3102/1076998610396900>, with normally distributed test scores and a binary outcome indicator. A pseudo-likelihood approach, Wolfinger (1993) <doi:10.1080/00949659308811554>, is used for the estimation of this joint generalized linear mixed model.  The inner loop of the pseudo-likelihood routine (estimation of a linear mixed model) occurs in the framework of the EM algorithm presented by Karl, Yang, and Lohr (2013) <DOI:10.1016/j.csda.2012.10.004>. This material is based upon work supported by the National Science Foundation under grants DRL-1336027 and DRL-1336265.  ",
    "version": "0.4-6",
    "maintainer": "Andrew Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6325,
    "package_name": "RecordTest",
    "title": "Inference Tools in Time Series Based on Record Statistics",
    "description": "Statistical tools based on the probabilistic properties of the \n  record occurrence in a sequence of independent and identically distributed \n  continuous random variables. In particular, tools to prepare a time series \n  as well as distribution-free trend and change-point tests and graphical \n  tools to study the record occurrence. Details about the implemented tools \n  can be found in Castillo-Mateo et al. (2023a) <doi:10.18637/jss.v106.i05> \n  and Castillo-Mateo et al. (2023b) <doi:10.1016/j.atmosres.2023.106934>.",
    "version": "2.2.0",
    "maintainer": "Jorge Castillo-Mateo <jorgecastillomateo@gmail.com>",
    "url": "https://github.com/JorgeCastilloMateo/RecordTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6337,
    "package_name": "RegAssure",
    "title": "Streamlined Integration of Regression Assumption",
    "description": "It streamlines the evaluation of regression model\n    assumptions, enhancing result reliability. With integrated tools for\n    assessing key aspects like linearity, homoscedasticity, and more. It's\n    a valuable asset for researchers and analysts working with regression\n    models.",
    "version": "1.0.0",
    "maintainer": "Nicolás Rubio García <nrubiogar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6339,
    "package_name": "RegCombin",
    "title": "Partially Linear Regression under Data Combination",
    "description": "We implement linear regression when the outcome of interest and some of the covariates are observed in two different datasets that cannot be linked, based on D'Haultfoeuille, Gaillac, Maurel (2022) <doi:10.3386/w29953>. The package allows for common regressors observed in both datasets, and for various shape constraints on the effect of covariates on the outcome of interest. It also provides the tools to perform a test of point identification. See the associated vignette <https://github.com/cgaillac/RegCombin/blob/master/RegCombin_vignette.pdf> for theory and code examples.",
    "version": "0.4.1",
    "maintainer": "Christophe Gaillac <christophe.gaillac@economics.ox.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6340,
    "package_name": "RegDDM",
    "title": "Generalized Linear Regression with DDM",
    "description": "Drift-Diffusion Model (DDM) has been widely used to model binary\n    decision-making tasks, and many research studies the relationship between\n    DDM parameters and other characteristics of the subject. This package uses\n    'RStan' to perform generalized liner regression analysis over DDM parameters\n    via a single Bayesian Hierarchical model. Compared to estimating DDM\n    parameters followed by a separate regression model, 'RegDDM' reduces bias\n    and improves statistical power.",
    "version": "1.1",
    "maintainer": "Zekai Jin <Jin.Zekai@nyspi.columbia.edu>",
    "url": "https://github.com/biorabbit/RegDDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6343,
    "package_name": "RegKink",
    "title": "Regression Kink with a Time-Varying Threshold",
    "description": "An algorithm is proposed to estimate regression kink model proposed by the paper, Lixiong Yang and Jen-Je Su (2018) <doi:10.1016/j.jimonfin.2018.06.002>.",
    "version": "0.1.0",
    "maintainer": "Lixiong Yang <ylx@lzu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6344,
    "package_name": "RegSDC",
    "title": "Information Preserving Regression-Based Tools for Statistical\nDisclosure Control",
    "description": "Implementation of the methods described in the paper with the above title: Langsrud, Ø. (2019) <doi:10.1007/s11222-018-9848-9>. The package can be used to generate synthetic or hybrid continuous microdata, and the relationship to the original data can be controlled in several ways. A function for replacing suppressed tabular cell frequencies with decimal numbers is included.",
    "version": "1.0.0",
    "maintainer": "Øyvind Langsrud <oyl@ssb.no>",
    "url": "https://github.com/olangsrud/RegSDC,\nhttps://olangsrud.github.io/RegSDC/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6348,
    "package_name": "RegressionFactory",
    "title": "Expander Functions for Generating Full Gradient and Hessian from\nSingle-Slot and Multi-Slot Base Distributions",
    "description": "The expander functions rely on the mathematics developed for the Hessian-definiteness invariance theorem for linear projection transformations of variables, described in authors' paper, to generate the full, high-dimensional gradient and Hessian from the lower-dimensional derivative objects. This greatly relieves the computational burden of generating the regression-function derivatives, which in turn can be fed into any optimization routine that utilizes such derivatives. The theorem guarantees that Hessian definiteness is preserved, meaning that reasoning about this property can be performed in the low-dimensional space of the base distribution. This is often a much easier task than its equivalent in the full, high-dimensional space. Definiteness of Hessian can be useful in selecting optimization/sampling algorithms such as Newton-Raphson optimization or its sampling equivalent, the Stochastic Newton Sampler. Finally, in addition to being a computational tool, the regression expansion framework is of conceptual value by offering new opportunities to generate novel regression problems.",
    "version": "0.7.4",
    "maintainer": "Alireza S. Mahani <alireza.s.mahani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6350,
    "package_name": "RelDists",
    "title": "Estimation for some Reliability Distributions",
    "description": "Parameters estimation and linear regression models for Reliability \n    distributions families reviewed by Almalki & Nadarajah (2014)\n    <doi:10.1016/j.ress.2013.11.010> using Generalized Additive\n    Models for Location, Scale and Shape, aka GAMLSS by Rigby & Stasinopoulos\n    (2005) <doi:10.1111/j.1467-9876.2005.00510.x>.",
    "version": "1.0.0",
    "maintainer": "Jaime Mosquera <jmosquerag@unal.edu.co>",
    "url": "https://ousuga.github.io/RelDists/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6358,
    "package_name": "ReliabilityTheory",
    "title": "Structural Reliability Analysis",
    "description": "Perform structural reliability analysis, including computation and\n    simulation with system signatures, Samaniego (2007)\n    <doi:10.1007/978-0-387-71797-5>, and survival signatures, Coolen and\n    Coolen-Maturi (2013) <doi:10.1007/978-3-642-30662-4_8>. Additionally\n    supports parametric and topological inference given system lifetime data,\n    Aslett (2012) <https://www.louisaslett.com/PhD_Thesis.pdf>.",
    "version": "0.3.1",
    "maintainer": "Louis Aslett <louis.aslett@durham.ac.uk>",
    "url": "https://www.louisaslett.com/,\nhttps://github.com/louisaslett/ReliabilityTheory",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6396,
    "package_name": "Rfast2",
    "title": "A Collection of Efficient and Extremely Fast R Functions II",
    "description": "A collection of fast statistical and utility functions for data analysis. Functions for regression, maximum likelihood, column-wise statistics and many more have been included. C++ has been utilized to speed up the functions. References: Tsagris M., Papadakis M. (2018). Taking R to its limits: 70+ tips. PeerJ Preprints 6:e26605v1 <doi:10.7287/peerj.preprints.26605v1>.",
    "version": "0.1.5.5",
    "maintainer": "Manos Papadakis <rfastofficial@gmail.com>",
    "url": "https://github.com/RfastOfficial/Rfast2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6406,
    "package_name": "Rgbp",
    "title": "Hierarchical Modeling and Frequency Method Checking on\nOverdispersed Gaussian, Poisson, and Binomial Data",
    "description": "We utilize  approximate Bayesian machinery to fit two-level conjugate hierarchical models on overdispersed Gaussian, Poisson, and Binomial data and evaluates whether the resulting approximate Bayesian interval estimates for random effects meet the nominal confidence levels via frequency coverage evaluation. The data that Rgbp assumes comprise observed sufficient statistic for each random effect, such as an average or a proportion of each group, without population-level data.  The approximate Bayesian tool equipped with the adjustment for density maximization produces approximate point and interval estimates for model parameters including second-level variance component, regression coefficients, and random effect. For the Binomial data, the package provides an option to produce  posterior samples of all the model parameters via the acceptance-rejection method. The package provides a quick way to evaluate coverage rates of the resultant Bayesian interval estimates for random effects via a parametric bootstrapping, which we call frequency method checking.",
    "version": "1.1.4",
    "maintainer": "Joseph Kelly <josephkelly@post.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6433,
    "package_name": "Ritc",
    "title": "Isothermal Titration Calorimetry (ITC) Data Analysis",
    "description": "Selected functions for simulation and regression of integrated Isothermal Titration Calorimetry (ITC) data with the most commonly used one-to-one binding model.",
    "version": "1.3",
    "maintainer": "Yingyun Liu <yingyunliu@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6440,
    "package_name": "Rlgt",
    "title": "Bayesian Exponential Smoothing Models with Trend Modifications",
    "description": "An implementation of a number of Global Trend models for time series forecasting \n    that are Bayesian generalizations and extensions of some Exponential Smoothing models. \n    The main differences/additions include 1) nonlinear global trend, 2) Student-t error \n    distribution, and 3) a function for the error size, so heteroscedasticity. The methods \n    are particularly useful for short time series. When tested on the well-known M3 dataset,\n    they are able to outperform all classical time series algorithms. The models are fitted \n    with MCMC using the 'rstan' package.",
    "version": "0.2-3",
    "maintainer": "Christoph Bergmeir <christoph.bergmeir@monash.edu>",
    "url": "https://github.com/cbergmeir/Rlgt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6445,
    "package_name": "Rlinsolve",
    "title": "Iterative Solvers for (Sparse) Linear System of Equations",
    "description": "Solving a system of linear equations is one of the most fundamental\n    computational problems for many fields of mathematical studies, such as\n    regression problems from statistics or numerical partial differential equations.\n    We provide basic stationary iterative solvers such as Jacobi, Gauss-Seidel,\n    Successive Over-Relaxation and SSOR methods. Nonstationary, also known as\n\tKrylov subspace methods are also provided. Sparse matrix computation is also supported\n    in that solving large and sparse linear systems can be manageable using 'Matrix' package\n    along with 'RcppArmadillo'. For a more detailed description, see a book by Saad (2003)\n    <doi:10.1137/1.9780898718003>.",
    "version": "0.3.3",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6474,
    "package_name": "RoBMA",
    "title": "Robust Bayesian Meta-Analyses",
    "description": "A framework for estimating ensembles of meta-analytic, meta-regression, and \n    multilevel models (assuming either presence or absence of the effect, heterogeneity,\n    publication bias, and moderators). The RoBMA framework uses Bayesian model-averaging to \n    combine the competing meta-analytic models into a model ensemble, weights \n    the posterior parameter distributions based on posterior model probabilities \n    and uses Bayes factors to test for the presence or absence of the\n    individual components (e.g., effect vs. no effect; Bartoš et al., 2022, \n    <doi:10.1002/jrsm.1594>; Maier, Bartoš & Wagenmakers, 2022, \n    <doi:10.1037/met0000405>; Bartoš et al., 2025, <doi:10.1037/met0000737>). \n    Users can define a wide range of prior distributions for the effect size, heterogeneity, \n    publication bias (including selection models and PET-PEESE), and moderator components. \n    The package provides convenient functions for summary, visualizations, and fit diagnostics.",
    "version": "3.6.1",
    "maintainer": "František Bartoš <f.bartos96@gmail.com>",
    "url": "https://fbartos.github.io/RoBMA/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6476,
    "package_name": "RoBTT",
    "title": "Robust Bayesian T-Test",
    "description": "An implementation of Bayesian model-averaged t-tests that allows \n    users to draw inferences about the presence versus absence of an effect, \n    variance heterogeneity, and potential outliers. The 'RoBTT' package estimates \n    ensembles of models created by combining competing hypotheses and applies \n    Bayesian model averaging using posterior model probabilities. Users can \n    obtain model-averaged posterior distributions and inclusion Bayes factors, \n    accounting for uncertainty in the data-generating process \n    (Maier et al., 2024, <doi:10.3758/s13423-024-02590-5>). The package also \n    provides a truncated likelihood version of the model-averaged t-test, \n    enabling users to exclude potential outliers without introducing bias \n    (Godmann et al., 2024, <doi:10.31234/osf.io/j9f3s>). Users can specify \n    a wide range of informative priors for all parameters of interest. \n    The package offers convenient functions for summary, visualization, \n    and fit diagnostics.",
    "version": "1.3.1",
    "maintainer": "František Bartoš <f.bartos96@gmail.com>",
    "url": "https://fbartos.github.io/RoBTT/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6480,
    "package_name": "RobGARCHBoot",
    "title": "Robust Bootstrap Forecast Densities for GARCH Models",
    "description": "Bootstrap forecast densities for GARCH (Generalized Autoregressive Conditional Heteroskedastic) returns and volatilities using the robust residual-based bootstrap procedure of Trucios, Hotta and Ruiz (2017) <DOI:10.1080/00949655.2017.1359601>.",
    "version": "1.2.0",
    "maintainer": "Carlos Trucios <ctrucios@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6481,
    "package_name": "RobKF",
    "title": "Innovative and/or Additive Outlier Robust Kalman Filtering",
    "description": "Implements a series of robust Kalman filtering approaches. It implements the additive outlier robust filters of Ruckdeschel et al. (2014) <arXiv:1204.3358> and Agamennoni et al. (2018) <doi:10.1109/ICRA.2011.5979605>, the innovative outlier robust filter of Ruckdeschel et al. (2014) <arXiv:1204.3358>, as well as the innovative and additive outlier robust filter of Fisch et al. (2020) <arXiv:2007.03238>.",
    "version": "1.0.2",
    "maintainer": "Daniel Grose <dan.grose@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6483,
    "package_name": "RobMixReg",
    "title": "Robust Mixture Regression",
    "description": "Finite mixture models are a popular technique for modelling unobserved heterogeneity or to approximate general distribution functions in a semi-parametric way. They are used in a lot of different areas such as astronomy, biology, economics, marketing or medicine.\n             This package is the implementation of popular robust mixture regression methods based on different algorithms including: fleximix, finite mixture models and latent class regression; CTLERob, component-wise adaptive trimming likelihood estimation; mixbi, bi-square estimation; mixL, Laplacian distribution; mixt, t-distribution; TLE, trimmed likelihood estimation.\n             The implemented algorithms includes:  CTLERob stands for Component-wise adaptive Trimming Likelihood Estimation based mixture regression; mixbi stands for mixture regression based on bi-square estimation; mixLstands for mixture regression based on Laplacian distribution; TLE stands for Trimmed Likelihood Estimation based mixture regression. For more detail of the algorithms, please refer to below references. \n             Reference: Chun Yu, Weixin Yao, Kun Chen (2017) <doi:10.1002/cjs.11310>.\n             NeyKov N, Filzmoser P, Dimova R et al. (2007) <doi:10.1016/j.csda.2006.12.024>.\n             Bai X, Yao W. Boyer JE (2012) <doi:10.1016/j.csda.2012.01.016>.\n             Wennan Chang, Xinyu Zhou, Yong Zang, Chi Zhang, Sha Cao (2020) <arXiv:2005.11599>.",
    "version": "1.1.0",
    "maintainer": "Wennan Chang <wnchang@iu.edu>",
    "url": "https://changwn.github.io/RobMixReg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6486,
    "package_name": "RobRegression",
    "title": "Robust Multivariate Regression",
    "description": "Robust methods for estimating the parameters of multivariate Gaussian linear models. ",
    "version": "0.1.0",
    "maintainer": "Antoine Godichon-Baggioni <antoine.godichon_baggioni@upmc.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6492,
    "package_name": "RobustAFT",
    "title": "Truncated Maximum Likelihood Fit and Robust Accelerated Failure\nTime Regression for Gaussian and Log-Weibull Case",
    "description": "R functions for the computation of the truncated maximum\n\t     likelihood and the robust accelerated failure time regression \n\t     for gaussian and log-Weibull case.",
    "version": "1.4-7",
    "maintainer": "A. Randriamiharisoa <exelami@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6494,
    "package_name": "RobustAdaptiveDecomposition",
    "title": "Decomposes a Univariate Time Series into Subcomponents",
    "description": "Provides a method to decompose a univariate time series into meaningful subcomponents for analysis and denoising.",
    "version": "0.1.0",
    "maintainer": "Laiba Sultan Dar <laibasultan@awkum.edu.pk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6496,
    "package_name": "RobustBayesianCopas",
    "title": "Robust Bayesian Copas Selection Model",
    "description": "Fits the robust Bayesian Copas (RBC) selection model of Bai et al. (2020) <arXiv:2005.02930> for correcting and quantifying publication bias in univariate meta-analysis. Also fits standard random effects meta-analysis and the Copas-like selection model of Ning et al. (2017) <doi:10.1093/biostatistics/kxx004>. ",
    "version": "2.0",
    "maintainer": "Ray Bai <raybaistat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6500,
    "package_name": "RobustLinearReg",
    "title": "Robust Linear Regressions",
    "description": "Provides an easy way to compute the Theil Sehn Regression method and also the Siegel Regression Method which are both robust methods base on the median of slopes between all pairs of data. In contrast with the least squared linear regression, these methods are not sensitive to outliers. Theil, H. (1992) <doi:10.1007/978-94-011-2546-8_20>, Sen, P. K. (1968) <doi:10.1080/01621459.1968.10480934>.",
    "version": "1.2.0",
    "maintainer": "Santiago I. Hurtado <santih@carina.fcaglp.unlp.edu.ar>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6504,
    "package_name": "Robyn",
    "title": "Semi-Automated Marketing Mix Modeling (MMM) from Meta Marketing\nScience",
    "description": "Semi-Automated Marketing Mix Modeling (MMM) aiming to reduce human bias by means of ridge regression and evolutionary algorithms, enables actionable decision making providing a budget allocation and diminishing returns curves and allows ground-truth calibration to account for causation.",
    "version": "3.12.1",
    "maintainer": "Bernardo Lares <laresbernardo@gmail.com>",
    "url": "https://github.com/facebookexperimental/Robyn,\nhttps://facebookexperimental.github.io/Robyn/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6509,
    "package_name": "RolWinMulCor",
    "title": "Subroutines to Estimate Rolling Window Multiple Correlation",
    "description": "Rolling Window Multiple Correlation ('RolWinMulCor') estimates the rolling (running) window correlation for the bi- and multi-variate cases between regular (sampled on identical time points) time series, with especial emphasis to ecological data although this can be applied to other kinds of data sets. 'RolWinMulCor' is based on the concept of rolling, running or sliding window and is useful to evaluate the evolution of correlation through time and time-scales. 'RolWinMulCor' contains six functions. The first two focus on the bi-variate case: (1) rolwincor_1win() and (2) rolwincor_heatmap(), which estimate the correlation coefficients and the their respective p-values for only one window-length (time-scale) and considering all possible window-lengths or a band of window-lengths, respectively. The second two functions: (3) rolwinmulcor_1win() and (4) rolwinmulcor_heatmap() are designed to analyze the multi-variate case, following the bi-variate case to visually display the results, but these two approaches are methodologically different. That is, the multi-variate case estimates the adjusted coefficients of determination instead of the correlation coefficients. The last two functions: (5) plot_1win() and (6) plot_heatmap() are used to represent graphically the outputs of the four aforementioned functions as simple plots or as heat maps. The functions contained in 'RolWinMulCor' are highly flexible since these contains several parameters to control the estimation of correlation and the features of the plot output, e.g. to remove the (linear) trend contained in the time series under analysis, to choose different p-value correction methods (which are used to address the multiple comparison problem) or to personalise the plot outputs. The 'RolWinMulCor' package also provides examples with synthetic and real-life ecological time series to exemplify its use. Methods derived from H. Abdi. (2007) <https://personal.utdallas.edu/~herve/Abdi-MCC2007-pretty.pdf>, R. Telford (2013) <https://quantpalaeo.wordpress.com/2013/01/04/, J. M. Polanco-Martinez (2019) <doi:10.1007/s11071-019-04974-y>, and J. M. Polanco-Martinez (2020) <doi:10.1016/j.ecoinf.2020.101163>. ",
    "version": "1.2.0",
    "maintainer": "Josue M. Polanco-Martinez <josue.m.polanco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6510,
    "package_name": "RolWinWavCor",
    "title": "Estimate Rolling Window Wavelet Correlation Between Two Time\nSeries",
    "description": "Estimates and plots as a heat map the rolling window wavelet correlation (RWWC) coefficients statistically significant (within the 95% CI) between two regular (evenly spaced) time series. 'RolWinWavCor' also plots at the same graphic the time series under study. The 'RolWinWavCor' was designed for financial time series, but this software can be used with other kinds of data (e.g., climatic, ecological, geological, etc). The functions contained in 'RolWinWavCor' are highly flexible since these contains some parameters to personalize the time series under analysis and the heat maps of the rolling window wavelet correlation coefficients. Moreover, we have also included a data set (named EU_stock_markets) that contains nine European stock market indices to exemplify the use of the functions contained in 'RolWinWavCor'. Methods derived from Polanco-Martínez et al (2018) <doi:10.1016/j.physa.2017.08.065>). ",
    "version": "0.4.0",
    "maintainer": "Josué M. Polanco-Martínez <josue.m.polanco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6511,
    "package_name": "Romeb",
    "title": "Robust Median-Based Bayesian Growth Curve Modeling",
    "description": "Implements robust median-based Bayesian growth curve models\n    that handle Missing Completely at Random (MCAR), Missing At Random (MAR),\n    and Missing Not At Random (MNAR) missing-data mechanisms, and allow\n    auxiliary variables. Models are fitted via 'rjags' (interface to 'JAGS')\n    and summarized with 'coda'.",
    "version": "0.1.2",
    "maintainer": "Dandan Tang <tangdd20@gmail.com>",
    "url": "https://github.com/DandanTang0/Romeb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6514,
    "package_name": "RootsExtremaInflections",
    "title": "Finds Roots, Extrema and Inflection Points of a Curve",
    "description": "Implementation of Taylor Regression Estimator (TRE), \n   Tulip Extreme Finding Estimator (TEFE), Bell Extreme Finding Estimator (BEFE),\n   Integration Extreme Finding Estimator (IEFE) and \n   Integration Root Finding Estimator (IRFE) for roots, extrema and inflections of a curve .     \n   Christopoulos, DT (2019) <doi:10.13140/RG.2.2.17158.32324> .\n   Christopoulos, DT (2016) <doi:10.2139/ssrn.3043076> .\n   Christopoulos, DT (2016) <https://demovtu.veltech.edu.in/wp-content/uploads/2016/04/Paper-04-2016.pdf> .\n   Christopoulos, DT (2014) <doi:10.48550/arXiv.1206.5478> .",
    "version": "1.2.5",
    "maintainer": "Demetris T. Christopoulos <dchristop@econ.uoa.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6517,
    "package_name": "Rosenbrock",
    "title": "Extended Rosenbrock-Type Densities for Markov Chain Monte Carlo\n(MCMC) Sampler Benchmarking",
    "description": "New Markov chain Monte Carlo (MCMC) samplers new to be thoroughly tested\n    and their performance accurately assessed. This requires densities\n    that offer challenging properties to the novel sampling algorithms.\n    One such popular problem is the Rosenbrock function. However, while its\n    shape lends itself well to a benchmark problem, no codified multivariate expansion\n    of the density exists. We have developed an extension to this class of distributions\n    and supplied densities and direct sampler functions to assess the performance\n    of novel MCMC algorithms. The functions are introduced in \"An n-dimensional Rosenbrock \n    Distribution for MCMC Testing\" by Pagani, Wiegand and Nadarajah (2019) <arXiv:1903.09556>.",
    "version": "0.1.0",
    "maintainer": "Martin Wiegand <Martin.Wiegand@mrc-bsu.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6546,
    "package_name": "Rsfar",
    "title": "Seasonal Functional Autoregressive Models",
    "description": "This is a collection of functions designed for simulating, estimating and forecasting seasonal functional autoregressive time series of order one. These methods are addressed in the manuscript: <https://www.monash.edu/business/ebs/research/publications/ebs/wp16-2019.pdf>.",
    "version": "0.0.1",
    "maintainer": "Hossein Haghbin <haghbinh@gmail.com>",
    "url": "https://github.com/haghbinh/Rsfar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6551,
    "package_name": "Rspc",
    "title": "Nelson Rules for Control Charts",
    "description": "Implementation of Nelson rules for control charts in 'R'. The 'Rspc' implements some Statistical Process Control methods, namely Levey-Jennings type of I (individuals) chart, Shewhart C (count) chart and Nelson rules (as described in Montgomery, D. C. (2013) Introduction to statistical quality control. Hoboken, NJ: Wiley.). Typical workflow is taking the time series, specify the control limits, and list of Nelson rules you want to evaluate. There are several options how to modify the rules (one sided limits, numerical parameters of rules, etc.). Package is also capable of calculating the control limits from the data (so far only for i-chart and c-chart are implemented).",
    "version": "1.2.2",
    "maintainer": "Stanislav Matousek (MSD) <rspc@merck.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6554,
    "package_name": "Rssa",
    "title": "A Collection of Methods for Singular Spectrum Analysis",
    "description": "Methods and tools for Singular Spectrum Analysis including decomposition,\n             forecasting and gap-filling for univariate and multivariate time series.\n             General description of the methods with many examples can be found in the book\n             Golyandina (2018, <doi:10.1007/978-3-662-57380-8>).\n             See 'citation(\"Rssa\")' for details.",
    "version": "1.1",
    "maintainer": "Anton Korobeynikov <anton@korobeynikov.info>",
    "url": "https://github.com/asl/rssa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6569,
    "package_name": "RtsEva",
    "title": "Performs the Transformed-Stationary Extreme Values Analysis",
    "description": "Adaptation of the 'Matlab' 'tsEVA' toolbox developed by Lorenzo Mentaschi\n    available here:\n    <https://github.com/menta78/tsEva>. It contains an implementation of the\n    Transformed-Stationary (TS) methodology for non-stationary extreme \n    value Analysis (EVA) as described in Mentaschi et al. (2016) <doi:10.5194/hess-20-3527-2016>.  \n    In synthesis this approach consists in:\n    (i) transforming a non-stationary time series into a\n    stationary one to which the stationary extreme value theory can be applied; and\n    (ii) reverse-transforming the result into a non-stationary extreme\n    value distribution.\n    'RtsEva' offers several options for trend estimation (mean, extremes, seasonal)\n    and contains multiple plotting functions displaying different aspects\n    of the non-stationarity of extremes.",
    "version": "1.1.0",
    "maintainer": "Alois Tilloy <alois.tilloy@ec.europa.eu>",
    "url": "https://github.com/r-lib/devtools,\nhttps://github.com/Alowis/RtsEva",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6576,
    "package_name": "Runuran",
    "title": "R Interface to the 'UNU.RAN' Random Variate Generators",
    "description": "Interface to the 'UNU.RAN' library for Universal Non-Uniform RANdom variate generators. \n\t     Thus it allows to build non-uniform random number generators from quite arbitrary\n\t     distributions. In particular, it provides an algorithm for fast numerical inversion\n\t     for distribution with given density function.\n\t     In addition, the package contains densities, distribution functions and quantiles\n\t     from a couple of distributions. ",
    "version": "0.41",
    "maintainer": "Josef Leydold <josef.leydold@wu.ac.at>",
    "url": "https://github.com/unuran/Runuran/,\nhttps://statmath.wu.ac.at/unuran/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6601,
    "package_name": "SALES",
    "title": "The (Adaptive) Elastic Net and Lasso Penalized Sparse Asymmetric\nLeast Squares (SALES) and Coupled Sparse Asymmetric Least\nSquares (COSALES) using Coordinate Descent and Proximal\nGradient Algorithms",
    "description": "A coordinate descent algorithm for computing the solution paths of\n    the sparse and coupled sparse asymmetric least squares, including the\n    (adaptive) elastic net and Lasso penalized SALES and COSALES regressions.",
    "version": "1.0.2",
    "maintainer": "Yuwen Gu <yuwen.gu@uconn.edu>",
    "url": "https://github.com/knightgu/SALES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6607,
    "package_name": "SAMTx",
    "title": "Sensitivity Assessment to Unmeasured Confounding with Multiple\nTreatments",
    "description": "A sensitivity analysis approach for unmeasured confounding in observational data with multiple treatments and a binary outcome. This approach derives the general bias formula and provides adjusted causal effect estimates in response to various assumptions about the degree of unmeasured confounding. Nested multiple imputation is embedded within the Bayesian framework to integrate   uncertainty about the sensitivity parameters and sampling variability.  Bayesian Additive Regression Model (BART) is used for outcome modeling. The causal estimands are the conditional average treatment effects (CATE) based on the risk difference.  For more details, see paper: Hu L et al. (2020) A flexible sensitivity analysis approach for unmeasured confounding with multiple treatments and a binary outcome with application to SEER-Medicare lung cancer data <arXiv:2012.06093>. ",
    "version": "0.3.0",
    "maintainer": "Jiayi Ji <Jiayi.Ji@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6612,
    "package_name": "SANple",
    "title": "Fitting Shared Atoms Nested Models via Markov Chains Monte Carlo",
    "description": "Estimate Bayesian nested mixture models via Markov Chain Monte Carlo methods. Specifically, the package implements the common atoms model (Denti et al., 2023), and hybrid finite-infinite models. \n             All models use Gaussian mixtures with a normal-inverse-gamma prior distribution on the parameters. Additional functions are provided to help analyzing the results of the fitting procedure.  \n             References:\n             Denti, Camerlenghi, Guindani, Mira (2023) <doi:10.1080/01621459.2021.1933499>,   \n             D’Angelo, Denti (2024) <doi:10.1214/24-BA1458>.",
    "version": "0.2.0",
    "maintainer": "Francesco Denti <francescodenti.personal@gmail.com>",
    "url": "https://github.com/laura-dangelo/SANple",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6623,
    "package_name": "SASmixed",
    "title": "Data sets from \"SAS System for Mixed Models\"",
    "description": "Data sets and sample lmer analyses corresponding\n  to the examples in Littell, Milliken, Stroup and Wolfinger\n  (1996), \"SAS System for Mixed Models\", SAS Institute.",
    "version": "1.0-4",
    "maintainer": "Steven Walker <steve.walker@utoronto.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6628,
    "package_name": "SBAGM",
    "title": "Search Best ARIMA, GARCH, and MS-GARCH Model",
    "description": "Get the most appropriate autoregressive integrated moving average, generalized auto-regressive conditional heteroscedasticity and Markov switching GARCH model. For method details see Haas M, Mittnik S, Paolella MS (2004). <doi:10.1093/jjfinec/nbh020>, Bollerslev T (1986). <doi:10.1016/0304-4076(86)90063-1>.",
    "version": "0.1.0",
    "maintainer": "Rajeev Ranjan Kumar <rrk.uasd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6632,
    "package_name": "SBICgraph",
    "title": "Structural Bayesian Information Criterion for Graphical Models",
    "description": "This is the implementation of the novel structural Bayesian information criterion by Zhou, 2020 (under review).\n   In this method, the prior structure is modeled and incorporated into the Bayesian information criterion framework. \n   Additionally, we also provide the implementation of a two-step algorithm to generate the candidate model pool.",
    "version": "1.0.0",
    "maintainer": "Quang Nguyen <Quang.P.Nguyen.GR@dartmouth.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6634,
    "package_name": "SBMSplitMerge",
    "title": "Inference for a Generalised SBM with a Split Merge Sampler",
    "description": "Inference in a Bayesian framework for a generalised stochastic block model. The generalised stochastic block model (SBM) can capture group structure in network data without requiring conjugate priors on the edge-states. Two sampling methods are provided to perform inference on edge parameters and block structure: a split-merge Markov chain Monte Carlo algorithm and a Dirichlet process sampler. Green, Richardson (2001) <doi:10.1111/1467-9469.00242>; Neal (2000) <doi:10.1080/10618600.2000.10474879>; Ludkin (2019) <arXiv:1909.09421>.",
    "version": "1.1.1",
    "maintainer": "Matthew Ludkin <m.ludkin1@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6638,
    "package_name": "SBmedian",
    "title": "Scalable Bayes with Median of Subset Posteriors",
    "description": "Median-of-means is a generic yet powerful framework for scalable and robust estimation. A framework for Bayesian analysis is called M-posterior, which estimates a median of subset posterior measures. For general exposition to the topic, see the paper by Minsker (2015) <doi:10.3150/14-BEJ645>.",
    "version": "0.1.2",
    "maintainer": "Kisung You <kisung.you@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6658,
    "package_name": "SCGLR",
    "title": "Supervised Component Generalized Linear Regression",
    "description": "\n    An extension of the Fisher Scoring Algorithm to combine PLS regression with GLM \n    estimation in the multivariate context. Covariates can also be grouped in themes.",
    "version": "3.1.0",
    "maintainer": "Guillaume Cornu <gcornu@cirad.fr>",
    "url": "https://scnext.github.io/SCGLR/, https://github.com/scnext/SCGLR,\nhttps://cran.r-project.org/package=SCGLR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6659,
    "package_name": "SCI",
    "title": "Standardized Climate Indices Such as SPI, SRI or SPEI",
    "description": "Functions for generating Standardized Climate Indices (SCI).\n  Functions for generating Standardized Climate Indices (SCI). \n  SCI is a transformation of (smoothed) climate (or environmental) time series \n  that removes seasonality and forces the data to take values of the standard \n  normal distribution. SCI was originally developed for precipitation. \n  In this case it is known as the Standardized Precipitation Index (SPI).",
    "version": "1.0-3",
    "maintainer": "Lukas Gudmundsson <lukas.gudmundsson@env.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6666,
    "package_name": "SCORNET",
    "title": "Semi-Supervised Calibration of Risk with Noisy Event Times",
    "description": "A consistent, semi-supervised, non-parametric survival curve estimator optimized for efficient use of Electronic Health Record (EHR) data with a limited number of current status labels. See van der Laan and Robins (1997) <doi:10.2307/2670119>.",
    "version": "0.1.1",
    "maintainer": "Yuri Ahuja <Yuri_Ahuja@hms.harvard.edu>",
    "url": "https://github.com/celehs/SCORNET",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6701,
    "package_name": "SEI",
    "title": "Calculating Standardised Indices",
    "description": "Convert a time series of observations to a time series of standardised indices that can be used to monitor variables on a common and probabilistically interpretable scale. The indices can be aggregated and rescaled to different time scales, visualised using plot capabilities, and calculated using a range of distributions. This includes flexible non-parametric and non-stationary methods.",
    "version": "0.2.0",
    "maintainer": "Sam Allen <sam.allen@stat.math.ethz.ch>",
    "url": "https://github.com/noeliaof/SEI, https://noeliaof.github.io/SEI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6706,
    "package_name": "SEMID",
    "title": "Identifiability of Linear Structural Equation Models",
    "description": "Provides routines to check identifiability or non-identifiability\n    of linear structural equation models as described in Drton, Foygel, and\n    Sullivant (2011) <doi:10.1214/10-AOS859>, Foygel, Draisma, and Drton (2012) \n    <doi:10.1214/12-AOS1012>, and other works. The routines are based on the graphical \n    representation of structural equation models.",
    "version": "0.4.2",
    "maintainer": "Nils Sturma <nils.sturma@epfl.ch>",
    "url": "https://github.com/Lucaweihs/SEMID",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6711,
    "package_name": "SEPaLS",
    "title": "Shrinkage for Extreme Partial Least-Squares (SEPaLS)",
    "description": "Regression context for the Partial Least Squares framework for \n    Extreme values. Estimations of the Shrinkage for Extreme Partial Least-Squares \n    (SEPaLS) estimators, an adaptation of the original Partial Least Squares \n    (PLS) method tailored to the extreme-value framework.\n    The SEPaLS project is a joint work by Stephane Girard, Hadrien Lorenzo and \n    Julyan Arbel.\n    R code to replicate the results of the paper is available at \n    <https://github.com/hlorenzo/SEPaLS_simus>.\n    Extremes within PLS was already studied by one of the authors, see M \n    Bousebeta, G Enjolras, S Girard (2023) <doi:10.1016/j.jmva.2022.105101>.",
    "version": "0.1.0",
    "maintainer": "Hadrien Lorenzo <hadrien.lorenzo@univ-amu.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6724,
    "package_name": "SGB",
    "title": "Simplicial Generalized Beta Regression",
    "description": "Main properties and regression procedures using a generalization of the Dirichlet distribution called Simplicial Generalized Beta distribution. It is a new distribution on the simplex (i.e. on the space of compositions or positive vectors with sum of components equal to 1). The Dirichlet distribution can be constructed from a random vector of independent Gamma variables divided by their sum. The SGB follows the same construction with generalized Gamma instead of Gamma variables. The Dirichlet exponents are supplemented by an overall shape parameter and a vector of scales. The scale vector is itself a composition and can be modeled with auxiliary variables through a log-ratio transformation. Graf, M. (2017, ISBN: 978-84-947240-0-8). See also the vignette enclosed in the package.",
    "version": "1.0.1.1",
    "maintainer": "Monique Graf <monique.p.n.graf@bluewin.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6726,
    "package_name": "SGDinference",
    "title": "Inference with Stochastic Gradient Descent",
    "description": "Estimation and inference methods for large-scale mean and quantile regression models via stochastic (sub-)gradient descent (S-subGD) algorithms. \n    The inference procedure handles cross-sectional data sequentially: \n    (i) updating the parameter estimate with each incoming \"new observation\", \n    (ii) aggregating it as a Polyak-Ruppert average, and \n    (iii) computing an asymptotically pivotal statistic for inference through random scaling. \n    The methodology used in the 'SGDinference' package is described in detail in the following papers: \n    (i) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2022) <doi:10.1609/aaai.v36i7.20701> \"Fast and robust online inference with stochastic gradient descent via random scaling\".\n    (ii) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2023) <arXiv:2209.14502> \"Fast Inference for Quantile Regression with Tens of Millions of Observations\". ",
    "version": "0.1.0",
    "maintainer": "Youngki Shin <shiny11@mcmaster.ca>",
    "url": "https://github.com/SGDinference-Lab/SGDinference/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6727,
    "package_name": "SGL",
    "title": "Fit a GLM (or Cox Model) with a Combination of Lasso and Group\nLasso Regularization",
    "description": "Fit a regularized generalized linear model via penalized\n        maximum likelihood.  The model is fit for a path of values of\n        the penalty parameter. Fits linear, logistic and Cox models.",
    "version": "1.3",
    "maintainer": "Noah Simon <nrsimon@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6736,
    "package_name": "SHELF",
    "title": "Tools to Support the Sheffield Elicitation Framework",
    "description": "Implements various methods for eliciting a probability\n    distribution for a single parameter from an expert or a group of\n    experts. The expert provides a small number of probability judgements,\n    corresponding to points on his or her cumulative distribution\n    function. A range of parametric distributions can then be fitted and\n    displayed, with feedback provided in the form of fitted probabilities\n    and percentiles. For multiple experts, a weighted linear pool can be\n    calculated. Also includes functions for eliciting beliefs about\n    population distributions; eliciting multivariate distributions using a\n    Gaussian copula; eliciting a Dirichlet distribution; eliciting\n    distributions for variance parameters in a random effects\n    meta-analysis model; survival extrapolation. R Shiny apps for most of the methods are\n    included.",
    "version": "1.12.1",
    "maintainer": "Jeremy Oakley <j.oakley@sheffield.ac.uk>",
    "url": "https://github.com/OakleyJ/SHELF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6742,
    "package_name": "SIBER",
    "title": "Stable Isotope Bayesian Ellipses in R",
    "description": "Fits bi-variate ellipses to stable isotope data using Bayesian\n    inference with the aim being to describe and compare their isotopic\n    niche.",
    "version": "2.1.9",
    "maintainer": "Andrew Jackson <jacksoan@tcd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6748,
    "package_name": "SIHR",
    "title": "Statistical Inference in High Dimensional Regression",
    "description": "The goal of SIHR is to provide inference procedures in the high-dimensional generalized linear regression setting for:\n    (1) linear functionals <doi:10.48550/arXiv.1904.12891> <doi:10.48550/arXiv.2012.07133>,\n    (2) conditional average treatment effects,\n    (3) quadratic functionals <doi:10.48550/arXiv.1909.01503>,\n    (4) inner product,\n    (5) distance.",
    "version": "2.1.0",
    "maintainer": "Zijian Guo <zijguo@stat.rutgers.edu>",
    "url": "https://zywang0701.github.io/SIHR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6755,
    "package_name": "SIMEXBoost",
    "title": "Boosting Method for High-Dimensional Error-Prone Data",
    "description": "Implementation of the boosting procedure with the simulation and extrapolation approach to address variable selection and estimation for high-dimensional data subject to measurement error in predictors. It can be used to address generalized linear models (GLM) in Chen (2023) <doi: 10.1007/s11222-023-10209-3> and the accelerated failure time (AFT) model in Chen and Qiu (2023) <doi: 10.1111/biom.13898>. Some relevant references include Chen and Yi (2021) <doi:10.1111/biom.13331> and Hastie, Tibshirani, and Friedman (2008, ISBN:978-0387848570).",
    "version": "0.2.0",
    "maintainer": "Bangxu Qiu <1135427976@qq.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6761,
    "package_name": "SIMle",
    "title": "Estimation and Inference for General Time Series Regression",
    "description": "We provide functions for estimation and inference of nonlinear and non-stationary time series regression using the sieve methods and bootstrapping procedure. ",
    "version": "0.1.0",
    "maintainer": "Xiucai Ding <xiucaiding89@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6769,
    "package_name": "SIRthresholded",
    "title": "Sliced Inverse Regression with Thresholding",
    "description": "Implements a thresholded version of the Sliced Inverse Regression method (Li, K. C. (1991) <doi:10.2307/2290563>), which allows to do variable selection.",
    "version": "1.0.2",
    "maintainer": "Clement Weinreich <clement@weinreich.fr>",
    "url": "https://clement-w.github.io/SIRthresholded/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6775,
    "package_name": "SKAT",
    "title": "SNP-Set (Sequence) Kernel Association Test",
    "description": "Functions for kernel-regression-based association tests including Burden test, SKAT and SKAT-O. These methods aggregate individual SNP score statistics in a SNP set and efficiently compute SNP-set level p-values.",
    "version": "2.2.5",
    "maintainer": "Seunggeun (Shawn) Lee <lee7801@snu.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6778,
    "package_name": "SLBDD",
    "title": "Statistical Learning for Big Dependent Data",
    "description": "Programs for analyzing large-scale time series data. They include functions for automatic specification and estimation of univariate time series, for clustering time series, for multivariate outlier detections, for quantile plotting of many time series, for dynamic factor models and for creating input data for deep learning programs. Examples of using the package can be found in the Wiley book 'Statistical Learning with Big Dependent Data' by Daniel Peña and Ruey S. Tsay (2021). ISBN 9781119417385.",
    "version": "0.0.4",
    "maintainer": "Antonio Elias <antonioefz91@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6785,
    "package_name": "SLIC",
    "title": "LIC for Distributed Skewed Regression",
    "description": "This comprehensive toolkit for skewed regression is designated as \"SLIC\" (The LIC for Distributed Skewed Regression Analysis). It is predicated on the assumption that the error term follows a skewed distribution, such as the Skew-Normal, Skew-t, or Skew-Laplace. The methodology and theoretical foundation of the package are described in Guo G.(2020) <doi:10.1080/02664763.2022.2053949>.",
    "version": "0.3",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6788,
    "package_name": "SLOPE",
    "title": "Sorted L1 Penalized Estimation",
    "description": "Efficient implementations for Sorted L-One Penalized Estimation\n    (SLOPE): generalized linear models regularized with the sorted L1-norm\n    (Bogdan et al. 2015). Supported models include\n    ordinary least-squares regression, binomial regression, multinomial\n    regression, and Poisson regression. Both dense and sparse  predictor\n    matrices are supported. In addition, the package features predictor\n    screening rules that enable fast and efficient solutions to high-dimensional\n    problems.",
    "version": "1.2.0",
    "maintainer": "Johan Larsson <johanlarsson@outlook.com>",
    "url": "https://jolars.github.io/SLOPE/, https://github.com/jolars/SLOPE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6791,
    "package_name": "SLRMss",
    "title": "Symmetric Linear Regression Models for Small Samples",
    "description": "Ordinary and modified statistics for symmetrical linear\n    regression models with small samples. The supported ordinary\n    statistics include Wald, score, likelihood ratio and gradient. The\n    modified statistics include score, likelihood ratio and gradient.\n    Diagnostic tools associated with the fitted model are implemented. For\n    more details see Medeiros and Ferrari (2017) <DOI:10.1111/stan.12107>.",
    "version": "1.0.0",
    "maintainer": "Ivonaldo S. da Silva-Júnior <ivosilvestresjr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6792,
    "package_name": "SLSEdesign",
    "title": "Optimal Regression Design under the Second-Order Least Squares\nEstimator",
    "description": "With given inputs that include number of points, discrete design space, a measure of skewness, models and parameter value, this package calculates the objective value, optimal designs and plot the equivalence theory under A- and D-optimal criteria under the second-order Least squares estimator. This package is based on the paper \"Properties of optimal regression designs under the second-order least squares estimator\" by Chi-Kuang Yeh and Julie Zhou (2021) <doi:10.1007/s00362-018-01076-6>. ",
    "version": "0.0.5",
    "maintainer": "Chi-Kuang Yeh <chi-kuang.yeh@mail.mcgill.ca>",
    "url": "https://github.com/chikuang/SLSEdesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6797,
    "package_name": "SMAHP",
    "title": "Survival Mediation Analysis of High-Dimensional Proteogenomic\nData",
    "description": "SMAHP (pronounced as SOO-MAP) is a novel multi-omics framework for causal mediation analysis of high-dimensional proteogenomic data with survival outcomes. The full methodological details can be found in our recent preprint by Ahn S et al. (2025) <doi:10.48550/arXiv.2503.08606>.",
    "version": "0.0.5",
    "maintainer": "Weijia Fu <weijia.fu@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6798,
    "package_name": "SMARTbayesR",
    "title": "Bayesian Set of Best Dynamic Treatment Regimes and Sample Size\nin SMARTs for Binary Outcomes",
    "description": "Permits determination of a set of \n    optimal dynamic treatment regimes and sample size for a SMART design \n    in the Bayesian setting with binary outcomes. Please see Artman (2020) <arXiv:2008.02341>.",
    "version": "2.0.0",
    "maintainer": "William Artman <William_Artman@URMC.Rochester.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6811,
    "package_name": "SMNCensReg",
    "title": "Fitting Univariate Censored Regression Model Under the Family of\nScale Mixture of Normal Distributions",
    "description": "Fit univariate right, left or interval censored regression model under the scale mixture of normal distributions.",
    "version": "3.1",
    "maintainer": "Aldo M. Garay <medina_garay@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6814,
    "package_name": "SMPracticals",
    "title": "Practicals for Use with Davison (2003) Statistical Models",
    "description": "Contains the datasets and a few functions for use with \n        the practicals outlined in Appendix A of the book\n        Statistical Models (Davison, 2003, Cambridge University Press), \n        which can be found at <doi:10.1017/CBO9780511815850>.",
    "version": "1.4-3.2",
    "maintainer": "Alessandra R. Brazzale <brazzale@stat.unipd.it>",
    "url": "doi:10.1017/CBO9780511815850",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6828,
    "package_name": "SNSchart",
    "title": "Sequential Normal Scores in Statistical Process Management",
    "description": "The methods discussed in this package are new non-parametric methods\n    based on sequential normal scores 'SNS' (Conover et al (2017)\n    <doi:10.1080/07474946.2017.1360091>), designed for sequences of observations,\n    usually time series data, which may occur singly or in batches,\n    and may be univariate or multivariate. These methods are designed\n    to detect changes in the process, which may occur as changes in location\n    (mean or median), changes in scale (standard deviation, or variance), or\n    other changes of interest in the distribution of the observations,\n    over the time observed. They usually apply to large data sets,\n    so computations need to be simple enough to be done in a reasonable\n    time on a computer, and easily updated as each new observation\n    (or batch of observations) becomes available. Some examples and more detail\n    in 'SNS' is presented in the work by Conover et al (2019) <arXiv:1901.04443>.",
    "version": "1.4.0",
    "maintainer": "Luis Benavides <luisbv@tec.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6829,
    "package_name": "SNSeg",
    "title": "Self-Normalization(SN) Based Change-Point Estimation for Time\nSeries",
    "description": "Implementations self-normalization (SN) based algorithms for \n    change-points estimation in time series data. This comprises nested \n    local-window algorithms for detecting changes in both univariate and \n    multivariate time series developed in Zhao, Jiang and Shao (2022) \n    <doi:10.1111/rssb.12552>.",
    "version": "1.0.3",
    "maintainer": "Zifeng Zhao <zzhao2@nd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6835,
    "package_name": "SOHPIE",
    "title": "Statistical Approach via Pseudo-Value Information and Estimation",
    "description": "'SOHPIE' (pronounced as SOFIE) is a novel pseudo-value regression approach for differential co-abundance network analysis of microbiome data, which can include additional clinical covariate in the model. The full methodological details can be found in Ahn S and Datta S (2023) <arXiv:2303.13702v1>.",
    "version": "1.0.6",
    "maintainer": "Seungjun Ahn <seungjun.ahn@mountsinai.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6843,
    "package_name": "SOP",
    "title": "Generalised Additive P-Spline Regression Models Estimation",
    "description": "Generalised additive P-spline regression models estimation using the separation of overlapping precision matrices (SOP) method. Estimation is based on the equivalence between P-splines and linear mixed models, and variance/smoothing parameters are estimated based on restricted maximum likelihood (REML). The package enables users to estimate P-spline models with overlapping penalties. Based on the work described in Rodriguez-Alvarez et al. (2015) <doi:10.1007/s11222-014-9464-2>; Rodriguez-Alvarez et al. (2019) <doi:10.1007/s11222-018-9818-2>, and Eilers and Marx (1996) <doi:10.1214/ss/1038425655>.",
    "version": "1.0-1",
    "maintainer": "Maria Xose Rodriguez-Alvarez <mxrodriguez@uvigo.gal>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6872,
    "package_name": "SPORTSCausal",
    "title": "Spillover Time Series Causal Inference",
    "description": "A time series causal inference model for Randomized Controlled Trial (RCT) under spillover effect. 'SPORTSCausal' (Spillover Time Series Causal Inference) separates treatment effect and spillover effect from given responses of experiment group and control group by predicting the response without treatment. It reports both effects by fitting the Bayesian Structural Time Series (BSTS) model based on 'CausalImpact', as described in Brodersen et al. (2015) <doi:10.1214/14-AOAS788>. ",
    "version": "1.0",
    "maintainer": "Feiyu Yue <yuefyopals@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6874,
    "package_name": "SPOUSE",
    "title": "Scatter Plots Over-Viewed Using Summary Ellipses",
    "description": "Summary ellipses superimposed on a scatter plot contain all bi-variate summary\n            statistics for regression analysis. Furthermore, the outer ellipse flags potential\n            outliers. Multiple groups can be compared in terms of centers and spreads as illustrated\n            in the examples.",
    "version": "0.1.0",
    "maintainer": "Siddhanta Phuyal <siddhantaphuyal7159@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6879,
    "package_name": "SPSP",
    "title": "Selection by Partitioning the Solution Paths",
    "description": "An implementation of the feature Selection procedure by Partitioning the entire Solution Paths\n            (namely SPSP) to identify the relevant features rather than using a single tuning parameter. \n            By utilizing the entire solution paths, this procedure can obtain better selection accuracy than \n            the commonly used approach of selecting only one tuning parameter based on existing criteria, \n            cross-validation (CV), generalized CV, AIC, BIC, and extended BIC (Liu, Y., & Wang, P. (2018) \n            <doi:10.1214/18-EJS1434>). It is more stable and accurate (low false positive and \n            false negative rates) than other variable selection approaches. In addition, it can be flexibly \n            coupled with the solution paths of Lasso, adaptive Lasso, ridge regression, and other penalized \n            estimators.",
    "version": "0.2.0",
    "maintainer": "Xiaorui (Jeremy) Zhu <zhuxiaorui1989@gmail.com>",
    "url": "https://xiaorui.site/SPSP/, https://github.com/XiaoruiZhu/SPSP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6881,
    "package_name": "SPreg",
    "title": "Bias Reduction in the Skew-Probit Model for a Binary Response",
    "description": "Provides a function for the estimation of parameters \n\t\tin a binary regression with the skew-probit link function. \n\t\tNaive MLE, Jeffrey type of prior and Cauchy prior type of penalization are implemented,\n\t\tas described in DongHyuk Lee and Samiran Sinha (2019+) <doi:10.1080/00949655.2019.1590579>.",
    "version": "1.0",
    "maintainer": "DongHyuk Lee <leedhyuk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6883,
    "package_name": "SQI",
    "title": "Soil Quality Index",
    "description": "The overall performance of soil ecosystem services and productivity greatly relies on soil health, making it a crucial indicator. The evaluation of soil physical, chemical, and biological parameters is necessary to determine the overall soil quality index. In our package, three commonly used methods, including linear scoring, regression-based, and principal component-based soil quality indexing, are employed to calculate the soil quality index. This package has been developed using concept of Bastida et al. (2008) and Doran and Parkin (1994) <doi:10.1016/j.geoderma.2008.08.007> <doi:10.2136/sssaspecpub35.c1>.  ",
    "version": "0.1.0",
    "maintainer": "Dr. Owais Ali Wani <owaisaliwani@skuastkashmir.ac.in>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6899,
    "package_name": "SSAforecast",
    "title": "SSA Based Decomposition and Forecasting",
    "description": "Singular spectrum analysis (SSA) decomposes a time series into interpretable components like trends, oscillations, and noise without strict distributional and structural assumptions. For method details see Golyandina N, Zhigljavsky A (2013). <doi:10.1007/978-3-642-34913-3>. ",
    "version": "0.1.1",
    "maintainer": "Rajeev Ranjan Kumar <rrk.uasd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6916,
    "package_name": "SSRMST",
    "title": "Sample Size Calculation using Restricted Mean Survival Time",
    "description": "Calculates the power and sample size based on the difference in Restricted Mean Survival Time.",
    "version": "0.1.1",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6918,
    "package_name": "SSVS",
    "title": "Functions for Stochastic Search Variable Selection (SSVS)",
    "description": "Functions for performing stochastic search variable selection (SSVS) \n    for binary and continuous outcomes and visualizing the results. \n    SSVS is a Bayesian variable selection method used to estimate the probability \n    that individual predictors should be included in a regression model. \n    Using MCMC estimation, the method samples thousands of regression models \n    in order to characterize the model uncertainty regarding both the predictor \n    set and the regression parameters. For details see Bainter, McCauley, Wager, \n    and Losin (2020) Improving practices for selecting a subset of important \n    predictors in psychology: An application to predicting pain, Advances in \n    Methods and Practices in Psychological Science 3(1), 66-80 \n    <DOI:10.1177/2515245919885617>.",
    "version": "2.1.0",
    "maintainer": "Sierra Bainter <sbainter@miami.edu>",
    "url": "https://github.com/sabainter/SSVS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6929,
    "package_name": "STB",
    "title": "Simultaneous Tolerance Bounds",
    "description": "Provides an implementation of simultaneous tolerance bounds (STB), useful for checking whether a numeric vector fits to a hypothetical null-distribution or not.\n             Furthermore, there are functions for computing STB (bands, intervals) for random variates of linear mixed models fitted with package 'VCA'. All kinds of, possibly transformed \n             (studentized, standardized, Pearson-type transformed) random variates (residuals, random effects), can be assessed employing STB-methodology. ",
    "version": "0.6.6",
    "maintainer": "Andre Schuetzenmeister <andre.schuetzenmeister@roche.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6935,
    "package_name": "STFTS",
    "title": "Statistical Tests for Functional Time Series",
    "description": "A collection of statistical hypothesis tests of functional time series. While it will include more tests when the related literature are enriched, this package contains the following key tests: functional stationarity test, functional trend stationarity test, functional unit root test, to name a few.",
    "version": "0.1.0",
    "maintainer": "Chi Seng Pun <cspun@ntu.edu.sg>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6939,
    "package_name": "STOPES",
    "title": "Selection Threshold Optimized Empirically via Splitting",
    "description": "Implements variable selection procedures for low to moderate size generalized linear regressions models. It includes the STOPES functions for linear regression (Capanu M, Giurcanu M, Begg C, Gonen M, Optimized variable selection via repeated data splitting, Statistics in Medicine, 2020, 19(6):2167-2184) as well as subsampling based optimization methods for generalized linear regression models (Marinela Capanu, Mihai Giurcanu, Colin B Begg, Mithat Gonen, Subsampling based variable selection for generalized linear models).",
    "version": "0.2",
    "maintainer": "Marinela Capanu <capanum@mskcc.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6951,
    "package_name": "SUSY",
    "title": "Surrogate Synchrony",
    "description": "Computes synchrony as windowed cross-correlation based on two-dimensional time series in a text file you can upload. 'SUSY' works as described in Tschacher & Meier (2020) <doi:10.1080/10503307.2019.1612114>.",
    "version": "0.1.0",
    "maintainer": "Wolfgang Tschacher <wolfgang.tschacher@upd.unibe.ch>",
    "url": "https://wtschacher.github.io/SUSY/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6954,
    "package_name": "SVDNF",
    "title": "Discrete Nonlinear Filtering for Stochastic Volatility Models",
    "description": "Implements the discrete nonlinear filter (DNF) of Kitagawa (1987) <doi:10.1080/01621459.1987.10478534> to a wide class of stochastic volatility (SV) models with return and volatility jumps following the work of Bégin and Boudreault (2021) <doi:10.1080/10618600.2020.1840995> to obtain likelihood evaluations and maximum likelihood parameter estimates. Offers several built-in SV models and a flexible framework for users to create customized models by specifying drift and diffusion functions along with an arrival distribution for the return and volatility jumps. Allows for the estimation of factor models with stochastic volatility (e.g., heteroskedastic volatility CAPM) by incorporating expected return predictors. Also includes functions to compute filtering and prediction distribution estimates, to simulate data from built-in and custom SV models with jumps, and to forecast future returns and volatility values using Monte Carlo simulation from a given SV model. ",
    "version": "0.1.11",
    "maintainer": "Louis Arsenault-Mahjoubi <larsenau@sfu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6955,
    "package_name": "SVEMnet",
    "title": "Self-Validated Ensemble Models with Lasso and Relaxed Elastic\nNet Regression",
    "description": "Tools for fitting self-validated ensemble models (SVEM; Lemkus et al. (2021) <doi:10.1016/j.chemolab.2021.104439>) in small-sample design-of-experiments and related workflows, using elastic net and relaxed elastic net regression via 'glmnet' (Friedman et al. (2010) <doi:10.18637/jss.v033.i01>). Fractional random-weight bootstraps with anti-correlated validation copies are used to tune penalty paths by validation-weighted AIC/BIC. Supports Gaussian and binomial responses, deterministic expansion helpers for shared factor spaces, prediction with bootstrap uncertainty, and a random-search optimizer that respects mixture constraints and combines multiple responses via desirability functions. Also includes a permutation-based whole-model test for Gaussian SVEM fits (Karl (2024) <doi:10.1016/j.chemolab.2024.105122>). Package code was drafted with assistance from generative AI tools.",
    "version": "3.1.4",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6962,
    "package_name": "SWMPr",
    "title": "Retrieving, Organizing, and Analyzing Estuary Monitoring Data",
    "description": "Tools for retrieving, organizing, and analyzing environmental\n    data from the System Wide Monitoring Program of the National Estuarine\n    Research Reserve System <https://cdmo.baruch.sc.edu/>. These tools\n    address common challenges associated with continuous time series data\n    for environmental decision making.",
    "version": "2.5.2",
    "maintainer": "Marcus W. Beck <mbeck@tbep.org>",
    "url": "http://fawda123.github.io/SWMPr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6971,
    "package_name": "SampleSizeMeans",
    "title": "Sample Size Calculations for Normal Means",
    "description": "Sample size requirements calculation\n        using three different Bayesian criteria in the\n        context of designing an experiment to estimate a normal mean or\n        the difference between two normal means.  Functions for\n        calculation of required sample sizes for the Average Length\n        Criterion, the Average Coverage Criterion and the Worst Outcome\n        Criterion in the context of normal means are provided.\n        Functions for both the fully Bayesian and the mixed\n        Bayesian/likelihood approaches are provided.\n        For reference see Joseph L. and Bélisle P. (1997) <https://www.jstor.org/stable/2988525>.",
    "version": "1.2.3",
    "maintainer": "Patrick Bélisle <patrickb.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6972,
    "package_name": "SampleSizeProportions",
    "title": "Calculating Sample Size Requirements when Estimating the\nDifference Between Two Binomial Proportions",
    "description": "Sample size requirements calculation \n        using three different Bayesian criteria in the\n        context of designing an experiment to estimate the difference\n        between two binomial proportions. Functions for calculation of\n        required sample sizes for the Average Length Criterion, the\n        Average Coverage Criterion and the Worst Outcome Criterion in\n        the context of binomial observations are provided. In all\n        cases, estimation of the difference between two binomial\n        proportions is considered. Functions for both the fully\n        Bayesian and the mixed Bayesian/likelihood approaches are\n        provided.\n        For reference see Joseph L., du Berger R. and Bélisle P. (1997)\n        <doi:10.1002/(sici)1097-0258(19970415)16:7%3C769::aid-sim495%3E3.0.co;2-v>.",
    "version": "1.1.3",
    "maintainer": "Patrick Bélisle <patrickb.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6973,
    "package_name": "SampleSizeSingleArmSurvival",
    "title": "Calculate Sample Size for Single-Arm Survival Studies",
    "description": "Provides methods to calculate sample size for single-arm survival studies using the arcsine transformation, incorporating uniform accrual and exponential survival assumptions. Includes functionality for detailed numerical integration and simulation. This method is based on Nagashima et al. (2021) <doi:10.1002/pst.2090>.",
    "version": "0.1.0",
    "maintainer": "Mohamed Kamal <mohamedkamalhospital@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6981,
    "package_name": "ScaleSpikeSlab",
    "title": "Scalable Spike-and-Slab",
    "description": "A scalable Gibbs sampling implementation for high dimensional Bayesian regression with the continuous spike-and-slab prior. Niloy Biswas, Lester Mackey and Xiao-Li Meng, \"Scalable Spike-and-Slab\" (2022) <arXiv:2204.01668>.",
    "version": "1.0",
    "maintainer": "Niloy Biswas <niloy_biswas@g.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 6997,
    "package_name": "SeBR",
    "title": "Semiparametric Bayesian Regression Analysis",
    "description": "Monte Carlo sampling algorithms for semiparametric\n    Bayesian regression analysis. These models feature a nonparametric\n    (unknown) transformation of the data paired with widely-used\n    regression models including linear regression, spline regression,\n    quantile regression, and Gaussian processes. The transformation\n    enables broader applicability of these key models, including for\n    real-valued, positive, and compactly-supported data with challenging\n    distributional features. The samplers prioritize computational\n    scalability and, for most cases, Monte Carlo (not MCMC) sampling for\n    greater efficiency. Details of the methods and algorithms are provided\n    in Kowal and Wu (2024) <doi:10.1080/01621459.2024.2395586>.",
    "version": "1.1.0",
    "maintainer": "Dan Kowal <daniel.r.kowal@gmail.com>",
    "url": "https://github.com/drkowal/SeBR, https://drkowal.github.io/SeBR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7000,
    "package_name": "SeaVal",
    "title": "Validation of Seasonal Weather Forecasts",
    "description": "Provides tools for processing and evaluating seasonal weather forecasts, \n    with an emphasis on tercile forecasts. We follow the World Meteorological Organization's \n    \"Guidance on Verification of Operational Seasonal Climate Forecasts\", \n    S.J.Mason (2018, ISBN: 978-92-63-11220-0, URL: <https://library.wmo.int/idurl/4/56227>). \n    The development was supported by the European Union’s Horizon 2020 research and innovation \n    programme under grant agreement no. 869730 (CONFER).\n    A comprehensive online tutorial is available at <https://seasonalforecastingengine.github.io/SeaValDoc/>.",
    "version": "1.2.0",
    "maintainer": "Claudio Heinrich-Mertsching <claudio.heinrich@hotmail.de>",
    "url": "https://seasonalforecastingengine.github.io/SeaValDoc/,\nhttps://github.com/SeasonalForecastingEngine/SeaVal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7013,
    "package_name": "SelectBoost.gamlss",
    "title": "Stability-Selection via Correlated Resampling for 'GAMLSS'\nModels",
    "description": "Extends the 'SelectBoost' approach to Generalized Additive Models for\n    Location, Scale and Shape (GAMLSS). Implements bootstrap stability-selection\n    across parameter-specific formulas (mu, sigma, nu, tau) via gamlss::stepGAIC().\n    Includes optional standardization of predictors and helper functions for\n    corrected AIC calculation. More details can be found in Bertrand and Maumy (2024) \n    <https://hal.science/hal-05352041> that highlights correlation-aware resampling to improve\n    variable selection for GAMLSS and quantile regression when predictors are\n    numerous and highly correlated.",
    "version": "0.2.2",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/SelectBoost.gamlss/,\nhttps://github.com/fbertran/SelectBoost.gamlss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7015,
    "package_name": "SelfControlledCaseSeries",
    "title": "Self-Controlled Case Series",
    "description": "Execute the self-controlled case series (SCCS) design using observational \n  data in the OMOP Common Data Model. Extracts all necessary data from the database and \n\ttransforms it to the format required for SCCS. Age and season can be modeled\n\tusing splines assuming constant hazard within calendar months. Event-dependent \n\tcensoring of the observation period can be corrected for. Many exposures can be\n\tincluded at once (MSCCS), with regularization on all coefficients except for the\n\texposure of interest. Includes diagnostics for all major assumptions of the SCCS.",
    "version": "6.1.1",
    "maintainer": "Martijn Schuemie <schuemie@ohdsi.org>",
    "url": "https://ohdsi.github.io/SelfControlledCaseSeries/,\nhttps://github.com/OHDSI/SelfControlledCaseSeries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7022,
    "package_name": "SemiCompRisks",
    "title": "Hierarchical Models for Parametric and Semi-Parametric Analyses\nof Semi-Competing Risks Data",
    "description": "Hierarchical multistate models are considered to perform the analysis of independent/clustered semi-competing risks data. The package allows to choose the specification for model components from a range of options giving users substantial flexibility, including: accelerated failure time or proportional hazards regression models; parametric or non-parametric specifications for baseline survival functions and cluster-specific random effects distribution; a Markov or semi-Markov specification for terminal event following non-terminal event. While estimation is mainly performed within the Bayesian paradigm, the package also provides the maximum likelihood estimation approach for several parametric models. The package also includes functions for univariate survival analysis as complementary analysis tools.",
    "version": "3.4",
    "maintainer": "Kyu Ha Lee <klee15239@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7024,
    "package_name": "SemiMarkov",
    "title": "Multi-States Semi-Markov Models",
    "description": "Functions for fitting multi-state semi-Markov models to longitudinal data. A parametric maximum likelihood estimation method adapted to deal with Exponential, Weibull and Exponentiated Weibull distributions is considered. Right-censoring can be taken into account and both constant and time-varying covariates can be included using a Cox proportional model. Reference: A. Krol and P. Saint-Pierre (2015) \t<doi:10.18637/jss.v066.i06>.",
    "version": "1.4.6",
    "maintainer": "Agnieszka Listwon-Krol <krol@lunenfeld.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7025,
    "package_name": "SemiPar.depCens",
    "title": "Copula Based Cox Proportional Hazards Models for Dependent\nCensoring",
    "description": "Copula based Cox proportional hazards models for survival data subject to dependent \n    censoring. This approach does not assume that the parameter defining the copula is known. The \n    dependency parameter is estimated with other finite model parameters by maximizing a Pseudo \n    likelihood function. The cumulative hazard function is estimated via estimating equations \n    derived based on martingale ideas. Available copula functions include Frank, Gumbel and Normal\n    copulas. Only Weibull and lognormal models are allowed for the censoring model, even though any\n    parametric model that satisfies certain identifiability conditions could be used. Implemented \n    methods are described in the article \"Copula based Cox proportional hazards models for dependent\n    censoring\" by Deresa and Van Keilegom (2024) <doi:10.1080/01621459.2022.2161387>. ",
    "version": "0.1.3",
    "maintainer": "Negera Wakgari Deresa <negera.deresa@gmail.com>",
    "url": "https://github.com/Nago2020/SemiPar.depCens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7027,
    "package_name": "SenSrivastava",
    "title": "Datasets from Sen & Srivastava",
    "description": "Collection of datasets from Sen & Srivastava: \"Regression\n        Analysis, Theory, Methods and Applications\", Springer.  Sources\n        for individual data files are more fully documented in the\n        book.",
    "version": "2015.6.25.1",
    "maintainer": "Kjetil B Halvorsen <kjetil1001@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7044,
    "package_name": "SequenceSpikeSlab",
    "title": "Exact Bayesian Model Selection Methods for the Sparse Normal\nSequence Model",
    "description": "Contains fast functions to calculate the exact Bayes posterior\n    for the Sparse Normal Sequence Model, implementing the algorithms\n    described in Van Erven and Szabo (2021,\n    <doi:10.1214/20-BA1227>). For general hierarchical\n    priors, sample sizes up to 10,000 are feasible within half an hour\n    on a standard laptop. For beta-binomial spike-and-slab priors, a\n    faster algorithm is provided, which can handle sample sizes of\n    100,000 in half an hour. In the implementation, special care has\n    been taken to assure numerical stability of the methods even for\n    such large sample sizes.",
    "version": "1.0.1",
    "maintainer": "Tim van Erven <tim@timvanerven.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7045,
    "package_name": "Sequential",
    "title": "Exact Sequential Analysis for Poisson and Binomial Data",
    "description": "Functions to calculate exact critical values, statistical power, expected time to signal, and required sample sizes for performing exact sequential analysis. All these\tcalculations can be done for either Poisson or binomial data, for continuous or group sequential analyses, and for different types of rejection boundaries. In case of group sequential analyses, the group sizes do not have to be specified in advance and the alpha spending can be arbitrarily settled. For regression versions of the methods, Monte Carlo and asymptotic methods are used. ",
    "version": "4.5.2",
    "maintainer": "Ivair Ramos Silva <ivair@ufop.edu.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7057,
    "package_name": "ShapeSelectForest",
    "title": "Shape Selection for Landsat Time Series of Forest Dynamics",
    "description": "Landsat satellites collect important data about global forest conditions. Documentation about Landsat's role in forest disturbance estimation is available at the site <https://landsat.gsfc.nasa.gov/>. By constrained quadratic B-splines, this package delivers an optimal shape-restricted trajectory to a time series of Landsat imagery for the purpose of modeling annual forest disturbance dynamics to behave in an ecologically sensible manner assuming one of seven possible \"shapes\", namely, flat, decreasing, one-jump (decreasing, jump up, decreasing), inverted vee (increasing then decreasing), vee (decreasing then increasing), linear increasing, and double-jump (decreasing, jump up, decreasing, jump up, decreasing). The main routine selects the best shape according to the minimum Bayes information criterion (BIC) or the cone information criterion (CIC), which is defined as the log of the estimated predictive squared error. The package also provides parameters summarizing the temporal pattern including year(s) of inflection, magnitude of change, pre- and post-inflection rates of growth or recovery. In addition, it contains routines for converting a flat map of disturbance agents to time-series disturbance maps and a graphical routine displaying the fitted trajectory of Landsat imagery. ",
    "version": "1.7",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7059,
    "package_name": "ShapleyValue",
    "title": "Shapley Value Regression for Relative Importance of Attributes",
    "description": "Shapley Value Regression for calculating the relative importance of independent variables in linear regression with avoiding the collinearity.",
    "version": "0.2.0",
    "maintainer": "Jingyi Liang <jingyiliang19@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7068,
    "package_name": "ShiftShareSE",
    "title": "Inference in Regressions with Shift-Share Structure",
    "description": "Provides confidence intervals in least-squares regressions when the\n  variable of interest has a shift-share structure, and in instrumental\n  variables regressions when the instrument has a shift-share structure. The\n  confidence intervals implement the AKM and AKM0 methods developed in Adão,\n  Kolesár, and Morales (2019) <doi:10.1093/qje/qjz025>.",
    "version": "1.1.0",
    "maintainer": "Michal Kolesár <kolesarmi@googlemail.com>",
    "url": "https://github.com/kolesarm/ShiftShareSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7077,
    "package_name": "ShrinkageTrees",
    "title": "Regression Trees with Shrinkage Priors",
    "description": "Bayesian regression tree models with shrinkage priors on \n  step heights. Supports continuous, binary, and right-censored (survival) \n  outcomes. Used for high-dimensional prediction and causal inference.",
    "version": "1.0.2",
    "maintainer": "Tijn Jacobs <t.jacobs@vu.nl>",
    "url": "https://github.com/tijn-jacobs/ShrinkageTrees",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7078,
    "package_name": "SiER",
    "title": "Signal Extraction Approach for Sparse Multivariate Response\nRegression",
    "description": "Methods for regression with high-dimensional predictors and  univariate or maltivariate response variables. It considers the decomposition of the coefficient matrix that leads to the best approximation to the signal part in the response given any rank, and estimates the decomposition by solving a penalized generalized eigenvalue problem followed by a least squares procedure. Ruiyan Luo and Xin Qi (2017) <doi:10.1016/j.jmva.2016.09.005>.",
    "version": "0.1.0",
    "maintainer": "Ruiyan Luo <rluo@gsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7084,
    "package_name": "Sie2nts",
    "title": "Sieve Methods for Non-Stationary Time Series",
    "description": "We provide functions for estimation and inference of locally-stationary time series using the sieve methods and bootstrapping procedure. In addition, it also contains functions to generate Daubechies and Coiflet wavelet by Cascade algorithm and to process data visualization.",
    "version": "0.1.0",
    "maintainer": "Xiucai Ding <xiucaiding89@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7093,
    "package_name": "SignifReg",
    "title": "Consistent Significance Controlled Variable Selection in\nGeneralized Linear Regression",
    "description": "Provides significance controlled variable selection algorithms with different directions (forward, backward, stepwise) based on diverse criteria (AIC, BIC, adjusted r-square, PRESS, or p-value). The algorithm selects a final model with only significant variables defined as those with significant p-values after multiple testing correction such as Bonferroni, False Discovery Rate, etc. See Zambom and Kim (2018) <doi:10.1002/sta4.210>.",
    "version": "4.3",
    "maintainer": "Adriano Zanin Zambom <adriano.zambom@csun.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7097,
    "package_name": "Sim.DiffProc",
    "title": "Simulation of Diffusion Processes",
    "description": "It provides users with a wide range of tools to simulate, estimate, analyze, and visualize the dynamics of stochastic differential systems in both forms Ito and Stratonovich. Statistical analysis with parallel Monte Carlo and moment equations methods of SDEs <doi:10.18637/jss.v096.i02>. Enabled many searchers in different domains to use these equations to modeling practical problems in financial and actuarial modeling and other areas of application, e.g., modeling and simulate of first passage time problem in shallow water using the attractive center (Boukhetala K, 1996) ISBN:1-56252-342-2. ",
    "version": "5.0",
    "maintainer": "Arsalane Chouaib Guidoum <acguidoum@univ-tam.dz>",
    "url": "https://github.com/acguidoum/Sim.DiffProc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7111,
    "package_name": "SimHaz",
    "title": "Simulated Survival and Hazard Analysis for Time-Dependent\nExposure",
    "description": "Generate power for the Cox proportional hazards model by simulating survival events data with time dependent exposure status for subjects. A dichotomous exposure variable is considered with a single transition from unexposed to exposed status during the subject's time on study.",
    "version": "0.1",
    "maintainer": "Nusrat Rabbee <rabbee@berkeley.edu>",
    "url": "http://www.stat.berkeley.edu/~rabbee/research_webpage.htm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7112,
    "package_name": "SimIndep",
    "title": "WISE: a Weighted Similarity Aggregation Test for Serial\nIndependence",
    "description": "A fast implementation of the weighted information similarity aggregation (WISE) \n    test for detecting serial dependence, particularly suited for high-dimensional and \n    non-Euclidean time series. Includes functions for constructing similarity matrices \n    and conducting hypothesis testing. Users can use different similarity \n    measures and define their own weighting schemes. For more details see Q Zhu, M Liu, \n    Y Han, D Zhou (2025) <doi:10.48550/arXiv.2509.05678>.",
    "version": "0.1.2",
    "maintainer": "Qihua Zhu <zhuqihua@u.nus.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7152,
    "package_name": "SlidingWindows",
    "title": "Methods for Time Series Analysis",
    "description": "A collection of functions to perform Detrended Fluctuation Analysis (DFA exponent), GUEDES et al. (2019) <doi:10.1016/j.physa.2019.04.132> , Detrended cross-correlation coefficient (RHODCCA), GUEDES & ZEBENDE (2019) <doi:10.1016/j.physa.2019.121286>, DMCA cross-correlation coefficient and Detrended multiple cross-correlation coefficient (DMC), GUEDES & SILVA-FILHO & ZEBENDE (2018) <doi:10.1016/j.physa.2021.125990>, both with sliding windows approach. ",
    "version": "0.2.0",
    "maintainer": "Everaldo Freitas Guedes <efgestatistico@gmail.com>",
    "url": "https://github.com/efguedes/SlidingWindows",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7162,
    "package_name": "SmoothHazard",
    "title": "Estimation of Smooth Hazard Models for Interval-Censored Data",
    "description": "Estimation of two-state (survival) models and irreversible illness-\n    death models with possibly interval-censored, left-truncated and right-censored\n    data. Proportional intensities regression models can be specified to allow for\n    covariates effects separately for each transition. We use either a parametric\n    approach with Weibull baseline intensities or a semi-parametric approach with\n    M-splines approximation of baseline intensities in order to obtain smooth\n    estimates of the hazard functions. Parameter estimates are obtained by maximum\n    likelihood in the parametric approach and by penalized maximum likelihood in the\n    semi-parametric approach.",
    "version": "2025.07.24",
    "maintainer": "Thomas Alexander Gerds <tag@biostat.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7188,
    "package_name": "SorptionAnalysis",
    "title": "Static Adsorption Experiment Plotting and Analysis",
    "description": "Provides tools to efficiently analyze and visualize laboratory data from aqueous static adsorption experiments. The package provides functions to plot Langmuir, Freundlich, and Temkin isotherms and functions to determine the statistical conformity of data points to the Langmuir, Freundlich, and Temkin adsorption models through statistical characterization of the isothermic least squares regressions lines. Scientific Reference: Dada, A.O, Olalekan, A., Olatunya, A. (2012) <doi:10.9790/5736-0313845>.",
    "version": "0.1.0",
    "maintainer": "Aurnov Chattopadhyay <aurnovcy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7223,
    "package_name": "SparseTSCGM",
    "title": "Sparse Time Series Chain Graphical Models",
    "description": "Computes sparse vector autoregressive coefficients and sparse\n  precision matrices for time series chain graphical models. Methods are\n  described in Abegaz and Wit (2013)\n  <doi:10.1093/biostatistics/kxt005>.",
    "version": "4.1",
    "maintainer": "Fentaw Abegaz <fentawabegaz@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7259,
    "package_name": "SpecsVerification",
    "title": "Forecast Verification Routines for Ensemble Forecasts of Weather\nand Climate",
    "description": "A collection of forecast verification routines developed for the SPECS\n    FP7 project. The emphasis is on comparative verification of ensemble forecasts of weather and climate.",
    "version": "0.5-3",
    "maintainer": "Stefan Siegert <s.siegert@exeter.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7271,
    "package_name": "SpiceFP",
    "title": "Sparse Method to Identify Joint Effects of Functional Predictors",
    "description": "A set of functions allowing to implement the 'SpiceFP' approach\n  which is iterative. It involves transformation of functional predictors into\n  several candidate explanatory matrices (based on contingency tables), to which\n  relative edge matrices with contiguity constraints are associated. Generalized\n  Fused Lasso regression are performed in order to identify the best candidate\n  matrix, the best class intervals and related coefficients at each iteration.\n  The approach is stopped when the maximal number of iterations is reached or\n  when retained coefficients are zeros. Supplementary functions allow to get\n  coefficients of any candidate matrix or mean of coefficients of many candidates.\n  The methods in this package are describing in Girault Gnanguenon Guesse, \n  Patrice Loisel, Bénedicte Fontez, Thierry Simonneau, Nadine Hilgert (2021)\n  \"An exploratory penalized regression to identify combined effects of functional\n  variables -Application to agri-environmental issues\" \n  <https://hal.archives-ouvertes.fr/hal-03298977>.",
    "version": "0.1.2",
    "maintainer": "Girault Gnanguenon Guesse <girault.gnanguenon@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7272,
    "package_name": "Spillover",
    "title": "Spillover/Connectedness Index Based on VAR Modelling",
    "description": "A user-friendly tool for estimating both total and directional connectedness spillovers based on Diebold and Yilmaz (2009, 2012). It also provides the user with rolling estimation for total and net indices. User can find both orthogonalized and generalized versions for each kind of measures. See Diebold and Yilmaz (2009, 2012) find them at  <doi:10.1111/j.1468-0297.2008.02208.x> and <doi:10.1016/j.ijforecast.2011.02.006>.",
    "version": "0.1.1",
    "maintainer": "Jilber Urbina <JilberUrbina@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7279,
    "package_name": "SplitReg",
    "title": "Split Regularized Regression",
    "description": "Functions for computing split regularized estimators defined in Christidis, Lakshmanan, \n             Smucler and Zamar (2019) <doi:10.48550/arXiv.1712.03561>. The approach fits linear regression models that\n             split the set of covariates into groups. The optimal split of the variables into groups and the \n             regularized estimation of the regression coefficients are performed by minimizing  an objective \n             function that encourages sparsity within each group and diversity among them. \n             The estimated coefficients are then pooled together to form the final fit.",
    "version": "1.0.3",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7280,
    "package_name": "SplitSplitPlot",
    "title": "Analysis of Split-Split-Plot Experiments (Analise De\nExperimentos Em Parcela Subsubdividida)",
    "description": "Performs analysis of split-split plot experiments in both completely randomized and randomized complete block designs. With the results, you can obtain ANOVA, mean tests, and regression analysis (Este pacote faz a analise de experimentos em parcela subsubdivididas no delineamento inteiramente casualizado e delineamento em blocos casualizados. Com resultados e possível obter a ANOVA, testes de medias e análise de regressao) <https://www.expstat.com/pacotes-do-r>.",
    "version": "0.0.1",
    "maintainer": "Alcinei Mistico Azevedo <alcineimistico@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7297,
    "package_name": "StReg",
    "title": "Student's t Regression Models",
    "description": "It contains functions to estimate multivariate Student's t dynamic and static regression models for given degrees of freedom and lag length. Users can also specify the trends and dummies of any kind in matrix form.\n    Poudyal, N., and Spanos, A. (2022) <doi:10.3390/econometrics10020017>.\n    Spanos, A. (1994) <http://www.jstor.org/stable/3532870>.",
    "version": "1.1",
    "maintainer": "Niraj Poudyal <niraj.poudyal@ku.edu.np>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7300,
    "package_name": "StabilizedRegression",
    "title": "Stabilizing Regression and Variable Selection",
    "description": "Contains an implementation of 'StabilizedRegression', a regression framework for heterogeneous data introduced in Pfister et al. (2021) <arXiv:1911.01850>. The procedure uses averaging to estimate a regression of a set of predictors X on a response variable Y by enforcing stability with respect to a given environment variable. The resulting regression leads to a variable selection procedure which allows to distinguish between stable and unstable predictors. The package further implements a visualization technique which illustrates the trade-off between stability and predictiveness of individual predictors.",
    "version": "1.1",
    "maintainer": "Niklas Pfister <np@math.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7301,
    "package_name": "StableEstim",
    "title": "Estimate the Four Parameters of Stable Laws using Different\nMethods",
    "description": "Estimate the four parameters of stable laws using maximum\n             likelihood method, generalised method of moments with\n             finite and continuum number of points, iterative\n             Koutrouvelis regression and Kogon-McCulloch method.  The\n             asymptotic properties of the estimators (covariance\n             matrix, confidence intervals) are also provided.",
    "version": "2.4",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/StableEstim/ (doc),\nhttps://CRAN.R-project.org/package=StableEstim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7302,
    "package_name": "StablePopulation",
    "title": "Calculates Alpha for a Stable Population",
    "description": "Provides tools to calculate the alpha parameter of the Weibull distribution, given beta\n and the age-specific fertility of a species, so that the population remains stable and stationary.\n Methods are inspired by \"Survival profiles from linear models versus Weibull models: Estimating stable\n and stationary population structures for Pleistocene large mammals\" (Martín-González et al. 2019)\n <doi:10.1016/j.jasrep.2019.03.031>.",
    "version": "1.0.3",
    "maintainer": "David Palacios-Morales <dpmorales@ubu.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7306,
    "package_name": "StanMoMo",
    "title": "Bayesian Mortality Modelling with 'Stan'",
    "description": "Implementation of popular mortality models using the 'rstan' \n    package, which provides the R interface to the 'Stan' C++ library for \n    Bayesian estimation. The package supports well-known models proposed in the \n    actuarial and demographic literature including the Lee-Carter (1992) \n    <doi:10.1080/01621459.1992.10475265> and the Cairns-Blake-Dowd (2006) \n    <doi:10.1111/j.1539-6975.2006.00195.x> models. By a simple call, the user \n    inputs deaths and exposures and the package outputs the MCMC simulations for\n    each parameter, the log likelihoods and predictions. Moreover, the package \n    includes tools for model selection and Bayesian model averaging by leave \n    future-out validation.",
    "version": "1.2.0",
    "maintainer": "Karim Barigou <karim290492@gmail.com>",
    "url": "https://github.com/kabarigou/StanMoMo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7307,
    "package_name": "Stat2Data",
    "title": "Datasets for Stat2",
    "description": "Datasets for the textbook Stat2: Modeling with Regression and ANOVA (second edition). \n    The package also includes data for the first edition, Stat2: Building Models for a World of Data\n    and a few functions for plotting diagnostics.",
    "version": "2.0.0",
    "maintainer": "Robin Lock <rlock@stlawu.edu>",
    "url": "https://github.com/statmanrobin/Stat2Data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7311,
    "package_name": "StatPerMeCo",
    "title": "Statistical Performance Measures to Evaluate Covariance Matrix\nEstimates",
    "description": "Statistical performance measures used in the econometric literature to evaluate conditional covariance/correlation matrix estimates (MSE, MAE, Euclidean distance, Frobenius distance, Stein distance, asymmetric loss function, eigenvalue loss function and the loss function defined in Eq. (4.6) of Engle et al. (2016) <doi:10.2139/ssrn.2814555>). Additionally, compute Eq. (3.1) and (4.2) of Li et al. (2016) <doi:10.1080/07350015.2015.1092975> to compare the factor loading matrix. The statistical performance measures implemented have been previously used in, for instance, Laurent et al. (2012) <doi:10.1002/jae.1248>,  Amendola et al. (2015) <doi:10.1002/for.2322> and  Becker et al. (2015) <doi:10.1016/j.ijforecast.2013.11.007>.",
    "version": "0.1.0",
    "maintainer": "Carlos Trucios <ctrucios@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7320,
    "package_name": "StealLikeBayes",
    "title": "A Compendium of Bayesian Statistical Routines Written in 'C++'",
    "description": "\n  This is a compendium of 'C++' routines useful for Bayesian statistics. We steal \n  other people's 'C++' code, repurpose it, and export it so developers of 'R' \n  packages can use it in their 'C++' code. We actually don't steal anything, or \n  claim that Thomas Bayes did, but copy code that is compatible with our GPL 3 \n  licence, fully acknowledging the authorship of the original code.",
    "version": "1.0",
    "maintainer": "Tomasz Woźniak <wozniak.tom@pm.me>",
    "url": "https://bsvars.org/StealLikeBayes/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7324,
    "package_name": "StepBeta",
    "title": "Stepwise Procedure for Beta, Beta-Binomial and Negative Binomial\nRegression Models",
    "description": "Starting from a Regression Model, it provides a stepwise \n              procedure to select the linear predictor.",
    "version": "2.1.0",
    "maintainer": "Sergio Garofalo <sergio.garofalo96@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7328,
    "package_name": "SteppedPower",
    "title": "Power Calculation for Stepped Wedge Designs",
    "description": "Tools for power and sample size \n    calculation as well as design diagnostics for \n    longitudinal mixed model settings, with a focus on stepped wedge designs.\n    All calculations are oracle estimates i.e. assume random effect variances \n    to be known (or guessed) in advance.  \n    The method is introduced in Hussey and Hughes (2007) <doi:10.1016/j.cct.2006.05.007>,\n    extensions are discussed in Li et al. (2020) <doi:10.1177/0962280220932962>.",
    "version": "0.3.5",
    "maintainer": "Philipp Mildenberger <pmildenb@uni-mainz.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7352,
    "package_name": "StroupGLMM",
    "title": "R Codes and Datasets for Generalized Linear Mixed Models: Modern\nConcepts, Methods and Applications by Walter W. Stroup",
    "description": "R Codes and Datasets for Stroup, W. W. (2012). Generalized Linear Mixed Models Modern Concepts, Methods and Applications, CRC Press.",
    "version": "0.3.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "url": "https://myaseen208.com/StroupGLMM/\nhttps://CRAN.R-project.org/package=StroupGLMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7354,
    "package_name": "StructuralDecompose",
    "title": "Decomposes a Level Shifted Time Series",
    "description": "Explains the behavior of a time series by decomposing it into its trend, seasonality and residuals. \n             It is built to perform very well in the presence of significant level shifts. It is designed to play \n             well with any breakpoint algorithm and any smoothing algorithm. Currently defaults to 'lowess' for smoothing\n             and 'strucchange' for breakpoint identification. The package is useful in areas such as trend analysis, time series \n             decomposition, breakpoint identification and anomaly detection.",
    "version": "0.1.1",
    "maintainer": "Allen Sunny <allensunny1242@gmail.com>",
    "url": "https://allen-1242.github.io/StructuralDecompose/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7373,
    "package_name": "Sunclarco",
    "title": "Survival Analysis using Copulas",
    "description": "Survival analysis for unbalanced clusters using Archimedean copulas (Prenen et al. (2016) <DOI:10.1111/rssb.12174>).",
    "version": "1.0.0",
    "maintainer": "Roel Braekers <roel.braekers@uhasselt.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7374,
    "package_name": "SunsVoc",
    "title": "Constructing Suns-Voc from Outdoor Time-Series I-V Curves",
    "description": "\n    Suns-Voc (or Isc-Voc) curves can provide the current-voltage (I-V) characteristics of the\n    diode of photovoltaic cells without the effect of series resistance.\n    Here, Suns-Voc curves can be constructed with outdoor time-series I-V\n    curves [1,2,3] of full-size photovoltaic (PV) modules instead of having to be measured in the lab.\n    Time series of four different power loss modes can be calculated based on obtained Isc-Voc curves.\n    This material is based upon work supported by the U.S. Department of\n    Energy's Office of Energy Efficiency and Renewable Energy (EERE) under\n    Solar Energy Technologies Office (SETO) Agreement Number DE-EE0008172. \n    Jennifer L. Braid is supported by the U.S. Department of Energy (DOE) Office of \n    Energy Efficiency and Renewable Energy administered by the Oak Ridge \n    Institute for Science and Education (ORISE) for the DOE. \n    ORISE is managed by Oak Ridge Associated Universities (ORAU) under DOE\n    contract number DE-SC0014664. \n    [1] Wang, M. et al, 2018. \n    <doi:10.1109/PVSC.2018.8547772>. \n    [2] Walters et al, 2018\n    <doi:10.1109/PVSC.2018.8548187>. \n    [3] Guo, S. et al, 2016. \n    <doi:10.1117/12.2236939>. ",
    "version": "0.1.2",
    "maintainer": "Tyler J. Burleyson <tjb152@case.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7379,
    "package_name": "SuperGauss",
    "title": "Superfast Likelihood Inference for Stationary Gaussian Time\nSeries",
    "description": "Likelihood evaluations for stationary Gaussian time series are typically obtained via the Durbin-Levinson algorithm, which scales as O(n^2) in the number of time series observations.  This package provides a \"superfast\" O(n log^2 n) algorithm written in C++, crossing over with Durbin-Levinson around n = 300.  Efficient implementations of the score and Hessian functions are also provided, leading to superfast versions of inference algorithms such as Newton-Raphson and Hamiltonian Monte Carlo.  The C++ code provides a Toeplitz matrix class packaged as a header-only library, to simplify low-level usage in other packages and outside of R.",
    "version": "2.0.4",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "https://github.com/mlysy/SuperGauss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7380,
    "package_name": "SuperLearner",
    "title": "Super Learner Prediction",
    "description": "Implements the super learner prediction method and contains a\n    library of prediction algorithms to be used in the super learner.",
    "version": "2.0-40",
    "maintainer": "Eric Polley <epolley@uchicago.edu>",
    "url": "https://github.com/ecpolley/SuperLearner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7390,
    "package_name": "SurrogateBMA",
    "title": "Flexible Evaluation of Surrogate Markers with Bayesian Model\nAveraging",
    "description": "Provides functions to estimate the proportion of treatment effect explained by the surrogate marker using a Bayesian Model Averaging approach. Duan and Parast (2023) <doi:10.1002/sim.9986>.",
    "version": "1.0",
    "maintainer": "Yunshan Duan <yunshan@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7391,
    "package_name": "SurrogateOutcome",
    "title": "Estimation of the Proportion of Treatment Effect Explained by\nSurrogate Outcome Information",
    "description": "Estimates the proportion of treatment effect on a censored primary outcome that is explained by the treatment effect on a censored surrogate outcome/event. All methods are described in detail in Parast, et al (2020) \"Assessing the Value of a Censored Surrogate Outcome\" <doi:10.1007/s10985-019-09473-1> and Wang et al (2025) \"Model-free Approach to Evaluate a Censored Intermediate Outcome as a Surrogate for Overall Survival\" <doi:10.1002/sim.70268>.  A tutorial for this package can be found at <https://www.laylaparast.com/surrogateoutcome>.",
    "version": "1.2",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7392,
    "package_name": "SurrogateParadoxTest",
    "title": "Empirical Testing of Surrogate Paradox Assumptions",
    "description": "Provides functions to nonparametrically assess assumptions necessary to prevent the surrogate paradox through hypothesis tests of stochastic dominance, monotonicity of regression functions, and non-negative residual treatment effects. More details are available in Hsiao et al 2025 (under review). A tutorial for this package can be found at <https://laylaparast.com/home/SurrogateParadoxTest.html>.",
    "version": "2.0",
    "maintainer": "Emily Hsiao <ehsiao@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7396,
    "package_name": "SurrogateTest",
    "title": "Early Testing for a Treatment Effect using Surrogate Marker\nInformation",
    "description": "Provides functions to test for a treatment effect in terms of the difference in survival between a treatment group and a control group using surrogate marker information obtained at some early time point in a time-to-event outcome setting. Nonparametric kernel estimation is used to estimate the test statistic and perturbation resampling is used for variance estimation. More details will be available in the future in: Parast L, Cai T, Tian L (2019) ``Using a Surrogate Marker for Early Testing of a Treatment Effect\" Biometrics, 75(4):1253-1263. <doi:10.1111/biom.13067>.",
    "version": "1.3",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7397,
    "package_name": "SurvCorr",
    "title": "Correlation of Bivariate Survival Times",
    "description": "Estimates correlation coefficients with associated\n   confidence limits for bivariate, partially censored survival times. Uses\n   the iterative multiple imputation approach proposed\n   by Schemper, Kaider, Wakounig and Heinze (2013) <doi:10.1002/sim.5874>. Provides a scatterplot function to visualize the bivariate \n   distribution, either on the original time scale or as copula.",
    "version": "1.1",
    "maintainer": "Georg Heinze <georg.heinze@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7398,
    "package_name": "SurvDisc",
    "title": "Discrete Time Survival and Longitudinal Data Analysis",
    "description": "Various functions for discrete time survival analysis and longitudinal analysis. SIMEX method for correcting for bias for errors-in-variables\n  in a mixed effects model. Asymptotic mean and variance of different proportional hazards test statistics using different ties methods given two\n  survival curves and censoring distributions. Score test and Wald test for regression analysis of grouped survival data. Calculation of survival\n  curves for events defined by the response variable in a mixed effects model crossing a threshold with or without confirmation.",
    "version": "0.1.1",
    "maintainer": "John Lawrence <john.lawrence@fda.hhs.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7399,
    "package_name": "SurvEval",
    "title": "Methods for the Evaluation of Survival Models",
    "description": "Provides predictive accuracy tools to evaluate time-to-event survival models. This includes calculating the concordance probability estimate that incorporates the follow-up time for a particular study developed by Devlin, Gonen, Heller (2020)<doi:10.1007/s10985-020-09503-3>. It also evaluates the concordance probability estimate for nested Cox proportional hazards models using a projection-based approach by Heller and Devlin (under review). ",
    "version": "1.1",
    "maintainer": "Sean Devlin <devlins@mskcc.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7400,
    "package_name": "SurvGME",
    "title": "Analysis of Survival Data under Graphical and Measurement Error\nModels",
    "description": "The estimation method proposed by Chen and Yi (2021) \n <doi:10.1111/biom.13331> is extended to the analysis of survival data, \n accommodating  commonly used survival models while accounting for measurement\n error and network structures among covariates.",
    "version": "0.1.0",
    "maintainer": "Li-Pang Chen <lchen723@nccu.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7403,
    "package_name": "SurvLong",
    "title": "Analysis of Proportional Hazards Model with Sparse Longitudinal\nCovariates",
    "description": "Provides kernel weighting methods for estimation of proportional \n    hazards models with intermittently observed longitudinal covariates. \n    Cao H., Churpek M. M., Zeng D., and Fine J. P. (2015) \n    <doi:10.1080/01621459.2014.957289>.",
    "version": "1.5",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7404,
    "package_name": "SurvMA",
    "title": "Model Averaging Prediction of Personalized Survival\nProbabilities",
    "description": "Provide model averaging-based approaches that can be used to predict personalized survival probabilities. The key underlying idea is to approximate the conditional survival function using a weighted average of multiple candidate models. Two scenarios of candidate models are allowed: (Scenario 1) partial linear Cox model and (Scenario 2) time-varying coefficient Cox model. A reference of the underlying methods is Li and Wang (2023) <doi:10.1016/j.csda.2023.107759>.",
    "version": "1.6.8",
    "maintainer": "Mengyu Li <mylilucky@163.com>",
    "url": "<https://github.com/Stat-WangXG/SurvMA>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7405,
    "package_name": "SurvMI",
    "title": "Multiple Imputation Method in Survival Analysis",
    "description": "In clinical trials, endpoints are sometimes evaluated with uncertainty. Adjudication is commonly adopted to ensure the study integrity. We propose to use multiple imputation (MI) introduced by Robin (1987) <doi:10.1002/9780470316696> to incorporate these uncertainties if reasonable event probabilities were provided. The method has been applied to Cox Proportional Hazard (PH) model, Kaplan-Meier (KM) estimation and Log-rank test in this package. Moreover, weighted estimations discussed in Cook (2004) <doi:10.1016/S0197-2456(00)00053-2> were also implemented with weights calculated from event probabilities. In conclusion, this package can handle time-to-event analysis if events presented with uncertainty by different methods. ",
    "version": "0.1.0",
    "maintainer": "Yiming Chen <yimingc1208@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7406,
    "package_name": "SurvMetrics",
    "title": "Predictive Evaluation Metrics in Survival Analysis",
    "description": "An implementation of popular evaluation metrics that are commonly used in survival prediction including Concordance Index, Brier Score, Integrated Brier Score, \n  Integrated Square Error, Integrated Absolute Error and Mean Absolute Error.\n  For a detailed information, see (Ishwaran H, Kogalur UB, Blackstone EH and Lauer MS (2008) <doi:10.1214/08-AOAS169>) , \n  (Moradian H, Larocque D and Bellavance F (2017) <doi:10.1007/s10985-016-9372-1>), (Hanpu Zhou, Hong Wang, Sizheng Wang and Yi Zou (2023) <doi:10.32614/rj-2023-009>) for different evaluation metrics.",
    "version": "0.5.1",
    "maintainer": "Hong Wang <wh@csu.edu.cn>",
    "url": "https://github.com/whcsu/SurvMetrics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7407,
    "package_name": "SurvRegCensCov",
    "title": "Weibull Regression for a Right-Censored Endpoint with\nInterval-Censored Covariate",
    "description": "The function SurvRegCens() of this package allows estimation of a Weibull Regression for a right-censored endpoint, one interval-censored covariate, and an arbitrary number of non-censored covariates. Additional functions allow to switch between different parametrizations of Weibull regression used by different R functions, inference for the mean difference of two arbitrarily censored Normal samples, and estimation of canonical parameters from censored samples for several distributional assumptions. Hubeaux, S. and Rufibach, K. (2014) <doi:10.48550/arXiv.1402.0432>.",
    "version": "1.8",
    "maintainer": "Stanislas Hubeaux <stan.hubeaux@bluewin.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7408,
    "package_name": "SurvSparse",
    "title": "Survival Analysis with Sparse Longitudinal Covariates",
    "description": "Survival analysis with sparse longitudinal covariates under right censoring scheme. Different hazards models are involved. Please cite the manuscripts corresponding to this package:  Sun, Z. et al. (2022) <doi:10.1007/s10985-022-09548-6>, Sun, Z. and Cao, H. (2023)  <arXiv:2310.15877> and Sun, D. et al. (2023) <arXiv:2308.15549>. ",
    "version": "0.1",
    "maintainer": "Zhuowei Sun <sunzw21@mails.jlu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7409,
    "package_name": "SurvTrunc",
    "title": "Analysis of Doubly Truncated Data",
    "description": "Package performs Cox regression and survival distribution function estimation when the survival times are subject to double truncation. In case that the survival and truncation times are quasi-independent, the estimation procedure for each method involves inverse probability weighting, where the weights correspond to the inverse of the selection probabilities and are estimated using the survival times and truncation times only. A test for checking this independence assumption is also included in this package. The functions available in this package for Cox regression, survival distribution function estimation, and testing independence under double truncation are based on the following methods, respectively: Rennert and Xie (2018) <doi:10.1111/biom.12809>, Shen (2010) <doi:10.1007/s10463-008-0192-2>, Martin and Betensky (2005) <doi:10.1198/016214504000001538>. When the survival times are dependent on at least one of the truncation times, an EM algorithm is employed to obtain point estimates for the regression coefficients. The standard errors are calculated using the bootstrap method. See Rennert and Xie (2022) <doi:10.1111/biom.13451>. Both the independent and dependent cases assume no censoring is present in the data. Please contact Lior Rennert <liorr@clemson.edu> for questions regarding function coxDT and Yidan Shi <yidan.shi@pennmedicine.upenn.edu> for questions regarding function coxDTdep.  ",
    "version": "0.2.0",
    "maintainer": "Lior Rennert <liorr@clemson.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7412,
    "package_name": "SurviMChd",
    "title": "High Dimensional Survival Data Analysis with Markov Chain Monte\nCarlo",
    "description": "High dimensional survival data analysis with Markov Chain Monte Carlo(MCMC). \n              Currently supports frailty data analysis. Allows for Weibull and \n              Exponential distribution. Includes function for interval censored data.",
    "version": "0.1.2",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7414,
    "package_name": "SurvivalTests",
    "title": "Survival Tests for One-Way Layout",
    "description": "Performs survival analysis for one-way layout. The package includes the generalized test for survival ANOVA (Tsui and Weerahandi (1989) <doi:10.2307/2289949> and (Weerahandi, 2004; ISBN:978-0471470175)). It also performs pairwise comparisons and graphical approaches. Moreover, it assesses the weibullness of data in each group via test. The package computes mean and confidence interval under Weibull distribution.",
    "version": "1.0",
    "maintainer": "Osman Dag <osman.dag@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7416,
    "package_name": "SvyNom",
    "title": "Nomograms for Right-Censored Outcomes from Survey Designs",
    "description": "Builds, evaluates and validates a nomogram with survey data\n    and right-censored outcomes. As described in Capanu (2015)\n    <doi:10.18637/jss.v064.c01>, the package contains functions to create\n    the nomogram, validate it using bootstrap, as well as produce the\n    calibration plots.",
    "version": "1.2",
    "maintainer": "Mithat Gonen <gonenm@mskcc.org>",
    "url": "https://github.com/MSKCC-Epi-Bio/SvyNom,\nhttps://mskcc-epi-bio.github.io/SvyNom/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7433,
    "package_name": "Synth",
    "title": "Synthetic Control Group Method for Comparative Case Studies",
    "description": "Implements the synthetic control group method for comparative case studies\n as described in Abadie and Gardeazabal (2003) and Abadie, Diamond, and Hainmueller\n (2010, 2011, 2014). The synthetic control method allows for effect estimation in\n settings where a single unit (a state, country, firm, etc.) is exposed to an event\n or intervention. It provides a data-driven procedure to construct synthetic control\n units based on a weighted combination of comparison units that approximates the\n characteristics of the unit that is exposed to the intervention. A combination of\n comparison units often provides a better comparison for the unit exposed to the\n intervention than any comparison unit alone.",
    "version": "1.1-9",
    "maintainer": "Jens Hainmueller <jhain@stanford.edu>",
    "url": "https://web.stanford.edu/~jhain/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7446,
    "package_name": "TAM",
    "title": "Test Analysis Modules",
    "description": "\n    Includes marginal maximum likelihood estimation and joint maximum\n    likelihood estimation for unidimensional and multidimensional \n    item response models. The package functionality covers the \n    Rasch model, 2PL model, 3PL model, generalized partial credit model, \n    multi-faceted Rasch model, nominal item response model, \n    structured latent class model, mixture distribution IRT models, \n    and located latent class models. Latent regression models and \n    plausible value imputation are also supported. For details see\n    Adams, Wilson and Wang, 1997 <doi:10.1177/0146621697211001>,\n    Adams, Wilson and Wu, 1997 <doi:10.3102/10769986022001047>,\n    Formann, 1982 <doi:10.1002/bimj.4710240209>,\n    Formann, 1992 <doi:10.1080/01621459.1992.10475229>.",
    "version": "4.3-25",
    "maintainer": "Alexander Robitzsch <robitzsch@ipn.uni-kiel.de>",
    "url": "http://www.edmeasurementsurveys.com/TAM/Tutorials/,\nhttps://github.com/alexanderrobitzsch/TAM,\nhttps://sites.google.com/view/alexander-robitzsch/software",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7450,
    "package_name": "TAR",
    "title": "Bayesian Modeling of Autoregressive Threshold Time Series Models",
    "description": "Identification and estimation of the autoregressive threshold models with Gaussian noise, as well as positive-valued time series. The package provides the identification of the number of regimes, the thresholds and the autoregressive orders, as well as the estimation of remain parameters. The package implements the methodology from the 2005 paper: Modeling Bivariate Threshold Autoregressive Processes in the Presence of Missing Data <DOI:10.1081/STA-200054435>.",
    "version": "1.0",
    "maintainer": "Hanwen Zhang <hanwengutierrez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7454,
    "package_name": "TBFmultinomial",
    "title": "TBF Methodology Extension for Multinomial Outcomes",
    "description": "Extends the test-based Bayes factor (TBF) methodology to multinomial regression models and discrete time-to-event models with competing risks. The TBF methodology has been well developed and implemented for the generalised linear model [Held et al. (2015) <doi:10.1214/14-STS510>] and for the Cox model [Held et al. (2016) <doi:10.1002/sim.7089>].",
    "version": "0.1.3",
    "maintainer": "Rachel Heyard <rachel.heyard@uzh.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7503,
    "package_name": "THETASVM",
    "title": "Time Series Forecasting using THETA-SVM Hybrid Model",
    "description": "Testing, Implementation, and Forecasting of the THETA-SVM hybrid model. The THETA-SVM hybrid model combines the distinct strengths of the THETA model and the Support Vector Machine (SVM) model for time series forecasting.For method details see Bhattacharyya et al. (2022) <doi:10.1007/s11071-021-07099-3>.",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7509,
    "package_name": "TITEgBOIN",
    "title": "Time-to-Event Dose-Finding Design for Multiple Toxicity Grades",
    "description": "In some phase I trials, the design goal is to find the dose associated with a certain target toxicity rate or the dose with a certain weighted sum of rates of various toxicity grades. 'TITEgBOIN' provides the set up and calculations needed to run a dose-finding trial using bayesian optimal interval (BOIN) (Yuan et al. (2016) <doi:10.1158/1078-0432.CCR-16-0592>), generalized bayesian optimal interval (gBOIN) (Mu et al. (2019) <doi:10.1111/rssc.12263>), time-to-event bayesian optimal interval (TITEBOIN) (Lin et al. (2020) <doi:10.1093/biostatistics/kxz007>) and time-to-event generalized bayesian optimal interval (TITEgBOIN) (Takeda et al. (2022) <doi:10.1002/pst.2182>) designs. 'TITEgBOIN' can conduct tasks: run simulations and get operating characteristics; determine the dose for the next cohort; select maximum tolerated dose (MTD). These functions allow customization of design characteristics to vary sample size, cohort sizes, target dose limiting toxicity (DLT) rates or target normalized equivalent toxicity score (ETS) rates to account for discrete toxicity score, and incorporate safety and/or stopping rules.",
    "version": "0.4.0",
    "maintainer": "Jing Zhu <zhujing716@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7512,
    "package_name": "TLIC",
    "title": "The LIC for T Distribution Regression Analysis",
    "description": "This comprehensive toolkit for T-distributed regression is designated as \"TLIC\" (The LIC for T Distribution Regression Analysis) analysis. It is predicated on the assumption that the error term adheres to a T-distribution. The philosophy of the package is described in Guo G. (2020) <doi:10.1080/02664763.2022.2053949>. ",
    "version": "0.4",
    "maintainer": "Guangbao Guo <ggb11111111@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7528,
    "package_name": "TOSI",
    "title": "Two-Directional Simultaneous Inference for High-Dimensional\nModels",
    "description": "A general framework of two directional simultaneous inference\n    is provided for high-dimensional as well as the fixed dimensional models with manifest\n    variable or latent variable structure, such as high-dimensional mean models, high-\n    dimensional sparse regression models, and high-dimensional latent factors models.\n    It is making the simultaneous inference on a set of parameters from two directions,\n    one is testing whether the estimated zero parameters indeed are zero and the other is\n    testing whether there exists zero in the parameter set of non-zero. More details can be \n    referred to Wei Liu, et al. (2022) <doi:10.48550/arXiv.2012.11100>.",
    "version": "0.3.0",
    "maintainer": "Wei Liu <weiliu@smail.swufe.edu.cn>",
    "url": "https://github.com/feiyoung/TOSI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7550,
    "package_name": "TRES",
    "title": "Tensor Regression with Envelope Structure",
    "description": "Provides three estimators for tensor response regression (TRR) and tensor predictor regression (TPR) models with tensor envelope structure. The three types of estimation approaches are generic and can be applied to any envelope estimation problems. The full Grassmannian (FG) optimization is often associated with likelihood-based estimation but requires heavy computation and good initialization; the one-directional optimization approaches (1D and ECD algorithms) are faster, stable and does not require carefully chosen initial values; the SIMPLS-type is motivated by the partial least squares regression and is computationally the least expensive. For details of TRR, see Li L, Zhang X (2017) <doi:10.1080/01621459.2016.1193022>. For details of TPR, see Zhang X, Li L (2017) <doi:10.1080/00401706.2016.1272495>. For details of 1D algorithm, see Cook RD, Zhang X (2016) <doi:10.1080/10618600.2015.1029577>. For details of ECD algorithm, see Cook RD, Zhang X (2018) <doi:10.5705/ss.202016.0037>. For more details of the package, see Zeng J, Wang W, Zhang X (2021) <doi:10.18637/jss.v099.i12>.",
    "version": "1.1.5",
    "maintainer": "Jing Zeng <jing.zeng@stat.fsu.edu>",
    "url": "https://github.com/leozeng15/TRES",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7554,
    "package_name": "TSA",
    "title": "Time Series Analysis",
    "description": "Contains R functions and datasets detailed in the book\n        \"Time Series Analysis with Applications in R (second edition)\" by Jonathan Cryer and Kung-Sik Chan.",
    "version": "1.3.1",
    "maintainer": "Kung-Sik Chan <kungsik.chan@gmail.com>",
    "url": "https://stat.uiowa.edu/~kchan/TSA.htm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7555,
    "package_name": "TSANN",
    "title": "Time Series Artificial Neural Network",
    "description": "The best ANN structure for time series data analysis is a demanding need in the present era.\n    This package will find the best-fitted ANN model based on forecasting accuracy.\n    The optimum size of the hidden layers was also determined after determining the number of lags to be included.\n    This package has been developed using the algorithm of Paul and Garai (2021) <doi:10.1007/s00500-021-06087-4>.",
    "version": "0.1.0",
    "maintainer": "Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7563,
    "package_name": "TSEAL",
    "title": "Time Series Analysis Library",
    "description": "The library allows to perform a multivariate time series\n    classification based on the use of Discrete Wavelet Transform for\n    feature extraction, a step wise discriminant to select the most\n    relevant features and finally, the use of a linear or quadratic\n    discriminant for classification. Note that all these steps can be done\n    separately which allows to implement new steps.  Velasco, I., Sipols,\n    A., de Blas, C. S., Pastor, L., & Bayona, S. (2023)\n    <doi:10.1186/S12938-023-01079-X>.  Percival, D. B., & Walden, A. T.\n    (2000,ISBN:0521640687).  Maharaj, E. A., & Alonso, A. M. (2014)\n    <doi:10.1016/j.csda.2013.09.006>.",
    "version": "0.1.3",
    "maintainer": "Iván Velasco <ivan.velasco@urjc.es>",
    "url": "https://github.com/vg-lab/TSEAL",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7565,
    "package_name": "TSEntropies",
    "title": "Time Series Entropies",
    "description": "Computes various entropies of given time series. This is the initial version that includes ApEn() and SampEn() functions for calculating approximate entropy and sample entropy. Approximate entropy was proposed by S.M. Pincus in \"Approximate entropy as a measure of system complexity\", Proceedings of the National Academy of Sciences of the United States of America, 88, 2297-2301 (March 1991). Sample entropy was proposed by J. S. Richman and J. R. Moorman in \"Physiological time-series analysis using approximate entropy and sample entropy\", American Journal of Physiology, Heart and Circulatory Physiology, 278, 2039-2049 (June 2000). This package also contains FastApEn() and FastSampEn() functions for calculating fast approximate entropy and fast sample entropy. These are newly designed very fast algorithms, resulting from the modification of the original algorithms. The calculated values of these entropies are not the same as the original ones, but the entropy trend of the analyzed time series determines equally reliably. Their main advantage is their speed, which is up to a thousand times higher. A scientific article describing their properties has been submitted to The Journal of Supercomputing and in present time it is waiting for the acceptance.",
    "version": "0.9",
    "maintainer": "Jiri Tomcala <jiri.tomcala@vsb.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7568,
    "package_name": "TSF",
    "title": "Two Stage Forecasting (TSF) for Long Memory Time Series in\nPresence of Structural Break",
    "description": "Forecasting of long memory time series in presence of structural break by using TSF algorithm by Papailias and Dias (2015) <doi:10.1016/j.ijforecast.2015.01.006>. ",
    "version": "0.1.1",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7571,
    "package_name": "TSHRC",
    "title": "Two Stage Hazard Rate Comparison",
    "description": "Two-stage procedure compares hazard rate functions,\n    which may or may not cross each other.",
    "version": "0.1-6",
    "maintainer": "Charles J. Geyer <charlie@stat.umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7574,
    "package_name": "TSLSTM",
    "title": "Long Short Term Memory (LSTM) Model for Time Series Forecasting",
    "description": "The LSTM (Long Short-Term Memory) model is a Recurrent Neural Network (RNN) based architecture that is widely used for time series forecasting. Min-Max transformation has been used for data preparation. Here, we have used one LSTM layer as a simple LSTM model and a Dense layer is used as the output layer. Then, compile the model using the loss function, optimizer and metrics. This package is based on Keras and TensorFlow modules and the algorithm of Paul and Garai (2021) <doi:10.1007/s00500-021-06087-4>.",
    "version": "0.1.0",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7575,
    "package_name": "TSLSTMplus",
    "title": "Long-Short Term Memory for Time-Series Forecasting, Enhanced",
    "description": "The LSTM (Long Short-Term Memory) model is a Recurrent Neural Network (RNN) based architecture that is widely used for time series forecasting. Customizable configurations for the model are allowed, improving the capabilities and usability of this model compared to other packages. This package is based on 'keras' and 'tensorflow' modules and the algorithm of Paul and Garai (2021) <doi:10.1007/s00500-021-06087-4>.",
    "version": "1.0.6",
    "maintainer": "Jaime Pizarroso Gonzalo <jpizarroso@comillas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7576,
    "package_name": "TSMCP",
    "title": "Fast Two Stage Multiple Change Point Detection",
    "description": "A novel and fast two stage method for simultaneous multiple change point detection and variable selection for piecewise stationary autoregressive (PSAR) processes and linear regression model. It also simultaneously performs variable selection for each autoregressive model and hence the order selection.",
    "version": "1.1",
    "maintainer": "Yaguang Li <liyg@mail.ustc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7582,
    "package_name": "TSS.RESTREND",
    "title": "Time Series Segmentation of Residual Trends",
    "description": "Time Series Segmented Residual Trends is a method for the automated detection of land degradation from remotely sensed vegetation and climate datasets. TSS-RESTREND incorporates aspects of two existing degradation detection methods: RESTREND which is used to control for climate variability, and BFAST which is used to look for structural changes in the ecosystem. The full details of the testing and justification of the TSS-RESTREND method (version 0.1.02) are published in Burrell et al., (2017). <doi:10.1016/j.rse.2017.05.018>. The changes to the method introduced in version 0.2.03 focus on the inclusion  of temperature as an additional climate variable. This allows for land  degradation assessment in temperature limited drylands. A paper that details this work is currently under review. There are also a number of bug fixes and speed improvements. Version 0.3.0 introduces additional attribution for eCO2,  climate change and climate variability the details of which are in press in Burrell et al., (2020).  The version under active development and additional example scripts showing  how the package can be applied can be found at <https://github.com/ArdenB/TSSRESTREND>. ",
    "version": "0.3.1",
    "maintainer": "Arden Burrell <arden.burrell@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7583,
    "package_name": "TSSS",
    "title": "Time Series Analysis with State Space Model",
    "description": "Functions for statistical analysis, modeling and simulation of time\n series with state space model, based on the methodology in Kitagawa\n (2020, ISBN: 978-0-367-18733-0).",
    "version": "1.3.4-5",
    "maintainer": "Masami Saga <msaga@mtb.biglobe.ne.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7585,
    "package_name": "TSTutorial",
    "title": "Fitting and Predict Time Series Interactive Laboratory",
    "description": "Interactive laboratory of Time Series based in Box-Jenkins methodology.",
    "version": "1.2.7",
    "maintainer": "Alberto Lopez Moreno <bertolomo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7587,
    "package_name": "TSclust",
    "title": "Time Series Clustering Utilities",
    "description": "A set of measures of dissimilarity between time series to perform time series clustering. Metrics based on raw data, on generating models and on the forecast behavior are implemented. Some additional utilities related to time series clustering are also provided, such as clustering algorithms and cluster evaluation metrics.",
    "version": "1.3.2",
    "maintainer": "Pablo Montero Manso <pmontm@gmail.com>",
    "url": "https://doi.org/10.18637/jss.v062.i01",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7588,
    "package_name": "TSdisaggregation",
    "title": "High-Dimensional Temporal Disaggregation",
    "description": "First - Generates (potentially high-dimensional) high-frequency and low-frequency series for simulation studies in temporal disaggregation; Second - a toolkit utilizing temporal disaggregation and benchmarking techniques with a low-dimensional matrix of indicator series previously proposed in Dagum and Cholette (2006, ISBN:978-0-387-35439-2) ; and Third - novel techniques proposed by Mosley, Gibberd and Eckley (2021) <arXiv:2108.05783> for disaggregating low-frequency series in the presence of high-dimensional indicator matrices.",
    "version": "2.0.0",
    "maintainer": "Luke Mosley <l.mosley@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7589,
    "package_name": "TSdist",
    "title": "Distance Measures for Time Series Data",
    "description": "A set of commonly used distance measures and some additional functions which, although initially not designed for this purpose, can be used to measure the dissimilarity between time series. These measures can be used to perform clustering, classification or other data mining tasks which require the definition of a distance measure between time series. U. Mori, A. Mendiburu and J.A. Lozano (2016), <doi:10.32614/RJ-2016-058>.",
    "version": "3.7.1",
    "maintainer": "Usue Mori <usue.mori@ehu.eus>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7590,
    "package_name": "TSeriesMMA",
    "title": "Multiscale Multifractal Analysis of Time Series Data",
    "description": "Multiscale multifractal analysis (MMA) (Gierałtowski et al.,\n    2012)<DOI:10.1103/PhysRevE.85.021915> is a time series analysis method,\n    designed to describe scaling properties of fluctuations within the signal\n    analyzed. The main result of this procedure is the so called Hurst surface\n    h(q,s) , which is a dependence of the local Hurst exponent h (fluctuation\n    scaling exponent) on the multifractal parameter q and the scale of observation s\n    (data window width).",
    "version": "0.1.1",
    "maintainer": "Vishakh Padmakumar <vishakhpadmakumar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7592,
    "package_name": "TSsmoothing",
    "title": "Trend Estimation of Univariate and Bivariate Time Series with\nControlled Smoothness",
    "description": "It performs the smoothing approach provided by penalized least squares for univariate and bivariate time series, as proposed by Guerrero (2007) and Gerrero et al. (2017). \n          This allows to estimate the time series trend by controlling the amount of resulting (joint) smoothness.\n          ---\n          Guerrero, V.M (2007)  <DOI:10.1016/j.spl.2007.03.006>.\n          Guerrero, V.M; Islas-Camargo, A. and Ramirez-Ramirez, L.L. (2017) <DOI:10.1080/03610926.2015.1133826>.",
    "version": "0.1.0",
    "maintainer": "L. Leticia Ramirez-Ramirez <leticia.ramirez@cimat.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7594,
    "package_name": "TTAinterfaceTrendAnalysis",
    "title": "Temporal Trend Analysis Graphical Interface",
    "description": "This interface was created to develop a standard procedure \n to analyse temporal trend in the framework of the OSPAR convention.\n The analysis process run through 4 successive steps : 1) manipulate your data, 2)\n select the parameters you want to analyse, 3) build your regulated \n time series, 4) perform diagnosis and analysis and 5) read the results. \n Statistical analysis call other package function such as Kendall tests\n or cusum() function.",
    "version": "1.5.11",
    "maintainer": "David DEVREKER <David.Devreker@ifremer.fr>",
    "url": "https://CRAN.R-project.org/package=TTAinterfaceTrendAnalysis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7619,
    "package_name": "TapeR",
    "title": "Flexible Tree Taper Curves Based on Semiparametric Mixed Models",
    "description": "Implementation of functions for fitting taper curves (a semiparametric \n  linear mixed effects taper model) to diameter measurements along stems. Further \n  functions are provided to estimate the uncertainty around the predicted curves, \n  to calculate timber volume (also by sections) and marginal (e.g., upper) diameters. \n  For cases where tree heights are not measured, methods for estimating\n  additional variance in volume predictions resulting from uncertainties in\n  tree height models (tariffs) are provided.  The example data include the taper \n  curve parameters for Norway spruce used in the 3rd German NFI fitted to 380 trees \n  and a subset of section-wise diameter measurements of these trees. The functions \n  implemented here are detailed in Kublin, E., Breidenbach, J., Kaendler, G. (2013)\n  <doi:10.1007/s10342-013-0715-0>.",
    "version": "0.5.3",
    "maintainer": "Christian Vonderach <christian.vonderach@forst.bwl.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7630,
    "package_name": "Tcomp",
    "title": "Data from the 2010 Tourism Forecasting Competition",
    "description": "The 1311 time series from the tourism forecasting competition conducted in 2010 and described in Athanasopoulos et al. (2011) <DOI:10.1016/j.ijforecast.2010.04.009>.",
    "version": "1.0.1",
    "maintainer": "Peter Ellis <peter.ellis2013nz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7636,
    "package_name": "TempCont",
    "title": "Temporal Contributions on Trends using Mixed Models",
    "description": "Method to estimate the effect of the trend in predictor variables on the observed trend\n of the response variable using mixed models with temporal autocorrelation. See Fernández-Martínez et\n al. (2017 and 2019) <doi:10.1038/s41598-017-08755-8> <doi:10.1038/s41558-018-0367-7>.",
    "version": "0.1.0",
    "maintainer": "Marcos Fernández-Martínez <marcos.fernandez-martinez@uantwerpen.be>",
    "url": "https://github.com/burriach/tempcont",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7638,
    "package_name": "Temporal",
    "title": "Parametric Time to Event Analysis",
    "description": "Performs maximum likelihood based estimation and inference on time to event data, possibly subject to non-informative right censoring. FitParaSurv() provides maximum likelihood estimates of model parameters and distributional characteristics, including the mean, median, variance, and restricted mean. CompParaSurv() compares the mean, median, and restricted mean survival experiences of two treatment groups. Candidate distributions include the exponential, gamma, generalized gamma, log-normal, and Weibull. ",
    "version": "0.3.0.1",
    "maintainer": "Zachary McCaw <zmccaw@alumni.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7643,
    "package_name": "TensorPreAve",
    "title": "Rank and Factor Loadings Estimation in Time Series Tensor Factor\nModels",
    "description": "A set of functions to estimate rank and factor loadings of time series tensor factor models. A tensor is a multidimensional array. To analyze high-dimensional tensor time series, factor model is a major dimension reduction tool. 'TensorPreAve' provides functions to estimate the rank of core tensors and factor loading spaces of tensor time series. More specifically, a pre-averaging method that accumulates information from tensor fibres is used to estimate the factor loading spaces. The estimated directions corresponding to the strongest factors are then used for projecting the data for a potentially improved re-estimation of the factor loading spaces themselves. A new rank estimation method is also implemented to utilizes correlation information from the projected data. \n    See Chen and Lam (2023) <arXiv:2208.04012> for more details.",
    "version": "1.1.0",
    "maintainer": "Weilin Chen <w.chen56@lse.ac.uk>",
    "url": "https://github.com/William-Chenwl/TensorPreAve",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7679,
    "package_name": "TiPS",
    "title": "Trajectories and Phylogenies Simulator",
    "description": "Generates stochastic time series and genealogies associated with a population dynamics model. Times series are simulated using the Gillespie exact and approximate algorithms and a new algorithm we introduce that uses both approaches to optimize the time execution of the simulations. Genealogies are simulated from a trajectory using a backwards-in-time based approach. Methods are described in Danesh G et al (2022) <doi:10.1111/2041-210X.14038>.",
    "version": "1.3.0",
    "maintainer": "Gonche Danesh <gonche.danesh@gmail.com>",
    "url": "https://gitlab.in2p3.fr/ete/tips/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7680,
    "package_name": "TideCurves",
    "title": "Analysis and Prediction of Tides",
    "description": "Tidal analysis of evenly spaced observed time series (time step 1 to 60 min) with or\n    without shorter gaps using the harmonic representation of inequalities.\n    The analysis should preferably cover an observation period of at least 19 years.\n    For shorter periods low frequency constituents are not taken into account, in accordance with the Rayleigh-Criterion.\n    The main objective of this package is to synthesize or predict a tidal time series.",
    "version": "0.0.5",
    "maintainer": "Moritz Mueller-Navarra <muellernavarra@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7683,
    "package_name": "Tides",
    "title": "Quasi-Periodic Time Series Characteristics",
    "description": "Calculate Characteristics of Quasi-Periodic Time Series, e.g. Estuarine Water Levels.",
    "version": "2.1",
    "maintainer": "Tom Cox <tom.cox@uantwerp.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7687,
    "package_name": "TimeDepFrail",
    "title": "Time Dependent Shared Frailty Cox Model",
    "description": "Fits time-dependent shared frailty Cox model (specifically the adapted Paik et al.'s Model) based on the paper \"Centre-Effect on Survival After Bone Marrow Transplantation: Application of Time-Dependent Frailty Models\", by C.M. Wintrebert, H. Putter, A.H. Zwinderman and J.C. van Houwelingen (2004) <doi:10.1002/bimj.200310051>.",
    "version": "0.1.0",
    "maintainer": "Alessandra Ragni <alessandra.ragni@polimi.it>",
    "url": "https://alessandragni.github.io/TimeDepFrail/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7689,
    "package_name": "TimeVarConcurrentModel",
    "title": "Concurrent Multivariate Models with Time-Varying Coefficients",
    "description": "Provides a hypothesis test and variable selection algorithm for use in time-varying, concurrent regression models. The hypothesis test function is also accompanied by a plotting function which will show the estimated beta(s) and confidence band(s) from the hypothesis test. The hypothesis test function helps the user identify significant covariates within the scope of a time-varying concurrent model. The plots will show the amount of area that falls outside the confidence band(s) which is used for the test statistic within the hypothesis test. ",
    "version": "1.0",
    "maintainer": "Chance Vandergeugten <leaveitalltochance@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7690,
    "package_name": "TimeVizPro",
    "title": "Dynamic Data Explorer: Visualize and Forecast with 'TimeVizPro'",
    "description": "Unleash the power of time-series data visualization with ease using our package. Designed with simplicity \n in mind, it offers three key features through the 'shiny' package output. The first tab shows time- series \n charts with forecasts, allowing users to visualize trends and changes effortlessly. The second one displays \n Averages per country presented in tables with accompanying sparklines, providing a quick and attractive \n overview of the data. The last tab presents A customizable world map colored based on user-defined \n variables for any chosen number of countries, offering an advanced visual approach to understanding \n geographical data distributions. This package operates with just a few simple arguments, enabling users \n to conduct sophisticated analyses without the need for complex programming skills. Transform your \n time-series data analysis experience with our user-friendly tool. ",
    "version": "1.0.1",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7709,
    "package_name": "ToxCrit",
    "title": "Calculates Safety Stopping Boundaries for a Single-Arm Trial\nusing Bayes",
    "description": "Computation of stopping boundaries for a single-arm trial using a\n    Bayesian criterion; i.e., for each m<=n (n= total patient number of the \n    trial) the smallest number of observed toxicities is calculated\n    leading to the termination of the trial/accrual according to the specified \n    criteria. The probabilities of stopping the trial/accrual at and up until \n    (resp.) the m-th patient (m<=n) is also calculated. This design is more \n    conservative than the frequentist approach (using Clopper Pearson CIs)\n    which might be preferred as it concerns safety.See also Aamot et.al.(2010) \n    \"Continuous monitoring of toxicity in clinical trials - simulating the risk \n    of stopping prematurely\" <doi:10.5414/cpp48476>.",
    "version": "1.0",
    "maintainer": "Lisa-Marie Lanz <studienzentrale@nct-heidelberg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7713,
    "package_name": "TraMineR",
    "title": "Trajectory Miner: a Sequence Analysis Toolkit",
    "description": "Set of sequence analysis tools for manipulating, describing and rendering categorical sequences, and more generally mining sequence data in the field of social sciences. Although this sequence analysis package is primarily intended for state or event sequences that describe time use or life courses such as family formation histories or professional careers, its features also apply to many other kinds of categorical sequence data. It accepts many different sequence representations as input and provides tools for converting sequences from one format to another. It offers several functions for describing and rendering sequences, for computing distances between sequences with different metrics (among which optimal matching), original dissimilarity-based analysis tools, and functions for extracting the most frequent event subsequences and identifying the most discriminating ones among them. A user's guide can be found on the TraMineR web page.",
    "version": "2.2-13",
    "maintainer": "Gilbert Ritschard <gilbert.ritschard@unige.ch>",
    "url": "http://traminer.unige.ch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7714,
    "package_name": "TraMineRextras",
    "title": "TraMineR Extension",
    "description": "Collection of ancillary functions and utilities to be used in conjunction with the 'TraMineR' package for sequence data exploration. Includes, among others, specific functions such as state survival plots, position-wise group-typical states, dynamic sequence indicators, and dissimilarities between event sequences. Also includes contributions by non-members of the TraMineR team such as methods for polyadic data and for the comparison of groups of sequences.",
    "version": "0.6.8",
    "maintainer": "Gilbert Ritschard <gilbert.ritschard@unige.ch>",
    "url": "http://traminer.unige.ch/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7715,
    "package_name": "TraceAssist",
    "title": "Nonparametric Trace Regression via Sign Series Representation",
    "description": "Efficient method for fitting nonparametric matrix trace regression model. The detailed description can be found in  C. Lee, L. Li, H. Zhang, and M. Wang (2021). Nonparametric Trace Regression via Sign Series Representation. <arXiv:2105.01783>. The method employs the aggregation of structured sign series for trace regression (ASSIST) algorithm.",
    "version": "0.1.0",
    "maintainer": "Chanwoo Lee <chanwoo.lee@wisc.edu>",
    "url": "https://arxiv.org/abs/2105.01783",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7717,
    "package_name": "TractorTsbox",
    "title": "Wrangle and Modify Ts Object with Classic Frequencies and Exact\nDates",
    "description": "The ts objects in R are managed using a very specific date format (in the form c(2022, 9) for September 2022 or c(2021, 2) for the second quarter of 2021, depending on the frequency, for example). We focus solely on monthly and quarterly series to manage the dates of ts objects. The general idea is to offer a set of functions to manage this date format without it being too restrictive or too imprecise depending on the rounding. This is a compromise between simplicity, precision and use of the basic 'stats' functions for creating and managing time series (ts(), window()).\n    Les objets ts en R sont gérés par un format de date très particulier (sous la forme c(2022, 9) pour septembre 2022 ou c(2021, 2) pour le deuxième trimestre 2021 selon la fréquence par exemple). On se concentre uniquement sur les séries mensuelles et trimestrielles pour gérer les dates des objets ts. Lidée générale est de proposer un ensemble de fonctions pour gérer ce format de date sans que ce soit trop contraignant ou trop imprécis selon les arrondis. Cest un compromis entre simplicité, précision et utilisation des fonctions du package 'stats' de création et de gestion des séries temporelles (ts(), window()).",
    "version": "0.1.1",
    "maintainer": "Tanguy Barthelemy <tangbarth@hotmail.fr>",
    "url": "https://github.com/TractorTom/TractorTsbox,\nhttps://tractortom.github.io/TractorTsbox/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7723,
    "package_name": "TransModel",
    "title": "Fit Linear Transformation Models for Right Censored Data",
    "description": "A unified estimation procedure for the analysis of right censored data using linear transformation models. An introduction can be found in Jie Zhou et al. (2022) <doi:10.18637/jss.v101.i09>. ",
    "version": "2.3",
    "maintainer": "Jie Zhou <zhoujie02569@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7747,
    "package_name": "TrendInTrend",
    "title": "Odds Ratio Estimation and Power Calculation for the Trend in\nTrend Model",
    "description": "Estimation of causal odds ratio and power calculation given trends in exposure prevalence    and outcome frequencies of stratified data.",
    "version": "1.1.3",
    "maintainer": "Ashkan Ertefaie <ashkan_ertefaie@urmc.rochester.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7753,
    "package_name": "TriDimRegression",
    "title": "Bayesian Statistics for 2D/3D Transformations",
    "description": "Fits 2D and 3D geometric transformations via 'Stan' probabilistic programming engine ( \n    Stan Development Team (2021) <https://mc-stan.org>). Returns posterior distribution for individual\n    parameters of the fitted distribution. Allows for computation of LOO and WAIC information criteria \n    (Vehtari A, Gelman A, Gabry J (2017) <doi:10.1007/s11222-016-9696-4>) as well as Bayesian R-squared\n    (Gelman A, Goodrich B, Gabry J, and Vehtari A (2018) <doi:10.1080/00031305.2018.1549100>).",
    "version": "1.0.3",
    "maintainer": "Alexander (Sasha) Pastukhov <pastukhov.alexander@gmail.com>",
    "url": "https://github.com/alexander-pastukhov/tridim-regression,\nhttps://alexander-pastukhov.github.io/tridim-regression/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7764,
    "package_name": "TrueSkillThroughTime",
    "title": "Skill Estimation Based on a Single Bayesian Network",
    "description": "\n    Most estimators implemented by the video game industry cannot obtain reliable initial estimates nor guarantee comparability between distant estimates. TrueSkill Through Time solves all these problems by modeling the entire history of activities using a single Bayesian network allowing the information to propagate correctly throughout the system. This algorithm requires only a few iterations to converge, allowing millions of observations to be analyzed using any low-end computer.\n    Landfried G, Mocskos E (2025). \"TrueSkill Through Time: Reliable Initial Skill Estimates and Historical Comparability with Julia, Python, and R.\" <doi:10.18637/jss.v112.i06>.\n    The core ideas implemented in this project were developed by Dangauthier P, Herbrich R, Minka T, Graepel T (2007). \"Trueskill through time: Revisiting the history of chess.\".",
    "version": "1.0.0",
    "maintainer": "Gustavo Landfried <gustavolandfried@gmail.com>",
    "url": "https://github.com/glandfried/TrueSkillThroughTime.R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7769,
    "package_name": "TukeyC",
    "title": "Conventional Tukey Test",
    "description": "Provides tools to perform multiple comparison analyses, based on the well-known Tukey's \"Honestly Significant Difference\" (HSD) test. In models involving interactions, 'TukeyC' stands out from other R packages by implementing intuitive and easy-to-use functions. In addition to accommodating traditional R methods such as lm() and aov(), it has also been extended to objects of the lmer() class, that is, mixed models with fixed effects. For more details see Tukey (1949) <doi:10.2307/3001913>.",
    "version": "1.3-43",
    "maintainer": "Ivan Bezerra Allaman <ivanalaman@gmail.com>",
    "url": "https://github.com/jcfaria/TukeyC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7776,
    "package_name": "TwoArmSurvSim",
    "title": "Simulate Survival Data for Randomized Clinical Trials",
    "description": "A system to simulate clinical trials with time to event endpoints. Event simulation is based on Cox models allowing for covariates in addition to the treatment or group factor. Specific drop-out rates (separate from administrative censoring) can be controlled in the simulation. Other features include stratified randomization, non-proportional hazards, different accrual patterns, and event projection (timing to reach the target event) based on interim data.",
    "version": "0.2",
    "maintainer": "Bo Zhang <bzhang@pumabiotechnology.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7779,
    "package_name": "TwoRegression",
    "title": "Develop and Apply Two-Regression Algorithms",
    "description": "Facilitates development and application of two-regression\n    algorithms for research-grade wearable devices. It provides an easy\n    way for users to access previously-developed algorithms, and also to\n    develop their own. Initial motivation came from Hibbing PR, LaMunion SR,\n    Kaplan AS, & Crouter SE (2018) <doi:10.1249/MSS.0000000000001532>.\n    However, other algorithms are now supported. Please see the associated\n    references in the package documentation for full details of the algorithms\n    that are supported.",
    "version": "1.1.1",
    "maintainer": "Paul R. Hibbing <paulhibbing@gmail.com>",
    "url": "https://github.com/paulhibbing/TwoRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7782,
    "package_name": "TwoTimeScales",
    "title": "Analysis of Event Data with Two Time Scales",
    "description": "Analyse time to event data with two time scales by estimating a smooth hazard that varies over two time scales and also, if covariates are available, to estimate a proportional hazards model with such a two-dimensional baseline hazard. \n  Functions are provided to prepare the raw data for estimation, to estimate and to plot the two-dimensional smooth hazard. \n  Extension to a competing risks model are implemented. For details about the method please refer to Carollo et al. (2024) <doi:10.1002/sim.10297>.",
    "version": "1.0.0",
    "maintainer": "Angela Carollo <carollo@demogr.mpg.de>",
    "url": "https://github.com/AngelaCar/TwoTimeScales,\nhttps://angelacar.github.io/TwoTimeScales/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7783,
    "package_name": "TwoWayFEWeights",
    "title": "Estimation of the Weights Attached to the Two-Way Fixed Effects\nRegressions",
    "description": "\n    Estimates the weights and measure of robustness to treatment effect heterogeneity attached to two-way fixed effects regressions.\n    Clément de Chaisemartin, Xavier D'Haultfœuille (2020) <DOI: 10.1257/aer.20181169>.",
    "version": "2.0.4",
    "maintainer": "Diego Ciccia <diego.ciccia@kellogg.northwestern.edu>",
    "url": "https://github.com/chaisemartinPackages",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7784,
    "package_name": "TxEffectsSurvival",
    "title": "Treatment Effect Inference for Terminal and Non-Terminal Events\nunder Competing Risks",
    "description": "\n    Provides several confidence interval and testing procedures, based on either \n    semiparametric (using event-specific win ratios) or nonparametric measures, \n    including the ratio of integrated cumulative hazard (RICH) and the ratio of \n    integrated transformed cumulative hazard (RITCH), for treatment effect inference \n    with terminal and non-terminal events under competing risks. The semiparametric \n    results were developed in Yang et al. (2022 <doi:10.1002/sim.9266>), and the \n    nonparametric results were developed in Yang (2025 <doi:10.1002/sim.70205>). \n    For comparison, results for the win ratio (Finkelstein and Schoenfeld 1999 \n    <doi:10.1002/(SICI)1097-0258(19990615)18:11%3C1341::AID-SIM129%3E3.0.CO;2-7>), \n    Pocock et al. 2012 <doi:10.1093/eurheartj/ehr352>, and Bebu and Lachin 2016 \n    <doi:10.1093/biostatistics/kxv032>) are included. The package also supports \n    univariate survival analysis with a single event. In this package, effect size \n    estimates and confidence intervals are obtained for each event type, and several \n    testing procedures are implemented for the global null hypothesis of no treatment \n    effect on either terminal or non-terminal events. Furthermore, a test of proportional \n    hazards assumptions, under which the event-specific win ratios converge to hazard \n    ratios, and a test of equal hazard ratios, are provided. For summarizing the treatment \n    effect across all events, confidence intervals for linear combinations of the \n    event-specific win ratios, RICH, or RITCH are available using pre-determined or \n    data-driven weights. Asymptotic properties of these inference procedures are \n    discussed in Yang et al. (2022 <doi:10.1002/sim.9266>) and Yang (2025 \n    <doi:10.1002/sim.70205>).",
    "version": "1.0.2",
    "maintainer": "Daewoo Pak <dpak@yonsei.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7792,
    "package_name": "UBayFS",
    "title": "A User-Guided Bayesian Framework for Ensemble Feature Selection",
    "description": "The framework proposed in Jenul et al., (2022) <doi:10.1007/s10994-022-06221-9>, together with an interactive Shiny dashboard. 'UBayFS' is an ensemble feature selection technique embedded in a Bayesian statistical framework. The method combines data and user knowledge, where the first is extracted via data-driven ensemble feature selection. The user can control the feature selection by assigning prior weights to features and penalizing specific feature combinations. 'UBayFS' can be used for common feature selection as well as block feature selection.",
    "version": "1.0",
    "maintainer": "Anna Jenul <anna.jenul@nmbu.no>",
    "url": "https://annajenul.github.io/UBayFS/,\nhttps://joss.theoj.org/papers/10.21105/joss.04848",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7798,
    "package_name": "UComp",
    "title": "Automatic Univariate Time Series Modelling of many Kinds",
    "description": "Comprehensive analysis and forecasting \n             of univariate time series using automatic \n             time series models of many kinds.\n             Harvey AC (1989) <doi:10.1017/CBO9781107049994>.\n             Pedregal DJ and Young PC (2002) <doi:10.1002/9780470996430>.\n             Durbin J and Koopman SJ (2012) <doi:10.1093/acprof:oso/9780199641178.001.0001>.\n             Hyndman RJ, Koehler AB, Ord JK, and Snyder RD (2008) <doi:10.1007/978-3-540-71918-2>.\n             Gómez V, Maravall A (2000) <doi:10.1002/9781118032978>.\n             Pedregal DJ, Trapero JR and Holgado E (2024) <doi:10.1016/j.ijforecast.2023.09.004>.",
    "version": "5.1.5",
    "maintainer": "Diego J. Pedregal <diego.pedregal@uclm.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7800,
    "package_name": "UGarima",
    "title": "The Unit-Garima Distribution",
    "description": "Density, distribution function, quantile function, and random generating function of the Unit-Garima distribution\n    based on Ayuyuen, S., & Bodhisuwan, W. (2024)<doi:10.18187/pjsor.v20i1.4307>. ",
    "version": "0.1.0",
    "maintainer": "Atchanut Rattanalertnusorn <atchanut_r@rmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7801,
    "package_name": "UHM",
    "title": "Unified Zero-Inflated Hurdle Regression Models",
    "description": "Run a Gibbs sampler for hurdle models to analyze data showing an excess of zeros, which is common in zero-inflated count and semi-continuous models. The package includes the hurdle model under Gaussian, Gamma, inverse Gaussian, Weibull, Exponential, Beta, Poisson, negative binomial, logarithmic, Bell, generalized Poisson, and binomial distributional assumptions. The models described in Ganjali et al. (2024).",
    "version": "0.3.0",
    "maintainer": "Taban Baghfalaki <t.baghfalaki@gmail.com>",
    "url": "https://github.com/tbaghfalaki/UHM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7821,
    "package_name": "UStatBookABSC",
    "title": "A Companion Package to the Book \"U-Statistics, M-Estimation and\nResampling\"",
    "description": "A set of functions leading to multivariate response L1 regression. \n    This includes functions on computing Euclidean inner products and norms, \n    weighted least squares estimates on multivariate responses, function to compute \n    fitted values and residuals. This package is a companion to the book \"U-Statistics,\n    M-estimation and Resampling\", by Arup Bose and Snigdhansu Chatterjee, to appear \n    in 2017 as part of the \"Texts and Readings in Mathematics\" (TRIM) series of \n    Hindustan Book Agency and Springer-Verlag.",
    "version": "1.0.0",
    "maintainer": "Snigdhansu Chatterjee <chatt019@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7826,
    "package_name": "Ultimixt",
    "title": "Bayesian Analysis of Location-Scale Mixture Models using a\nWeakly Informative Prior",
    "description": "A generic reference Bayesian analysis of unidimensional mixture distributions obtained by a location-scale parameterisation of the model is implemented. The including functions simulate and summarize posterior samples for location-scale mixture models using a weakly informative prior. There is no need to define priors for scale-location parameters except two hyperparameters in which are associated with a Dirichlet prior for weights and a simplex.",
    "version": "2.1",
    "maintainer": "Kaniav Kamary <kamary@ceremade.dauphine.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7833,
    "package_name": "UniIsoRegression",
    "title": "Unimodal and Isotonic L1, L2 and Linf Regression",
    "description": "Perform L1 or L2 isotonic and unimodal regression on 1D weighted or unweighted input vector and isotonic regression on 2D weighted or unweighted input vector. It also performs L infinity isotonic and unimodal regression on 1D unweighted input vector. Reference: Quentin F. Stout (2008) <doi:10.1016/j.csda.2008.08.005>. Spouge, J., Wan, H. & Wilbur, W.(2003) <doi:10.1023/A:1023901806339>. Q.F. Stout (2013) <doi:10.1007/s00453-012-9628-4>.",
    "version": "0.0-0",
    "maintainer": "Zhipeng Xu <xzhipeng@umich.edu>",
    "url": "https://github.com/xzp1995/UniIsoRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7853,
    "package_name": "VAJointSurv",
    "title": "Variational Approximation for Joint Survival and Marker Models",
    "description": "Estimates joint marker (longitudinal) and\n    survival (time-to-event) outcomes using variational approximations.\n    The package supports multivariate markers allowing for\n    correlated error terms and multiple types of survival outcomes which may be\n    left-truncated, right-censored, and recurrent. Time-varying fixed and\n    random covariate effects are supported along with non-proportional hazards.",
    "version": "0.1.1",
    "maintainer": "Benjamin Christoffersen <boennecd@gmail.com>",
    "url": "https://github.com/boennecd/VAJointSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7857,
    "package_name": "VAR.etp",
    "title": "VAR Modelling: Estimation, Testing, and Prediction",
    "description": "A collection of the functions for estimation, hypothesis testing, prediction for stationary vector autoregressive models.",
    "version": "1.1",
    "maintainer": "Jae H. Kim <jaekim8080@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7858,
    "package_name": "VAR.spec",
    "title": "Allows Specifying a Bivariate VAR (Vector Autoregression) with\nDesired Spectral Characteristics",
    "description": "The spectral characteristics of a bivariate series (Marginal Spectra, Coherency- and Phase-Spectrum) determine whether there is a strong presence of short-, medium-, or long-term fluctuations (components of certain frequencies in the spectral representation of the series) in each one of them.  These are induced by strong peaks of the marginal spectra of each series at the corresponding frequencies. The spectral characteristics also determine how strongly these short-, medium-, or long-term fluctuations of the two series are correlated between the two series. Information on this is provided by the Coherency spectrum at the corresponding frequencies. Finally, certain fluctuations of the two series may be lagged to each other. Information on this is provided by the Phase spectrum at the corresponding frequencies. The idea in this package is to define a VAR (Vector autoregression) model with desired spectral characteristics by specifying a number of polynomials, required to define the VAR. See Ioannidis(2007) <doi:10.1016/j.jspi.2005.12.013>. These are specified via their roots, instead of via their coefficients. This is an idea borrowed from the Time Series Library of R. Dahlhaus, where it is used for defining ARMA models for univariate time series. This way, one may e.g. specify a VAR inducing a strong presence of long-term fluctuations in series 1 and in series 2, which are weakly correlated, but lagged by a number of time units to each other, while short-term fluctuations in series 1 and in series 2, are strongly present only in one of the two series, while they are strongly correlated to each other between the two series. Simulation from such models allows studying the behavior of data-analysis tools, such as estimation of the spectra, under different circumstances, as e.g. peaks in the spectra, generating bias, induced by leakage.",
    "version": "1.0",
    "maintainer": "Evangelos Ioannidis <eioannid@aueb.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7860,
    "package_name": "VARcpDetectOnline",
    "title": "Sequential Change Point Detection for High-Dimensional VAR\nModels",
    "description": "Implements the algorithm introduced in Tian, Y., and Safikhani, A. (2024)\n    <doi:10.5705/ss.202024.0182>, \"Sequential Change Point Detection in High-dimensional \n    Vector Auto-regressive Models\". This package provides tools for detecting change points \n    in the transition matrices of VAR models, effectively identifying shifts in temporal \n    and cross-correlations within high-dimensional time series data.",
    "version": "0.2.0",
    "maintainer": "Yuhan Tian <yuhan.tian@ufl.edu>",
    "url": "https://github.com/Helloworld9293/VARcpDetectOnline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7861,
    "package_name": "VARshrink",
    "title": "Shrinkage Estimation Methods for Vector Autoregressive Models",
    "description": "\n    Vector autoregressive (VAR) model is a fundamental and effective approach\n    for multivariate time series analysis. Shrinkage estimation methods can be\n    applied to high-dimensional VAR models with dimensionality greater than\n    the number of observations, contrary to the standard ordinary least squares\n    method. This package is an integrative package delivering nonparametric,\n    parametric, and semiparametric methods in a unified and consistent manner,\n    such as the multivariate ridge regression in Golub, Heath, and Wahba (1979)\n    <doi:10.2307/1268518>, a James-Stein type nonparametric shrinkage method in\n    Opgen-Rhein and Strimmer (2007) <doi:10.1186/1471-2105-8-S2-S3>, and\n    Bayesian estimation methods using noninformative and informative priors\n    in Lee, Choi, and S.-H. Kim (2016) <doi:10.1016/j.csda.2016.03.007> and\n    Ni and Sun (2005) <doi:10.1198/073500104000000622>.",
    "version": "0.3.3",
    "maintainer": "Namgil Lee <namgil.lee@kangwon.ac.kr>",
    "url": "https://github.com/namgillee/VARshrink/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7863,
    "package_name": "VBJM",
    "title": "Variational Inference for Joint Model",
    "description": "The shared random effects joint model is one of the most widely used approaches to study the associations between longitudinal biomarkers and a survival outcome and make dynamic risk predictions using the longitudinally measured biomarkers. \n    One major limitation of joint models is that they could be computationally expensive for complex models where the number of the shared random effects is large.  \n    This package can be used to fit complex multivariate joint models using our newly developed algorithm Jieqi Tu and Jiehuan Sun (2023) <doi:10.1002/sim.9619>, which is based on Gaussian variational approximate inference and is computationally efficient.",
    "version": "0.1.0",
    "maintainer": "Jiehuan Sun <jiehuan.sun@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7867,
    "package_name": "VBV",
    "title": "The Generalized Berlin Method for Time Series Decomposition",
    "description": "Time series decomposition for univariate time series using the\n    \"Verallgemeinerte Berliner Verfahren\" (Generalized Berlin Method) as\n    described in 'Kontinuierliche Messgrößen und Stichprobenstrategien in\n    Raum und Zeit mit Anwendungen in den Natur-, Umwelt-, Wirtschafts-\n    und Finanzwissenschaften', by\n    Hebbel and Steuer, Springer Berlin Heidelberg, 2022\n    <doi:10.1007/978-3-662-65638-9>, or\n    'Decomposition of Time Series using the Generalised Berlin \n    Method (VBV)' by Hebbel and Steuer, in Jan Beran, Yuanhua Feng, Hartmut\n    Hebbel (Eds.): Empirical Economic and Financial Research - Theory,\n    Methods and Practice, Festschrift in Honour of Prof. Siegfried Heiler.\n    Series: Advanced Studies in Theoretical and Applied Econometrics.\n    Springer 2014, p. 9-40.",
    "version": "0.6.2",
    "maintainer": "Detlef Steuer <steuer@hsu-hh.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7868,
    "package_name": "VBel",
    "title": "Variational Bayes for Fast and Accurate Empirical Likelihood\nInference",
    "description": "Computes the Gaussian variational approximation of the Bayesian \n    empirical likelihood posterior. This is an implementation of the function \n    found in Yu, W., & Bondell, H. D. (2023) <doi:10.1080/01621459.2023.2169701>.",
    "version": "1.1.7",
    "maintainer": "Weichang Yu <weichang.yu@unimelb.edu.au>",
    "url": "https://github.com/jlimrasc/VBel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7870,
    "package_name": "VBsparsePCA",
    "title": "The Variational Bayesian Method for Sparse PCA",
    "description": "Contains functions for a variational Bayesian method for sparse PCA proposed by Ning (2020) <arXiv:2102.00305>. There are two algorithms: the PX-CAVI algorithm (if assuming the loadings matrix is jointly row-sparse) and the batch PX-CAVI algorithm (if without this assumption). The outputs of the main function, VBsparsePCA(), include the mean and covariance of the loadings matrix, the score functions, the variable selection results, and the estimated variance of the random noise. ",
    "version": "0.1.0",
    "maintainer": "Bo (Yu-Chien) Ning <bo.ning@upmc.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7872,
    "package_name": "VCA",
    "title": "Variance Component Analysis",
    "description": "\n ANOVA and REML estimation of linear mixed models is implemented, once following\n Searle et al. (1991, ANOVA for unbalanced data), once making use of the 'lme4' package.\n The primary objective of this package is to perform a variance component analysis (VCA)\n according to CLSI EP05-A3 guideline \"Evaluation of Precision of Quantitative Measurement\n Procedures\" (2014). There are plotting methods for visualization of an experimental design,\n plotting random effects and residuals. For ANOVA type estimation two methods for computing\n ANOVA mean squares are implemented (SWEEP and quadratic forms). The covariance matrix of \n variance components can be derived, which is used in estimating confidence intervals. Linear\n hypotheses of fixed effects and LS means can be computed. LS means can be computed at specific\n values of covariables and with custom weighting schemes for factor variables. See ?VCA for a\n more comprehensive description of the features. ",
    "version": "1.5.2",
    "maintainer": "Andre Schuetzenmeister <andre.schuetzenmeister@roche.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7873,
    "package_name": "VCBART",
    "title": "Fit Varying Coefficient Models with Bayesian Additive Regression\nTrees",
    "description": "Fits linear varying coefficient (VC) models, which assert a linear relationship between an outcome and several covariates but allow that relationship (i.e., the coefficients or slopes in the linear regression) to change as functions of additional variables known as effect modifiers, by approximating the coefficient functions with Bayesian Additive Regression Trees. Implements a Metropolis-within-Gibbs sampler to simulate draws from the posterior over coefficient function evaluations. VC models with independent observations or repeated observations can be fit. For more details see Deshpande et al. (2024) <doi:10.1214/24-BA1470>.",
    "version": "1.2.4",
    "maintainer": "Sameer K. Deshpande <sameer.deshpande@wisc.edu>",
    "url": "https://github.com/skdeshpande91/VCBART",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7880,
    "package_name": "VDSPCalibration",
    "title": "Statistical Methods for Designing and Analyzing a Calibration\nStudy",
    "description": "Provides statistical methods for the design and analysis of a calibration study, which aims for calibrating measurements using two different methods. The package includes sample size calculation, sample selection,  regression analysis with error-in measurements and change-point regression. The method is described in Tian, Durazo-Arvizu, Myers, et al. (2014) <DOI:10.1002/sim.6235>.",
    "version": "1.0",
    "maintainer": "Lu Tian <lutian@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7886,
    "package_name": "VGAM",
    "title": "Vector Generalized Linear and Additive Models",
    "description": "An implementation of about 6 major classes of\n    statistical regression models. The central algorithm is\n    Fisher scoring and iterative reweighted least squares.\n    At the heart of this package are the vector generalized linear\n    and additive model (VGLM/VGAM) classes. VGLMs can be loosely\n    thought of as multivariate GLMs. VGAMs are data-driven\n    VGLMs that use smoothing. The book \"Vector Generalized\n    Linear and Additive Models: With an Implementation in R\"\n    (Yee, 2015) <DOI:10.1007/978-1-4939-2818-7> gives details of\n    the statistical framework and the package. Currently only\n    fixed-effects models are implemented. Many (100+) models and\n    distributions are estimated by maximum likelihood estimation\n    (MLE) or penalized MLE. The other classes are RR-VGLMs\n    (reduced-rank VGLMs), quadratic RR-VGLMs, doubly constrained\n    RR-VGLMs, quadratic RR-VGLMs, reduced-rank VGAMs,\n    RCIMs (row-column interaction models)---these classes perform\n    constrained and unconstrained quadratic ordination (CQO/UQO)\n    models in ecology, as well as constrained additive ordination\n    (CAO). Hauck-Donner effect detection is implemented.\n    Note that these functions are subject to change;\n    see the NEWS and ChangeLog files for latest changes.",
    "version": "1.1-14",
    "maintainer": "Thomas Yee <t.yee@auckland.ac.nz>",
    "url": "https://www.stat.auckland.ac.nz/~yee/VGAM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7887,
    "package_name": "VGAMdata",
    "title": "Data Supporting the 'VGAM' Package",
    "description": "Mainly data sets to accompany the VGAM package and\n\tthe book \"Vector Generalized Linear and\n\tAdditive Models: With an Implementation in R\" (Yee, 2015)\n\t<DOI:10.1007/978-1-4939-2818-7>.\n\tThese are used to illustrate vector generalized\n\tlinear and additive models (VGLMs/VGAMs), and associated models\n\t(Reduced-Rank VGLMs, Quadratic RR-VGLMs, Row-Column\n\tInteraction Models, and constrained and unconstrained ordination\n\tmodels in ecology). This package now contains some\n\told VGAM family functions which have been replaced by newer\n\tones (often because they are now special cases).",
    "version": "1.1-13",
    "maintainer": "Thomas Yee <t.yee@auckland.ac.nz>",
    "url": "https://www.stat.auckland.ac.nz/~yee/VGAMdata/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7896,
    "package_name": "VIRF",
    "title": "Computation of Volatility Impulse Response Function of\nMultivariate Time Series",
    "description": "Computation of volatility impulse response function for multivariate time series model using algorithm by Jin, Lin and Tamvakis (2012) <doi:10.1016/j.eneco.2012.03.003>.",
    "version": "0.1.1",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7899,
    "package_name": "VLMCX",
    "title": "Variable Length Markov Chain with Exogenous Covariates",
    "description": "Models categorical time series through a Markov Chain when a) covariates are predictors for transitioning into the next state/symbol and b) when the dependence in the past states has variable length. The probability of transitioning to the next state in the Markov Chain is defined by a multinomial regression whose parameters depend on the past states of the chain and, moreover, the number of states in the past needed to predict the next state also depends on the observed states themselves. See Zambom, Kim, and Garcia (2022) <doi:10.1111/jtsa.12615>.",
    "version": "1.0",
    "maintainer": "Adriano Zanin Zambom Developer <adriano.zambom@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7916,
    "package_name": "VarReg",
    "title": "Semi-Parametric Variance Regression",
    "description": "Methods for fitting semi-parametric mean and variance models, with normal or censored data. Extended to allow a regression in the location, scale and shape parameters, and further for multiple regression in each.",
    "version": "2.0",
    "maintainer": "Kristy Robledo <robledo.kristy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7918,
    "package_name": "VariableScreening",
    "title": "High-Dimensional Screening for Semiparametric Longitudinal\nRegression",
    "description": "Implements variable screening techniques for ultra-high\n    dimensional regression settings.  Techniques for independent (iid) data,\n    varying-coefficient models, and longitudinal data are implemented. The package\n     currently contains three screen functions: screenIID(), screenLD() and screenVCM(),\n     and six methods for simulating dataset: simulateDCSIS(), simulateLD, simulateMVSIS(),\n     simulateMVSISNY(), simulateSIRS() and simulateVCM(). The package is based on the work of\n    Li-Ping ZHU, Lexin LI, Runze LI, and Li-Xing ZHU (2011) <DOI:10.1198/jasa.2011.tm10563>,\n    Runze LI, Wei ZHONG, & Liping ZHU (2012) <DOI:10.1080/01621459.2012.695654>,\n    Jingyuan LIU, Runze LI, & Rongling WU (2014) <DOI:10.1080/01621459.2013.850086>\n    Hengjian CUI, Runze LI, & Wei ZHONG (2015) <DOI:10.1080/01621459.2014.920256>, and\n    Wanghuan CHU, Runze LI and Matthew REIMHERR (2016) <DOI:10.1214/16-AOAS912>.",
    "version": "0.2.1",
    "maintainer": "John Dziak <dziakj1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7941,
    "package_name": "VineCopula",
    "title": "Statistical Inference of Vine Copulas",
    "description": "Provides tools for the statistical analysis of regular vine copula",
    "version": "2.6.1",
    "maintainer": "",
    "url": "https://github.com/tnagler/VineCopula",
    "exports": [],
    "topics": ["copula", "estimation", "statistics", "vine"],
    "score": "NA",
    "stars": 96
  },
  {
    "id": 7946,
    "package_name": "VisitorCounts",
    "title": "Modeling and Forecasting Visitor Counts Using Social Media",
    "description": "Performs modeling and forecasting of park visitor counts\n\tusing social media data and (partial) on-site visitor counts.\n\tSpecifically, the model is built based on an automatic decomposition\n\tof the trend and seasonal components of the social media-based park visitor counts,\n\tfrom which short-term forecasts of the visitor counts and percent changes\n\tin the visitor counts can be made. A reference for the underlying model that 'VisitorCounts' uses can be found at \n    Russell Goebel, Austin Schmaltz, Beth Ann Brackett, Spencer A. Wood, Kimihiro Noguchi (2023) <doi:10.1002/for.2965> .",
    "version": "2.0.3",
    "maintainer": "Robert Bowen <robertbowen.bham@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7948,
    "package_name": "VisualDom",
    "title": "Visualize Dominant Variables in Wavelet Multiple Correlation",
    "description": "Estimates and plots as a heat map the correlation coefficients obtained via the wavelet local multiple correlation 'WLMC' (Fernández-Macho 2018) and the 'dominant' variable/s, i.e., the variable/s that maximizes the multiple correlation through time and scale (Polanco-Martínez et al. 2020, Polanco-Martínez 2022). We improve the graphical outputs of WLMC proposing a didactic and useful way to visualize the 'dominant' variable(s) for a set of time series. The WLMC was designed for financial time series, but other kinds of data (e.g., climatic, ecological, etc.) can be used. The functions contained in 'VisualDom' are highly flexible since these contains several parameters to personalize the time series under analysis and the heat maps. In addition, we have also included two data sets (named 'rdata_climate' and 'rdata_Lorenz') to exemplify the use of the functions contained in 'VisualDom'. Methods derived from Fernández-Macho (2018) <doi:10.1016/j.physa.2017.11.050>, Polanco-Martínez et al. (2020) <doi:10.1038/s41598-020-77767-8> and Polanco-Martínez (2023, in press). ",
    "version": "0.8.0",
    "maintainer": "Josué M. Polanco-Martínez <josue.m.polanco@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7965,
    "package_name": "WALS",
    "title": "Weighted-Average Least Squares Model Averaging",
    "description": "Implements Weighted-Average Least Squares model averaging\n    for negative binomial regression models of Huynh (2024) <doi:10.48550/arXiv.2404.11324>,\n    generalized linear models of De Luca, Magnus, Peracchi (2018) \n    <doi:10.1016/j.jeconom.2017.12.007> and linear regression models of \n    Magnus, Powell, Pruefer (2010) <doi:10.1016/j.jeconom.2009.07.004>, see also \n    Magnus, De Luca (2016) <doi:10.1111/joes.12094>. Weighted-Average Least Squares\n    for the linear regression model is based on the original 'MATLAB' code by \n    Magnus and De Luca <https://www.janmagnus.nl/items/WALS.pdf>, see also \n    Kumar, Magnus (2013) <doi:10.1007/s13571-013-0060-9> and \n    De Luca, Magnus (2011) <doi:10.1177/1536867X1201100402>.",
    "version": "0.2.6",
    "maintainer": "Kevin Huynh <kevin.huynh-dev@gmx.ch>",
    "url": "https://github.com/kevhuy/WALS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7967,
    "package_name": "WARN",
    "title": "Weaning Age Reconstruction with Nitrogen Isotope Analysis",
    "description": "This estimates precise weaning ages\n\tfor a given skeletal population\n\tby analyzing the stable nitrogen isotope ratios of them.\n\tBone collagen turnover rates estimated anew and\n\tthe approximate Bayesian computation (ABC)\n\twere adopted in this package.",
    "version": "1.2-5",
    "maintainer": "Takumi Tsutaya <tsutayatakumi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7970,
    "package_name": "WCE",
    "title": "Weighted Cumulative Exposure Models",
    "description": "A flexible method for modeling cumulative effects of time-varying exposures, weighted according to their relative proximity in time, and represented by time-dependent covariates. The current implementation estimates the weight function in the Cox proportional hazards model. The function that assigns weights to doses taken in the past is estimated using cubic regression splines.",
    "version": "1.0.4",
    "maintainer": "Marie-Pierre Sylvestre <marie-pierre.sylvestre@umontreal.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7983,
    "package_name": "WLreg",
    "title": "Regression Analysis Based on Win Loss Endpoints",
    "description": "Use various regression models for the analysis of win loss endpoints \n             adjusting for non-binary and multivariate covariates.",
    "version": "1.0.0.1",
    "maintainer": "Xiaodong Luo <Xiaodong.Luo@sanofi.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7990,
    "package_name": "WR",
    "title": "Win Ratio Analysis of Composite Time-to-Event Outcomes",
    "description": "Implements various win ratio methodologies for composite endpoints of death and \n  non-fatal events, including the (stratified) proportional win-fractions (PW) regression models \n  (Mao and Wang, 2020 <doi:10.1111/biom.13382>), (stratified) two-sample tests with possibly \n  recurrent nonfatal event, and sample size calculation for standard win ratio test (Mao et al., \n  2021 <doi:10.1111/biom.13501>).",
    "version": "1.0",
    "maintainer": "Lu Mao <lmao@biostat.wisc.edu>",
    "url": "https://sites.google.com/view/lmaowisc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7991,
    "package_name": "WRI",
    "title": "Wasserstein Regression and Inference",
    "description": "Implementation of the methodologies described in 1) Alexander Petersen, Xi Liu and Afshin A. Divani (2021) <doi:10.1214/20-aos1971>, including global F tests, partial F tests, intrinsic Wasserstein-infinity bands and Wasserstein density bands, and 2) Chao Zhang, Piotr Kokoszka and Alexander Petersen (2022) <doi:10.1111/jtsa.12590>, including estimation, prediction, and inference of the Wasserstein autoregressive models.",
    "version": "0.2.1",
    "maintainer": "Xi Liu <xiliu@ucsb.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7994,
    "package_name": "WRTDStidal",
    "title": "Weighted Regression for Water Quality Evaluation in Tidal Waters",
    "description": "An adaptation for estuaries (tidal waters) of weighted regression\n    on time, discharge, and season to evaluate trends in water quality time series. \n    Please see Beck and Hagy (2015) <doi:10.1007/s10666-015-9452-8> for \n    details.",
    "version": "1.1.4",
    "maintainer": "Marcus W. Beck <mbeck@tbep.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 7998,
    "package_name": "WWGbook",
    "title": "Functions and Datasets for WWGbook",
    "description": "Book is \"Linear Mixed Models: A Practical Guide Using\n        Statistical Software\" published in 2006 by Chapman Hall / CRC\n        Press.",
    "version": "1.0.2",
    "maintainer": "Shu Chen <chenshu@umich.edu>",
    "url": "http://www-personal.umich.edu/~bwest/almmussp.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8005,
    "package_name": "WaveletANN",
    "title": "Wavelet ANN Model",
    "description": "The wavelet and ANN technique have been combined to reduce the effect of data noise. This wavelet-ANN conjunction model is able to forecast time series data with better accuracy than the traditional time series model. This package fits hybrid Wavelet ANN model for time series forecasting using algorithm by Anjoy and Paul (2017) <DOI: 10.1007/s00521-017-3289-9>.",
    "version": "0.1.2",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8006,
    "package_name": "WaveletArima",
    "title": "Wavelet-ARIMA Model for Time Series Forecasting",
    "description": "Noise in the time-series data significantly affects the accuracy of the ARIMA model. Wavelet transformation decomposes the time series data into subcomponents to reduce the noise and help to improve the model performance. The wavelet-ARIMA model can achieve higher prediction accuracy than the traditional ARIMA model. This package provides Wavelet-ARIMA model for time series forecasting based on the algorithm by Aminghafari and Poggi (2012) and Paul and Anjoy (2018) <doi:10.1142/S0219691307002002> <doi:10.1007/s00704-017-2271-x>.",
    "version": "0.1.2",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8007,
    "package_name": "WaveletComp",
    "title": "Computational Wavelet Analysis",
    "description": "Wavelet analysis and reconstruction of time series, cross-wavelets and phase-difference (with filtering options), significance with simulation algorithms.",
    "version": "1.2",
    "maintainer": "Angi Roesch <angi@angi-stat.com>",
    "url": "http://www.hs-stat.com/projects/WaveletComp/WaveletComp_guided_tour.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8008,
    "package_name": "WaveletETS",
    "title": "Wavelet Based Error Trend Seasonality Model",
    "description": "ETS stands for Error, Trend, and Seasonality, and it is a popular time series forecasting method. Wavelet decomposition can be used for denoising, compression, and feature extraction of signals. By removing the high-frequency components, wavelet decomposition can remove noise from the data while preserving important features. A hybrid Wavelet ETS  (Error Trend-Seasonality) model has been developed for time series forecasting using algorithm of Anjoy and Paul (2017) <DOI:10.1007/s00521-017-3289-9>.",
    "version": "0.1.0",
    "maintainer": "Dr. Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8009,
    "package_name": "WaveletGARCH",
    "title": "Fit the Wavelet-GARCH Model to Volatile Time Series Data",
    "description": "Fits the combination of Wavelet-GARCH model for time series forecasting using algorithm by Paul (2015) <doi:10.3233/MAS-150328>.",
    "version": "0.1.1",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8010,
    "package_name": "WaveletGBM",
    "title": "Wavelet Based Gradient Boosting Method",
    "description": "Wavelet decomposition method is very useful for modelling noisy time series data. Wavelet decomposition using 'haar' algorithm has been implemented to developed hybrid Wavelet GBM (Gradient Boosting Method) model for time series forecasting using algorithm by Anjoy and Paul (2017) <DOI:10.1007/s00521-017-3289-9>.",
    "version": "0.1.0",
    "maintainer": "Dr. Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8011,
    "package_name": "WaveletKNN",
    "title": "Wavelet Based K-Nearest Neighbor Model",
    "description": "The employment of the Wavelet decomposition technique proves to be highly advantageous in the modelling of noisy time series data. Wavelet decomposition technique using the \"haar\" algorithm has been incorporated to formulate a hybrid Wavelet KNN (K-Nearest Neighbour) model for time series forecasting, as proposed by Anjoy and Paul (2017) <DOI:10.1007/s00521-017-3289-9>.",
    "version": "0.1.0",
    "maintainer": "Dr. Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8016,
    "package_name": "WaveletSVR",
    "title": "Wavelet-SVR Hybrid Model for Time Series Forecasting",
    "description": "The main aim of this package is to combine the advantage of wavelet and support vector machine models for time series forecasting. This package also gives the accuracy measurements in terms of RMSE and MAPE. This package fits the hybrid Wavelet SVR model for time series forecasting The main aim of this package is to combine the advantage of wavelet and Support Vector Regression (SVR) models for time series forecasting. This package also gives the accuracy measurements in terms of Root Mean Square Error (RMSE) and Mean Absolute Prediction Error (MAPE). This package is based on the algorithm of Raimundo and Okamoto (2018) <DOI: 10.1109/INFOCT.2018.8356851>.",
    "version": "0.1.0",
    "maintainer": "Ranjit Kumar Paul <ranjitstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8017,
    "package_name": "WaverR",
    "title": "Data Estimation using Weighted Averages of Multiple Regressions",
    "description": "For multivariate datasets, this function enables the estimation of missing data using the Weighted AVERage of all possible Regressions using the data available.",
    "version": "1.0",
    "maintainer": "Olivia Cheronet <cheronetolivia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8021,
    "package_name": "Wcompo",
    "title": "Semiparametric Proportional Means Regression of Weighted\nComposite Endpoint",
    "description": "Implements inferential and graphic procedures for the semiparametric proportional \n  means regression of weighted composite endpoint of recurrent event and death (Mao and Lin, \n  2016, <doi:10.1093/biostatistics/kxv050>).",
    "version": "1.0",
    "maintainer": "Lu Mao <lmao@biostat.wisc.edu>",
    "url": "https://sites.google.com/view/lmaowisc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8022,
    "package_name": "WeMix",
    "title": "Weighted Mixed-Effects Models Using Multilevel Pseudo Maximum\nLikelihood Estimation",
    "description": "Run mixed-effects models that include weights at every level. The WeMix package fits a weighted mixed model, also known as a multilevel, mixed, or hierarchical linear model (HLM). The weights could be inverse selection probabilities, such as those developed for an education survey where schools are sampled probabilistically, and then students inside of those schools are sampled probabilistically. Although mixed-effects models are already available in R, WeMix is unique in implementing methods for mixed models using weights at multiple levels. Both linear and logit models are supported. Models may have up to three levels. Random effects are estimated using the PIRLS algorithm from 'lme4pureR' (Walker and Bates (2013) <https://github.com/lme4/lme4pureR>).",
    "version": "4.0.3",
    "maintainer": "Paul Bailey <pbailey@air.org>",
    "url": "https://american-institutes-for-research.github.io/WeMix/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8028,
    "package_name": "WeibullModiAMR",
    "title": "Fit Modified Weibull-Type Distributions",
    "description": "Provides maximum likelihood estimation methods for eight modified \n    Weibull-type distributions. It returns parameter estimates, \n    log-likelihood, AIC, and BIC, and also supports model fitting, validation, \n    and comparison across different distributional forms. These methods can be \n    applied to reliability, survival, and lifetime data analysis, making the package \n    useful for researchers and practitioners in statistics, engineering, and medicine. \n    The following distributions are included: Rangoli2023, Peng2014, Lai2003, Xie1996, \n    Sarhan2009, Rangoli2025, Mustafa2012, and Alwasel2009.",
    "version": "0.1.0",
    "maintainer": "Dr Ajaykumar Rangoli <drajaykumarmr1008@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8036,
    "package_name": "WeightedEnsemble",
    "title": "Weighted Ensemble for Hybrid Model",
    "description": "The weighted ensemble method is a valuable approach for combining forecasts. This algorithm employs several optimization techniques to generate optimized weights. This package has been developed using algorithm of Armstrong (1989) <doi:10.1016/0024-6301(90)90317-W>.",
    "version": "0.1.0",
    "maintainer": "Dr. Md Yeasin <yeasin.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8037,
    "package_name": "WeightedPortTest",
    "title": "Weighted Portmanteau Tests for Time Series Goodness-of-Fit",
    "description": "An implementation of the Weighted Portmanteau Tests described\n      in \"New Weighted Portmanteau Statistics for Time Series Goodness-of-Fit Testing\"\n      published by the Journal of the American Statistical Association, Volume 107, \n      Issue 498, pages 777-787, 2012.",
    "version": "1.1",
    "maintainer": "Thomas J. Fisher <fishert4@miamioh.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8048,
    "package_name": "WinRatio",
    "title": "Win Ratio for Prioritized Outcomes and 95% Confidence Interval",
    "description": "Calculate the win ratio for prioritized outcomes and the 95% confidence interval based on Bebu and Lachin (2016) <doi:10.1093/biostatistics/kxv032>. Three type of outcomes can be analyzed: survival \"failure-time\" events, repeated survival \"failure-time\" events and continuous or ordinal \"non-failure time\" events that are captured at specific time-points in the study.",
    "version": "1.0",
    "maintainer": "Kevin Duarte <k.duarte@chru-nancy.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8067,
    "package_name": "XDNUTS",
    "title": "Discontinuous Hamiltonian Monte Carlo with Varying Trajectory\nLength",
    "description": "Hamiltonian Monte Carlo for both continuous and discontinuous posterior \n    distributions with a customizable trajectory length termination criterion. \n    See Nishimura et al. (2020) <doi:10.1093/biomet/asz083> for the original \n    Discontinuous Hamiltonian Monte Carlo; Hoffman et al. (2014) \n    <doi:10.48550/arXiv.1111.4246> and Betancourt (2016) <doi:10.48550/arXiv.1601.00225> \n    for the definition of possible Hamiltonian Monte Carlo termination criteria.",
    "version": "1.6.5",
    "maintainer": "Paolo Manildo <paolo.manildo@phd.unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8082,
    "package_name": "Xcertainty",
    "title": "Estimating Lengths and Uncertainty from Photogrammetric Imagery",
    "description": "Implementation of Bayesian models for estimating object lengths and morphological relationships between object lengths using photographic data collected from drones.  The Bayesian model is described in \"Bayesian approach for predicting photogrammetric uncertainty in morphometric measurements derived from drones\" (Bierlich et al., 2021, <doi:10.3354/meps13814>).",
    "version": "1.0.1",
    "maintainer": "K.C. Bierlich <bierlick@oregonstate.edu>",
    "url": "https://github.com/MMI-CODEX/Xcertainty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8088,
    "package_name": "YPBP",
    "title": "Yang and Prentice Model with Baseline Distribution Modeled by\nBernstein Polynomials",
    "description": "Semiparametric modeling of lifetime data with crossing survival curves via Yang and Prentice model with baseline hazard/odds modeled with Bernstein polynomials. Details about the model can be found in Demarqui et al. (2019) <arXiv:1910.04475>. Model fitting can be carried out via both maximum likelihood and Bayesian approaches. The package also provides point and interval estimation for the crossing survival times.",
    "version": "0.0.1",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/YPBP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8090,
    "package_name": "YPPE",
    "title": "Yang and Prentice Model with Piecewise Exponential Baseline\nDistribution",
    "description": "Semiparametric modeling of lifetime data with crossing survival curves via Yang and Prentice model with piecewise exponential baseline distribution. Details about the model can be found in Demarqui and Mayrink (2019) <arXiv:1910.02406>. Model fitting carried out via likelihood-based and Bayesian approaches. The package also provides point and interval estimation for the crossing survival times.",
    "version": "1.0.1",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/YPPE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8091,
    "package_name": "YPmodel",
    "title": "The Short-Term and Long-Term Hazard Ratio Model for Survival\nData",
    "description": "Inference procedures accommodate a flexible range of hazard ratio patterns with a two-sample semi-parametric model. This model contains the proportional hazards model and the proportional odds model as sub-models, and accommodates non-proportional hazards situations to the extreme of having crossing hazards and crossing survivor functions. Overall, this package has four major functions: 1) the parameter estimation, namely short-term and long-term hazard ratio parameters; 2) 95 percent and 90 percent point-wise confidence intervals and simultaneous confidence bands for the hazard ratio function; 3) p-value of the adaptive weighted log-rank test; 4) p-values of two lack-of-fit tests for the model. See the included \"read_me_first.pdf\" for brief instructions. In this version (1.1), there is no need to sort the data before applying this package.",
    "version": "1.4",
    "maintainer": "Junlong Sun <junlong.sun@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8092,
    "package_name": "YPmodelPhreg",
    "title": "The Short-Term and Long-Term Hazard Ratio Model with\nProportional Adjustment",
    "description": "Provides covariate-adjusted comparison of two groups of right \n    censored data, where the binary group variable has separate short-term \n    and long-term effects on the hazard function, while effects of covariates \n    such as age, blood pressure, etc. are proportional on the hazard. The model \n    was studied in Yang and Prentice (2015) <doi:10.1002/sim.6453> and it extends \n    the two sample version of the short-term and long-term hazard ratio model\n    proposed in Yang and Prentice (2005) <doi:10.1093/biomet/92.1.1>. The model \n    extends the usual Cox proportional hazards model to allow more flexible \n    hazard ratio patterns, such as gradual onset of effect, diminishing effect, \n    and crossing hazard or survival functions. This package provides \n    the following: 1) point estimates and confidence intervals for model \n    parameters; 2) point estimate and confidence interval of the average hazard \n    ratio; and 3) plots of estimated hazard ratio function with point-wise and \n    simultaneous confidence bands.",
    "version": "1.0.0",
    "maintainer": "Daewoo Pak <dpak@yonsei.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8093,
    "package_name": "YRmisc",
    "title": "Y&R Miscellaneous R Functions",
    "description": "Miscellaneous functions for data analysis, portfolio management, graphics, data manipulation, statistical investigation, including descriptive statistics, creating leading and lagging variables, portfolio return analysis, time series difference and percentage change calculation, stacking data for higher efficient analysis.",
    "version": "0.1.6",
    "maintainer": "Xuanhua (Peter) Yin <peteryin.sju@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8102,
    "package_name": "ZIBR",
    "title": "A Zero-Inflated Beta Random Effect Model",
    "description": "A two-part zero-inflated Beta regression model with random \n    effects (ZIBR) for testing the association between microbial abundance \n    and clinical covariates for longitudinal microbiome data. Eric Z. Chen \n    and Hongzhe Li (2016) <doi:10.1093/bioinformatics/btw308>.",
    "version": "1.0.2",
    "maintainer": "Charlie Bushman <ctbushman@gmail.com>",
    "url": "https://github.com/PennChopMicrobiomeProgram/ZIBR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8103,
    "package_name": "ZIDW",
    "title": "Zero-Inflated Discrete Weibull Models",
    "description": "Parameter estimation for zero-inflated discrete Weibull (ZIDW) regression models, the univariate setting, distribution functions, functions to generate randomized quantile residuals a pseudo R2, and plotting of rootograms. For more details, see Kalktawi (2017)  <https://bura.brunel.ac.uk/handle/2438/14476>, Taconeli and Rodrigues de Lara (2022) <doi:10.1080/00949655.2021.2005597>, and Yeh and Young (2025) <doi:10.1080/03610918.2025.2464076>.",
    "version": "0.1.0",
    "maintainer": "Derek S. Young <derek.young@uky.edu>",
    "url": "https://github.com/dsy109/ZIDW",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8104,
    "package_name": "ZIHINAR1",
    "title": "Zero-Inflated and Hurdle INAR(1) Models",
    "description": "Provides tools for estimating Zero-Inflated INAR(1) \n    (ZI-INAR(1)) and Hurdle INAR(1) (H-INAR(1)) models using 'Stan'. \n    It allows users to simulate time series data for these models, \n    estimate parameters, and evaluate model fit using various criteria. \n    Functions include model estimation, simulation, and likelihood-based metrics.",
    "version": "0.1.0",
    "maintainer": "Fusheng Yang <fusheng.yang@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8105,
    "package_name": "ZIM",
    "title": "Zero-Inflated Models (ZIM) for Count Time Series with Excess\nZeros",
    "description": "Analyze count time series with excess zeros. \n    Two types of statistical models are supported: Markov regression by Yang et al.\n    (2013) <doi:10.1016/j.stamet.2013.02.001> and state-space models by Yang et al. \n    (2015) <doi:10.1177/1471082X14535530>. They are also known as observation-driven and \n    parameter-driven models respectively in the time series literature. The functions used for \n    Markov regression or observation-driven models can also be used to fit ordinary regression models \n    with independent data under the zero-inflated Poisson (ZIP) or zero-inflated negative binomial (ZINB) \n    assumption. Besides, the package contains some miscellaneous functions to compute density, distribution, \n    quantile, and generate random numbers from ZIP and ZINB distributions.",
    "version": "1.1.0",
    "maintainer": "Ming Yang <mingyang@biostatstudio.com>",
    "url": "https://github.com/biostatstudio/ZIM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8108,
    "package_name": "ZINARp",
    "title": "Simulate INAR/ZINAR(p) Models and Estimate Its Parameters",
    "description": "Simulation, exploratory data analysis and Bayesian analysis of the p-order Integer-valued Autoregressive (INAR(p)) and Zero-inflated p-order Integer-valued Autoregressive (ZINAR(p)) processes, as described in Garay et al. (2020) <doi:10.1080/00949655.2020.1754819>. ",
    "version": "0.1.0",
    "maintainer": "Tharso Augustus Rossiter Araújo Monteiro <tharso.augustus@ufpe.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8110,
    "package_name": "ZIPFA",
    "title": "Zero Inflated Poisson Factor Analysis",
    "description": "Estimation methods for zero-inflated Poisson factor analysis (ZIPFA) on sparse data. \n    It provides estimates of coefficients in a new type of zero-inflated regression. \n    It provides a cross-validation method to determine the potential rank of the data in the ZIPFA \n    and conducts zero-inflated Poisson factor analysis based on the determined rank.",
    "version": "0.8.1",
    "maintainer": "Tianchen Xu <tx2155@columbia.edu>",
    "url": "https://zjph602xtc.github.io/ZIPFA/,\nhttps://arxiv.org/abs/1910.11985",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8111,
    "package_name": "ZIPG",
    "title": "Zero-Inflated Poisson-Gamma Regression",
    "description": "We provide a flexible Zero-inflated Poisson-Gamma Model (ZIPG) by connecting both the mean abundance and the variability to different covariates, and build valid statistical inference procedures for both parameter estimation and hypothesis testing. These functions can be used to analyze microbiome count data with zero-inflation and overdispersion. The model is discussed in Jiang et al (2023) <doi:10.1080/01621459.2022.2151447>.",
    "version": "1.1",
    "maintainer": "Roulan Jiang <roulan2000@gmail.com>",
    "url": "https://github.com/roulan2000/ZIPG",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8114,
    "package_name": "ZRA",
    "title": "Dynamic Plots for Time Series Forecasting",
    "description": "Combines a forecast of a time series, using the function forecast(), with the dynamic plots from dygraphs.",
    "version": "0.2",
    "maintainer": "David Beiner <zra.r.package@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8117,
    "package_name": "Zelig",
    "title": "Everyone's Statistical Software",
    "description": "A framework that brings together an abundance of common",
    "version": "5.1.7",
    "maintainer": "",
    "url": "https://github.com/IQSS/Zelig",
    "exports": [],
    "topics": ["regression-diagnostics", "regression-models", "rstats", "simulation", "statistics", "visualization"],
    "score": "NA",
    "stars": 111
  },
  {
    "id": 8130,
    "package_name": "aLBI",
    "title": "Estimating Length-Based Indicators for Fish Stock",
    "description": "Provides tools for estimating length-based indicators from length frequency data to assess fish stock status and manage fisheries sustainably. Implements methods from Cope and Punt (2009) <doi:10.1577/C08-025.1> for data-limited stock assessment and Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> for detecting overfishing using simple indicators. Key functions include:\n    FrequencyTable(): Calculate the frequency table from the collected and also the extract the length frequency data from the frequency table with the upper length_range. A numeric value specifying the bin width for class intervals. If not provided, the bin width is automatically calculated using Wang (2020) <doi:10.1016/j.fishres.2019.105474> formula.\n    FreqTM(): Creates a frequency distribution table for fish length data across multiple months using a consistent length class structure. The bin width is determined by either a custom value or Wang's formula, applied uniformly across all months. The function dynamically detects and renames columns to 'Month' and 'Length' from the input dataframe. The maximum observed length is included as part of the last class, with the upper bound set to the smallest multiple of the bin width greater than or equal to the maximum length. Months can be converted to dates using a configurable day and year, with dates assigned sequentially in 'day.month.year' format (e.g., 15.01.26).\n    FishPar(): Calculates length-based indicators (LBIs) proposed by Froese (2004) <doi:10.1111/j.1467-2979.2004.00144.x> such as the percentage of mature fish (Pmat), percentage of optimal length fish (Popt), percentage of mega spawners (Pmega), and the sum of these as Pobj. This function also estimates confidence intervals for different lengths, visualizes length frequency distributions, and provides data frames containing calculated values.\n    FishSS(): Makes decisions based on input from Cope and Punt (2009) <doi:10.1577/C08-025.1> and parameters calculated by FishPar() (e.g., Pobj, Pmat, Popt, LM_ratio) to determine stock status as target spawning biomass (TSB40) and limit spawning biomass (LSB25), and selectivity.\n    LWR(): Fits and visualizes length-weight relationships using linear regression, with options for log-transformation and customizable plotting.",
    "version": "0.1.9",
    "maintainer": "Ataher Ali <ataher.cu.ms@gmail.com>",
    "url": "https://github.com/Ataher76/aLBI",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8135,
    "package_name": "aRD",
    "title": "Adjusted Risk Differences via Specifically Penalized Likelihood",
    "description": "Fits a linear-binomial model using a modified Newton-type \n    algorithm for solving the maximum likelihood \n    estimation problem under linear box constraints. Similar methods are \n    described in Wagenpfeil, Schöpe and Bekhit (2025, ISBN:9783111341972) \n    \"Estimation of adjusted relative risks in log-binomial regression \n    using the BSW algorithm\". In: Mau, Mukhin, Wang and Xu (Eds.), \n    Biokybernetika. De Gruyter, Berlin, pp. 665–676.",
    "version": "0.1.0",
    "maintainer": "Thomas Wolf <imbei@med-imbei.uni-saarland.de>",
    "url": "https://github.com/UdS-MF-IMBEI/aRD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8146,
    "package_name": "abc",
    "title": "Tools for Approximate Bayesian Computation (ABC)",
    "description": "Implements several ABC algorithms for\n        performing parameter estimation, model selection, and goodness-of-fit.\n        Cross-validation tools are also available for measuring the\n        accuracy of ABC estimates, and to calculate the\n        misclassification probabilities of different models.",
    "version": "2.2.2",
    "maintainer": "Blum Michael <michael.blum.temp@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8147,
    "package_name": "abc.data",
    "title": "Data Only: Tools for Approximate Bayesian Computation (ABC)",
    "description": "Contains data which are used by functions of the 'abc' package.",
    "version": "1.1",
    "maintainer": "Blum Michael <michael.blum.temp@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8148,
    "package_name": "abcel",
    "title": "Empirical Likelihood-Based Approximate Bayesian Computation",
    "description": "Empirical likelihood-based approximate Bayesian Computation.  Approximates the required posterior using empirical likelihood and estimated differential entropy.  This is achieved without requiring any specification of the likelihood or estimating equations that connects the observations with the underlying parameters. The procedure is known to be posterior consistent.  More details can be found in Chaudhuri, Ghosh, and Kim (2024) <doi:10.1002/SAM.11711>.",
    "version": "1.0",
    "maintainer": "Sanjay Chaudhuri <schaudhuri2@unl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8150,
    "package_name": "abcrf",
    "title": "Approximate Bayesian Computation via Random Forests",
    "description": "Performs Approximate Bayesian Computation (ABC) model choice and parameter inference via random forests.\n    Pudlo P., Marin J.-M., Estoup A., Cornuet J.-M., Gautier M. and Robert C. P. (2016) <doi:10.1093/bioinformatics/btv684>.\n    Raynal L., Marin J.-M., Pudlo P., Ribatet M., Robert C. P. and Estoup A. (2019) <doi:10.1093/bioinformatics/bty867>.",
    "version": "2.0",
    "maintainer": "Jean-Michel Marin <jean-michel.marin@umontpellier.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8152,
    "package_name": "abctools",
    "title": "Tools for ABC Analyses",
    "description": "Tools for approximate Bayesian computation including summary statistic selection and assessing coverage.\n    See Nunes and Prangle (2015) <doi:10.32614/RJ-2015-030> and Rodrigues, Prangle and Sisson (2018) <doi:10.1016/j.csda.2018.04.004>.",
    "version": "1.1.8",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "https://github.com/dennisprangle/abctools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8158,
    "package_name": "abglasso",
    "title": "Adaptive Bayesian Graphical Lasso",
    "description": "Implements a Bayesian adaptive graphical lasso data-augmented block Gibbs sampler. The sampler simulates the posterior distribution of precision matrices of a Gaussian Graphical Model. This sampler was adapted from the original MATLAB routine proposed in Wang (2012) <doi:10.1214/12-BA729>.",
    "version": "0.1.1",
    "maintainer": "Jarod Smith <jarodsmith706@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8165,
    "package_name": "abn",
    "title": "Modelling Multivariate Data with Additive Bayesian Networks",
    "description": "The 'abn' R package facilitates Bayesian network analysis, a\n    probabilistic graphical model that derives from empirical data a\n    directed acyclic graph (DAG). This DAG describes the dependency\n    structure between random variables. The R package 'abn' provides\n    routines to help determine optimal Bayesian network models for a given\n    data set. These models are used to identify statistical dependencies\n    in messy, complex data. Their additive formulation is equivalent to\n    multivariate generalised linear modelling, including mixed models with\n    independent and identically distributed (iid) random effects. The core\n    functionality of the 'abn' package revolves around model selection,\n    also known as structure discovery. It supports both exact and\n    heuristic structure learning algorithms and does not restrict the data\n    distribution of parent-child combinations, providing flexibility in\n    model creation and analysis. The 'abn' package uses Laplace\n    approximations for metric estimation and includes wrappers to the\n    'INLA' package. It also employs 'JAGS' for data simulation purposes.\n    For more resources and information, visit the 'abn' website.",
    "version": "3.1.13",
    "maintainer": "Matteo Delucchi <matteo.delucchi@math.uzh.ch>",
    "url": "https://r-bayesian-networks.org/,\nhttps://github.com/furrer-lab/abn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8170,
    "package_name": "absorber",
    "title": "Variable Selection in Nonparametric Models using B-Splines",
    "description": "A variable selection method using B-Splines in multivariate nOnparametric Regression models Based on partial dErivatives Regularization (ABSORBER) implements a novel variable selection method in a nonlinear multivariate model using B-splines. For further details we refer the reader to the paper Savino, M. E. and Lévy-Leduc, C. (2024), <https://hal.science/hal-04434820>.",
    "version": "1.0",
    "maintainer": "Mary E. Savino <mary.savino@outlook.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8173,
    "package_name": "abtest",
    "title": "Bayesian A/B Testing",
    "description": "Provides functions for Bayesian A/B testing including prior elicitation\n    options based on Kass and Vaidyanathan (1992) <doi:10.1111/j.2517-6161.1992.tb01868.x>. \n    Gronau, Raj K. N., & Wagenmakers (2021) <doi:10.18637/jss.v100.i17>.",
    "version": "1.0.1",
    "maintainer": "Quentin F. Gronau <Quentin.F.Gronau@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8174,
    "package_name": "abundant",
    "title": "High-Dimensional Principal Fitted Components and Abundant\nRegression",
    "description": "Fit and predict with the high-dimensional principal fitted\n        components model.  This model is described by Cook, Forzani, and Rothman (2012)\n\t<doi:10.1214/11-AOS962>.",
    "version": "1.2",
    "maintainer": "Adam J. Rothman <arothman@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8187,
    "package_name": "accrual",
    "title": "Bayesian Accrual Prediction",
    "description": "Participant recruitment for medical research is challenging. Slow accrual leads to delays in research. Accrual monitoring during the process of recruitment is critical. Researchers need reliable tools to manage the accrual rate. We developed a Bayesian method that integrates the researcher's experience with previous trials and data from the current study, providing reliable predictions on accrual rate for clinical studies. For more details and background on these methodologies, see the publications of Byron, Stephen and Susan (2008) <doi:10.1002/sim.3128>, and Yu et al. (2015) <doi:10.1002/sim.6359>. In this R package, Bayesian accrual prediction functions are presented, which can be easily used by statisticians and clinical researchers.",
    "version": "1.4",
    "maintainer": "Junhao Liu <liujunhao2008@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8196,
    "package_name": "acebayes",
    "title": "Optimal Bayesian Experimental Design using the ACE Algorithm",
    "description": "Optimal Bayesian experimental design using the approximate coordinate exchange (ACE) algorithm.",
    "version": "1.11",
    "maintainer": "Antony M. Overstall <A.M.Overstall@soton.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8208,
    "package_name": "acp",
    "title": "Autoregressive Conditional Poisson",
    "description": "Analysis of count data exhibiting autoregressive properties, using the Autoregressive Conditional Poisson model (ACP(p,q)) proposed by Heinen (2003).",
    "version": "2.1",
    "maintainer": "Siakoulis Vasilios <Siakoulis.Vasilios@atticabank.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8211,
    "package_name": "acss",
    "title": "Algorithmic Complexity for Short Strings",
    "description": "Main functionality is to provide the algorithmic complexity for\n    short strings, an approximation of the Kolmogorov Complexity of a short\n    string using the coding theorem method (see ?acss). The database containing\n    the complexity is provided in the data only package acss.data, this package\n    provides functions accessing the data such as prob_random returning the\n    posterior probability that a given string was produced by a random process.\n    In addition, two traditional (but problematic) measures of complexity are\n    also provided: entropy and change complexity.",
    "version": "0.3-2",
    "maintainer": "Henrik Singmann <singmann+acss@gmail.com>",
    "url": "https://complexity-calculator.com/methodology.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8215,
    "package_name": "actel",
    "title": "Acoustic Telemetry Data Analysis",
    "description": "Designed for studies where animals tagged with acoustic tags are expected\n    to move through receiver arrays. This package combines the advantages of automatic sorting and checking \n    of animal movements with the possibility for user intervention on tags that deviate from expected \n    behaviour. The three analysis functions (explore(), migration() and residency()) \n    allow the users to analyse their data in a systematic way, making it easy to compare results from \n    different studies.\n    CJS calculations are based on Perry et al. (2012) <https://www.researchgate.net/publication/256443823_Using_mark-recapture_models_to_estimate_survival_from_telemetry_data>.",
    "version": "1.3.0",
    "maintainer": "Hugo Flávio <hflavio@wlu.ca>",
    "url": "https://github.com/hugomflavio/actel,\nhttps://hugomflavio.github.io/actel-website/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8216,
    "package_name": "actfts",
    "title": "Autocorrelation Tools Featured for Time Series",
    "description": "The 'actfts' package provides tools for performing autocorrelation analysis of time series data. It includes functions to compute and visualize the autocorrelation function (ACF) and the partial autocorrelation function (PACF). Additionally, it performs the Dickey-Fuller, KPSS, and Phillips-Perron unit root tests to assess the stationarity of time series. Theoretical foundations are based on Box and Cox (1964) <doi:10.1111/j.2517-6161.1964.tb00553.x>, Box and Jenkins (1976) <isbn:978-0-8162-1234-2>, and Box and Pierce (1970) <doi:10.1080/01621459.1970.10481180>. Statistical methods are also drawn from Kolmogorov (1933) <doi:10.1007/BF00993594>, Kwiatkowski et al. (1992) <doi:10.1016/0304-4076(92)90104-Y>, and Ljung and Box (1978) <doi:10.1093/biomet/65.2.297>. The package integrates functions from 'forecast' (Hyndman & Khandakar, 2008) <https://CRAN.R-project.org/package=forecast>, 'tseries' (Trapletti & Hornik, 2020) <https://CRAN.R-project.org/package=tseries>, 'xts' (Ryan & Ulrich, 2020) <https://CRAN.R-project.org/package=xts>, and 'stats' (R Core Team, 2023) <https://stat.ethz.ch/R-manual/R-devel/library/stats/html/00Index.html>. Additionally, it provides visualization tools via 'plotly' (Sievert, 2020) <https://CRAN.R-project.org/package=plotly> and 'reactable' (Glaz, 2023) <https://CRAN.R-project.org/package=reactable>. The package also incorporates macroeconomic datasets from the U.S. Bureau of Economic Analysis: Disposable Personal Income (DPI) <https://fred.stlouisfed.org/series/DPI>, Gross Domestic Product (GDP) <https://fred.stlouisfed.org/series/GDP>, and Personal Consumption Expenditures (PCEC) <https://fred.stlouisfed.org/series/PCEC>.",
    "version": "0.3.0",
    "maintainer": "Sergio Sierra <sergiochess95@gmail.com>",
    "url": "https://github.com/SergioFinances/actfts,\nhttps://sergiofinances.github.io/actfts/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8226,
    "package_name": "actuaRE",
    "title": "Handling Hierarchically Structured Risk Factors using Random\nEffects Models",
    "description": "Using this package, you can fit a random effects model using either the hierarchical credibility model, a combination of the hierarchical credibility model with a generalized linear model or a Tweedie generalized linear mixed model. See Campo, B.D.C. and Antonio, K. (2023) <doi:10.1080/03461238.2022.2161413>.",
    "version": "0.1.7",
    "maintainer": "Campo Bavo D.C. <bavo.decock@kuleuven.be>",
    "url": "https://bavodc.github.io/websiteactuaRE/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8229,
    "package_name": "actverse",
    "title": "Process, Visualize, and Analyze Actigraphy Data",
    "description": "A comprehensive toolkit to process, analyze and visualize",
    "version": "0.2.0",
    "maintainer": "",
    "url": "https://github.com/danielvartan/actverse",
    "exports": [],
    "topics": ["accelerometry", "actigraphy", "actimetry", "actogram", "biomedical-data-science", "chronobiology", "data-interpolation", "health-data", "periodograms", "physiological-monitoring", "physiology", "r", "r-packages", "rstats", "sensor-data", "sleep", "spectograms", "time-series", "time-series-analysis", "wearables"],
    "score": "NA",
    "stars": 14
  },
  {
    "id": 8245,
    "package_name": "adaptMCMC",
    "title": "Implementation of a Generic Adaptive Monte Carlo Markov Chain\nSampler",
    "description": "Enables sampling from arbitrary distributions if the log density is known up to a constant; a common situation in the context of Bayesian inference. The implemented sampling algorithm was proposed by Vihola (2012) <DOI:10.1007/s11222-011-9269-5> and achieves often a high efficiency by tuning the proposal distributions to a user defined acceptance rate.",
    "version": "1.5",
    "maintainer": "Andreas Scheidegger <andreas.scheidegger@eawag.ch>",
    "url": "https://github.com/scheidan/adaptMCMC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8254,
    "package_name": "adass",
    "title": "Adaptive Smoothing Spline (AdaSS) Estimator for the\nFunction-on-Function Linear Regression",
    "description": "Implements the adaptive smoothing spline estimator for the function-on-function linear regression model described in Centofanti et al. (2023) <doi:10.1007/s00180-022-01223-6>.",
    "version": "1.0.1",
    "maintainer": "Fabio Centofanti <fabio.centofanti@unina.it>",
    "url": "https://github.com/unina-sfere/adass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8263,
    "package_name": "addhazard",
    "title": "Fit Additive Hazards Models for Survival Analysis",
    "description": "Contains tools to fit the additive hazards model to data from a cohort,\n    random sampling, two-phase Bernoulli sampling and two-phase finite population sampling,\n    as well as calibration tool to incorporate phase I auxiliary information into the\n    two-phase data model fitting.  This package provides regression parameter estimates and\n    their model-based and robust standard errors. It also offers tools to make prediction of\n    individual specific hazards.",
    "version": "1.1.0",
    "maintainer": "Jie (Kate) Hu <hujie0704@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8269,
    "package_name": "addreg",
    "title": "Additive Regression for Discrete Data",
    "description": "Methods for fitting identity-link GLMs and GAMs to discrete data,\n    using EM-type algorithms with more stable convergence properties than standard methods.",
    "version": "3.0",
    "maintainer": "Mark W. Donoghoe <markdonoghoe@gmail.com>",
    "url": "https://github.com/mdonoghoe/addreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8280,
    "package_name": "adelie",
    "title": "Group Lasso and Elastic Net Solver for Generalized Linear Models",
    "description": "Extremely efficient procedures for fitting the entire group lasso and group elastic net regularization path for GLMs, multinomial, the Cox model and multi-task Gaussian models. Similar to the R package 'glmnet' in scope of models, and in computational speed.  This package provides  R bindings to the C++ code underlying the corresponding Python package 'adelie'. These bindings offer a general purpose group elastic net solver, \n    a wide range of matrix classes that can exploit special structure \n    to allow large-scale inputs, and an assortment of \n    generalized linear model classes for fitting various types of data. \n    The package is an implementation of Yang, J. and Hastie, T. (2024) <doi:10.48550/arXiv.2405.08631>.",
    "version": "1.0.8",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "https://github.com/JamesYang007/adelie-r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8299,
    "package_name": "adlift",
    "title": "An Adaptive Lifting Scheme Algorithm",
    "description": "Adaptive wavelet lifting transforms for signal denoising using optimal local neighbourhood regression, from Nunes et al. (2006) <doi:10.1007/s11222-006-6560-y>.",
    "version": "1.4-6",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8314,
    "package_name": "adnuts",
    "title": "No-U-Turn MCMC Sampling for 'ADMB' Models",
    "description": "Bayesian inference using the no-U-turn (NUTS) algorithm by \n Hoffman and Gelman (2014) <https://www.jmlr.org/papers/v15/hoffman14a.html>. \n Designed for 'AD Model Builder' ('ADMB') models,\n or when R functions for log-density and log-density gradient\n are available, such as 'Template Model Builder'\n models and other special cases. Functionality is similar to 'Stan', \n and the 'rstan' and 'shinystan' packages are used for diagnostics and \n inference.",
    "version": "1.1.2",
    "maintainer": "Cole Monnahan <monnahc@uw.edu>",
    "url": "https://github.com/Cole-Monnahan-NOAA/adnuts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8321,
    "package_name": "adsoRptionCMF",
    "title": "Classical Model Fitting of Adsorption Isotherms",
    "description": "\n  Provides tools for classical parameter estimation of adsorption isotherm models, including both \n  linear and nonlinear forms of the Freundlich, Langmuir, and Temkin isotherms. This package allows \n  users to fit these models to experimental data, providing parameter estimates along with fit statistics \n  such as Akaike Information Criterion (AIC) and Bayesian Information Criterion (BIC). Error metrics are \n  computed to evaluate model performance, and the package produces model fit plots with bootstrapped 95% \n  confidence intervals. Additionally, it generates residual plots for diagnostic assessment of the models. \n  Researchers and engineers in material science, environmental engineering, and chemical engineering can \n  rigorously analyze adsorption behavior in their systems using this straightforward, non-Bayesian approach. \n  For more details, see Harding (1907) <doi:10.2307/2987516>.",
    "version": "0.1.1",
    "maintainer": "Paul Angelo C. Manlapaz <pacmanlapaz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8323,
    "package_name": "adsoRptionMCMC",
    "title": "Bayesian Estimation of Adsorption Isotherms via MCMC",
    "description": "\n  Provides tools for Bayesian parameter estimation of adsorption isotherm models using Markov Chain Monte Carlo (MCMC) methods. \n  This package enables users to fit non-linear and linear adsorption isotherm models—Freundlich, Langmuir, and Temkin—within a \n  probabilistic framework, capturing uncertainty and parameter correlations. It provides posterior summaries, 95% credible intervals, \n  convergence diagnostics (Gelman-Rubin), and visualizations through trace and density plots. With this R package, researchers can \n  rigorously analyze adsorption behavior in environmental and chemical systems using robust Bayesian inference. For more details, \n  see Gilks et al. (1995) <doi:10.1201/b14835>, and Gamerman & Lopes (2006) <doi:10.1201/9781482296426>.",
    "version": "0.1.0",
    "maintainer": "Paul Angelo C. Manlapaz <pacmanlapaz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8339,
    "package_name": "afex",
    "title": "Analysis of Factorial Experiments",
    "description": "Convenience functions for analyzing factorial experiments using ANOVA or\n         mixed models. aov_ez(), aov_car(), and aov_4() allow specification of\n         between, within (i.e., repeated-measures), or mixed (i.e., split-plot) \n         ANOVAs for data in long format (i.e., one observation per row),\n         automatically aggregating multiple observations per individual and cell \n         of the design. mixed() fits mixed models using lme4::lmer() and computes \n         p-values for all fixed effects using either Kenward-Roger or Satterthwaite \n         approximation for degrees of freedom (LMM only), parametric bootstrap \n         (LMMs and GLMMs), or likelihood ratio tests (LMMs and GLMMs). \n         afex_plot() provides a high-level interface for interaction or one-way \n         plots using ggplot2, combining raw data and model estimates. afex uses \n         type 3 sums of squares as default (imitating commercial statistical software).",
    "version": "1.5-1",
    "maintainer": "Henrik Singmann <singmann@gmail.com>",
    "url": "https://afex.singmann.science/, https://github.com/singmann/afex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8356,
    "package_name": "africamonitor",
    "title": "Africa Macroeconomic Monitor Database API",
    "description": "An R API providing access to a relational database with macroeconomic data for Africa. \n             The database contains >700 macroeconomic time series from mostly international sources, \n             grouped into 50 macroeconomic and development-related topics. Series are carefully selected\n             on the basis of data coverage for Africa, frequency, and relevance to the macro-development context. \n             The project is part of the 'Kiel Institute Africa Initiative' \n             <https://www.ifw-kiel.de/institute/initiatives/kiel-institute-africa-initiative/>, \n             which, amongst other things, aims to develop a parsimonious database with highly relevant indicators \n             to monitor macroeconomic developments in Africa, accessible through a fast API and a web-based platform\n             at <https://africamonitor.ifw-kiel.de/>. \n             The database is maintained at the Kiel Institute for the World Economy <https://www.ifw-kiel.de/>. ",
    "version": "0.2.4",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "url": "https://africamonitor.ifw-kiel.de/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8360,
    "package_name": "aftgee",
    "title": "Accelerated Failure Time Model with Generalized Estimating\nEquations",
    "description": "A collection of methods for both the rank-based estimates and least-square estimates to the Accelerated Failure Time (AFT) model. For rank-based estimation, it provides approaches that include the computationally efficient Gehan's weight and the general's weight such as the logrank weight. Details of the rank-based estimation can be found in Chiou et al. (2014) <doi:10.1007/s11222-013-9388-2> and Chiou et al. (2015) <doi:10.1002/sim.6415>. For the least-square estimation, the estimating equation is solved with generalized estimating equations (GEE). Moreover, in multivariate cases, the dependence working correlation structure can be specified in GEE's setting. Details on the least-squares estimation can be found in Chiou et al. (2014) <doi:10.1007/s10985-014-9292-x>.",
    "version": "1.2.1",
    "maintainer": "Sy Han Chiou <schiou@smu.edu>",
    "url": "https://github.com/stc04003/aftgee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8362,
    "package_name": "aftsem",
    "title": "Semiparametric Accelerated Failure Time Model",
    "description": "Implements several basic algorithms for estimating regression parameters for semiparametric accelerated failure time (AFT) model. The main methods are: Jin rank-based method (Jin (2003) <doi:10.1093/biomet/90.2.341>), Heller’s estimating method (Heller (2012) <doi:10.1198/016214506000001257>), Polynomial smoothed Gehan function method (Chung (2013) <doi:10.1007/s11222-012-9333-9>), Buckley-James method (Buckley (1979) <doi:10.2307/2335161>) and Jin`s improved least squares method (Jin (2006) <doi:10.1093/biomet/93.1.147>). This package can be used for modeling right-censored data and for comparing different estimation algorithms.",
    "version": "1.0",
    "maintainer": "Martin Benedikt <benedma2@cvut.cz>",
    "url": "https://github.com/benedma2/aftsem-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8368,
    "package_name": "agena.ai",
    "title": "R Wrapper for 'agena.ai' API",
    "description": "An R wrapper for 'agena.ai' <https://www.agena.ai> which provides users capabilities to work with 'agena.ai' using the R environment. Users can create Bayesian network models from scratch or import existing models in R and export to 'agena.ai' cloud or local API for calculations. Note: running calculations requires a valid 'agena.ai' API license (past the initial trial period of the local API).",
    "version": "1.1.2",
    "maintainer": "Eugene Dementiev <support@agenarisk.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8376,
    "package_name": "aggutils",
    "title": "Utilities for Aggregating Probabilistic Forecasts",
    "description": "Provides several methods for aggregating probabilistic forecasts. You have a group of\n    people who have made probabilistic forecasts for the same event. You want to take advantage of\n    the \"wisdom of the crowd\" and combine these forecasts in some sensible way. This package\n    provides implementations of several strategies, including geometric mean of odds, an extremized\n    aggregate (Neyman, Roughgarden (2021) <doi:10.1145/3490486.3538243>), and \"high-density trimmed\n    mean\" (Powell et al. (2022) <doi:10.1037/dec0000191>).",
    "version": "1.0.2",
    "maintainer": "Molly Hickman <molly@forecastingresearch.org>",
    "url": "https://github.com/forecastingresearch/aggutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8377,
    "package_name": "aghq",
    "title": "Adaptive Gauss Hermite Quadrature for Bayesian Inference",
    "description": "Adaptive Gauss Hermite Quadrature for Bayesian inference.\n    The AGHQ method for normalizing posterior distributions\n    and making Bayesian inferences based on them. Functions are provided for doing\n    quadrature and marginal Laplace approximations, and summary methods are provided\n    for making inferences based on the results. \n    See Stringer (2021). \"Implementing Adaptive Quadrature for Bayesian Inference: \n    the aghq Package\" <arXiv:2101.04468>.",
    "version": "0.4.1",
    "maintainer": "Alex Stringer <alex.stringer@uwaterloo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8379,
    "package_name": "aglm",
    "title": "Accurate Generalized Linear Model",
    "description": "Provides functions to fit Accurate Generalized Linear Model (AGLM) models, visualize them, and predict for new data. AGLM is defined as a regularized GLM which applies a sort of feature transformations using a discretization of numerical features and specific coding methodologies of dummy variables. For more information on AGLM, see Suguru Fujita, Toyoto Tanaka, Kenji Kondo and Hirokazu Iwasawa (2020) <https://www.institutdesactuaires.com/global/gene/link.php?doc_id=16273&fg=1>.",
    "version": "0.4.1",
    "maintainer": "Kenji Kondo <kkondo.odnokk@gmail.com>",
    "url": "https://github.com/kkondo1981/aglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8392,
    "package_name": "agrostab",
    "title": "Stability Analysis for Agricultural Research",
    "description": "Statistical procedures to perform stability analysis in plant breeding and to identify stable genotypes under diverse environments. It is possible to calculate coefficient of homeostaticity by Khangildin et al. (1979), variance of specific adaptive ability by Kilchevsky&Khotyleva (1989), weighted homeostaticity index by Martynov (1990), steadiness of stability index by Udachin (1990), superiority measure by Lin&Binn (1988) <doi:10.4141/cjps88-018>, regression on environmental index by Erberhart&Rassel (1966) <doi:10.2135/cropsci1966.0011183X000600010011x>, Tai's (1971) stability parameters <doi:10.2135/cropsci1971.0011183X001100020006x>, stability variance by Shukla (1972) <doi:10.1038/hdy.1972.87>, ecovalence by Wricke (1962), nonparametric stability parameters by Nassar&Huehn (1987) <doi:10.2307/2531947>, Francis&Kannenberg's parameters of stability (1978) <doi:10.4141/cjps78-157>.",
    "version": "0.1.0",
    "maintainer": "Anna Cheshkova <cheshanf@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8393,
    "package_name": "ags",
    "title": "Crosswalk Municipality and District Statistics in Germany",
    "description": "Construct time series for Germany's municipalities (Gemeinden) and districts (Kreise) using a annual crosswalk constructed by the Federal Office for Building and Regional Planning (BBSR). ",
    "version": "1.0.1",
    "maintainer": "Moritz Marbach <m.marbach@ucl.ac.uk>",
    "url": "https://sumtxt.github.io/ags/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8396,
    "package_name": "ahaz",
    "title": "Regularization for Semiparametric Additive Hazards Regression",
    "description": "Computationally efficient procedures for regularized\n        estimation with the semiparametric additive hazards regression\n        model.",
    "version": "1.15.1",
    "maintainer": "Anders Gorst-Rasmussen <agorstras@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8402,
    "package_name": "aiRthermo",
    "title": "Atmospheric Thermodynamics and Visualization",
    "description": "Deals with many computations related to the thermodynamics of atmospheric processes. It includes many functions designed to consider the density of air with varying degrees of water vapour in it, saturation pressures and mixing ratios, conversion of moisture indices, computation of atmospheric states of parcels subject to dry or pseudoadiabatic vertical evolutions and atmospheric instability indices that are routinely used for operational weather forecasts or meteorological diagnostics.",
    "version": "1.2.2",
    "maintainer": "Santos J. González-Rojí <santosjose.gonzalez@ehu.eus>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8410,
    "package_name": "airGRdatasets",
    "title": "Hydro-Meteorological Catchments Datasets for the 'airGR'\nPackages",
    "description": "Sample of hydro-meteorological datasets extracted from the 'CAMELS-FR' French database <doi:10.57745/WH7FJR>. \n  It provides metadata and catchment-scale aggregated hydro-meteorological time series on a pool of French catchments for use by the 'airGR' packages. ",
    "version": "0.2.3",
    "maintainer": "Olivier Delaigue <airGR@inrae.fr>",
    "url": "https://gitlab.irstea.fr/HYCAR-Hydro/airgrgalaxy/airgrdatasets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8415,
    "package_name": "airnow",
    "title": "Retrieve 'AirNow' Air Quality Observations and Forecasts",
    "description": "Retrieve air quality data via the 'AirNow'\n    <https://www.airnow.gov/> API.",
    "version": "0.1.0",
    "maintainer": "Brian Connelly <bdc@bconnelly.net>",
    "url": "https://github.com/briandconnelly/airnow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8423,
    "package_name": "aisoph",
    "title": "Additive Isotonic Proportional Hazards Model",
    "description": "Nonparametric estimation of additive isotonic covariate effects for proportional hazards model.",
    "version": "0.4",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8449,
    "package_name": "alcoholSurv",
    "title": "Light Daily Alcohol and Longevity",
    "description": "Contains data from an observational study concerning possible effects of light daily alcohol consumption on survival and on HDL cholesterol.  It also replicates various simple analyses in Rosenbaum (2025a) <doi:10.1080/09332480.2025.2473291>.  Finally, it includes new R code in wgtRankCef() that implements and replicates a new method for constructing evidence factors in observational block designs. ",
    "version": "0.7.0",
    "maintainer": "Paul Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8451,
    "package_name": "ald",
    "title": "The Asymmetric Laplace Distribution",
    "description": "It provides the density, distribution function, quantile function, \n             random number generator, likelihood function, moments and Maximum Likelihood estimators for a given sample, all this for\n             the three parameter Asymmetric Laplace Distribution defined \n             in Koenker and Machado (1999). This is a special case of the skewed family of distributions\n             available in Galarza et.al. (2017) <doi:10.1002/sta4.140> useful for quantile regression. ",
    "version": "1.3.1",
    "maintainer": "Christian E. Galarza <cgalarza88@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8456,
    "package_name": "alfred",
    "title": "Downloading Time Series from ALFRED Database for Various\nVintages",
    "description": "Provides direct access to the ALFRED (<https://alfred.stlouisfed.org>) and FRED (<https://fred.stlouisfed.org>) databases.\n    Its functions return tidy data frames for different releases of the specified time series. \n    Note that this product uses the FRED© API but is not endorsed or certified by the Federal Reserve Bank of St. Louis.",
    "version": "0.2.1",
    "maintainer": "Onno Kleen <r@onnokleen.de>",
    "url": "https://github.com/onnokleen/alfred/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8464,
    "package_name": "align",
    "title": "A Modified DTW Algorithm for Stratigraphic Time Series Alignment",
    "description": "A dynamic time warping (DTW) algorithm for stratigraphic alignment,\n    translated into R from the original published 'MATLAB' code by Hay et al. (2019)\n    <doi:10.1130/G46019.1>. The DTW algorithm incorporates two geologically relevant\n    parameters (g and edge) for augmenting the typical DTW cost matrix, allowing\n    for a range of sedimentologic and chronologic conditions to be explored, as \n    well as the generation of an alignment library (as opposed to a single alignment\n    solution). The g parameter relates to the relative sediment accumulation rate\n    between the two time series records,  while the edge parameter relates to the \n    amount of total shared time between the records. Note that this algorithm is\n    used for all DTW alignments in the Align Shiny application, detailed in Hagen\n    et al. (in review).",
    "version": "0.1.0",
    "maintainer": "Cedric Hagen <ch0934@princeton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8470,
    "package_name": "allestimates",
    "title": "Effect Estimates from All Models",
    "description": "Estimates and plots effect estimates from models with all possible \n    combinations of a list of variables. It can be used for assessing treatment \n    effects in clinical trials or risk factors in bio-medical and epidemiological \n    research. Like Stata command 'confall' (Wang Z (2007) <doi:10.1177/1536867X0700700203> ), \n    'allestimates' calculates and stores all effect estimates, and plots them against p values or \n    Akaike information criterion (AIC) values. It currently has functions for linear \n    regression: all_lm(), logistic and Poisson regression: all_glm(), \n    and Cox proportional hazards regression: all_cox(). ",
    "version": "0.2.3",
    "maintainer": "Zhiqiang Wang <menzies.uq@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8476,
    "package_name": "alluvial",
    "title": "Alluvial Diagrams",
    "description": "Creating alluvial diagrams (also known as parallel sets plots) for multivariate\n  and time series-like data.",
    "version": "0.1-2",
    "maintainer": "Michal Bojanowski <michal2992@gmail.com>",
    "url": "https://github.com/mbojan/alluvial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8477,
    "package_name": "almanac",
    "title": "Tools for Working with Recurrence Rules",
    "description": "Provides tools for defining recurrence rules and recurrence\n    sets. Recurrence rules are a programmatic way to define a recurring\n    event, like the first Monday of December. Multiple recurrence rules\n    can be combined into larger recurrence sets. A full holiday and\n    calendar interface is also provided that can generate holidays within\n    a particular year, can detect if a date is a holiday, can respect\n    holiday observance rules, and allows for custom holidays.",
    "version": "1.0.0",
    "maintainer": "Davis Vaughan <davis@posit.co>",
    "url": "https://github.com/DavisVaughan/almanac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8478,
    "package_name": "alone",
    "title": "Datasets from the Survival TV Series Alone",
    "description": "A collection of datasets on the Alone survival TV series in tidy format. \n  Included in the package are 4 datasets detailing the survivors, their loadouts,\n  episode details and season information.",
    "version": "0.7",
    "maintainer": "Daniel Oehm <danieloehm@gmail.com>",
    "url": "https://github.com/doehm/alone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8481,
    "package_name": "alpaca",
    "title": "Fit GLM's with High-Dimensional k-Way Fixed Effects",
    "description": "Provides a routine to partial out factors with many levels during the optimization of the log-likelihood function of the corresponding generalized linear model (glm). The package is based on the algorithm described in Stammann (2018) <doi:10.48550/arXiv.1707.01815> and is restricted to glm's that are based on maximum likelihood estimation and nonlinear. It also offers an efficient algorithm to recover estimates of the fixed effects in a post-estimation routine and includes robust and multi-way clustered standard errors. Further the package provides analytical bias corrections for binary choice models derived by Fernandez-Val and Weidner (2016) <doi:10.1016/j.jeconom.2015.12.014> and Hinz, Stammann, and Wanner (2020) <doi:10.48550/arXiv.2004.12655>.",
    "version": "0.3.5",
    "maintainer": "Amrei Stammann <amrei.stammann@uni-bayreuth.de>",
    "url": "https://github.com/amrei-stammann/alpaca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8483,
    "package_name": "alphaN",
    "title": "Set Alpha Based on Sample Size Using Bayes Factors",
    "description": "Sets the alpha level for coefficients in a regression model\n    as a decreasing function of the sample size through the use of\n    Jeffreys' Approximate Bayes factor. You tell alphaN() your sample\n    size, and it tells you to which value you must lower alpha to avoid\n    Lindley's Paradox. For details, see Wulff and Taylor (2024)\n    <doi:10.1177/14761270231214429>.",
    "version": "0.1.2",
    "maintainer": "Jesper Wulff <jwulff@econ.au.dk>",
    "url": "https://github.com/jespernwulff/alphaN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8491,
    "package_name": "alpmixBayes",
    "title": "Bayesian Estimation for Alpha-Mixture Survival Models",
    "description": "Implements Bayesian estimation and inference for alpha-mixture survival models,\n  including Weibull and Exponential based components, with tools for simulation and posterior \n  summaries. The methods target applications in reliability and biomedical survival analysis.\n  The package implements Bayesian estimation for the alpha-mixture methodology introduced in \n  Asadi et al. (2019) <doi:10.1017/jpr.2019.72>.",
    "version": "0.1.0",
    "maintainer": "Feng Luan <fluan1@niu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8492,
    "package_name": "alqrfe",
    "title": "Adaptive Lasso Quantile Regression with Fixed Effects",
    "description": "Quantile regression with fixed effects solves longitudinal data, considering the individual intercepts as fixed effects. The parametric set of this type of problem used to be huge. Thus penalized methods such as Lasso are currently applied. Adaptive Lasso presents oracle proprieties, which include Gaussianity and correct model selection. Bayesian information criteria (BIC) estimates the optimal tuning parameter lambda. Plot tools are also available.",
    "version": "1.3",
    "maintainer": "Ian Meneghel Danilevicz <iandanilevicz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8493,
    "package_name": "alr4",
    "title": "Data to Accompany Applied Linear Regression 4th Edition",
    "description": "Datasets to Accompany S. Weisberg (2014, ISBN: 978-1-118-38608-8), \n \"Applied Linear Regression,\" 4th edition.  Many data files \n in this package are included in the `alr3` package as well, so only one of them \n should be used.",
    "version": "1.0.6",
    "maintainer": "Sanford Weisberg <sandy@umn.edu>",
    "url": "http://www.z.umn.edu/alr4ed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8518,
    "package_name": "amen",
    "title": "Additive and Multiplicative Effects Models for Networks and\nRelational Data",
    "description": "Analysis of dyadic network and relational data using additive and\n    multiplicative effects (AME) models. The basic model includes\n    regression terms, the covariance structure of the social relations model\n    (Warner, Kenny and Stoto (1979) <DOI:10.1037/0022-3514.37.10.1742>, \n    Wong (1982) <DOI:10.2307/2287296>), and multiplicative factor\n    models (Hoff(2009) <DOI:10.1007/s10588-008-9040-4>). \n    Several different link functions accommodate different\n    relational data structures, including binary/network data, normal\n    relational data, zero-inflated positive outcomes using a tobit model, ordinal relational data and data from\n    fixed-rank nomination schemes. Several of these link functions are\n    discussed in Hoff, Fosdick, Volfovsky and Stovel (2013) \n    <DOI:10.1017/nws.2013.17>. Development of this\n    software was supported in part by NIH grant R01HD067509.",
    "version": "1.4.5",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "https://github.com/pdhoff/amen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8522,
    "package_name": "ammiBayes",
    "title": "Bayesian Ammi Model for Continuous Data",
    "description": "Flexible multi-environment trials analysis via MCMC method for Additive Main Effects and Multiplicative Model (AMMI) for continuous data. \n Biplot with the averages and regions of confidence can be generated. The chains run in parallel on Linux systems and run serially on Windows.",
    "version": "1.0-3",
    "maintainer": "Fabio M. Correa <fmcron@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8528,
    "package_name": "amregtest",
    "title": "Runs Allelematch Regression Tests",
    "description": "Automates regression testing of package 'allelematch'. Over\n    2500 tests covers all functions in 'allelematch', reproduces the\n    examples from the documentation and includes negative tests. The\n    implementation is based on 'testthat'.",
    "version": "1.0.5",
    "maintainer": "Torvald Staxler <torvald.staxler@telia.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8548,
    "package_name": "aniSNA",
    "title": "Statistical Network Analysis of Animal Social Networks",
    "description": "Obtain network structures from animal GPS telemetry observations and \n    statistically analyse them to assess their adequacy for social network analysis. Methods include \n    pre-network data permutations, bootstrapping techniques to obtain confidence intervals \n    for global and node-level network metrics, and correlation and regression analysis of the local network metrics. ",
    "version": "1.1.1",
    "maintainer": "Prabhleen Kaur <prabhleen.kaur.ucd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8554,
    "package_name": "animation",
    "title": "A Gallery of Animations in Statistics and Utilities to Create Animations",
    "description": "Provides functions for animations in statistics, covering topics",
    "version": "2.8.1",
    "maintainer": "",
    "url": "https://github.com/yihui/animation",
    "exports": [],
    "topics": ["animation", "r", "r-package", "statistical-computing", "statistical-graphics", "statistics"],
    "score": "NA",
    "stars": 210
  },
  {
    "id": 8572,
    "package_name": "anomaly",
    "title": "Detecting Anomalies in Data",
    "description": "Implements Collective And Point Anomaly (CAPA) Fisch, Eckley, and Fearnhead (2022) <doi:10.1002/sam.11586>, Multi-Variate Collective And Point Anomaly (MVCAPA) Fisch, Eckley, and Fearnhead (2021) <doi:10.1080/10618600.2021.1987257>, Proportion Adaptive Segment Selection (PASS) Jeng, Cai, and Li (2012) <doi:10.1093/biomet/ass059>, and Bayesian Abnormal Region Detector (BARD) Bardwell and Fearnhead (2015) <doi:10.1214/16-BA998>. These methods are for the detection of anomalies in time series data. Further information regarding the use of this package along with detailed examples can be found in Fisch, Grose, Eckley, Fearnhead, and Bardwell (2024) <doi:10.18637/jss.v110.i01>.",
    "version": "4.3.3",
    "maintainer": "Daniel Grose <dan.grose@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8577,
    "package_name": "anovir",
    "title": "Analysis of Virulence",
    "description": "Epidemiological population dynamics models traditionally define \n    a pathogen's virulence as the increase in the per capita rate of mortality \n    of infected hosts due to infection. This package provides functions \n    allowing virulence to be estimated by maximum likelihood techniques. The \n    approach is based on the analysis of relative survival comparing survival \n    in matching cohorts of infected vs. uninfected hosts (Agnew 2019) \n    <doi:10.1101/530709>. ",
    "version": "0.1.0",
    "maintainer": "Philip Agnew <philip.agnew@ird.fr>",
    "url": "https://www.biorxiv.org/content/10.1101/530709v1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8578,
    "package_name": "anscombiser",
    "title": "Create Datasets with Identical Summary Statistics",
    "description": "Anscombe's quartet are a set of four two-variable datasets that \n    have several common summary statistics but which have very different joint \n    distributions.  This becomes apparent when the data are plotted, which \n    illustrates the importance of using graphical displays in Statistics.  This\n    package enables the creation of datasets that have identical marginal sample\n    means and sample variances, sample correlation, least squares regression \n    coefficients and coefficient of determination.  The user supplies an initial \n    dataset, which is shifted, scaled and rotated in order to achieve target \n    summary statistics.  The general shape of the initial dataset is retained. \n    The target statistics can be supplied directly or calculated based on a \n    user-supplied dataset.  The 'datasauRus' package \n    <https://cran.r-project.org/package=datasauRus> provides further examples \n    of datasets that have markedly different scatter plots but share many \n    sample summary statistics.",
    "version": "1.1.0",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "url": "https://paulnorthrop.github.io/anscombiser/,\nhttps://github.com/paulnorthrop/anscombiser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8595,
    "package_name": "anytime",
    "title": "Anything to 'POSIXct' or 'Date' Converter",
    "description": "Convert input in any one of character, integer, numeric, factor,\n or ordered type into 'POSIXct' (or 'Date') objects, using one of a number of\n predefined formats, and relying on Boost facilities for date and time parsing.",
    "version": "0.3.12",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "url": "https://github.com/eddelbuettel/anytime,\nhttps://dirk.eddelbuettel.com/code/anytime.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8597,
    "package_name": "aod",
    "title": "Analysis of Overdispersed Data",
    "description": "Provides a set of functions to analyse\n        overdispersed counts or proportions. Most of the methods are\n        already available elsewhere but are scattered in different\n        packages. The proposed functions should be considered as\n        complements to more sophisticated methods such as generalized\n        estimating equations (GEE) or generalized linear mixed effect\n        models (GLMM).",
    "version": "1.3.3",
    "maintainer": "Renaud Lancelot <renaud.lancelot@cirad.fr>",
    "url": "https://cran.r-project.org/package=aod",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8598,
    "package_name": "aods3",
    "title": "Analysis of Overdispersed Data using S3 Methods",
    "description": "Provides functions to analyse overdispersed counts or proportions. These functions should be considered as complements to more sophisticated methods such as generalized estimating equations (GEE) or generalized linear mixed effect models (GLMM). aods3 is an S3 re-implementation of the deprecated S4 package aod.",
    "version": "0.5",
    "maintainer": "Aurélie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8611,
    "package_name": "apc",
    "title": "Age-Period-Cohort Analysis",
    "description": "Functions for age-period-cohort analysis. Aggregate data can be organised in matrices indexed by age-cohort, age-period or cohort-period. The data can include dose and response or just doses. The statistical model is a generalized linear model (GLM) allowing for 3,2,1 or 0 of the age-period-cohort factors. 2-sample analysis is possible. Mixed frequency data are possible. Individual-level data should have a row for each individual and columns for each of age, period, and cohort. The statistical model for repeated cross-section is a generalized linear model. The statistical model for panel data is ordinary least squares. The canonical parametrisation of Kuang, Nielsen and Nielsen (2008) <DOI:10.1093/biomet/asn026> is used. Thus, the analysis does not rely on ad hoc identification.",
    "version": "3.0.0",
    "maintainer": "Bent Nielsen <bent.nielsen@nuffield.ox.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8622,
    "package_name": "api2lm",
    "title": "Functions and Data Sets for the Book \"A Progressive Introduction\nto Linear Models\"",
    "description": "Simplifies aspects of linear regression analysis, particularly simultaneous inference. Additionally, supports \"A Progressive Introduction to Linear Models\" by Joshua French (<https://jfrench.github.io/LinearRegression/>).",
    "version": "0.2",
    "maintainer": "Joshua P. French <joshua.french@ucdenver.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8624,
    "package_name": "aplms",
    "title": "Additive Partial Linear Models with Symmetric Autoregressive\nErrors",
    "description": "Set of tools for fitting the additive partial linear models with symmetric autoregressive errors of order p, or APLMS-AR(p). This setup enables the modeling of a time series response variable using linear and nonlinear structures of a set of explanatory variables, with nonparametric components approximated by natural cubic splines or P-splines. It also accounts for autoregressive error terms with distributions that have lighter or heavier tails than the normal distribution. The package includes various error distributions, such as normal, generalized normal, Student's t, generalized Student's t, power-exponential, and Cauchy distributions. Chou-Chen, S.W., Oliveira, R.A., Raicher, I., Gilberto A. Paula (2024) <doi:10.1007/s00362-024-01590-w>.",
    "version": "0.1.0",
    "maintainer": "Shu Wei Chou-Chen <shuwei.chou@ucr.ac.cr>",
    "url": "https://github.com/shuwei325/aplms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8629,
    "package_name": "apm",
    "title": "Averaged Prediction Models",
    "description": "In panel data settings, specifies set of candidate models, fits them to data from pre-treatment validation periods, and selects model as average over candidate models, weighting each by posterior probability of being most robust given its differential average prediction errors in pre-treatment validation periods. Subsequent estimation and inference of causal effect's bounds accounts for both model and sampling uncertainty, and calculates the robustness changepoint value at which bounds go from excluding to including 0. The package also includes a range of diagnostic plots, such as those illustrating models' differential average prediction errors and the posterior distribution of which model is most robust.",
    "version": "0.1.1",
    "maintainer": "Thomas Leavitt <thomas.leavitt@baruch.cuny.edu>",
    "url": "https://github.com/tl2624/apm/, https://tl2624.github.io/apm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8632,
    "package_name": "apollo",
    "title": "Tools for Choice Model Estimation and Application",
    "description": "Choice models are a widely used technique across numerous scientific disciplines. The Apollo package is a very flexible tool for the estimation and application \n    of choice models in R. Users are able to write their own \n    model functions or use a mix of already available ones. Random heterogeneity, \n    both continuous and discrete and at the level of individuals and \n    choices, can be incorporated for all models. There is support for both standalone \n    models and hybrid model structures.  Both classical \n    and Bayesian estimation is available, and multiple discrete \n    continuous models are covered in addition to discrete choice. \n    Multi-threading processing is supported for estimation and a large\n    number of pre and post-estimation routines, including for computing posterior\n    (individual-level) distributions are available. \n    For examples, a manual, and a support forum, visit \n    <https://www.ApolloChoiceModelling.com>. For more information on choice \n    models see Train, K. (2009) <isbn:978-0-521-74738-7> and Hess, \n    S. & Daly, A.J. (2014) <isbn:978-1-781-00314-5> for an overview \n    of the field.",
    "version": "0.3.6",
    "maintainer": "Stephane Hess <S.Hess@leeds.ac.uk>",
    "url": "https://www.ApolloChoiceModelling.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8642,
    "package_name": "approximator",
    "title": "Bayesian Prediction of Complex Computer Codes",
    "description": "Performs Bayesian prediction of complex computer codes when fast approximations are available.  It uses a hierarchical version of the Gaussian process, originally proposed by Kennedy and O'Hagan (2000), Biometrika 87(1):1.",
    "version": "1.2-8",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8643,
    "package_name": "approxmatch",
    "title": "Approximately Optimal Fine Balance Matching with Multiple Groups",
    "description": "Tools for constructing a matched design with multiple comparison groups.\n Further specifications of refined covariate balance restriction and exact match on \n covariate can be imposed. Matches are approximately optimal in  the sense that the \n cost of the solution is at most twice the optimal cost, Crama and Spieksma (1992) \n <doi:10.1016/0377-2217(92)90078-N>, Karmakar, Small and Rosenbaum (2019)\n <doi:10.1080/10618600.2019.1584900>.",
    "version": "2.0",
    "maintainer": "Bikram Karmakar <bkarmakar@ufl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8646,
    "package_name": "aprean3",
    "title": "Datasets from Draper and Smith \"Applied Regression Analysis\"\n(3rd Ed., 1998)",
    "description": "An unofficial companion to the textbook \"Applied Regression\n    Analysis\" by N.R. Draper and H. Smith (3rd Ed., 1998) including all the\n    accompanying datasets.",
    "version": "1.0.2",
    "maintainer": "Luca Braglia <lbraglia@gmail.com>",
    "url": "https://github.com/lbraglia/aprean3",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8674,
    "package_name": "archiDART",
    "title": "Plant Root System Architecture Analysis Using DART and RSML\nFiles",
    "description": "Analysis of complex plant root system architectures (RSA) using the output files created by Data Analysis of Root Tracings (DART), an open-access software dedicated to the study of plant root architecture and development across time series (Le Bot et al (2010) \"DART: a software to analyse root system architecture and development from captured images\", Plant and Soil, <DOI:10.1007/s11104-009-0005-2>), and RSA data encoded with the Root System Markup Language (RSML) (Lobet et al (2015) \"Root System Markup Language: toward a unified root architecture description language\", Plant Physiology, <DOI:10.1104/pp.114.253625>). More information can be found in Delory et al (2016) \"archiDART: an R package for the automated computation of plant root architectural traits\", Plant and Soil, <DOI:10.1007/s11104-015-2673-4>.",
    "version": "3.4",
    "maintainer": "Benjamin M Delory <Benjamin.Delory@leuphana.de>",
    "url": "https://archidart.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8683,
    "package_name": "ardl.nardl",
    "title": "Linear and Nonlinear Autoregressive Distributed Lag Models:\nGeneral-to-Specific Approach",
    "description": "Estimate the linear and nonlinear autoregressive distributed lag (ARDL & NARDL) models and the corresponding error correction models, and test for longrun and short-run asymmetric. The general-to-specific approach is also available in estimating the ARDL and NARDL models. The Pesaran, Shin & Smith (2001) (<doi:10.1002/jae.616>) bounds test for level relationships is also provided. The 'ardl.nardl' package also performs short-run and longrun symmetric restrictions available at Shin et al. (2014) <doi:10.1007/978-1-4899-8008-3_9> and their corresponding tests.",
    "version": "1.3.0",
    "maintainer": "Eric I. Otoakhia <otoakhiai@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8693,
    "package_name": "arfima",
    "title": "Fractional ARIMA (and Other Long Memory) Time Series Modeling",
    "description": "Simulates, fits, and predicts long-memory and anti-persistent\n\ttime series, possibly mixed with ARMA, regression, transfer-function\n\tcomponents.\n\tExact methods (MLE, forecasting, simulation) are used.\n\tBug reports should be done via GitHub (at\n\t<https://github.com/JQVeenstra/arfima>), where the development version\n\tof this package lives; it can be installed using devtools.",
    "version": "1.8-2",
    "maintainer": "JQ Veenstra <jqveenstra@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8708,
    "package_name": "arm",
    "title": "Data Analysis Using Regression and Multilevel/Hierarchical\nModels",
    "description": "Functions to accompany A. Gelman and J. Hill, Data Analysis Using Regression and Multilevel/Hierarchical Models, Cambridge University Press, 2007.",
    "version": "1.14-4",
    "maintainer": "Yu-Sung Su <suyusung@tsinghua.edu.cn>",
    "url": "https://CRAN.R-project.org/package=arm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8729,
    "package_name": "arsenal",
    "title": "An Arsenal of 'R' Functions for Large-Scale Statistical Summaries",
    "description": "An Arsenal of 'R' functions for large-scale statistical summaries,",
    "version": "3.6.4.0000",
    "maintainer": "",
    "url": "https://github.com/mayoverse/arsenal",
    "exports": [],
    "topics": ["baseline-characteristics", "cran", "descriptive-statistics", "modeling", "paired-comparisons", "r", "r-package", "reporting", "statistics", "tableone"],
    "score": "NA",
    "stars": 225
  },
  {
    "id": 8740,
    "package_name": "asaur",
    "title": "Data Sets for \"Applied Survival Analysis Using R\"\"",
    "description": "Data sets are referred to in the text \"Applied Survival Analysis Using R\"\n by Dirk F. Moore, Springer, 2016, ISBN: 978-3-319-31243-9, <DOI:10.1007/978-3-319-31245-3>.",
    "version": "0.50",
    "maintainer": "Dirk F. Moore <dirkfmoore@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8741,
    "package_name": "asbio",
    "title": "A Collection of Statistical Tools for Biologists",
    "description": "Contains functions from: Aho, K. (2014) Foundational and Applied Statistics for Biologists using R.  CRC/Taylor and Francis, Boca Raton, FL, ISBN: 978-1-4398-7338-0.",
    "version": "1.12-2",
    "maintainer": "Ken Aho <kenaho1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8747,
    "package_name": "asciichartr",
    "title": "Lightweight ASCII Line Graphs",
    "description": "Create ASCII line graphs of a time series directly on\n    your terminal in an easy way. There are some configurations you\n    can add to make the plot the way you like. This project was\n    inspired by the original 'asciichart' package by Igor Kroitor.",
    "version": "0.1.0",
    "maintainer": "Brian <bleemayer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8754,
    "package_name": "ashr",
    "title": "Methods for Adaptive Shrinkage, using Empirical Bayes",
    "description": "The R package 'ashr' implements an Empirical Bayes\n    approach for large-scale hypothesis testing and false discovery\n    rate (FDR) estimation based on the methods proposed in\n    M. Stephens, 2016, \"False discovery rates: a new deal\",\n    <DOI:10.1093/biostatistics/kxw041>. These methods can be applied\n    whenever two sets of summary statistics---estimated effects and\n    standard errors---are available, just as 'qvalue' can be applied\n    to previously computed p-values. Two main interfaces are\n    provided: ash(), which is more user-friendly; and ash.workhorse(),\n    which has more options and is geared toward advanced users. The\n    ash() and ash.workhorse() also provides a flexible modeling\n    interface that can accommodate a variety of likelihoods (e.g.,\n    normal, Poisson) and mixture priors (e.g., uniform, normal).",
    "version": "2.2-63",
    "maintainer": "Peter Carbonetto <pcarbo@uchicago.edu>",
    "url": "https://github.com/stephens999/ashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8759,
    "package_name": "asm",
    "title": "Optimal Convex M-Estimation for Linear Regression via Antitonic\nScore Matching",
    "description": "Performs linear regression with respect to a data-driven convex loss function that is chosen to minimize the asymptotic covariance of the resulting M-estimator. The convex loss function is estimated in 5 steps: (1) form an initial OLS (ordinary least squares) or LAD (least absolute deviation) estimate of the regression coefficients; (2) use the resulting residuals to obtain a kernel estimator of the error density; (3) estimate the score function of the errors by differentiating the logarithm of the kernel density estimate; (4) compute the L2 projection of the estimated score function onto the set of decreasing functions; (5) take a negative antiderivative of the projected score function estimate. Newton's method (with Hessian modification) is then used to minimize the convex empirical risk function. Further details of the method are given in Feng et al. (2024) <doi:10.48550/arXiv.2403.16688>.",
    "version": "0.2.4",
    "maintainer": "Min Xu <min.cut@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8765,
    "package_name": "aspline",
    "title": "Spline Regression with Adaptive Knot Selection",
    "description": "Perform one-dimensional spline regression with automatic knot selection.\n      This package uses a penalized approach to select the most relevant knots.\n      B-splines of any degree can be fitted. More details in 'Goepp et al. (2018)',\n      \"Spline Regression with Automatic Knot Selection\", <arXiv:1808.01770>.",
    "version": "0.2.0",
    "maintainer": "Vivien Goepp <vivien.goepp@gmail.com>",
    "url": "https://github.com/goepp/aspline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8773,
    "package_name": "assessor",
    "title": "Assessment Tools for Regression Models with Discrete and\nSemicontinuous Outcomes",
    "description": "Provides assessment tools for regression models with discrete and semicontinuous outcomes proposed in Yang (2023) <doi:10.48550/arXiv.2308.15596>. It calculates the double probability integral transform (DPIT) residuals, constructs QQ plots of residuals and the ordered curve for assessing mean structures.",
    "version": "1.1.1",
    "maintainer": "Jeonghwan Lee <lee03938@umn.edu>",
    "url": "https://jhlee1408.github.io/assessor/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8777,
    "package_name": "assist",
    "title": "A Suite of R Functions Implementing Spline Smoothing Techniques",
    "description": "Fit various smoothing spline models. Includes an ssr() function for smoothing \n    spline regression, an nnr() function for nonparametric nonlinear regression, an snr() \n    function for semiparametric nonlinear regression, an slm() function for semiparametric \n    linear mixed-effects models, and an snm() function for semiparametric nonlinear \n    mixed-effects models. See Wang (2011) <doi:10.1201/b10954> for an overview.",
    "version": "3.1.9",
    "maintainer": "Yuedong Wang <yuedong@ucsb.edu>",
    "url": "https://yuedong.faculty.pstat.ucsb.edu/software.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8781,
    "package_name": "assurance",
    "title": "Perform assurance computations for a clinical trial.",
    "description": "Assurance (unconditional power) calculations for clinical",
    "version": "0.999",
    "maintainer": "",
    "url": "https://github.com/scientific-computing-solutions/assurance",
    "exports": [],
    "topics": ["bayesian", "clinical-trials", "probability-technical-success"],
    "score": "NA",
    "stars": 5
  },
  {
    "id": 8783,
    "package_name": "aster",
    "title": "Aster Models",
    "description": "Aster models (Geyer, Wagenius, and Shaw, 2007,\n    <doi:10.1093/biomet/asm030>; Shaw, Geyer, Wagenius, Hangelbroek, and\n    Etterson, 2008, <doi:10.1086/588063>; Geyer, Ridley, Latta, Etterson,\n    and Shaw, 2013, <doi:10.1214/13-AOAS653>) are exponential family\n    regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, life table analysis,\n    zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    There are also random effects versions of these models.",
    "version": "1.3-7",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8784,
    "package_name": "aster2",
    "title": "Aster Models",
    "description": "Aster models are exponential family regression models for life\n    history analysis.  They are like generalized linear models except that\n    elements of the response vector can have different families (e. g.,\n    some Bernoulli, some Poisson, some zero-truncated Poisson, some normal)\n    and can be dependent, the dependence indicated by a graphical structure.\n    Discrete time survival analysis, zero-inflated Poisson regression, and\n    generalized linear models that are exponential family (e. g., logistic\n    regression and Poisson regression with log link) are special cases.\n    Main use is for data in which there is survival over discrete time periods\n    and there is additional data about what happens conditional on survival\n    (e. g., number of offspring).  Uses the exponential family canonical\n    parameterization (aster transform of usual parameterization).\n    Unlike the aster package, this package does dependence groups (nodes of\n    the graph need not be conditionally independent given their predecessor\n    node), including multinomial and two-parameter normal as families.  Thus\n    this package also generalizes mark-capture-recapture analysis.",
    "version": "0.3-2",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "url": "https://www.stat.umn.edu/geyer/aster/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8787,
    "package_name": "astrochron",
    "title": "A Computational Tool for Astrochronology",
    "description": "Routines for astrochronologic testing, astronomical time scale construction, and time series analysis <doi:10.1016/j.earscirev.2018.11.015>. Also included are a range of statistical analysis and modeling routines that are relevant to time scale development and paleoclimate analysis.",
    "version": "1.5",
    "maintainer": "Stephen Meyers <smeyers@geology.wisc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8788,
    "package_name": "astsa",
    "title": "Applied Statistical Time Series Analysis",
    "description": "Contains data sets and scripts for analyzing time series in both the frequency and time domains including state space modeling as well as supporting the texts Time Series Analysis and Its Applications: With R Examples (5th ed), by R.H. Shumway and D.S. Stoffer. Springer Texts in Statistics, 2025, <DOI:10.1007/978-3-031-70584-7>, and Time Series: A Data Analysis Approach Using R (2nd ed). Chapman-Hall, 2026, <https://www.routledge.com/Time-Series-A-Data-Analysis-Approach-Using-R/Shumway-Stoffer/p/book/9781041031642>. Most scripts are designed to require minimal input to produce aesthetically pleasing output for ease of use in live demonstrations and course work.",
    "version": "2.4",
    "maintainer": "David Stoffer <stoffer@pitt.edu>",
    "url": "https://dsstoffer.github.io/, https://nickpoison.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8795,
    "package_name": "asympDiag",
    "title": "Diagnostic Tools for Asymptotic Theory",
    "description": "Leveraging Monte Carlo simulations, this package provides\n    tools for diagnosing regression models. It implements a parametric\n    bootstrap framework to compute statistics, generates diagnostic\n    envelopes to assess goodness-of-fit, and evaluates type I error\n    control for Wald tests. By simulating data under the assumption that\n    the model is true, it helps to identify model mis-specifications and\n    enhances the reliability of the model inferences.",
    "version": "0.3.1",
    "maintainer": "Álvaro Kothe <kothe65@gmail.com>",
    "url": "https://github.com/Alvaro-Kothe/asympDiag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8820,
    "package_name": "auRoc",
    "title": "Various Methods to Estimate the AUC",
    "description": "Estimate the AUC using a variety of methods as follows: \n             (1) frequentist nonparametric methods based on the Mann-Whitney statistic or kernel methods. \n             (2) frequentist parametric methods using the likelihood ratio test based on higher-order \n             asymptotic results, the signed log-likelihood ratio test, the Wald test, \n             or the approximate ''t'' solution to the Behrens-Fisher problem. \n             (3) Bayesian parametric MCMC methods.",
    "version": "0.2-1",
    "maintainer": "Dai Feng <daifeng.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8827,
    "package_name": "audrex",
    "title": "Automatic Dynamic Regression using Extreme Gradient Boosting",
    "description": "Dynamic regression for time series using Extreme Gradient Boosting with hyper-parameter tuning via Bayesian Optimization or Random Search.",
    "version": "3.0.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/audrex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8839,
    "package_name": "autoCovariateSelection",
    "title": "Automated Covariate Selection Using HDPS Algorithm",
    "description": "Contains functions to implement automated covariate selection using methods described in the\n             high-dimensional propensity score (HDPS) algorithm by Schneeweiss et.al. Covariate adjustment in real-world-observational-data (RWD) is important for\n             for estimating adjusted outcomes and this can be done by using methods such as, but not limited to, propensity score \n             matching, propensity score weighting and regression analysis. While these methods strive to statistically adjust for \n             confounding, the major challenge is in selecting the potential covariates that can bias the outcomes comparison estimates \n             in observational RWD (Real-World-Data). This is where the utility of automated covariate selection comes in. \n             The functions in this package help to implement the three major steps of automated covariate selection as described by\n             Schneeweiss et. al elsewhere. These three functions, in order of the steps required to execute automated covariate \n             selection are, get_candidate_covariates(), get_recurrence_covariates() and get_prioritised_covariates(). \n             In addition to these functions, a sample real-world-data from publicly available de-identified medical claims data is \n             also available for running examples and also for further exploration. The original article where the algorithm is described \n             by Schneeweiss et.al. (2009) <doi:10.1097/EDE.0b013e3181a663cc> .",
    "version": "1.0.0",
    "maintainer": "Dennis Robert <dennis.robert.nm@gmail.com>",
    "url": "https://github.com/technOslerphile/autoCovariateSelection",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8846,
    "package_name": "autoTS",
    "title": "Automatic Model Selection and Prediction for Univariate Time\nSeries",
    "description": "Offers a set of functions to easily make predictions for univariate time series. \n             'autoTS' is a wrapper of existing functions of the 'forecast' and 'prophet' packages, \n             harmonising their outputs in tidy dataframes and using default values for each.\n             The core function getBestModel() allows the user to effortlessly benchmark seven \n             algorithms along with a bagged estimator to identify which one performs the best \n             for a given time series.",
    "version": "0.9.11",
    "maintainer": "Vivien Roussez <vivien.roussez@gmail.com>",
    "url": "https://github.com/vivienroussez/autoTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8865,
    "package_name": "autoplotly",
    "title": "Automatic Generation of Interactive Visualizations for Statistical Results",
    "description": "Functionalities to automatically generate interactive visualizations for",
    "version": "0.1.4",
    "maintainer": "",
    "url": "https://github.com/terrytangyuan/autoplotly",
    "exports": [],
    "topics": ["data-visualization", "ggplot2", "interactive-visualizations", "machine-learning", "plotly", "plotlyjs", "rstats", "statistics"],
    "score": "NA",
    "stars": 92
  },
  {
    "id": 8869,
    "package_name": "autostsm",
    "title": "Automatic Structural Time Series Models",
    "description": "Automatic model selection for structural time series decomposition into trend, cycle, and seasonal components, plus optionality for structural interpolation, using the Kalman filter. \n  Koopman, Siem Jan and Marius Ooms (2012) \"Forecasting Economic Time Series Using Unobserved Components Time Series Models\" <doi:10.1093/oxfordhb/9780195398649.013.0006>.\n  Kim, Chang-Jin and Charles R. Nelson (1999) \"State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications\" <doi:10.7551/mitpress/6444.001.0001><http://econ.korea.ac.kr/~cjkim/>. ",
    "version": "3.1.5",
    "maintainer": "Alex Hubbard <hubbard.alex@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8874,
    "package_name": "autovi",
    "title": "Auto Visual Inference with Computer Vision Models",
    "description": "Provides automated visual inference of residual plots using computer vision models, facilitating diagnostic checks for classical normal linear regression models.",
    "version": "0.4.1",
    "maintainer": "Weihao Li <llreczx@gmail.com>",
    "url": "https://tengmcing.github.io/autovi/,\nhttps://github.com/TengMCing/autovi/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8888,
    "package_name": "awdb",
    "title": "Query the USDA NWCC Air and Water Database REST API",
    "description": "Query the four endpoints of the 'Air and Water Database (AWDB) REST\n    API' maintained by the National Water and Climate Center (NWCC) at the \n    United States Department of Agriculture (USDA). Endpoints include data, \n    forecast, reference-data, and metadata. The package is extremely light \n    weight, with 'Rust' via 'extendr' doing most of the heavy lifting to \n    deserialize and flatten deeply nested 'JSON' responses. The AWDB can be \n    found at <https://wcc.sc.egov.usda.gov/awdbRestApi/swagger-ui/index.html>.",
    "version": "0.1.3",
    "maintainer": "Kenneth Blake Vernon <kenneth.b.vernon@gmail.com>",
    "url": "https://github.com/kbvernon/awdb, https://kbvernon.github.io/awdb/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8913,
    "package_name": "bGWAS",
    "title": "Bayesian Genome-Wide Association Study",
    "description": "Package regrouping functions to perform Bayesian Genome-Wide Association Studies (bGWAS). See McDaid et al (2017) for more information about the method.",
    "version": "1.0.3",
    "maintainer": "Ninon Mounier <mounier.ninon@gmail.com>",
    "url": "https://github.com/n-mounier/bGWAS",
    "exports": [],
    "topics": ["bayesian", "gwas", "r-package", "statistical-genetics"],
    "score": "NA",
    "stars": 41
  },
  {
    "id": 8939,
    "package_name": "bacondecomp",
    "title": "Goodman-Bacon Decomposition",
    "description": "Decomposition for differences-in-differences with variation in\n    treatment timing from Goodman-Bacon (2018) <doi:10.3386/w25018>.",
    "version": "0.1.1",
    "maintainer": "Evan Flack <evanjflack@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8940,
    "package_name": "bacr",
    "title": "Bayesian Adjustment for Confounding",
    "description": "Estimating the average causal effect based on the Bayesian Adjustment for Confounding (BAC) algorithm.",
    "version": "1.0.1",
    "maintainer": "Chi Wang <chi.wang@uky.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8945,
    "package_name": "bage",
    "title": "Bayesian Estimation and Forecasting of Age-Specific Rates",
    "description": "Fast Bayesian estimation and forecasting of age-specific\n    rates, probabilities, and means, based on 'Template Model Builder'.",
    "version": "0.10.2",
    "maintainer": "John Bryant <john@bayesiandemography.com>",
    "url": "https://bayesiandemography.github.io/bage/,\nhttps://github.com/bayesiandemography/bage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8947,
    "package_name": "baggingbwsel",
    "title": "Bagging Bandwidth Selection in Kernel Density and Regression\nEstimation",
    "description": "Bagging bandwidth selection methods for the Parzen-Rosenblatt and Nadaraya-Watson estimators. These \n  bandwidth selectors can achieve greater statistical precision than their non-bagged counterparts while being \n    computationally fast. See Barreiro-Ures et al. (2020) <doi:10.1093/biomet/asaa092> and \n  Barreiro-Ures et al. (2021) <doi:10.48550/arXiv.2105.04134>.",
    "version": "1.1",
    "maintainer": "Ruben Fernandez-Casal <rubenfcasal@gmail.com>",
    "url": "https://rubenfcasal.github.io/baggingbwsel/,\nhttps://github.com/rubenfcasal/baggingbwsel/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8948,
    "package_name": "baggr",
    "title": "Bayesian Aggregate Treatment Effects",
    "description": "Running and comparing meta-analyses of data with hierarchical \n    Bayesian models in Stan, including convenience functions for formatting\n    data, plotting and pooling measures specific to meta-analysis. This implements many models\n    from Meager (2019) <doi:10.1257/app.20170299>.",
    "version": "0.7.11",
    "maintainer": "Witold Wiecek <witold.wiecek@gmail.com>",
    "url": "https://github.com/wwiecek/baggr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8952,
    "package_name": "bain",
    "title": "Bayes Factors for Informative Hypotheses",
    "description": "Computes approximated adjusted fractional Bayes factors for\n    equality, inequality, and about equality constrained hypotheses.\n    For a tutorial on this method, see Hoijtink, Mulder, van Lissa, & Gu,\n    (2019) <doi:10.1037/met0000201>. For applications in structural equation\n    modeling, see: Van Lissa, Gu, Mulder, Rosseel, Van Zundert, &\n    Hoijtink, (2021) <doi:10.1080/10705511.2020.1745644>. For the statistical\n    underpinnings, see Gu, Mulder, and Hoijtink (2018) <doi:10.1111/bmsp.12110>;\n    Hoijtink, Gu, & Mulder, J. (2019) <doi:10.1111/bmsp.12145>; Hoijtink, Gu, \n    Mulder, & Rosseel, (2019) <doi:10.31234/osf.io/q6h5w>.",
    "version": "0.2.11",
    "maintainer": "Caspar J van Lissa <c.j.vanlissa@tilburguniversity.edu>",
    "url": "https://informative-hypotheses.sites.uu.nl/software/bain/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8959,
    "package_name": "baldur",
    "title": "Bayesian Hierarchical Modeling for Label-Free Proteomics",
    "description": "Statistical decision in proteomics data using a hierarchical\n    Bayesian model. There are two regression models for describing the \n    mean-variance trend, a gamma regression or a latent gamma mixture\n    regression. The regression model is then used as an Empirical Bayes\n    estimator for the prior on the variance in a peptide. Further, it assumes\n    that each measurement has an uncertainty (increased variance) associated\n    with it that is also inferred. Finally, it tries to estimate the posterior\n    distribution (by Hamiltonian Monte Carlo) for the differences in means for\n    each peptide in the data. Once the posterior is inferred, it integrates the\n    tails to estimate the probability of error from which a statistical decision\n    can be made.\n    See Berg and Popescu for details (<doi:10.1016/j.mcpro.2023.100658>).",
    "version": "0.0.4",
    "maintainer": "Philip Berg <pberg@live.se>",
    "url": "https://github.com/PhilipBerg/baldur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8961,
    "package_name": "bama",
    "title": "High Dimensional Bayesian Mediation Analysis",
    "description": "Perform mediation analysis in the presence of high-dimensional\n    mediators based on the potential outcome framework. Bayesian Mediation\n    Analysis (BAMA), developed by Song et al (2019) <doi:10.1111/biom.13189> and\n    Song et al (2020) <doi:10.48550/arXiv.2009.11409>,\n    relies on two Bayesian sparse linear mixed models to simultaneously analyze\n    a relatively large number of mediators for a continuous exposure and outcome\n    assuming a small number of mediators are truly active. This sparsity\n    assumption also allows the extension of univariate mediator analysis by\n    casting the identification of active mediators as a variable selection\n    problem and applying Bayesian methods with continuous shrinkage priors on\n    the effects.",
    "version": "1.3.1",
    "maintainer": "Mike Kleinsasser <mkleinsa@umich.edu>",
    "url": "https://github.com/umich-cphds/bama",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8964,
    "package_name": "bamdit",
    "title": "Bayesian Meta-Analysis of Diagnostic Test Data",
    "description": "Provides a new class of Bayesian meta-analysis models that incorporates a model for internal and external validity bias. In this way, it is possible to combine studies of diverse quality and different types. For example, we can combine the results of randomized control trials (RCTs) with the results of observational studies (OS). ",
    "version": "3.4.4",
    "maintainer": "Pablo Emilio Verde <pabloemilio.verde@hhu.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8965,
    "package_name": "bamlss",
    "title": "Bayesian Additive Models for Location, Scale, and Shape (and\nBeyond)",
    "description": "Infrastructure for estimating probabilistic distributional regression models in a Bayesian framework.\n  The distribution parameters may capture location, scale, shape, etc. and every parameter may depend\n  on complex additive terms (fixed, random, smooth, spatial, etc.) similar to a generalized additive model.\n  The conceptual and computational framework is introduced in Umlauf, Klein, Zeileis (2019)\n  <doi:10.1080/10618600.2017.1407325> and the R package in Umlauf, Klein, Simon, Zeileis (2021)\n  <doi:10.18637/jss.v100.i04>.",
    "version": "1.2-5",
    "maintainer": "Nikolaus Umlauf <Nikolaus.Umlauf@uibk.ac.at>",
    "url": "http://www.bamlss.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8976,
    "package_name": "bandsfdp",
    "title": "Compute Upper Prediction Bounds on the FDP in Competition-Based\nSetups",
    "description": "Implements functions that calculate upper prediction \n  bounds on the false discovery proportion (FDP) in the list of discoveries \n  returned by competition-based setups, implementing Ebadi et al. (2022)\n  <arXiv:2302.11837>. Such setups include target-decoy competition (TDC) \n  in computational mass spectrometry and the knockoff construction in linear \n  regression (note this package typically uses the terminology of TDC). Included \n  is the standardized (TDC-SB) and uniform (TDC-UB) bound on TDC's FDP, and the \n  simultaneous standardized and uniform bands. Requires \n  pre-computed Monte Carlo statistics available at \n  <https://github.com/uni-Arya/fdpbandsdata>. This data can be downloaded by\n  running the command 'devtools::install_github(\"uni-Arya/fdpbandsdata\")' in R\n  and restarting R after installation. The size of this data is roughly 81Mb.",
    "version": "1.1.0",
    "maintainer": "Arya Ebadi <aeba3842@uni.sydney.edu.au>",
    "url": "https://github.com/uni-Arya/bandsfdp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8985,
    "package_name": "bapred",
    "title": "Batch Effect Removal and Addon Normalization (in Phenotype\nPrediction using Gene Data)",
    "description": "Various tools dealing with batch effects, in particular enabling the \n  removal of discrepancies between training and test sets in prediction scenarios.\n  Moreover, addon quantile normalization and addon RMA normalization (Kostka & Spang, \n  2008) is implemented to enable integrating the quantile normalization step into \n  prediction rules. The following batch effect removal methods are implemented: \n  FAbatch, ComBat, (f)SVA, mean-centering, standardization, Ratio-A and Ratio-G. \n  For each of these we provide  an additional function which enables a posteriori \n  ('addon') batch effect removal in independent batches ('test data'). Here, the\n  (already batch effect adjusted) training data is not altered. For evaluating the\n  success of batch effect adjustment several metrics are provided. Moreover, the \n  package implements a plot for the visualization of batch effects using principal \n  component analysis. The main functions of the package for batch effect adjustment \n  are ba() and baaddon() which enable batch effect removal and addon batch effect \n  removal, respectively, with one of the seven methods mentioned above. Another \n  important function here is bametric() which is a wrapper function for all implemented\n  methods for evaluating the success of batch effect removal. For (addon) quantile \n  normalization and (addon) RMA normalization the functions qunormtrain(), \n  qunormaddon(), rmatrain() and rmaaddon() can be used.",
    "version": "1.1",
    "maintainer": "Roman Hornung <hornung@ibe.med.uni-muenchen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8993,
    "package_name": "bartCause",
    "title": "Causal Inference using Bayesian Additive Regression Trees",
    "description": "Contains a variety of methods to generate typical causal inference estimates using Bayesian Additive Regression Trees (BART) as the underlying regression model (Hill (2012) <doi:10.1198/jcgs.2010.08162>).",
    "version": "1.0-10",
    "maintainer": "Vincent Dorie <vdorie@gmail.com>",
    "url": "https://github.com/vdorie/bartCause",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8994,
    "package_name": "bartMachine",
    "title": "Bayesian Additive Regression Trees",
    "description": "An advanced implementation of Bayesian Additive Regression Trees with expanded features for data analysis and visualization.",
    "version": "1.3.4.1",
    "maintainer": "Adam Kapelner <kapelner@qc.cuny.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8996,
    "package_name": "bartMan",
    "title": "Create Visualisations for BART Models",
    "description": "Investigating and visualising Bayesian Additive Regression Tree (BART) (Chipman, H. A., George, E. I., & McCulloch, R. E. 2010) <doi:10.1214/09-AOAS285> model fits.  We construct conventional plots to analyze a model’s performance and stability  as well as create new tree-based plots to analyze variable importance, interaction, and tree structure.  We employ Value Suppressing Uncertainty Palettes (VSUP) to construct heatmaps that display variable importance  and interactions jointly using colour scale to represent posterior uncertainty.  Our visualisations are designed to work with the most popular BART R packages available, namely 'BART' Rodney Sparapani and Charles Spanbauer and Robert McCulloch 2021 <doi:10.18637/jss.v097.i01>,  'dbarts'  (Vincent Dorie 2023) <https://CRAN.R-project.org/package=dbarts>,  and 'bartMachine' (Adam Kapelner and Justin Bleich 2016) <doi:10.18637/jss.v070.i04>.",
    "version": "0.2.1",
    "maintainer": "Alan Inglis <alan.inglis@mu.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8998,
    "package_name": "bartcs",
    "title": "Bayesian Additive Regression Trees for Confounder Selection",
    "description": "Fit Bayesian Regression Additive Trees (BART) models to\n    select true confounders from a large set of potential confounders and\n    to estimate average treatment effect. For more information, see Kim et\n    al. (2023) <doi:10.1111/biom.13833>.",
    "version": "1.3.0",
    "maintainer": "Yeonghoon Yoo <yooyh.stat@gmail.com>",
    "url": "https://github.com/yooyh/bartcs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 8999,
    "package_name": "basad",
    "title": "Bayesian Variable Selection with Shrinking and Diffusing Priors",
    "description": "Provides a Bayesian variable selection approach using continuous spike and slab prior distributions. The prior choices here are motivated by the shrinking and diffusing priors studied in Narisetty & He (2014) <DOI:10.1214/14-AOS1207>.",
    "version": "0.3.0",
    "maintainer": "Qingyan Xiang <qyxiang@bu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9015,
    "package_name": "bases",
    "title": "Basis Expansions for Regression Modeling",
    "description": "Provides various basis expansions for flexible regression modeling,\n    including random Fourier features (Rahimi & Recht, 2007)\n    <https://proceedings.neurips.cc/paper_files/paper/2007/file/013a006f03dbc5392effeb8f18fda755-Paper.pdf>,\n    exact kernel / Gaussian process feature maps, Bayesian Additive Regression\n    Trees (BART) (Chipman et al., 2010) <doi:10.1214/09-AOAS285> prior features,\n    and a helpful interface for n-way interactions. The provided functions may\n    be used within any modeling formula, allowing the use of kernel methods and\n    other basis expansions in modeling functions that do not otherwise support\n    them. Along with the basis expansions, a number of kernel functions are also\n    provided, which support kernel arithmetic to form new kernels. Basic ridge\n    regression functionality is included as well.",
    "version": "0.1.2",
    "maintainer": "Cory McCartan <mccartan@psu.edu>",
    "url": "https://corymccartan.com/bases/,\nhttps://github.com/CoryMcCartan/bases/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9018,
    "package_name": "basicMCMCplots",
    "title": "Trace Plots, Density Plots and Chain Comparisons for MCMC\nSamples",
    "description": "Provides methods for examining posterior MCMC samples\n    from a single chain using trace plots and density plots, and from\n    multiple chains by comparing posterior medians and credible intervals\n    from each chain.  These plotting functions have a variety of options,\n    such as figure sizes, legends, parameters to plot, and saving plots to file.\n    Functions interface with the NIMBLE software package, see\n    de Valpine, Turek, Paciorek, Anderson-Bergman, Temple Lang and Bodik (2017)\n    <doi:10.1080/10618600.2016.1172487>.",
    "version": "0.2.7",
    "maintainer": "Daniel Turek <danielturek@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9031,
    "package_name": "batchmix",
    "title": "Semi-Supervised Bayesian Mixture Models Incorporating Batch\nCorrection",
    "description": "Semi-supervised and unsupervised Bayesian mixture models that\n  simultaneously infer the cluster/class structure and a batch correction.\n  Densities available are the multivariate normal and the multivariate t.\n  The model sampler is implemented in C++. This package is aimed at analysis of\n  low-dimensional data generated across several batches. See Coleman et al.\n  (2022) <doi:10.1101/2022.01.14.476352> for details of the model.",
    "version": "2.2.1",
    "maintainer": "Stephen Coleman <stcolema@tcd.ie>",
    "url": "https://github.com/stcolema/batchmix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9039,
    "package_name": "bayMDS",
    "title": "Bayesian Multidimensional Scaling and Choice of Dimension",
    "description": "Bayesian approach to multidimensional scaling. The package consists of implementations of the methods of Oh and Raftery (2001)  <doi:10.1198/016214501753208690>. ",
    "version": "2.1",
    "maintainer": "Man-Suk Oh <msoh@ewha.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9042,
    "package_name": "bayclumpr",
    "title": "Bayesian Analysis of Clumped Isotope Datasets",
    "description": "Simulating synthetic clumped isotope dataset, fitting\n    linear regression models under Bayesian and non-Bayesian frameworks,\n    and generating temperature reconstructions for the same two approaches.\n    Please note that models implemented in this package are described\n    in Roman-Palacios et al. (2021) <doi:10.1002/essoar.10507995.1>.",
    "version": "0.1.0",
    "maintainer": "Cristian Roman Palacios <cromanpa94@arizona.edu>",
    "url": "https://bayclump.tripatilab.epss.ucla.edu/,\nhttps://tripati-lab.github.io/bayclumpr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9043,
    "package_name": "baycn",
    "title": "Bayesian Inference for Causal Networks",
    "description": "A Bayesian hybrid approach for inferring Directed Acyclic Graphs\n    (DAGs) for continuous, discrete, and mixed data. The algorithm can use the \n    graph inferred by another more efficient graph inference method as input;\n    the input graph may contain false edges or undirected edges but can help\n    reduce the search space to a more manageable size. A Bayesian Markov chain\n    Monte Carlo algorithm is then used to infer the probability of direction and\n    absence for the edges in the network.\n    References:\n    Martin and Fu (2019) <arXiv:1909.10678>.",
    "version": "1.2.0",
    "maintainer": "Evan A Martin <evanamartin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9044,
    "package_name": "bayefdr",
    "title": "Bayesian Estimation and Optimisation of Expected False Discovery\nRate",
    "description": "\n    Implements the Bayesian FDR control described by \n    Newton et al. (2004), <doi:10.1093/biostatistics/5.2.155>.\n    Allows optimisation and visualisation of expected error rates based on\n    tail posterior probability tests.\n    Based on code written by Catalina Vallejos for BASiCS, see\n    Beyond comparisons of means: understanding changes in gene expression at the\n    single-cell level Vallejos et al. (2016) <doi:10.1186/s13059-016-0930-3>.",
    "version": "0.2.1",
    "maintainer": "Alan O'Callaghan <alan.ocallaghan@outlook.com>",
    "url": "https://github.com/VallejosGroup/bayefdr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9045,
    "package_name": "bayes4psy",
    "title": "User Friendly Bayesian Data Analysis for Psychology",
    "description": "Contains several Bayesian models for data analysis of psychological tests. A user friendly interface for these models should enable students and researchers to perform professional level Bayesian data analysis without advanced knowledge in programming and Bayesian statistics. This package is based on the Stan platform (Carpenter et el. 2017 <doi:10.18637/jss.v076.i01>).",
    "version": "1.2.13",
    "maintainer": "Jure Demšar <jure.demsar@fri.uni-lj.si>",
    "url": "https://github.com/bstatcomp/bayes4psy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9048,
    "package_name": "bayesDccGarch",
    "title": "Methods and Tools for Bayesian Dynamic Conditional Correlation\nGARCH(1,1) Model",
    "description": "Bayesian estimation of dynamic conditional correlation GARCH model for multivariate time series volatility (Fioruci, J.A., Ehlers, R.S. and Andrade-Filho, M.G., (2014). <doi:10.1080/02664763.2013.839635>.",
    "version": "3.0.4",
    "maintainer": "Jose Augusto Fiorucci <jafiorucci@gmail.com>",
    "url": "https://ui.adsabs.harvard.edu/abs/2014arXiv1412.2967F/abstract",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9051,
    "package_name": "bayesImageS",
    "title": "Bayesian Methods for Image Segmentation using a Potts Model",
    "description": "Various algorithms for segmentation of 2D and 3D images, such\n    as computed tomography and satellite remote sensing. This package implements\n    Bayesian image analysis using the hidden Potts model with external field\n    prior of Moores et al. (2015) <doi:10.1016/j.csda.2014.12.001>.\n    Latent labels are sampled using chequerboard updating or Swendsen-Wang.\n    Algorithms for the smoothing parameter include pseudolikelihood, path sampling,\n    the exchange algorithm, approximate Bayesian computation (ABC-MCMC and ABC-SMC),\n    and the parametric functional approximate Bayesian (PFAB) algorithm. Refer to\n    Moores, Pettitt & Mengersen (2020) <doi:10.1007/978-3-030-42553-1_6> for an overview\n    and also to <doi:10.1007/s11222-014-9525-6> and <doi:10.1214/18-BA1130> for\n    further details of specific algorithms.",
    "version": "0.7-0",
    "maintainer": "Matt Moores <mmoores@gmail.com>",
    "url": "https://bitbucket.org/Azeari/bayesimages,\nhttps://mooresm.github.io/bayesImageS/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9052,
    "package_name": "bayesLife",
    "title": "Bayesian Projection of Life Expectancy",
    "description": "Making probabilistic projections of life expectancy for all countries of the world, using a Bayesian hierarchical model <doi:10.1007/s13524-012-0193-x>. Subnational projections are also supported.",
    "version": "5.3-1",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "https://bayespop.csss.washington.edu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9053,
    "package_name": "bayesMRM",
    "title": "Bayesian Multivariate Receptor Modeling",
    "description": "Bayesian analysis of multivariate receptor modeling. The package consists of implementations of the methods of Park and Oh (2015) <doi:10.1016/j.chemolab.2015.08.021>.The package uses 'JAGS'(Just Another Gibbs Sampler) to generate Markov chain Monte Carlo samples of parameters. ",
    "version": "2.4.0",
    "maintainer": "Man-Suk Oh <msoh@ewha.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9054,
    "package_name": "bayesMeanScale",
    "title": "Bayesian Post-Estimation on the Mean Scale",
    "description": "Computes Bayesian posterior distributions of predictions, marginal effects, and differences of marginal effects for various generalized linear models. Importantly, the posteriors are on the mean (response) scale, allowing for more natural interpretation than summaries on the link scale. Also, predictions and marginal effects of the count probabilities for Poisson and negative binomial models can be computed. ",
    "version": "0.2.1",
    "maintainer": "David M. Dalenberg <dalenbe2@gmail.com>",
    "url": "https://github.com/dalenbe2/bayesMeanScale",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9055,
    "package_name": "bayesMig",
    "title": "Bayesian Projection of Migration",
    "description": "Producing probabilistic projections of net migration rate for all countries of the world\n    or for subnational units using a Bayesian hierarchical model by Azose an Raftery (2015) <doi:10.1007/s13524-015-0415-0>.",
    "version": "1.0-0",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "http://bayespop.csss.washington.edu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9056,
    "package_name": "bayesPO",
    "title": "Bayesian Inference for Presence-Only Data",
    "description": "Presence-Only data is best modelled with a Point Process Model.\n    The work of Moreira and Gamerman (2022) <doi:10.1214/21-AOAS1569> provides\n    a way to use exact Bayesian inference to model this type of data, which is\n    implemented in this package.",
    "version": "0.5.0",
    "maintainer": "Guido Alberti Moreira <guidoalber@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9058,
    "package_name": "bayesQR",
    "title": "Bayesian Quantile Regression",
    "description": "Bayesian quantile regression using the asymmetric Laplace distribution, both continuous as well as binary dependent variables are supported. The package consists of implementations of the methods of Yu & Moyeed (2001) <doi:10.1016/S0167-7152(01)00124-9>, Benoit & Van den Poel (2012) <doi:10.1002/jae.1216> and Al-Hamzawi, Yu & Benoit (2012) <doi:10.1177/1471082X1101200304>. To speed up the calculations, the Markov Chain Monte Carlo core of all algorithms is programmed in Fortran and called from R.",
    "version": "2.4",
    "maintainer": "Dries F. Benoit <Dries.Benoit@UGent.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9059,
    "package_name": "bayesQRsurvey",
    "title": "Bayesian Quantile Regression Models for Complex Survey Data\nAnalysis",
    "description": "Provides Bayesian quantile regression models for complex survey data \n    under informative sampling using survey-weighted estimators. Both single- and \n    multiple-output models are supported. To accelerate computation, all algorithms \n    are implemented in 'C++' using 'Rcpp', 'RcppArmadillo', and 'RcppEigen', and \n    are called from 'R'. See Nascimento and Gonçalves (2024) <doi:10.1093/jssam/smae015> \n    and Nascimento and Gonçalves (2025, in press) <https://academic.oup.com/jssam>.",
    "version": "0.1.4",
    "maintainer": "Tomás Rodríguez Taborda <torodriguezt@unal.edu.co>",
    "url": "https://github.com/torodriguezt/bayesQRsurvey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9060,
    "package_name": "bayesROE",
    "title": "Bayesian Regions of Evidence",
    "description": "Computation and visualization of Bayesian Regions of Evidence\n    to systematically evaluate the sensitivity of a superiority or\n    non-inferiority claim against any prior assumption of its assessors.\n    Methodological details are elaborated by Hoefler and Miller\n    (<https://osf.io/jxnsv>).  Besides generic functions, the package also\n    provides an intuitive 'Shiny' application, that can be run in local R\n    environments.",
    "version": "0.2",
    "maintainer": "Robert Miller <robert.miller@tu-dresden.de>",
    "url": "https://github.com/waidschrat/bayesROE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9061,
    "package_name": "bayesRecon",
    "title": "Probabilistic Reconciliation via Conditioning",
    "description": "Provides methods for probabilistic reconciliation of hierarchical forecasts of time series. \n             The available methods include analytical Gaussian reconciliation (Corani et al., 2021) \n             <doi:10.1007/978-3-030-67664-3_13>, \n             MCMC reconciliation of count time series (Corani et al., 2024) \n             <doi:10.1016/j.ijforecast.2023.04.003>, \n             Bottom-Up Importance Sampling (Zambon et al., 2024) \n             <doi:10.1007/s11222-023-10343-y>,\n             methods for the reconciliation of mixed hierarchies (Mix-Cond and TD-cond)\n             (Zambon et al., 2024) <https://proceedings.mlr.press/v244/zambon24a.html>. ",
    "version": "0.3.3",
    "maintainer": "Dario Azzimonti <dario.azzimonti@gmail.com>",
    "url": "https://github.com/IDSIA/bayesRecon,\nhttps://idsia.github.io/bayesRecon/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9063,
    "package_name": "bayesSurv",
    "title": "Bayesian Survival Regression with Flexible Error and Random\nEffects Distributions",
    "description": "Contains Bayesian implementations of the Mixed-Effects Accelerated Failure Time (MEAFT) models\n             for censored data. Those can be not only right-censored but also interval-censored,\n\t     doubly-interval-censored or misclassified interval-censored. The methods implemented in the package\n\t     have been published in Komárek and Lesaffre (2006, Stat. Modelling) <doi:10.1191/1471082X06st107oa>,\n\t     Komárek, Lesaffre and Legrand (2007, Stat. in Medicine) <doi:10.1002/sim.3083>, Komárek and Lesaffre (2007, Stat. Sinica) <https://www3.stat.sinica.edu.tw/statistica/oldpdf/A17n27.pdf>, Komárek and Lesaffre (2008, JASA) <doi:10.1198/016214507000000563>,\n\t     García-Zattera, Jara and Komárek (2016, Biometrics) <doi:10.1111/biom.12424>.\t     ",
    "version": "3.8",
    "maintainer": "Arnošt Komárek <arnost.komarek@mff.cuni.cz>",
    "url": "https://msekce.karlin.mff.cuni.cz/~komarek/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9064,
    "package_name": "bayesTFR",
    "title": "Bayesian Fertility Projection",
    "description": "Making probabilistic projections of total fertility rate for all countries of the world, using a Bayesian hierarchical model <doi:10.1007/s13524-011-0040-5> <doi:10.18637/jss.v106.i08>. Subnational probabilistic projections are also supported <doi:10.4054/DemRes.2018.38.60>.",
    "version": "7.4-4",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "https://bayespop.csss.washington.edu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9065,
    "package_name": "bayesWatch",
    "title": "Bayesian Change-Point Detection for Process Monitoring with\nFault Detection",
    "description": "Bayes Watch fits an array of Gaussian Graphical Mixture Models to groupings of homogeneous data in time, called regimes, which are modeled as the observed states of a Markov process with unknown transition probabilities.  In doing so, Bayes Watch defines a posterior distribution on a vector of regime assignments, which gives meaningful expressions on the probability of every possible change-point.  Bayes Watch also allows for an effective and efficient fault detection system that assesses what features in the data where the most responsible for a given change-point.  For further details, see: Alexander C. Murph et al. (2023) <doi:10.48550/arXiv.2310.02940>.",
    "version": "0.1.4",
    "maintainer": "Alexander C. Murph <murph290@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9066,
    "package_name": "bayesZIB",
    "title": "Bayesian Zero-Inflated Bernoulli Regression Model",
    "description": "Fits a Bayesian zero-inflated Bernoulli regression model handling (potentially) different covariates for the zero-inflated\n    and non zero-inflated parts. See Moriña D, Puig P, Navarro A. (2021) <doi:10.1186/s12874-021-01427-2>.",
    "version": "0.0.5",
    "maintainer": "David Moriña Soler <dmorina@ub.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9067,
    "package_name": "bayesammi",
    "title": "Bayesian Estimation of the Additive Main Effects and\nMultiplicative Interaction Model",
    "description": "Performs Bayesian estimation of the additive main effects and multiplicative interaction (AMMI) model. The method is explained in Crossa, J., Perez-Elizalde, S., Jarquin, D., Cotes, J.M., Viele, K., Liu, G. and Cornelius, P.L. (2011) (<doi:10.2135/cropsci2010.06.0343>).",
    "version": "0.3.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9068,
    "package_name": "bayesanova",
    "title": "Bayesian Inference in the Analysis of Variance via Markov Chain\nMonte Carlo in Gaussian Mixture Models",
    "description": "Provides a Bayesian version of the analysis of variance based on a three-component Gaussian mixture for which a Gibbs sampler produces posterior draws. For details about the Bayesian ANOVA based on Gaussian mixtures, see Kelter (2019) <arXiv:1906.07524>.",
    "version": "1.6",
    "maintainer": "Riko Kelter <riko_k@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9069,
    "package_name": "bayesbio",
    "title": "Miscellaneous Functions for Bioinformatics and Bayesian\nStatistics",
    "description": "A hodgepodge of hopefully helpful functions. Two of these perform\n    shrinkage estimation: one using a simple weighted method where the user can\n    specify the degree of shrinkage required, and one using James-Stein shrinkage\n    estimation for the case of unequal variances.",
    "version": "1.0.0",
    "maintainer": "Andrew McKenzie <amckenz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9070,
    "package_name": "bayesboot",
    "title": "An Implementation of Rubin's (1981) Bayesian Bootstrap",
    "description": "Functions for performing the Bayesian bootstrap as introduced by\n    Rubin (1981) <doi:10.1214/aos/1176345338> and for summarizing the result.\n    The implementation can handle both summary statistics that works on a\n    weighted version of the data and summary statistics that works on a\n    resampled data set.",
    "version": "0.2.3",
    "maintainer": "Rasmus Bååth <rasmus.baath@gmail.com>",
    "url": "https://github.com/rasmusab/bayesboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9071,
    "package_name": "bayescount",
    "title": "Power Calculations and Bayesian Analysis of Count Distributions\nand FECRT Data using MCMC",
    "description": "A set of functions to allow analysis of count data (such\n        as faecal egg count data) using Bayesian MCMC methods.  Returns\n        information on the possible values for mean count, coefficient\n        of variation and zero inflation (true prevalence) present in\n        the data.  A complete faecal egg count reduction test (FECRT)\n        model is implemented, which returns inference on the true\n        efficacy of the drug from the pre- and post-treatment data\n        provided, using non-parametric bootstrapping as well as using\n        Bayesian MCMC.  Functions to perform power analyses for faecal\n        egg counts (including FECRT) are also provided.",
    "version": "0.9.99-9",
    "maintainer": "Matthew Denwood <md@sund.ku.dk>",
    "url": "https://bayescount.sourceforge.net",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9072,
    "package_name": "bayesdfa",
    "title": "Bayesian Dynamic Factor Analysis (DFA) with 'Stan'",
    "description": "Implements Bayesian dynamic factor analysis with 'Stan'. Dynamic \n    factor analysis is a dimension reduction tool for multivariate time series.\n    'bayesdfa' extends conventional dynamic factor models in several ways. \n    First, extreme events may be estimated in the latent trend by modeling\n    process error with a student-t distribution. Second, alternative constraints\n    (including proportions are allowed). Third, the estimated\n    dynamic factors can be analyzed with hidden Markov models to evaluate\n    support for latent regimes.",
    "version": "1.3.4",
    "maintainer": "Eric J. Ward <eric.ward@noaa.gov>",
    "url": "https://fate-ewi.github.io/bayesdfa/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9073,
    "package_name": "bayesdistreg",
    "title": "Bayesian Distribution Regression",
    "description": "Implements Bayesian Distribution Regression methods. This package contains functions for three estimators (non-asymptotic, semi-asymptotic and asymptotic) and related routines for Bayesian Distribution Regression in Huang and Tsyawo (2018) <doi:10.2139/ssrn.3048658> which is also the recommended reference to cite for this package. The functions can be grouped into three (3) categories. The first computes the logit likelihood function and posterior densities under uniform and normal priors. The second contains Independence and Random Walk Metropolis-Hastings Markov Chain Monte Carlo (MCMC) algorithms as functions and the third category of functions are useful for semi-asymptotic and asymptotic Bayesian distribution regression inference.",
    "version": "0.1.0",
    "maintainer": "Emmanuel Tsyawo <estsyawo@temple.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9074,
    "package_name": "bayesestdft",
    "title": "Estimating the Degrees of Freedom of the Student's\nt-Distribution under a Bayesian Framework",
    "description": "A Bayesian framework to estimate the Student's t-distribution's degrees of freedom is developed. Markov Chain Monte Carlo sampling routines are developed as in <doi:10.3390/axioms11090462> to sample from the posterior distribution of the degrees of freedom. A random walk Metropolis algorithm is used for sampling when Jeffrey's and Gamma priors are endowed upon the degrees of freedom. In addition, the Metropolis-adjusted Langevin algorithm for sampling is used under the Jeffrey's prior specification. The Log-normal prior over the degrees of freedom is posed as a viable choice with comparable performance in simulations and real-data application, against other prior choices, where an Elliptical Slice Sampler is used to sample from the concerned posterior.",
    "version": "1.0.0",
    "maintainer": "Somjit Roy <sroy_123@tamu.edu>",
    "url": "https://github.com/Roy-SR-007/bayesestdft",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9075,
    "package_name": "bayesforecast",
    "title": "Bayesian Time Series Modeling with Stan",
    "description": "Fit Bayesian time series models using 'Stan' for full Bayesian inference. A wide range \n  of distributions and models are supported, allowing users to fit Seasonal ARIMA, ARIMAX, Dynamic \n  Harmonic Regression, GARCH, t-student innovation GARCH models, asymmetric GARCH, Random Walks, stochastic \n  volatility models for univariate time series. Prior specifications are flexible and explicitly encourage \n  users to apply prior distributions that actually reflect their beliefs. Model fit can easily be assessed \n  and compared with typical visualization methods, information criteria such as loglik, AIC, BIC WAIC, Bayes \n  factor and leave-one-out cross-validation methods. References: Hyndman (2017)\n    <doi:10.18637/jss.v027.i03>; Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>.",
    "version": "1.0.5",
    "maintainer": "Asael Alonzo Matamoros <asael.alonzo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9077,
    "package_name": "bayesianOU",
    "title": "Bayesian Nonlinear Ornstein-Uhlenbeck Models with Stochastic\nVolatility",
    "description": "Fits Bayesian nonlinear Ornstein-Uhlenbeck models with cubic drift,\n    stochastic volatility, and Student-t innovations. The package implements\n    hierarchical priors for sector-specific parameters and supports parallel\n    MCMC sampling via 'Stan'. Model comparison is performed using Pareto\n    Smoothed Importance Sampling Leave-One-Out (PSIS-LOO) cross-validation\n    following Vehtari, Gelman, and Gabry (2017)\n    <doi:10.1007/s11222-016-9696-4>. Prior specifications follow recommendations\n    from Gelman (2006) <doi:10.1214/06-BA117A> for scale parameters.",
    "version": "0.1.3",
    "maintainer": "José Mauricio Gómez Julián <isadore.nabi@pm.me>",
    "url": "https://github.com/isadorenabi/bayesianOU",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9079,
    "package_name": "bayeslist",
    "title": "Bayesian Analysis of List Experiments with Prior Information",
    "description": "Estimates Bayesian models of list experiments with informative priors. It includes functionalities to estimate different types of list experiment models with varying prior information. See Lu and Traunmüller (2021) <doi:10.2139/ssrn.3871089> for examples and details of estimation.",
    "version": "0.0.1.5",
    "maintainer": "Xiao Lu <xiao.lu.research@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9080,
    "package_name": "bayeslongitudinal",
    "title": "Adjust Longitudinal Regression Models Using Bayesian Methodology",
    "description": "Adjusts longitudinal regression models using Bayesian methodology \n            for covariance structures of composite symmetry (SC), \n            autoregressive ones of order 1 AR (1) and \n            autoregressive moving average of order (1,1) ARMA (1,1).",
    "version": "0.1.0",
    "maintainer": "Edwin Javier Castillo Carreño <edjcastilloca@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9081,
    "package_name": "bayesm",
    "title": "Bayesian Inference for Marketing/Micro-Econometrics",
    "description": "Covers many important models used\n  in marketing and micro-econometrics applications. \n  The package includes:\n  Bayes Regression (univariate or multivariate dep var),\n  Bayes Seemingly Unrelated Regression (SUR),\n  Binary and Ordinal Probit,\n  Multinomial Logit (MNL) and Multinomial Probit (MNP),\n  Multivariate Probit,\n  Negative Binomial (Poisson) Regression,\n  Multivariate Mixtures of Normals (including clustering),\n  Dirichlet Process Prior Density Estimation with normal base,\n  Hierarchical Linear Models with normal prior and covariates,\n  Hierarchical Linear Models with a mixture of normals prior and covariates,\n  Hierarchical Multinomial Logits with a mixture of normals prior\n     and covariates,\n  Hierarchical Multinomial Logits with a Dirichlet Process prior and covariates,\n  Hierarchical Negative Binomial Regression Models,\n  Bayesian analysis of choice-based conjoint data,\n  Bayesian treatment of linear instrumental variables models,\n  Analysis of Multivariate Ordinal survey data with scale\n     usage heterogeneity (as in Rossi et al, JASA (01)),\n  Bayesian Analysis of Aggregate Random Coefficient Logit Models as in BLP (see\n  Jiang, Manchanda, Rossi 2009)\n  For further reference, consult our book, Bayesian Statistics and\n  Marketing by Rossi, Allenby and McCulloch (Wiley second edition 2024) and Bayesian Non- and Semi-Parametric\n  Methods and Applications (Princeton U Press 2014).",
    "version": "3.1-7",
    "maintainer": "Peter Rossi <perossichi@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9082,
    "package_name": "bayesmeta",
    "title": "Bayesian Random-Effects Meta-Analysis and Meta-Regression",
    "description": "A collection of functions allowing to derive the posterior distribution of the model parameters in random-effects meta-analysis or meta-regression, and providing functionality to evaluate joint and marginal posterior probability distributions, predictive distributions, shrinkage effects, posterior predictive p-values, etc.; For more details, see also Roever C (2020) <doi:10.18637/jss.v093.i06>, or Roever C and Friede T (2022) <doi:10.1016/j.cmpb.2022.107303>.",
    "version": "3.5",
    "maintainer": "Christian Roever <christian.roever@med.uni-goettingen.de>",
    "url": "https://gitlab.gwdg.de/croever/bayesmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9083,
    "package_name": "bayesmix",
    "title": "Bayesian Mixture Models with JAGS",
    "description": "Fits finite mixture models of univariate Gaussian distributions using JAGS within a Bayesian framework.",
    "version": "0.7-6",
    "maintainer": "Bettina Gruen <Bettina.Gruen@R-project.org>",
    "url": "https://statmath.wu.ac.at/~gruen/BayesMix/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9084,
    "package_name": "bayesmlogit",
    "title": "A Multistate Life Table (MSLT) Methodology Based on Bayesian\nApproach",
    "description": "Create life tables with a Bayesian approach, which can be very useful for modelling a complex health process when considering multiple predisposing factors and multiple coexisting health conditions. Details for this method can be found in: Lynch, Scott, et al., (2022) <doi:10.1177/00811750221112398>; Zang, Emma, et al., (2022) <doi:10.1093/geronb/gbab149>.",
    "version": "1.0.1",
    "maintainer": "Xuezhixing Zhang <xuezhixing.zhang@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9085,
    "package_name": "bayesmodels",
    "title": "The 'Tidymodels' Extension for Bayesian Models",
    "description": "Bayesian framework for use with the 'tidymodels' ecosystem. Includes the following models: Sarima, Garch,",
    "version": "0.1.1",
    "maintainer": "",
    "url": "https://github.com/AlbertoAlmuinha/bayesmodels",
    "exports": [],
    "topics": ["bayesian", "forecasting", "modeltime", "parsnip", "tidymodels", "time-series"],
    "score": "NA",
    "stars": 55
  },
  {
    "id": 9086,
    "package_name": "bayesmove",
    "title": "Non-Parametric Bayesian Analyses of Animal Movement",
    "description": "Methods for assessing animal movement from telemetry and biologging\n    data using non-parametric Bayesian methods. This includes features for pre-\n    processing and analysis of data, as well as the visualization of results\n    from the models. This framework does not rely on standard parametric density\n    functions, which provides flexibility during model fitting. Further details \n    regarding part of this framework can be found in Cullen et al. (2022) <doi:10.1111/2041-210X.13745>.",
    "version": "0.2.4",
    "maintainer": "Joshua Cullen <joshcullen10@gmail.com>",
    "url": "https://github.com/joshcullen/bayesmove,\nhttps://joshcullen.github.io/bayesmove/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9088,
    "package_name": "bayesnec",
    "title": "A Bayesian No-Effect- Concentration (NEC) Algorithm",
    "description": "Implementation of No-Effect-Concentration estimation that uses 'brms' (see Burkner (2017)<doi:10.18637/jss.v080.i01>; Burkner (2018)<doi:10.32614/RJ-2018-017>; Carpenter 'et al.' (2017)<doi:10.18637/jss.v076.i01> to fit concentration(dose)-response data using Bayesian methods for the purpose of estimating 'ECx' values, but more particularly 'NEC' (see Fox (2010)<doi:10.1016/j.ecoenv.2009.09.012>), 'NSEC' (see Fisher and Fox (2023)<doi:10.1002/etc.5610>), and 'N(S)EC (see Fisher et al. 2023<doi:10.1002/ieam.4809>). A full description of this package can be found in Fisher 'et al.' (2024)<doi:10.18637/jss.v110.i05>. This package expands and supersedes an original version implemented in 'R2jags' (see Su and Yajima (2020)<https://CRAN.R-project.org/package=R2jags>; Fisher et al. (2020)<doi:10.5281/ZENODO.3966864>).",
    "version": "2.1.3.1",
    "maintainer": "Rebecca Fisher <r.fisher@aims.gov.au>",
    "url": "https://open-aims.github.io/bayesnec/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9089,
    "package_name": "bayespin",
    "title": "An R package for Bayesian estimation of the probability of informed",
    "description": "An R package for Bayesian estimation of the probability of informed",
    "version": "1.0",
    "maintainer": "Lars Simon Zehnder <simon.zehnder@neway.ai>",
    "url": "https://github.com/simonsays1980/bayespin",
    "exports": [],
    "topics": ["bayesian", "finite-mixture-models", "market-microstructure", "pin"],
    "score": "NA",
    "stars": 5
  },
  {
    "id": 9092,
    "package_name": "bayespm",
    "title": "Bayesian Statistical Process Monitoring",
    "description": "The R-package bayespm implements Bayesian Statistical Process Control and Monitoring (SPC/M) methodology. These methods utilize available prior information and/or historical data, providing efficient online quality monitoring of a process, in terms of identifying moderate/large transient shifts (i.e., outliers) or persistent shifts of medium/small size in the process. These self-starting, sequentially updated tools can also run under complete absence of any prior information. The Predictive Control Charts (PCC) are introduced for the quality monitoring of data from any discrete or continuous distribution that is a member of the regular exponential family. The Predictive Ratio CUSUMs (PRC) are introduced for the Binomial, Poisson and Normal data (a later version of the library will cover all the remaining distributions from the regular exponential family). The PCC targets transient process shifts of typically large size (a.k.a. outliers), while PRC is focused in detecting persistent (structural) shifts that might be of medium or even small size. Apart from monitoring, both PCC and PRC provide the sequentially updated posterior inference for the monitored parameter. Bourazas K., Kiagias D. and Tsiamyrtzis P. (2022) \"Predictive Control Charts (PCC): A Bayesian approach in online monitoring of short runs\" <doi:10.1080/00224065.2021.1916413>, Bourazas K., Sobas F. and Tsiamyrtzis, P. 2023. \"Predictive ratio CUSUM (PRC): A Bayesian approach in online change point detection of short runs\" <doi:10.1080/00224065.2022.2161434>, Bourazas K., Sobas F. and Tsiamyrtzis, P. 2023. \"Design and properties of the predictive ratio cusum (PRC) control charts\" <doi:10.1080/00224065.2022.2161435>. ",
    "version": "0.2.0",
    "maintainer": "Dimitrios Kiagias <kiagias.dim@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9094,
    "package_name": "bayesrules",
    "title": "Datasets and Supplemental Functions from Bayes Rules! Book",
    "description": "Provides datasets and functions used for analysis \n  and visualizations in the Bayes Rules! book (<https://www.bayesrulesbook.com>). \n  The package contains a set of functions that summarize and plot Bayesian models from some conjugate families \n  and another set of functions for evaluation of some Bayesian models.",
    "version": "0.0.2",
    "maintainer": "Mine Dogucu <mdogucu@gmail.com>",
    "url": "https://bayes-rules.github.io/bayesrules/docs/,\nhttps://github.com/bayes-rules/bayesrules/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9095,
    "package_name": "bayess",
    "title": "Bayesian Essentials with R",
    "description": "Allows the reenactment of the R programs used in \n        the book Bayesian Essentials with R without further programming. \n        R code being available as well, they can be modified by the user\n        to conduct one's own simulations. \n\t    Marin J.-M. and Robert C. P. (2014) <doi:10.1007/978-1-4614-8687-9>.",
    "version": "1.6",
    "maintainer": "Jean-Michel Marin <jean-michel.marin@umontpellier.fr>",
    "url": "https://www.r-project.org, https://github.com/jmm34/bayess",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9096,
    "package_name": "bayest",
    "title": "Effect Size Targeted Bayesian Two-Sample t-Tests via Markov\nChain Monte Carlo in Gaussian Mixture Models",
    "description": "Provides an Markov-Chain-Monte-Carlo algorithm for Bayesian t-tests on the effect size. The underlying Gibbs sampler is based on a two-component Gaussian mixture and approximates the posterior distributions of the effect size, the difference of means and difference of standard deviations. A posterior analysis of the effect size via the region of practical equivalence is provided, too. For more details about the Gibbs sampler see Kelter (2019) <arXiv:1906.07524>.",
    "version": "1.5",
    "maintainer": "Riko Kelter <riko.kelter@uni-siegen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9100,
    "package_name": "bayfoxr",
    "title": "Global Bayesian Foraminifera Core Top Calibration",
    "description": "A Bayesian, global planktic foraminifera core top calibration to \n    modern sea-surface temperatures. Includes four calibration models, \n    considering species-specific calibration parameters and seasonality.",
    "version": "0.0.1",
    "maintainer": "Steven Malevich <malevich@email.arizona.edu>",
    "url": "https://github.com/brews/bayfoxr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9101,
    "package_name": "baygel",
    "title": "Bayesian Shrinkage Estimators for Precision Matrices in Gaussian\nGraphical Models",
    "description": "This R package offers block Gibbs samplers for the Bayesian (adaptive) graphical lasso, ridge, and naive elastic net priors. These samplers facilitate the simulation of the posterior distribution of precision matrices for Gaussian distributed data and were originally proposed by: Wang (2012) <doi:10.1214/12-BA729>; Smith et al. (2022) <doi:10.48550/arXiv.2210.16290> and Smith et al. (2023) <doi:10.48550/arXiv.2306.14199>, respectively.",
    "version": "0.3.0",
    "maintainer": "Jarod Smith <jarodsmith706@gmail.com>",
    "url": "https://github.com/Jarod-Smithy/baygel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9102,
    "package_name": "baymedr",
    "title": "Computation of Bayes Factors for Common Biomedical Designs",
    "description": "BAYesian inference for MEDical designs in R. Functions for the \n    computation of Bayes factors for common biomedical research designs. \n    Implemented are functions to test the equivalence (equiv_bf), \n    non-inferiority (infer_bf), and superiority (super_bf) of an experimental \n    group compared to a control group on a continuous outcome measure. Bayes \n    factors for these three tests can be computed based on raw data (x, y) or \n    summary statistics (n_x, n_y, mean_x, mean_y, sd_x, sd_y [or ci_margin \n    and ci_level]).",
    "version": "0.1.1",
    "maintainer": "Maximilian Linde <maximilian.linde.92@gmail.com>",
    "url": "https://github.com/maxlinde/baymedr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9103,
    "package_name": "baystability",
    "title": "Bayesian Stability Analysis of Genotype by Environment\nInteraction (GEI)",
    "description": "Performs general Bayesian estimation method of linear–bilinear models for genotype × environment interaction. The method is explained in Perez-Elizalde, S., Jarquin, D., and Crossa, J. (2011) (<doi:10.1007/s13253-011-0063-9>).",
    "version": "0.2.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9106,
    "package_name": "bbk",
    "title": "Client for Central Bank APIs",
    "description": "A client for retrieving data and metadata from major central\n    bank APIs. It supports access to the 'Bundesbank SDMX Web Service API'\n    (<https://www.bundesbank.de/en/statistics/time-series-databases/help-for-sdmx-web-service/web-service-interface-data>),\n    the 'Swiss National Bank Data Portal' (<https://data.snb.ch/en>), the\n    'European Central Bank Data Portal API'\n    (<https://data.ecb.europa.eu/help/api/overview>), the 'Bank of England\n    Interactive Statistical Database'\n    (<https://www.bankofengland.co.uk/boeapps/database>), the 'Banco de\n    España API'\n    (<https://www.bde.es/webbe/en/estadisticas/recursos/api-estadisticas-bde.html>),\n    the 'Banque de France Web Service'\n    (<https://webstat.banque-france.fr/en/pages/guide-migration-api/>),\n    and 'Bank of Canada Valet API'\n    (<https://www.bankofcanada.ca/valet/docs>).",
    "version": "0.8.0",
    "maintainer": "Maximilian Mücke <muecke.maximilian@gmail.com>",
    "url": "https://m-muecke.github.io/bbk/, https://github.com/m-muecke/bbk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9111,
    "package_name": "bbnet",
    "title": "Create Simple Predictive Models on Bayesian Belief Networks",
    "description": "A system to build, visualise and evaluate Bayesian belief networks. The methods are described in Stafford et al. (2015) <doi:10.12688/f1000research.5981.1>.",
    "version": "1.2.1",
    "maintainer": "Victoria Dominguez Almela <vda1r22@soton.ac.uk>",
    "url": "https://github.com/vda1r22/bbnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9116,
    "package_name": "bbreg",
    "title": "Bessel and Beta Regressions via Expectation-Maximization\nAlgorithm for Continuous Bounded Data",
    "description": "Functions to fit, via Expectation-Maximization (EM) algorithm, the Bessel and Beta regressions to a data set with a bounded continuous response variable. The Bessel regression is a new and robust approach proposed in the literature. The EM version for the well known Beta regression is another major contribution of this package. See details in the references Barreto-Souza, Mayrink and Simas (2022) <doi:10.1111/anzs.12354> and Barreto-Souza, Mayrink and Simas (2020) <arXiv:2003.05157>.  ",
    "version": "2.0.2",
    "maintainer": "Vinicius Mayrink <vdinizm@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9129,
    "package_name": "bcf",
    "title": "Causal Inference using Bayesian Causal Forests",
    "description": "Causal inference for a binary treatment and continuous outcome using Bayesian Causal Forests. See Hahn, Murray and Carvalho (2020) <doi:10.1214/19-BA1195> for additional information. This implementation relies on code originally accompanying Pratola et. al. (2013) <arXiv:1309.1906>.",
    "version": "2.0.2",
    "maintainer": "Jared S. Murray <jared.murray@mccombs.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9134,
    "package_name": "bcpa",
    "title": "Behavioral Change Point Analysis of Animal Movement",
    "description": "The Behavioral Change Point Analysis (BCPA) is a method of\n    identifying hidden shifts in the underlying parameters of a time series,\n    developed specifically to be applied to animal movement data which is\n    irregularly sampled.  The method is based on: E. Gurarie, R. Andrews and \n    K. Laidre A novel method for identifying behavioural changes in animal \n    movement data (2009) Ecology Letters 12:5 395-408. A development version is \n    on <https://github.com/EliGurarie/bcpa>. NOTE: the BCPA method may be useful \n    for any univariate, irregularly sampled Gaussian time-series, but animal \n    movement analysts are encouraged to apply correlated velocity change point \n    analysis as implemented in the smoove package, as of this writing on GitHub \n    at <https://github.com/EliGurarie/smoove>. An example of a univariate analysis\n    is provided in the UnivariateBCPA vignette. ",
    "version": "1.3.2",
    "maintainer": "Eliezer Gurarie <egurarie@esf.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9146,
    "package_name": "bdlim",
    "title": "Bayesian Distributed Lag Interaction Models",
    "description": "Estimation and interpretation of Bayesian distributed lag interaction models (BDLIMs). A BDLIM regresses a scalar outcome on repeated measures of exposure and allows for modification by a categorical variable under four specific patterns of modification. The main function is bdlim(). There are also summary and plotting files. Details on methodology are described in Wilson et al. (2017) <doi:10.1093/biostatistics/kxx002>.",
    "version": "0.5.0",
    "maintainer": "Ander Wilson <ander.wilson@colostate.edu>",
    "url": "https://anderwilson.github.io/bdlim/,\nhttps://github.com/AnderWilson/bdlim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9148,
    "package_name": "bdots",
    "title": "Bootstrapped Differences of Time Series",
    "description": "Analyze differences among time series curves with p-value\n        adjustment for multiple comparisons introduced in Oleson et al\n        (2015) <DOI:10.1177/0962280215607411>.",
    "version": "2.0.0",
    "maintainer": "Collin Nolte <noltecollin@grinnell.edu>",
    "url": "https://github.com/collinn/bdots",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9151,
    "package_name": "bdrc",
    "title": "Bayesian Discharge Rating Curves",
    "description": "Fits a discharge rating curve based on the power-law and the generalized power-law from data on paired stage and discharge measurements in a given river using a Bayesian hierarchical model as described in Hrafnkelsson et al. (2020) <arXiv:2010.04769>.",
    "version": "1.1.0",
    "maintainer": "Solvi Rognvaldsson <solviro@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9152,
    "package_name": "bdribs",
    "title": "Bayesian Detection of Potential Risk Using Inference on Blinded\nSafety Data",
    "description": "Implements Bayesian inference to detect signal from blinded \n    clinical trial when total number of adverse events of special \n    concerns and total risk exposures from all patients are available in the study.  \n    For more details see the article by Mukhopadhyay et. al. (2018) \n    titled 'Bayesian Detection of Potential Risk Using Inference on Blinded Safety Data', \n    in Pharmaceutical Statistics (to appear). ",
    "version": "1.0.4",
    "maintainer": "Saurabh Mukhopadhyay <saurabh.mukhopadhyay@abbvie.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9154,
    "package_name": "bdsm",
    "title": "Bayesian Dynamic Systems Modeling",
    "description": "Implements methods for building and analyzing models based on panel\n    data as described in the paper by \n    Moral-Benito (2013, <doi:10.1080/07350015.2013.818003>).\n    The package provides functions to estimate dynamic panel data models\n    and analyze the results of the estimation.",
    "version": "0.3.0",
    "maintainer": "Marcin Dubel <marcindubel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9168,
    "package_name": "beast",
    "title": "Bayesian Estimation of Change-Points in the Slope of\nMultivariate Time-Series",
    "description": "Assume that a temporal process is composed of contiguous segments with differing slopes and replicated noise-corrupted time series measurements are observed. The unknown mean of the data generating process is modelled as a piecewise linear function of time with an unknown number of change-points. The package infers the joint posterior distribution of the number and position of change-points as well as the unknown mean parameters per time-series by MCMC sampling. A-priori, the proposed model uses an overfitting number of mean parameters but, conditionally on a set of change-points, only a subset of them influences the likelihood. An exponentially decreasing prior distribution on the number of change-points gives rise to a posterior distribution concentrating on sparse representations of the underlying sequence, but also available is the Poisson distribution. See Papastamoulis et al (2017) <arXiv:1709.06111> for a detailed presentation of the method.",
    "version": "1.1",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9172,
    "package_name": "beaver",
    "title": "Bayesian Model Averaging of Covariate Adjusted Negative-Binomial\nDose-Response",
    "description": "Dose-response modeling for negative-binomial distributed data with\n  a variety of dose-response models. Covariate adjustment and Bayesian\n  model averaging is supported. Functions are provided to easily obtain\n  inference on the dose-response relationship and plot the dose-response curve.",
    "version": "1.0.0",
    "maintainer": "Hollins Showalter <hollins.showalter@gmail.com>",
    "url": "https://github.com/rich-payne/beaver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9180,
    "package_name": "beezdiscounting",
    "title": "Behavioral Economic Easy Discounting",
    "description": "Facilitates some of the analyses performed in studies of\n    behavioral economic discounting. The package supports scoring of the 27-Item Monetary Choice\n    Questionnaire (see Kaplan et al., 2016; <doi:10.1007/s40614-016-0070-9>), calculating k\n    values (Mazur's simple hyperbolic and exponential) using nonlinear regression, calculating\n    various Area Under the Curve (AUC) measures, plotting regression curves for both fit-to-group and\n    two-stage approaches, checking for unsystematic discounting\n    (Johnson & Bickel, 2008; <doi:10.1037/1064-1297.16.3.264>) and scoring of the\n    minute discounting task (see Koffarnus & Bickel, 2014; <doi:10.1037/a0035973>) using the\n    Qualtrics 5-trial discounting template (see the Qualtrics Minute Discounting User Guide;\n    <doi:10.13140/RG.2.2.26495.79527>), which is also available as a .qsf file in this package.",
    "version": "0.3.2",
    "maintainer": "Brent A. Kaplan <bkaplan.ku@gmail.com>",
    "url": "https://github.com/brentkaplan/beezdiscounting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9182,
    "package_name": "behaviorchange",
    "title": "Tools for Behavior Change Researchers and Professionals",
    "description": "Contains specialised analyses and\n    visualisation tools for behavior change science.\n    These facilitate conducting determinant studies\n    (for example, using confidence interval-based\n    estimation of relevance, CIBER, or CIBERlite\n    plots, see Crutzen, Noijen & Peters (2017)\n    <doi:10/ghtfz9>),\n    systematically developing, reporting,\n    and analysing interventions (for example, using\n    Acyclic Behavior Change Diagrams), and reporting\n    about intervention effectiveness (for example, using\n    the Numbers Needed for Change, see Gruijters & Peters\n    (2017) <doi:10/jzkt>), and computing the\n    required sample size (using the Meaningful Change\n    Definition, see Gruijters & Peters (2020)\n    <doi:10/ghpnx8>).\n    This package is especially useful for\n    researchers in the field of behavior change or\n    health psychology and to behavior change\n    professionals such as intervention developers and\n    prevention workers.",
    "version": "25.8.0",
    "maintainer": "Gjalt-Jorn Peters <behaviorchange@opens.science>",
    "url": "https://behaviorchange.opens.science",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9186,
    "package_name": "bellreg",
    "title": "Count Regression Models Based on the Bell Distribution",
    "description": "Bell regression models for count data with overdispersion. The implemented models account for ordinary and zero-inflated regression models under both frequentist and Bayesian approaches. Theoretical details regarding the models implemented in the package can be found in Castellares et al. (2018) <doi:10.1016/j.apm.2017.12.014> and Lemonte et al. (2020) <doi:10.1080/02664763.2019.1636940>.",
    "version": "0.0.2.2",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/bellreg,\nhttps://fndemarqui.github.io/bellreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9195,
    "package_name": "bennu",
    "title": "Bayesian Estimation of Naloxone Kit Number Under-Reporting",
    "description": "Bayesian model and associated tools for generating estimates of \n    total naloxone kit numbers distributed and used from naloxone kit orders \n    data. Provides functions for generating simulated data of naloxone kit use\n    and functions for generating samples from the posterior.",
    "version": "0.3.2",
    "maintainer": "Mike Irvine <mike.irvine@bccdc.ca>",
    "url": "https://sempwn.github.io/bennu/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9196,
    "package_name": "bentcableAR",
    "title": "Bent-Cable Regression for Independent Data or Autoregressive\nTime Series",
    "description": "\n\tIncluded are two main interfaces, bentcable.ar() and\n\tbentcable.dev.plot(), for fitting and diagnosing bent-cable\n\tregressions for autoregressive time-series data (Chiu and\n\tLockhart 2010, <doi:10.1002/cjs.10070>) or independent data (time\n\tseries or otherwise - Chiu, Lockhart and Routledge 2006,\n\t<doi:10.1198/016214505000001177>). Some components in the package\n\tcan also be used as stand-alone functions. The bent cable\n\t(linear-quadratic-linear) generalizes the broken stick\n\t(linear-linear), which is also handled by this package. Version\n\t0.2 corrected a glitch in the computation of confidence intervals\n\tfor the CTP. References that were updated from Versions 0.2.1 and\n\t0.2.2 appear in Version 0.2.3 and up. Version 0.3.0 improved\n\trobustness of the error-message producing mechanism. Version 0.3.1\n\timproves the NAMESPACE file of the package. It is the author's\n\tintention to distribute any future updates via GitHub.",
    "version": "0.3.1",
    "maintainer": "Grace Chiu <bentcable@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9202,
    "package_name": "bestglm",
    "title": "Best Subset GLM and Regression Utilities",
    "description": "Best subset glm using information criteria or cross-validation,\n        carried by using 'leaps' algorithm (Furnival and Wilson, 1974) <doi:10.2307/1267601>\n        or complete enumeration (Morgan and Tatar, 1972) <doi:10.1080/00401706.1972.10488918>.\n        Implements PCR and PLS using AIC/BIC.\n        Implements one-standard deviation rule for use with the 'caret' package.",
    "version": "0.37.3",
    "maintainer": "Yuanhao Lai <ylai72@uwo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9204,
    "package_name": "betaBayes",
    "title": "Bayesian Beta Regression",
    "description": "Provides a class of Bayesian beta regression models for the analysis of continuous data with support restricted to an unknown finite support. The response variable is modeled using a four-parameter beta distribution with the mean or mode parameter depending linearly on covariates through a link function. When the response support is known to be (0,1), the above class of models reduce to traditional (0,1) supported beta regression models. Model choice is carried out via the logarithm of the pseudo marginal likelihood (LPML), the deviance information criterion (DIC), and the Watanabe-Akaike information criterion (WAIC). See Zhou and Huang (2022) <doi:10.1016/j.csda.2021.107345>.",
    "version": "1.0.1",
    "maintainer": "Haiming Zhou <haiming2019@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9205,
    "package_name": "betaDelta",
    "title": "Confidence Intervals for Standardized Regression Coefficients",
    "description": "Generates confidence intervals for standardized regression coefficients\n    using delta method standard errors for models fitted by lm()\n    as described in Yuan and Chan (2011) <doi:10.1007/s11336-011-9224-6>\n    and Jones and Waller (2015) <doi:10.1007/s11336-013-9380-y>.\n    The package can also be used to generate confidence intervals for\n    differences of standardized regression coefficients \n    and as a general approach to performing the delta method.\n    A description of the package and code examples\n    are presented in Pesigan, Sun, and Cheung (2023) <doi:10.1080/00273171.2023.2201277>.",
    "version": "1.0.6",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "url": "https://github.com/jeksterslab/betaDelta,\nhttps://jeksterslab.github.io/betaDelta/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9207,
    "package_name": "betaMC",
    "title": "Monte Carlo for Regression Effect Sizes",
    "description": "Generates Monte Carlo confidence intervals\n    for standardized regression coefficients (beta) and other effect sizes,\n    including multiple correlation, semipartial correlations,\n    improvement in R-squared, squared partial correlations,\n    and differences in standardized regression coefficients,\n    for models fitted by lm().\n    'betaMC' combines ideas from Monte Carlo confidence intervals for the indirect effect\n    (Pesigan and Cheung, 2024 <doi:10.3758/s13428-023-02114-4>)\n    and the sampling covariance matrix of regression coefficients\n    (Dudgeon, 2017 <doi:10.1007/s11336-017-9563-z>)\n    to generate confidence intervals effect sizes in regression.",
    "version": "1.3.3",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "url": "https://github.com/jeksterslab/betaMC,\nhttps://jeksterslab.github.io/betaMC/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9208,
    "package_name": "betaNB",
    "title": "Bootstrap for Regression Effect Sizes",
    "description": "Generates nonparametric bootstrap confidence intervals\n    (Efron and Tibshirani, 1993: <doi:10.1201/9780429246593>)\n    for standardized regression coefficients (beta) and other effect sizes,\n    including multiple correlation, semipartial correlations,\n    improvement in R-squared, squared partial correlations,\n    and differences in standardized regression coefficients,\n    for models fitted by lm().",
    "version": "1.0.6",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "url": "https://github.com/jeksterslab/betaNB,\nhttps://jeksterslab.github.io/betaNB/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9209,
    "package_name": "betaSandwich",
    "title": "Robust Confidence Intervals for Standardized Regression\nCoefficients",
    "description": "Generates robust confidence intervals for standardized regression coefficients\n    using heteroskedasticity-consistent standard errors for models fitted by lm()\n    as described in Dudgeon (2017) <doi:10.1007/s11336-017-9563-z>.\n    The package can also be used to generate confidence intervals for R-squared,\n    adjusted R-squared, and differences of standardized regression coefficients.\n    A description of the package and code examples\n    are presented in Pesigan, Sun, and Cheung (2023) <doi:10.1080/00273171.2023.2201277>.",
    "version": "1.0.8",
    "maintainer": "Ivan Jacob Agaloos Pesigan <r.jeksterslab@gmail.com>",
    "url": "https://github.com/jeksterslab/betaSandwich,\nhttps://jeksterslab.github.io/betaSandwich/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9217,
    "package_name": "betategarch",
    "title": "Simulation, Estimation and Forecasting of Beta-Skew-t-EGARCH\nModels",
    "description": "Simulation, estimation and forecasting of first-order Beta-Skew-t-EGARCH models with leverage (one-component, two-component, skewed versions).",
    "version": "3.4",
    "maintainer": "Genaro Sucarrat <genaro.sucarrat@bi.no>",
    "url": "https://www.sucarrat.net/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9223,
    "package_name": "beyondWhittle",
    "title": "Bayesian Spectral Inference for Time Series",
    "description": "Implementations of Bayesian parametric, nonparametric and semiparametric procedures for univariate and multivariate time series. The package is based on the methods presented in C. Kirch et al (2018) <doi:10.1214/18-BA1126>, A. Meier (2018) <https://opendata.uni-halle.de//handle/1981185920/13470> and Y. Tang et al (2023) <doi:10.48550/arXiv.2303.11561>. It was supported by DFG grants KI 1443/3-1 and KI 1443/3-2.",
    "version": "1.3.0",
    "maintainer": "Renate Meyer <renate.meyer@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9226,
    "package_name": "bfast",
    "title": "Breaks for Additive Season and Trend",
    "description": "Decomposition of time series into\n    trend, seasonal, and remainder components with methods for detecting and\n    characterizing abrupt changes within the trend and seasonal components. 'BFAST'\n    can be used to analyze different types of satellite image time series and can\n    be applied to other disciplines dealing with seasonal or non-seasonal time\n    series, such as hydrology, climatology, and econometrics. The algorithm can be\n    extended to label detected changes with information on the parameters of the\n    fitted piecewise linear models. 'BFAST' monitoring functionality is described\n    in Verbesselt et al. (2010) <doi:10.1016/j.rse.2009.08.014>. 'BFAST monitor'\n    provides functionality to detect disturbance in near real-time based on 'BFAST'-\n    type models, and is described in Verbesselt et al. (2012) <doi:10.1016/j.rse.2012.02.022>.\n    'BFAST Lite' approach is a flexible approach that handles missing data\n    without interpolation, and will be described in an upcoming paper.\n    Furthermore, different models can now be used to fit the\n    time series data and detect structural changes (breaks).",
    "version": "1.7.1",
    "maintainer": "Dainius Masiliūnas <pastas4@gmail.com>",
    "url": "https://bfast2.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9227,
    "package_name": "bfboin",
    "title": "Operating Characteristics for the Bayesian Optimal Interval\nDesign with Back Filling",
    "description": "Calculate the operating characteristics of the \n    Bayesian Optimal Interval with Back Filling Design for dose escalation\n    in early-phase oncology trials.",
    "version": "0.1.1",
    "maintainer": "Dominic Magirr <dominic.magirr@novartis.com>",
    "url": "https://openpharma.github.io/bfboin/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9228,
    "package_name": "bfboinet",
    "title": "Backfill Bayesian Optimal Interval Design Using Efficacy and\nToxicity",
    "description": "The backfill Bayesian optimal interval design using efficacy and toxicity outcomes for \n    dose optimization (BF-BOIN-ET) design is a novel clinical trial design to allow patients to be \n    backfilled at lower doses during a dose-finding trial while prioritizing the dose-escalation \n    cohort to explore a higher dose. The advantages compared to the other designs in terms of the \n    percentage of correct optimal dose (OD) selection, reducing the sample size, and shortening the \n    duration of the trial, in various realistic setting.",
    "version": "0.4.0",
    "maintainer": "Jing Zhu <zhujing716@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9230,
    "package_name": "bfp",
    "title": "Bayesian Fractional Polynomials",
    "description": "Implements the Bayesian paradigm for fractional\n polynomial models under the assumption of normally distributed error terms, see\n Sabanes Bove, D. and Held, L. (2011) <doi:10.1007/s11222-010-9170-7>.",
    "version": "0.0-49",
    "maintainer": "Daniel Sabanes Bove <daniel.sabanesbove@gmx.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9234,
    "package_name": "bfw",
    "title": "Bayesian Framework for Computational Modeling",
    "description": "Derived from the work of Kruschke (2015,\n    <ISBN:9780124058880>), the present package aims to provide a framework\n    for conducting Bayesian analysis using Markov chain Monte Carlo (MCMC)\n    sampling utilizing the Just Another Gibbs Sampler ('JAGS', Plummer,\n    2003, <https://mcmc-jags.sourceforge.io>).  The initial version\n    includes several modules for conducting Bayesian equivalents of\n    chi-squared tests, analysis of variance (ANOVA), multiple\n    (hierarchical) regression, softmax regression, and for fitting data\n    (e.g., structural equation modeling).",
    "version": "0.4.2",
    "maintainer": "Øystein Olav Skaar <bayesianfw@gmail.com>",
    "url": "https://github.com/oeysan/bfw/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9236,
    "package_name": "bgeva",
    "title": "Binary Generalized Extreme Value Additive Models",
    "description": "Routine for fitting regression models for binary rare events with linear and nonlinear covariate effects when using the quantile function of the Generalized Extreme Value random variable.",
    "version": "0.3-1",
    "maintainer": "Giampiero Marra <giampiero.marra@ucl.ac.uk>",
    "url": "http://www.ucl.ac.uk/statistics/people/giampieromarra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9237,
    "package_name": "bggum",
    "title": "Bayesian Estimation of Generalized Graded Unfolding Model\nParameters",
    "description": "Provides a Metropolis-coupled Markov chain Monte Carlo sampler,\n    post-processing and parameter estimation functions, and plotting utilities\n    for the generalized graded unfolding model of Roberts, Donoghue, and\n    Laughlin (2000) <doi:10.1177/01466216000241001>.",
    "version": "1.0.2",
    "maintainer": "JBrandon Duck-Mayr <j.duckmayr@gmail.com>",
    "url": "https://github.com/duckmayr/bggum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9240,
    "package_name": "bgms",
    "title": "Bayesian Analysis of Networks of Binary and/or Ordinal Variables",
    "description": "Bayesian variable selection methods for analyzing the structure of a Markov random field model for a network of binary and/or ordinal variables. ",
    "version": "0.1.6.1",
    "maintainer": "Maarten Marsman <m.marsman@uva.nl>",
    "url": "https://Bayesian-Graphical-Modelling-Lab.github.io/bgms/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9243,
    "package_name": "bgvars",
    "title": "Bayesian Inference of Global Vector Autoregressive and Gloal Vector Error Correction Models",
    "description": "Assists in the set-up and inference of Bayesian global vector autoregressive (BGVAR) and error correction (BGVEC) models.",
    "version": "0.1.0",
    "maintainer": "",
    "url": "https://github.com/franzmohr/bgvars",
    "exports": [],
    "topics": ["bayesian", "bayesian-inference", "contagion", "econometrics", "gvar", "gvec", "international-economics"],
    "score": "NA",
    "stars": 17
  },
  {
    "id": 9246,
    "package_name": "bhetGP",
    "title": "Bayesian Heteroskedastic Gaussian Processes",
    "description": "Performs Bayesian posterior inference for heteroskedastic Gaussian processes.\n    Models are trained through MCMC including elliptical slice sampling (ESS) of \n    latent noise processes and Metropolis-Hastings sampling of \n    kernel hyperparameters. Replicates are handled efficientyly through a\n    Woodbury formulation of the joint likelihood for the mean and noise process \n    (Binois, M., Gramacy, R., Ludkovski, M. (2018) <doi:10.1080/10618600.2018.1458625>)\n    For large data, Vecchia-approximation for faster \n    computation is leveraged (Sauer, A., Cooper, A., and Gramacy, R.,\n    (2023), <doi:10.1080/10618600.2022.2129662>). Incorporates 'OpenMP' and \n    SNOW parallelization and utilizes 'C'/'C++' under the hood.",
    "version": "1.0.1",
    "maintainer": "Parul V. Patil <parulvijay@vt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9247,
    "package_name": "bhm",
    "title": "Biomarker Threshold Models",
    "description": "Contains tools to fit both predictive and prognostic biomarker effects using biomarker threshold models and continuous threshold models. Evaluate the treatment effect, biomarker effect and treatment-biomarker interaction using probability index measurement. Test for treatment-biomarker interaction using residual bootstrap method.",
    "version": "1.19",
    "maintainer": "Bingshu E. Chen <bingshu.chen@queensu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9248,
    "package_name": "bhmbasket",
    "title": "Bayesian Hierarchical Models for Basket Trials",
    "description": "Provides functions for the evaluation of basket\n    trial designs with binary endpoints. Operating characteristics of a\n    basket trial design are assessed by simulating trial data according to\n    scenarios, analyzing the data with Bayesian hierarchical models (BHMs), and\n    assessing decision probabilities on stratum and trial-level based on Go / No-go decision making.\n    The package is build for high flexibility regarding decision rules,\n    number of interim analyses, number of strata, and recruitment.\n    The BHMs proposed by\n    Berry et al. (2013) <doi:10.1177/1740774513497539>\n    and Neuenschwander et al. (2016) <doi:10.1002/pst.1730>,\n    as well as a model that combines both approaches are implemented.\n    Functions are provided to implement Bayesian decision rules as for example\n    proposed by Fisch et al. (2015) <doi:10.1177/2168479014533970>.\n    In addition, posterior point estimates (mean/median) and credible intervals\n    for response rates and some model parameters can be calculated.\n    For simulated trial data, bias and mean squared errors of posterior\n    point estimates for response rates can be provided.",
    "version": "0.9.5",
    "maintainer": "Stephan Wojciekowski <stephan.wojciekowski@boehringer-ingelheim.com>",
    "url": "https://CRAN.R-project.org/package=bhmbasket",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9257,
    "package_name": "bibs",
    "title": "Bayesian Inference for the Birnbaum-Saunders Distribution",
    "description": "Developed for the following tasks. 1- Simulating and computing the maximum likelihood \n              estimator for the Birnbaum-Saunders (BS) distribution, 2- Computing the Bayesian estimator for\n              the parameters of the BS distribution based on reference prior proposed by Xu and Tang (2010)\n              <doi:10.1016/j.csda.2009.08.004> and conjugate prior. 3- Computing the Bayesian estimator for\n              the BS distribution based on conjugate prior. 4- Computing the Bayesian estimator for the BS\n              distribution based on Jeffrey prior given by Achcar (1993) <doi:10.1016/0167-9473(93)90170-X>\n              5- Computing the Bayesian estimator for the BS distribution under progressive type-II censoring\n              scheme.",
    "version": "1.1.1",
    "maintainer": "Mahdi Teimouri <teimouri@aut.ac.ir>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9267,
    "package_name": "bigGP",
    "title": "Distributed Gaussian Process Calculations",
    "description": "Distributes Gaussian process calculations across nodes\n    in a distributed memory setting, using Rmpi. The bigGP class \n    provides high-level methods for maximum likelihood with normal data, \n    prediction, calculation of uncertainty (i.e., posterior covariance \n    calculations), and simulation of realizations. In addition, bigGP \n    provides an API for basic matrix calculations with distributed \n    covariance matrices, including Cholesky decomposition, back/forwardsolve, \n    crossproduct, and matrix multiplication.",
    "version": "0.1.9",
    "maintainer": "Christopher Paciorek <paciorek@stat.berkeley.edu>",
    "url": "https://doi.org/10.18637/jss.v063.i10",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9270,
    "package_name": "bigPLScox",
    "title": "Partial Least Squares for Cox Models with Big Matrices",
    "description": "Provides Partial least squares Regression and various regular, sparse \n    or kernel, techniques for fitting Cox models for big data. Provides a Partial \n    Least Squares (PLS) algorithm adapted to Cox proportional hazards models that \n    works with 'bigmemory' matrices without loading the entire dataset in memory. \n    Also implements a gradient-descent based solver for Cox proportional hazards \n    models that works directly on 'bigmemory' matrices.     \n    Bertrand and Maumy (2023) <https://hal.science/hal-05352069>, and \n    <https://hal.science/hal-05352061> highlighted \n    fitting and cross-validating PLS-based Cox models to censored big data.",
    "version": "0.8.1",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/bigPLScox/,\nhttps://github.com/fbertran/bigPLScox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9272,
    "package_name": "bigReg",
    "title": "Generalized Linear Models (GLM) for Large Data Sets",
    "description": "Allows the user to carry out GLM on very large\n    data sets. Data can be created using the data_frame() function and appended\n    to the object with object$append(data); data_frame and data_matrix objects\n    are available that allow the user to store large data on disk. The data is\n    stored as doubles in binary format and any character columns are transformed\n    to factors and then stored as numeric (binary) data while a look-up table is\n    stored in a separate .meta_data file in the same folder. The data is stored in\n    blocks and GLM regression algorithm is modified and carries out a MapReduce-\n    like algorithm to fit the model. The functions bglm(), and summary()\n    and bglm_predict() are available for creating and post-processing of models.\n    The library requires Armadillo installed on your system. It may not \n    function on windows since multi-core processing is done using mclapply() \n    which forks R on Unix/Linux type operating systems.",
    "version": "0.1.5",
    "maintainer": "Chibisi Chima-Okereke <chibisi@active-analytics.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9273,
    "package_name": "bigSurvSGD",
    "title": "Big Survival Analysis Using Stochastic Gradient Descent",
    "description": "Fits Cox model via stochastic gradient descent. This implementation avoids computational instability of the standard Cox Model when dealing large datasets. Furthermore, it scales up with large datasets that do not fit the memory. It also handles large sparse datasets using proximal stochastic gradient descent algorithm. For more details about the method, please see Aliasghar Tarkhan and Noah Simon (2020) <arXiv:2003.00116v2>.",
    "version": "0.0.1",
    "maintainer": "Aliasghar Tarkhan <atarkhan@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9279,
    "package_name": "biglasso",
    "title": "Extending Lasso Model Fitting to Big Data",
    "description": "Extend lasso and elastic-net model fitting for large data sets that\n  cannot be loaded into memory. Designed to be more memory- and\n  computation-efficient than existing lasso-fitting packages like 'glmnet' and\n  'ncvreg', thus allowing the user to analyze big data with limited RAM\n  <doi:10.32614/RJ-2021-001>.",
    "version": "1.6.1",
    "maintainer": "Patrick Breheny <patrick-breheny@uiowa.edu>",
    "url": "https://pbreheny.github.io/biglasso/,\nhttps://github.com/pbreheny/biglasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9281,
    "package_name": "biglm",
    "title": "Bounded Memory Linear and Generalized Linear Models",
    "description": "Regression for data too large to fit in memory.",
    "version": "0.9-3",
    "maintainer": "Thomas Lumley <t.lumley@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9282,
    "package_name": "biglmm",
    "title": "Bounded Memory Linear and Generalized Linear Models",
    "description": "Regression for data too large to fit in memory. This package functions exactly like the 'biglm' package, but works with later versions of R.",
    "version": "0.9-3",
    "maintainer": "Mark Purver <mark_purver@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9296,
    "package_name": "bigsplines",
    "title": "Smoothing Splines for Large Samples",
    "description": "Fits smoothing spline regression models using scalable algorithms designed for large samples. Seven marginal spline types are supported: linear, cubic, different cubic, cubic periodic, cubic thin-plate, ordinal, and nominal. Random effects and parametric effects are also supported. Response can be Gaussian or non-Gaussian: Binomial, Poisson, Gamma, Inverse Gaussian, or Negative Binomial.",
    "version": "1.1-1",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9298,
    "package_name": "bigstep",
    "title": "Stepwise Selection for Large Data Sets",
    "description": "Selecting linear and generalized linear models for large data sets\n    using modified stepwise procedure and modern selection criteria (like\n    modifications of Bayesian Information Criterion). Selection can be \n    performed on data which exceed RAM capacity. Bogdan et al., (2004)\n    <doi:10.1534/genetics.103.021683>.",
    "version": "1.1.2",
    "maintainer": "Piotr Szulc <piotr.michal.szulc@gmail.com>",
    "url": "https://github.com/pmszulc/bigstep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9301,
    "package_name": "bigtime",
    "title": "Sparse Estimation of Large Time Series Models",
    "description": "Estimation of large Vector AutoRegressive (VAR), Vector AutoRegressive with Exogenous Variables X (VARX) and Vector AutoRegressive Moving Average (VARMA) Models with Structured Lasso Penalties, see Nicholson, Wilms, Bien and Matteson (2020) <https://jmlr.org/papers/v21/19-777.html> and Wilms, Basu, Bien and Matteson (2021) <doi:10.1080/01621459.2021.1942013>.",
    "version": "0.2.3",
    "maintainer": "Ines Wilms <i.wilms@maastrichtuniversity.nl>",
    "url": "https://github.com/ineswilms/bigtime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9310,
    "package_name": "bin2norm",
    "title": "Hierarchical Probit Estimation for Dichotomized Data",
    "description": "Provides likelihood-based and hierarchical estimation methods\n    for thresholded (binomial-probit) data. Supports fixed-mean and random-mean\n    models with maximum likelihood estimation (MLE), generalized linear mixed model\n    (GLMM), and Bayesian Markov chain Monte Carlo (MCMC) implementations.\n    For methodological background, see Albert and Chib (1993)\n    <doi:10.1080/01621459.1993.10476321> and McCulloch (1994)\n    <doi:10.2307/2297959>.",
    "version": "0.1.0",
    "maintainer": "Zhaoze Liu <786633848@qq.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9311,
    "package_name": "binGroup",
    "title": "Evaluation and Experimental Design for Binomial Group Testing",
    "description": "Methods for estimation and hypothesis testing of proportions\n in group testing designs: methods for estimating a proportion in a single\n        population (assuming sensitivity and specificity equal to 1 in designs\n        with equal group sizes), as well as hypothesis tests and\n        functions for experimental design for this situation. For\n        estimating one proportion or the difference of proportions, a\n        number of confidence interval methods are included, which can\n        deal with various different pool sizes. Further, regression\n        methods are implemented for simple pooling and matrix pooling\n        designs.\n        Methods for identification of positive items in group\n        testing designs: Optimal testing configurations can be found \n        for hierarchical and array-based algorithms. Operating \n        characteristics can be calculated for testing configurations \n        across a wide variety of situations.",
    "version": "2.2-3",
    "maintainer": "Frank Schaarschmidt <schaarschmidt@biostat.uni-hannover.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9337,
    "package_name": "binsegRcpp",
    "title": "Efficient Implementation of Binary Segmentation",
    "description": "Standard template library \n containers are used to implement an efficient binary segmentation\n algorithm, which is log-linear on average and quadratic in the\n worst case.",
    "version": "2025.5.13",
    "maintainer": "Toby Dylan Hocking <toby.hocking@r-project.org>",
    "url": "https://github.com/tdhock/binsegRcpp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9340,
    "package_name": "binspp",
    "title": "Bayesian Inference for Neyman-Scott Point Processes",
    "description": "The Bayesian MCMC estimation of parameters for Thomas-type cluster\n    point process with various inhomogeneities. It allows for inhomogeneity in\n    (i) distribution of parent points, (ii) mean number of points in a cluster,\n    (iii) cluster spread. The package also allows for the Bayesian MCMC\n    algorithm for the homogeneous generalized Thomas process. The cluster size\n    is allowed to have a variance that is greater or less than the expected\n    value (cluster sizes are over or under dispersed). Details are described in\n    Dvořák, Remeš, Beránek & Mrkvička (2022) <arXiv: 10.48550/arXiv.2205.07946>.",
    "version": "0.2.3",
    "maintainer": "Remes Radim <inrem@jcu.cz>",
    "url": "https://github.com/tomasmrkvicka/binspp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9376,
    "package_name": "bioinactivation",
    "title": "Mathematical Modelling of (Dynamic) Microbial Inactivation",
    "description": "Functions for modelling microbial inactivation under\n    isothermal or dynamic conditions. The calculations are based on several mathematical models broadly\n    used by the scientific community and industry. Functions enable to make predictions for cases where the\n    kinetic parameters are known. It also implements functions for parameter estimation for isothermal and\n    dynamic conditions. The model fitting capabilities include an Adaptive Monte Carlo method for a Bayesian\n    approach to parameter estimation.",
    "version": "1.3.1",
    "maintainer": "Alberto Garre <garre.alberto@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9382,
    "package_name": "biometryassist",
    "title": "Functions to Assist Design and Analysis of Agronomic Experiments",
    "description": "Provides functions to aid in the design and analysis of\n    agronomic and agricultural experiments through easy access to\n    documentation and helper functions, especially for users who are\n    learning these concepts. While not required for most functionality,\n    this package enhances the `asreml` package which provides a \n    computationally efficient algorithm for fitting mixed models \n    using Residual Maximum Likelihood. It is a commercial package \n    that can be purchased as 'asreml-R' from 'VSNi' \n    <https://vsni.co.uk/>, who will supply a zip file for local \n    installation/updating (see <https://asreml.kb.vsni.co.uk/>). ",
    "version": "1.3.3",
    "maintainer": "Sam Rogers <biometrytraining@adelaide.edu.au>",
    "url": "https://biometryhub.github.io/biometryassist/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9384,
    "package_name": "biomod2",
    "title": "Ensemble Platform for Species Distribution Modeling",
    "description": "Functions for species distribution modeling, calibration and evaluation, \n  ensemble of models, ensemble forecasting and visualization. The package permits to run\n  consistently up to 10 single models on a presence/absences (resp presences/pseudo-absences)\n  dataset and to combine them in ensemble models and ensemble projections. Some bench of other \n  evaluation and visualisation tools are also available within the package.",
    "version": "4.3-4-3",
    "maintainer": "Maya Guéguen <maya.gueguen@univ-grenoble-alpes.fr>",
    "url": "https://biomodhub.github.io/biomod2/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9391,
    "package_name": "biosensors.usc",
    "title": "Distributional Data Analysis Techniques for Biosensor Data",
    "description": "Unified and user-friendly framework for using new \n             distributional representations of biosensors data in different statistical modeling \n             tasks: regression models, hypothesis testing, cluster analysis, visualization, and \n             descriptive analysis. Distributional representations are a functional extension of \n             compositional time-range metrics and we have used them successfully so far in modeling \n             glucose profiles and accelerometer data. However, these functional representations can \n             be used to represent any biosensor data such as ECG or medical imaging such as fMRI.\n             Matabuena M, Petersen A, Vidal JC, Gude F. \"Glucodensities: A new representation of \n             glucose profiles using distributional data analysis\" (2021) \n             <doi:10.1177/0962280221998064>.",
    "version": "1.0",
    "maintainer": "Juan C. Vidal <juan.vidal@usc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9395,
    "package_name": "biospear",
    "title": "Biomarker Selection in Penalized Regression Models",
    "description": "Provides some tools for developing and validating prediction models, estimate expected survival of patients and visualize them graphically. Most of the implemented methods are based on penalized regressions such as: the lasso (Tibshirani R (1996)), the elastic net (Zou H et al. (2005) <doi:10.1111/j.1467-9868.2005.00503.x>), the adaptive lasso (Zou H (2006) <doi:10.1198/016214506000000735>), the stability selection (Meinshausen N et al. (2010) <doi:10.1111/j.1467-9868.2010.00740.x>), some extensions of the lasso (Ternes et al. (2016) <doi:10.1002/sim.6927>), some methods for the interaction setting (Ternes N et al. (2016) <doi:10.1002/bimj.201500234>), or others. A function generating simulated survival data set is also provided.",
    "version": "1.0.2",
    "maintainer": "Stefan Michiels <stefan.michiels@gustaveroussy.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9405,
    "package_name": "bipd",
    "title": "Bayesian Individual Patient Data Meta-Analysis using 'JAGS'",
    "description": "We use a Bayesian approach to run individual patient data meta-analysis and network meta-analysis using 'JAGS'. The methods incorporate shrinkage methods and calculate patient-specific treatment effects as described in Seo et al. (2021) <DOI:10.1002/sim.8859>. This package also includes user-friendly functions that impute missing data in an individual patient data using mice-related packages.",
    "version": "0.3",
    "maintainer": "Michael Seo <swj8874@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9421,
    "package_name": "bistablehistory",
    "title": "Cumulative History Analysis for Bistable Perception Time Series",
    "description": "Estimates cumulative history for time-series for continuously\n    viewed bistable perceptual rivalry displays. Computes cumulative history\n    via a homogeneous first order differential process. I.e., it assumes\n    exponential growth/decay of the history as a function time and perceptually\n    dominant state, Pastukhov & Braun (2011) <doi:10.1167/11.10.12>.\n    Supports Gamma, log normal, and normal distribution families.\n    Provides a method to compute history directly and example of using the\n    computation on a custom Stan code.",
    "version": "1.1.3",
    "maintainer": "Alexander Pastukhov <pastukhov.alexander@gmail.com>",
    "url": "https://github.com/alexander-pastukhov/bistablehistory/,\nhttps://alexander-pastukhov.github.io/bistablehistory/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9429,
    "package_name": "bivarhr",
    "title": "Bivariate Hurdle Regression with Bayesian Model Averaging",
    "description": "Provides tools for fitting bivariate hurdle negative binomial \n    models with horseshoe priors, Bayesian Model Averaging (BMA) via stacking, \n    and comprehensive causal inference methods including G-computation, \n    transfer entropy, Threshold Vector Autoregressive (TVAR) and Smooth \n    Transition Autoregressive (STAR) models, Dynamic Bayesian Networks (DBN), \n    Hidden Markov Models (HMM), and sensitivity analysis.",
    "version": "0.1.5",
    "maintainer": "José Mauricio Gómez Julián <isadore.nabi@pm.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9432,
    "package_name": "bivgeom",
    "title": "Roy's Bivariate Geometric Distribution",
    "description": "Implements Roy's bivariate geometric model (Roy (1993) <doi:10.1006/jmva.1993.1065>): joint probability mass function, distribution function, survival function, random generation, parameter estimation, and more.",
    "version": "1.0",
    "maintainer": "Alessandro Barbiero <alessandro.barbiero@unimi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9435,
    "package_name": "biwavelet",
    "title": "Conduct Univariate and Bivariate Wavelet Analyses",
    "description": "This is a port of the WTC MATLAB package written by Aslak Grinsted\n    and the wavelet program written by Christopher Torrence and Gibert P.\n    Compo. This package can be used to perform univariate and bivariate\n    (cross-wavelet, wavelet coherence, wavelet clustering) analyses.",
    "version": "0.20.22",
    "maintainer": "Tarik Gouhier <tarik.gouhier@gmail.com>",
    "url": "https://github.com/tgouhier/biwavelet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9438,
    "package_name": "bizicount",
    "title": "Bivariate Zero-Inflated Count Models Using Copulas",
    "description": "Maximum likelihood estimation of copula-based zero-inflated \n    (and non-inflated) Poisson and negative binomial count models, based on the \n    article <doi:10.18637/jss.v109.i01>. Supports Frank and Gaussian copulas. \n    Allows for mixed margins (e.g., one margin Poisson, the other zero-inflated \n    negative binomial), and several marginal link functions. Built-in methods for \n    publication-quality tables using 'texreg', post-estimation diagnostics using \n    'DHARMa', and testing for marginal zero-modification via <doi:10.1177/0962280217749991>. \n    For information on copula regression for count data, see Genest and Nešlehová (2007) \n    <doi:10.1017/S0515036100014963> as well as Nikoloulopoulos (2013) <doi:10.1007/978-3-642-35407-6_11>. \n    For information on zero-inflated count regression generally, see Lambert (1992) \n    <https://www.jstor.org/stable/1269547>. The author acknowledges \n    support by NSF DMS-1925119 and DMS-212324.",
    "version": "1.3.4",
    "maintainer": "John Niehaus <jniehaus2257@gmail.com>",
    "url": "https://github.com/jmniehaus/bizicount",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9440,
    "package_name": "bkmrhat",
    "title": "Parallel Chain Tools for Bayesian Kernel Machine Regression",
    "description": "Bayesian kernel machine regression (from the 'bkmr' package)\n    is a Bayesian semi-parametric generalized linear model approach under\n    identity and probit links. There are a number of functions in this\n    package that extend Bayesian kernel machine regression fits to allow\n    multiple-chain inference and diagnostics, which leverage functions\n    from the 'future', 'rstan', and 'coda' packages.  Reference: Bobb, J.\n    F., Henn, B. C., Valeri, L., & Coull, B. A. (2018). Statistical\n    software for analyzing the health effects of multiple concurrent\n    exposures via Bayesian kernel machine regression. ;\n    <doi:10.1186/s12940-018-0413-y>.",
    "version": "1.1.7",
    "maintainer": "Alexander Keil <alex.keil@nih.gov>",
    "url": "https://github.com/alexpkeil1/bkmrhat/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9446,
    "package_name": "blapsr",
    "title": "Bayesian Inference with Laplace Approximations and P-Splines",
    "description": "Laplace approximations and penalized B-splines are combined\n    for fast Bayesian inference in latent Gaussian models. The routines can be\n    used to fit survival models, especially proportional hazards and promotion \n    time cure models (Gressani, O. and Lambert, P. (2018) \n    <doi:10.1016/j.csda.2018.02.007>). The Laplace-P-spline methodology can also\n    be implemented for inference in (generalized) additive models\n    (Gressani, O. and Lambert, P. (2021) <doi:10.1016/j.csda.2020.107088>).\n    See the associated website for more information and examples.",
    "version": "0.7.0",
    "maintainer": "Oswaldo Gressani <oswaldo_gressani@hotmail.fr>",
    "url": "<https://github.com/oswaldogressani/blapsr>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9449,
    "package_name": "blatent",
    "title": "Bayesian Latent Variable Models",
    "description": "Estimation of latent variable models using Bayesian methods. Currently estimates the loglinear cognitive diagnosis model of Henson, Templin, and Willse (2009) <doi:10.1007/s11336-008-9089-5>.",
    "version": "0.1.2",
    "maintainer": "Jonathan Templin <jonathan-templin@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9451,
    "package_name": "blavaan",
    "title": "Bayesian Latent Variable Analysis",
    "description": "Fit a variety of Bayesian latent variable models, including confirmatory\n   factor analysis, structural equation models, and latent growth curve models. References: Merkle & Rosseel (2018) <doi:10.18637/jss.v085.i04>; Merkle et al. (2021) <doi:10.18637/jss.v100.i06>.",
    "version": "0.5-9",
    "maintainer": "Edgar Merkle <merklee@missouri.edu>",
    "url": "https://ecmerkle.github.io/blavaan/,\nhttps://github.com/ecmerkle/blavaan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9457,
    "package_name": "bliss",
    "title": "Bayesian Functional Linear Regression with Sparse Step Functions",
    "description": "A method for the Bayesian functional linear regression model (scalar-on-function),\n  including two estimators of the coefficient function and an estimator of its support.\n  A representation of the posterior distribution is also available. Grollemund P-M., Abraham C., \n  Baragatti M., Pudlo P. (2019) <doi:10.1214/18-BA1095>.",
    "version": "1.1.1",
    "maintainer": "Paul-Marie Grollemund <paul_marie.grollemund@uca.fr>",
    "url": "https://github.com/pmgrollemund/bliss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9459,
    "package_name": "blm",
    "title": "Binomial Linear Regression",
    "description": "Implements regression models for binary data on the absolute risk scale. These models are applicable to cohort and population-based case-control data.",
    "version": "2022.0.0.1",
    "maintainer": "S.Kovalchik <s.a.kovalchik@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9460,
    "package_name": "blme",
    "title": "Bayesian Linear Mixed-Effects Models",
    "description": "Maximum a posteriori estimation for linear and generalized linear mixed-effects models in a Bayesian setting, implementing the methods of Chung, et al. (2013) <doi:10.1007/s11336-013-9328-2>. Extends package 'lme4' (Bates, Maechler, Bolker, and Walker (2015) <doi:10.18637/jss.v067.i01>).",
    "version": "1.0-7",
    "maintainer": "Vincent Dorie <vdorie@gmail.com>",
    "url": "https://github.com/vdorie/blme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9461,
    "package_name": "blmeco",
    "title": "Data Files and Functions Accompanying the Book \"Bayesian Data\nAnalysis in Ecology using R, BUGS and Stan\"",
    "description": "Data files and functions accompanying the book Korner-Nievergelt, Roth, von Felten, Guelat, Almasi, Korner-Nievergelt (2015) \"Bayesian Data Analysis in Ecology using R, BUGS and Stan\", Elsevier, New York.",
    "version": "1.4",
    "maintainer": "Fraenzi Korner-Nievergelt <fraenzi.korner@oikostat.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9470,
    "package_name": "blockmatrix",
    "title": "blockmatrix: Tools to solve algebraic systems with partitioned\nmatrices",
    "description": "Some elementary matrix algebra tools are implemented to manage\n    block matrices or partitioned matrix, i.e. \"matrix of matrices\"\n    (http://en.wikipedia.org/wiki/Block_matrix). The block matrix is here\n    defined as a new S3 object. In this package, some methods for \"matrix\"\n    object are rewritten for \"blockmatrix\" object. New methods are implemented.\n    This package was created to solve equation systems with block matrices for\n    the analysis of environmental vector time series .\n    Bugs/comments/questions/collaboration of any kind are warmly welcomed.",
    "version": "1.0",
    "maintainer": "Emanuele Cordano <emanuele.cordano@gmail.com>",
    "url": "http://cri.gmpf.eu/Research/Sustainable-Agro-Ecosystems-and-Bioresources/Dynamics-in-the-agro-ecosystems/people/Emanuele-Cordano",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9481,
    "package_name": "blocksdesign",
    "title": "Nested and Crossed Block Designs for Factorial and Unstructured\nTreatment Sets",
    "description": "Constructs treatment and block designs for linear treatment models\n  with crossed or nested block factors. The treatment design can be any feasible \n  linear model and the block design can be any feasible combination of crossed or \n  nested block factors. The block design is a sum of one or more block factors\n  and the block design is optimized sequentially with the levels of each successive\n  block factor optimized conditional on all previously optimized block factors. \n  D-optimality is used throughout except for square or rectangular lattice block designs \n  which are constructed algebraically using mutually orthogonal Latin squares.\n  Crossed block designs with interaction effects are optimized using a weighting scheme\n  which allows for differential weighting of first and second-order block effects. \n  Outputs include a table showing the allocation of treatments to blocks and tables showing\n  the achieved D-efficiency factors for each block and treatment design.  \n  Edmondson, R.N. Multi-level Block Designs for Comparative Experiments. \n  JABES 25, 500–522 (2020) <doi:10.1007/s13253-020-00416-0>.",
    "version": "4.9",
    "maintainer": "Rodney Edmondson <rodney.edmondson@gmail.com>",
    "url": "<doi:10.1007/s13253-020-00416-0>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9489,
    "package_name": "blsR",
    "title": "Make Requests from the Bureau of Labor Statistics API",
    "description": "Implements v2 of the B.L.S. API for requests of survey information\n  and time series data through 3-tiered API that allows users to interact with\n  the raw API directly, create queries through a functional interface, and\n  re-shape the data structures returned to fit common uses. The API definition \n  is located at: <https://www.bls.gov/developers/api_signature_v2.htm>.",
    "version": "0.5.0",
    "maintainer": "Guillermo Roditi Dominguez <guillermo@newriverinvestments.com>",
    "url": "https://github.com/groditi/blsR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9491,
    "package_name": "bltm",
    "title": "Bayesian Latent Threshold Modeling",
    "description": "Fits latent threshold model for simulated data\n    and describes how to adjust model using real data. Implements algorithm\n    proposed by Nakajima and West (2013) <doi:10.1080/07350015.2012.747847>. \n    This package has a function to generate data, a function to configure \n    priors and a function to fit the model. Examples may be checked inside \n    the demonstration files.",
    "version": "0.1.0",
    "maintainer": "Julio Trecenti <julio.trecenti@gmail.com>",
    "url": "https://github.com/curso-r/bltm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9495,
    "package_name": "bmabart",
    "title": "Bayesian Mediation Analysis Using BART",
    "description": "Used for Bayesian mediation analysis based on Bayesian additive Regression Trees (BART).\n\tThe analysis method is described in Yu and Li (2025) \"Mediation Analysis with Bayesian Additive Regression Trees\", submitted for publication.  ",
    "version": "2.0",
    "maintainer": "Qingzhao Yu <qyu@lsuhsc.edu>",
    "url": "https://www.r-project.org,\nhttps://publichealth.lsuhsc.edu/Faculty_pages/qyu/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9498,
    "package_name": "bmem",
    "title": "Mediation Analysis with Missing Data Using Bootstrap",
    "description": "Four methods for mediation analysis with missing data: Listwise deletion, Pairwise deletion, Multiple imputation, and Two Stage Maximum Likelihood algorithm. For MI and TS-ML, auxiliary variables can be included. Bootstrap confidence intervals for mediation effects are obtained. The robust method is also implemented for TS-ML. Since version 1.4, bmem adds the capability to conduct power analysis for mediation models. Details about the methods used can be found in these articles. Zhang and Wang (2003) <doi:10.1007/s11336-012-9301-5>. Zhang (2014) <doi:10.3758/s13428-013-0424-0>.",
    "version": "2.2",
    "maintainer": "Zhiyong Zhang <zhiyongzhang@nd.edu>",
    "url": "https://bigdatalab.nd.edu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9500,
    "package_name": "bmet",
    "title": "Bayesian Multigroup Equivalence Testing",
    "description": "Calculates the necessary quantities to perform Bayesian multigroup equivalence testing. \n             Currently the package includes the Bayesian models and equivalence criteria outlined in Pourmohamad and Lee (2023) \n             <doi:10.1002/sta4.645>, but more models and equivalence testing features may be added over time.",
    "version": "0.1.0",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9501,
    "package_name": "bmgarch",
    "title": "Bayesian Multivariate GARCH Models",
    "description": "Fit Bayesian multivariate GARCH models using 'Stan' for full Bayesian inference. Generate (weighted) forecasts for means, variances (volatility) and correlations. Currently DCC(P,Q), CCC(P,Q), pdBEKK(P,Q), and BEKK(P,Q) parameterizations are implemented, based either on a multivariate gaussian normal or student-t distribution. DCC and CCC models are based on Engle (2002) <doi:10.1198/073500102288618487> and Bollerslev (1990). The BEKK parameterization follows Engle and Kroner (1995) <doi:10.1017/S0266466600009063> while the pdBEKK as well as the estimation approach for this package is described in Rast et al. (2020) <doi:10.31234/osf.io/j57pk>. The fitted models contain 'rstan' objects and can be examined with 'rstan' functions.  ",
    "version": "2.0.0",
    "maintainer": "Philippe Rast <rast.ph@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9502,
    "package_name": "bmggum",
    "title": "Bayesian Multidimensional Generalized Graded Unfolding Model",
    "description": "Full Bayesian estimation of Multidimensional Generalized Graded Unfolding Model (MGGUM) using 'rstan' (See Stan Development Team (2020) <https://mc-stan.org/>).\n    Functions are provided for estimation, result extraction, model fit statistics, and plottings.",
    "version": "0.1.0",
    "maintainer": "Naidan Tu <naidantu@usf.edu>",
    "url": "https://github.com/Naidantu/bmggum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9504,
    "package_name": "bmixture",
    "title": "Bayesian Estimation for Finite Mixture of Distributions",
    "description": "Provides statistical tools for Bayesian estimation of mixture distributions, mainly a mixture of Gamma, Normal, and t-distributions. The package is implemented based on the Bayesian literature for the finite mixture of distributions, including Mohammadi and et al. (2013) <doi:10.1007/s00180-012-0323-3> and Mohammadi and Salehi-Rad (2012) <doi:10.1080/03610918.2011.588358>.",
    "version": "1.7",
    "maintainer": "Reza Mohammadi <a.mohammadi@uva.nl>",
    "url": "https://www.uva.nl/profile/a.mohammadi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9505,
    "package_name": "bmlm",
    "title": "Bayesian Multilevel Mediation",
    "description": "Easy estimation of Bayesian multilevel mediation models with Stan.",
    "version": "1.3.15",
    "maintainer": "Matti Vuorre <m.j.vuorre@tilburguniversity.edu>",
    "url": "https://github.com/mvuorre/bmlm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9506,
    "package_name": "bmm",
    "title": "Easy and Accessible Bayesian Measurement Models Using 'brms'",
    "description": "Fit computational and measurement models using full Bayesian\n    inference. The package provides a simple and accessible interface by\n    translating complex domain-specific models into 'brms' syntax, a\n    powerful and flexible framework for fitting Bayesian regression models\n    using 'Stan'. The package is designed so that users can easily apply\n    state-of-the-art models in various research fields, and so that\n    researchers can use it as a new model development framework.\n    References: Frischkorn and Popov (2023) <doi:10.31234/osf.io/umt57>.",
    "version": "1.2.0",
    "maintainer": "Vencislav Popov <vencislav.popov@gmail.com>",
    "url": "https://github.com/venpopov/bmm, https://venpopov.github.io/bmm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9510,
    "package_name": "bnRep",
    "title": "A Repository of Bayesian Networks from the Academic Literature",
    "description": "A collection of Bayesian networks (discrete, Gaussian, and conditional linear Gaussian) collated from recent academic literature. The 'bnRep_summary' object provides an overview of the Bayesian networks in the repository and the package documentation includes details about the variables in each network. A Shiny app to explore the repository can be launched with 'bnRep_app()' and is available online at <https://manueleleonelli.shinyapps.io/bnRep>. Reference: 'M. Leonelli' (2025) <doi:10.1016/j.neucom.2025.129502>.",
    "version": "0.0.6",
    "maintainer": "Manuele Leonelli <manuele.leonelli@ie.edu>",
    "url": "https://github.com/manueleleonelli/bnRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9512,
    "package_name": "bnclassify",
    "title": "Learning Discrete Bayesian Network Classifiers from Data",
    "description": "State-of-the art algorithms for learning discrete Bayesian network classifiers from data, including a number of those described in Bielza & Larranaga (2014) <doi:10.1145/2576868>, with functions for prediction, model evaluation and inspection.",
    "version": "0.4.8",
    "maintainer": "Mihaljevic Bojan <boki.mihaljevic@gmail.com>",
    "url": "https://github.com/bmihaljevic/bnclassify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9515,
    "package_name": "bnlearn",
    "title": "Bayesian Network Structure Learning, Parameter Learning and\nInference",
    "description": "Bayesian network structure learning, parameter learning and inference.\n  This package implements constraint-based (PC, GS, IAMB, Inter-IAMB, Fast-IAMB, MMPC,\n  Hiton-PC, HPC), pairwise (ARACNE and Chow-Liu), score-based (Hill-Climbing and Tabu\n  Search) and hybrid (MMHC, RSMAX2, H2PC) structure learning algorithms for discrete,\n  Gaussian and conditional Gaussian networks, along with many score functions and\n  conditional independence tests.\n  The Naive Bayes and the Tree-Augmented Naive Bayes (TAN) classifiers are also implemented.\n  Some utility functions (model comparison and manipulation, random data generation, arc\n  orientation testing, simple and advanced plots) are included, as well as support for\n  parameter estimation (maximum likelihood and Bayesian) and inference, conditional\n  probability queries, cross-validation, bootstrap and model averaging.\n  Development snapshots with the latest bugfixes are available from <https://www.bnlearn.com/>.",
    "version": "5.1",
    "maintainer": "Marco Scutari <scutari@bnlearn.com>",
    "url": "https://www.bnlearn.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9516,
    "package_name": "bnma",
    "title": "Bayesian Network Meta-Analysis using 'JAGS'",
    "description": "Network meta-analyses using Bayesian framework following Dias et al. (2013) <DOI:10.1177/0272989X12458724>. Based on the data input, creates prior, model file, and initial values needed to run models in 'rjags'. Able to handle binomial, normal and multinomial arm-level data. Can handle multi-arm trials and includes methods to incorporate covariate and baseline risk effects. Includes standard diagnostics and visualization tools to evaluate the results.",
    "version": "1.6.1",
    "maintainer": "Michael Seo <swj8874@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9517,
    "package_name": "bnmonitor",
    "title": "An Implementation of Sensitivity Analysis in Bayesian Networks",
    "description": "An implementation of sensitivity and robustness methods in Bayesian networks in R. It includes methods to perform parameter variations via a variety of co-variation schemes, to compute sensitivity functions and to quantify the dissimilarity of two Bayesian networks via distances and divergences. It further includes diagnostic methods to assess the goodness of fit of a Bayesian networks to data, including global, node and parent-child monitors. Reference: M. Leonelli, R. Ramanathan, R.L. Wilkerson (2022) <doi:10.1016/j.knosys.2023.110882>. ",
    "version": "0.2.2",
    "maintainer": "Manuele Leonelli <manuele.leonelli@ie.edu>",
    "url": "https://manueleleonelli.github.io/bnmonitor/,\nhttps://github.com/manueleleonelli/bnmonitor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9518,
    "package_name": "bnnSurvival",
    "title": "Bagged k-Nearest Neighbors Survival Prediction",
    "description": "Implements a bootstrap aggregated (bagged) version of\n    the k-nearest neighbors survival probability prediction method (Lowsky et\n    al. 2013). In addition to the bootstrapping of training samples, the\n    features can be subsampled in each baselearner to break the correlation\n    between them. The Rcpp package is used to speed up the computation.",
    "version": "0.1.5",
    "maintainer": "Marvin N. Wright <marv@wrig.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9520,
    "package_name": "bnpMTP",
    "title": "Bayesian Nonparametric Sensitivity Analysis of Multiple Testing\nProcedures for p Values",
    "description": "Bayesian Nonparametric sensitivity analysis of multiple testing procedures for p values with arbitrary dependencies, based on the Dirichlet process prior distribution.",
    "version": "1.0.0",
    "maintainer": "George Karabatsos <gkarabatsos1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9523,
    "package_name": "bnstruct",
    "title": "Bayesian Network Structure Learning from Data with Missing\nValues",
    "description": "Bayesian Network Structure Learning from Data with Missing Values.\n    The package implements the Silander-Myllymaki complete search,\n    the Max-Min Parents-and-Children, the Hill-Climbing, the\n    Max-Min Hill-climbing heuristic searches, and the Structural\n    Expectation-Maximization algorithm. Available scoring functions are\n    BDeu, AIC, BIC. The package also implements methods for generating and using\n    bootstrap samples, imputed data, inference.",
    "version": "1.0.15",
    "maintainer": "Alberto Franzin <afranzin@ulb.ac.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9525,
    "package_name": "boa",
    "title": "Bayesian Output Analysis Program (BOA) for MCMC",
    "description": "A menu-driven program and library of functions for carrying out\n    convergence diagnostics and statistical and graphical analysis of Markov\n    chain Monte Carlo sampling output.",
    "version": "1.1.8-2",
    "maintainer": "Brian J. Smith <brian-j-smith@uiowa.edu>",
    "url": "http://www.jstatsoft.org/v21/i11",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9528,
    "package_name": "bodycomp",
    "title": "Percent Body Fat Values Using Anthropometric Prediction\nEquations",
    "description": "Skinfold measurements is one of the most popular and practical methods for estimating percent body fat. Body composition is a term that describes the relative proportions of fat, bone, and muscle mass in the human body. Following the collection of skinfold measurements, regression analysis (a statistical procedure used to predict a dependent variable based on one or more independent or predictor variables) is used to estimate total percent body fat in humans. <doi:10.4324/9780203868744>. ",
    "version": "1.0.0",
    "maintainer": "Gleidson Mendes Rebouças <gleidsonreboucas@uern.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9533,
    "package_name": "boinet",
    "title": "Conduct Simulation Study of Bayesian Optimal Interval Design\nwith BOIN-ET Family",
    "description": "Bayesian optimal interval based on both efficacy and toxicity outcomes (BOIN-ET) design is a model-assisted oncology phase I/II trial design, aiming to establish an optimal biological dose accounting for efficacy and toxicity in the framework of dose-finding. Some extensions of BOIN-ET design are also available to allow for time-to-event efficacy and toxicity outcomes based on cumulative and pending data (time-to-event BOIN-ET: TITE-BOIN-ET), ordinal graded efficacy and toxicity outcomes (generalized BOIN-ET: gBOIN-ET), and their combination (TITE-gBOIN-ET). 'boinet' is a package to implement the BOIN-ET design family and supports the conduct of simulation studies to assess operating characteristics of BOIN-ET, TITE-BOIN-ET, gBOIN-ET, and TITE-gBOIN-ET, where users can choose design parameters in flexible and straightforward ways depending on their own application.",
    "version": "1.5.0",
    "maintainer": "Yusuke Yamaguchi <yamagubed@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9534,
    "package_name": "boiwsa",
    "title": "Seasonal Adjustment of Weekly Data",
    "description": "Perform seasonal adjustment and forecasting of weekly data.\n    The package provides a user-friendly interface for computing seasonally\n    adjusted estimates and forecasts of weekly time series and includes\n    functions for the construction of country-specific prior adjustment\n    variables, as well as diagnostic tools to assess the quality of the\n    adjustments. The methodology is described in more detail in\n    Ginker (2024) <doi:10.13140/RG.2.2.12221.44000>.",
    "version": "1.1.4",
    "maintainer": "Tim Ginker <tim.ginker@gmail.com>",
    "url": "https://github.com/timginker/boiwsa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9542,
    "package_name": "bonsaiforest",
    "title": "Shrinkage Based Forest Plots",
    "description": "Subgroup analyses are routinely performed in clinical trial\n    analyses. From a methodological perspective, two key issues of\n    subgroup analyses are multiplicity (even if only predefined subgroups\n    are investigated) and the low sample sizes of subgroups which lead to\n    highly variable estimates, see e.g. Yusuf et al (1991)\n    <doi:10.1001/jama.1991.03470010097038>.  This package implements\n    subgroup estimates based on Bayesian shrinkage priors, see Carvalho et\n    al (2019) <https://proceedings.mlr.press/v5/carvalho09a.html>. In\n    addition, estimates based on penalized likelihood inference are\n    available, based on Simon et al (2011) <doi:10.18637/jss.v039.i05>.\n    The corresponding shrinkage based forest plots address the\n    aforementioned issues and can complement standard forest plots in\n    practical clinical trial analyses.",
    "version": "0.1.1",
    "maintainer": "Isaac Gravestock <isaac.gravestock@roche.com>",
    "url": "https://github.com/insightsengineering/bonsaiforest/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9549,
    "package_name": "boostime",
    "title": "The Tidymodels Extension for Time Series Boosting Models",
    "description": "The time series forecasting package using boosting to improve modeling errors (residuals). The methods",
    "version": "0.1.0",
    "maintainer": "",
    "url": "https://github.com/AlbertoAlmuinha/boostime",
    "exports": [],
    "topics": ["boosting-algorithms", "forecasting", "r", "tidymodels", "tidyverse", "time-series"],
    "score": "NA",
    "stars": 58
  },
  {
    "id": 9552,
    "package_name": "boostrq",
    "title": "Boosting Regression Quantiles",
    "description": "Boosting Regression Quantiles is a component-wise boosting algorithm, \n             that embeds all boosting steps in the well-established framework \n             of quantile regression. It is initialized with the corresponding quantile, \n             uses a quantile-specific learning rate, and uses quantile regression as its base learner.\n             The package implements this algorithm and allows cross-validation and stability selection. ",
    "version": "1.0.0",
    "maintainer": "Stefan Linner <stefan.linner97@gmail.com>",
    "url": "https://github.com/stefanlinner/boostrq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9553,
    "package_name": "boot",
    "title": "Bootstrap Functions",
    "description": "Functions and datasets for bootstrapping from the\n  book \"Bootstrap Methods and Their Application\" by A. C. Davison and \n  D. V. Hinkley (1997, CUP), originally written by Angelo Canty for S.",
    "version": "1.3-32",
    "maintainer": "Alessandra R. Brazzale <brazzale@stat.unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9555,
    "package_name": "boot.pval",
    "title": "Bootstrap p-Values",
    "description": "Computation of bootstrap p-values through inversion of confidence intervals, including convenience functions for regression models and tests of location.",
    "version": "0.7.0",
    "maintainer": "Måns Thulin <mans@statistikkonsult.com>",
    "url": "https://github.com/mthulin/boot.pval,\nhttps://mthulin.github.io/boot.pval/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9563,
    "package_name": "bootPLS",
    "title": "Bootstrap Hyperparameter Selection for PLS Models and Extensions",
    "description": "Several implementations of non-parametric stable bootstrap-based techniques to determine the numbers of components for Partial Least Squares linear or generalized linear regression models as well as and sparse Partial Least Squares linear or generalized linear regression models. The package collects techniques that were published in a book chapter (Magnanensi et al. 2016, 'The Multiple Facets of Partial Least Squares and Related Methods', <doi:10.1007/978-3-319-40643-5_18>) and two articles (Magnanensi et al. 2017, 'Statistics and Computing', <doi:10.1007/s11222-016-9651-4>) and (Magnanensi et al. 2021, 'Frontiers in Applied Mathematics and Statistics', <doi:10.3389/fams.2021.693126>).",
    "version": "1.1.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/bootPLS/,\nhttps://github.com/fbertran/bootPLS/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9578,
    "package_name": "boral",
    "title": "Bayesian Ordination and Regression AnaLysis",
    "description": "Bayesian approaches for analyzing multivariate data in ecology. Estimation is performed using Markov Chain Monte Carlo (MCMC) methods via Three. JAGS types of models may be fitted: 1) With explanatory variables only, boral fits independent column Generalized Linear Models (GLMs) to each column of the response matrix; 2) With latent variables only, boral fits a purely latent variable model for model-based unconstrained ordination; 3) With explanatory and latent variables, boral fits correlated column GLMs with latent variables to account for any residual correlation between the columns of the response matrix. ",
    "version": "2.0.3",
    "maintainer": "Francis K.C. Hui <fhui28@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9580,
    "package_name": "borrowr",
    "title": "Estimate Causal Effects with Borrowing Between Data Sources",
    "description": "Estimate population average treatment effects from a primary data source \n  with borrowing from supplemental sources. Causal estimation is done with either a \n  Bayesian linear model or with Bayesian additive regression trees (BART) to adjust \n  for confounding. Borrowing is done with multisource exchangeability models (MEMs). For \n  information on BART, see Chipman, George, & McCulloch (2010) <doi:10.1214/09-AOAS285>. \n  For information on MEMs, see Kaizer, Koopmeiners, & \n  Hobbs (2018) <doi:10.1093/biostatistics/kxx031>.",
    "version": "0.2.0",
    "maintainer": "Jeffrey A. Boatman <jeffrey.boatman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9604,
    "package_name": "bpcp",
    "title": "Beta Product Confidence Procedure for Right Censored Data",
    "description": "Calculates nonparametric pointwise confidence intervals for the survival distribution for right censored data, and for medians [Fay and Brittain <DOI:10.1002/sim.6905>]. Has two-sample tests for dissimilarity (e.g., difference, ratio or odds ratio) in survival at a fixed time, and differences in medians [Fay, Proschan, and Brittain <DOI:10.1111/biom.12231>]. Basically, the package gives exact inference methods for one- and two-sample exact inferences for Kaplan-Meier curves (e.g., generalizing Fisher's exact test to allow for right censoring), which are especially important for latter parts of the survival curve, small sample sizes or heavily censored data. Includes mid-p options.",
    "version": "1.4.2",
    "maintainer": "Michael P. Fay <mfay@niaid.nih.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9605,
    "package_name": "bpcs",
    "title": "Bayesian Paired Comparison Analysis with Stan",
    "description": "Models for the analysis of paired comparison data using Stan. The models include Bayesian versions of the Bradley-Terry model, including random effects, generalized model for predictors, order effect (home advantage) and the  variations for the Davidson (1970) model to handle ties. Additionally, we provide a number of functions to facilitate inference and obtaining results with these models. References: Bradley and Terry (1952) <doi:10.2307/2334029>; Davidson (1970) <doi:10.1080/01621459.1970.10481082>; Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>.",
    "version": "1.2.2",
    "maintainer": "",
    "url": "https://github.com/davidissamattos/bpcs",
    "exports": [],
    "topics": ["bayesian", "bradley-terry-model", "paired-comparisons", "stan"],
    "score": "NA",
    "stars": 12
  },
  {
    "id": 9607,
    "package_name": "bplsr",
    "title": "Bayesian partial least squares regression",
    "description": "Fits the Bayesian partial least squares regression model\n    introduced in Urbas et al. (2024) <doi:10.1214/24-AOAS1947>. Suitable\n    for univariate and multivariate regression with high-dimensional data.",
    "version": "1.0.4",
    "maintainer": "Szymon Urbas <szymon.urbas@mu.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9611,
    "package_name": "bpnreg",
    "title": "Bayesian Projected Normal Regression Models for Circular Data",
    "description": "Fitting Bayesian multiple and mixed-effect regression models for \n    circular data based on the projected normal distribution. Both continuous \n    and categorical predictors can be included. Sampling from the posterior is \n    performed via an MCMC algorithm. Posterior descriptives of all parameters, \n    model fit statistics and Bayes factors for hypothesis tests for inequality \n    constrained hypotheses are provided. See Cremers, Mulder & Klugkist (2018) \n    <doi:10.1111/bmsp.12108> and Nuñez-Antonio & Guttiérez-Peña (2014) \n    <doi:10.1016/j.csda.2012.07.025>.",
    "version": "2.0.3",
    "maintainer": "Jolien Cremers <joliencremers@gmail.com>",
    "url": "https://github.com/joliencremers/bpnreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9612,
    "package_name": "bpp",
    "title": "Computations Around Bayesian Predictive Power",
    "description": "Implements functions to update Bayesian Predictive Power Computations after not stopping a clinical trial at an interim analysis. Such an interim analysis can either be blinded or unblinded. Code is provided for Normally distributed endpoints with known variance, with a prominent example being the hazard ratio.",
    "version": "1.0.6",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9613,
    "package_name": "bpr",
    "title": "Fitting Bayesian Poisson Regression",
    "description": "Posterior sampling and inference for Bayesian Poisson regression models. The model specification makes use of Gaussian (or conditionally Gaussian) prior distributions on the regression coefficients. Details on the algorithm are found in D'Angelo and Canale (2023) <doi:10.1080/10618600.2022.2123337>.",
    "version": "1.0.8",
    "maintainer": "Laura D'Angelo <laura.dangelo@live.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9616,
    "package_name": "bqtl",
    "title": "Bayesian QTL Mapping Toolkit",
    "description": "QTL mapping toolkit for inbred crosses and recombinant\n        inbred lines. Includes maximum likelihood and Bayesian tools.",
    "version": "1.0-39",
    "maintainer": "Charles C. Berry <cberry@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9624,
    "package_name": "brainGraph",
    "title": "Graph Theory Analysis of Brain MRI Data",
    "description": "A set of tools for performing graph theory analysis of brain MRI",
    "version": "3.0.3",
    "maintainer": "",
    "url": "https://github.com/cwatson/brainGraph",
    "exports": [],
    "topics": ["brain-connectivity", "brain-imaging", "complex-networks", "connectome", "connectomics", "fmri", "graph-theory", "mri", "network-analysis", "neuroimaging", "neuroscience", "r", "statistics", "tractography"],
    "score": "NA",
    "stars": 191
  },
  {
    "id": 9633,
    "package_name": "brant",
    "title": "Test for Parallel Regression Assumption",
    "description": "Tests the parallel regression assumption wit the brant test by Brant (1990) <doi: 10.2307/2532457> for ordinal logit models generated with the function polr() from the package 'MASS'.",
    "version": "0.3-0",
    "maintainer": "Benjamin Schlegel <kontakt@benjaminschlegel.ch>",
    "url": "https://benjaminschlegel.ch/r/brant/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9636,
    "package_name": "bravo",
    "title": "Bayesian Screening and Variable Selection",
    "description": "Performs Bayesian variable screening and selection for ultra-high dimensional linear regression models.",
    "version": "3.2.2",
    "maintainer": "Dongjin Li <liyangxiaobei@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9638,
    "package_name": "brea",
    "title": "Bayesian Recurrent Events Analysis",
    "description": "Functions to produce MCMC samples for posterior inference in semiparametric Bayesian discrete time competing risks recurrent events models and multistate models.",
    "version": "0.4.2",
    "maintainer": "Adam J King <king@cpp.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9642,
    "package_name": "breakfast",
    "title": "Methods for Fast Multiple Change-Point/Break-Point Detection and\nEstimation",
    "description": "A developing software suite for multiple change-point and change-point-type feature detection/estimation (data segmentation) in data sequences.",
    "version": "2.5",
    "maintainer": "Yining Chen <y.chen101@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9644,
    "package_name": "breathtestcore",
    "title": "Core Functions to Read and Fit 13c Time Series from Breath Tests",
    "description": "Reads several formats of 13C data (IRIS/Wagner,\n    BreathID) and CSV.  Creates artificial sample data for testing.  Fits\n    Maes/Ghoos, Bluck-Coward self-correcting formula using 'nls', 'nlme'.\n    Methods to fit breath test curves with Bayesian Stan methods are\n    refactored to package 'breathteststan'. For a Shiny GUI, see package\n    'dmenne/breathtestshiny' on github.",
    "version": "0.8.10",
    "maintainer": "Dieter Menne <dieter.menne@menne-biomed.de>",
    "url": "https://github.com/dmenne/breathtestcore,",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9646,
    "package_name": "bregr",
    "title": "Easy and Efficient Batch Processing of Regression Models",
    "description": "Easily processes batches of univariate or multivariate\n    regression models. Returns results in a tidy format and generates\n    visualization plots for straightforward interpretation (Wang,\n    Shixiang, et al. (2025) <DOI:10.1002/mdr2.70028>).",
    "version": "1.3.1",
    "maintainer": "Shixiang Wang <w_shixiang@163.com>",
    "url": "https://github.com/WangLabCSU/bregr,\nhttps://wanglabcsu.github.io/bregr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9649,
    "package_name": "brfinance",
    "title": "Simplified Access to Brazilian Financial and Macroeconomic Data",
    "description": "It offers simplified access to Brazilian macroeconomic and financial indicators selected from official sources, such as the 'IBGE' (Brazilian Institute of Geography and Statistics) via the 'SIDRA' API and the 'Central Bank of Brazil' via the 'SGS' API. It allows users to quickly retrieve and visualize data series such as the unemployment rate and the Selic interest rate. This package was developed for data access and visualization purposes, without generating forecasts or statistical results. For more information, see the official APIs: <https://sidra.ibge.gov.br/> and <https://dadosabertos.bcb.gov.br/dataset/>.",
    "version": "0.6.0",
    "maintainer": "João Paulo dos Santos Pereira Barbosa <joao.31582129@gmail.com>",
    "url": "https://github.com/efram2/brfinance",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9657,
    "package_name": "bridgesampling",
    "title": "Bridge Sampling for Marginal Likelihoods and Bayes Factors",
    "description": "Provides functions for estimating marginal likelihoods, Bayes\n    factors, posterior model probabilities, and normalizing constants in general,\n    via different versions of bridge sampling (Meng & Wong, 1996, \n    <https://www3.stat.sinica.edu.tw/statistica/j6n4/j6n43/j6n43.htm>).\n    Gronau, Singmann, & Wagenmakers (2020) <doi:10.18637/jss.v092.i10>.",
    "version": "1.2-1",
    "maintainer": "Quentin F. Gronau <Quentin.F.Gronau@gmail.com>",
    "url": "https://github.com/quentingronau/bridgesampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9658,
    "package_name": "bridgr",
    "title": "Bridging Data Frequencies for Timely Economic Forecasts",
    "description": "Implements bridge models for nowcasting and forecasting macroeconomic variables by linking high-frequency indicator variables (e.g., monthly data) to low-frequency target variables (e.g., quarterly GDP). Simplifies forecasting and aggregating indicator variables to match the target frequency, enabling timely predictions ahead of official data releases. For more on bridge models, see Baffigi, A., Golinelli, R., & Parigi, G. (2004) <doi:10.1016/S0169-2070(03)00067-0>, Burri (2023) <https://www5.unine.ch/RePEc/ftp/irn/pdfs/WP23-02.pdf> or Schumacher (2016) <doi:10.1016/j.ijforecast.2015.07.004>. ",
    "version": "0.1.1",
    "maintainer": "Marc Burri <marc.burri91@gmail.com>",
    "url": "https://github.com/marcburri/bridgr,\nhttps://marcburri.github.io/bridgr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9660,
    "package_name": "brinla",
    "title": "Bayesian Regression with INLA",
    "description": "Data and code to support a book regarding Bayesian",
    "version": "0.1.0",
    "maintainer": "Julian Faraway <julian.faraway@gmail.com>",
    "url": "https://github.com/julianfaraway/brinla",
    "exports": [],
    "topics": ["bayesian", "inference", "r", "r-package", "rstats"],
    "score": "NA",
    "stars": 52
  },
  {
    "id": 9664,
    "package_name": "brisk",
    "title": "Bayesian Benefit Risk Analysis",
    "description": "Quantitative methods for benefit-risk analysis help to condense\n    complex decisions into a univariate metric describing the overall benefit\n    relative to risk.  One approach is to use the multi-criteria decision\n    analysis framework (MCDA), as in Mussen, Salek, and Walker\n    (2007) <doi:10.1002/pds.1435>.  Bayesian benefit-risk\n    analysis incorporates uncertainty through posterior distributions which are\n    inputs to the benefit-risk framework.  The brisk package provides functions\n    to assist with Bayesian benefit-risk analyses, such as MCDA.\n    Users input posterior samples, utility functions, weights, and the package\n    outputs quantitative benefit-risk scores.  The posterior of the benefit-risk\n    scores for each group can be compared.  Some plotting capabilities are also\n    included.",
    "version": "0.1.0",
    "maintainer": "Richard Payne <paynestatistics@gmail.com>",
    "url": "https://rich-payne.github.io/brisk/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9666,
    "package_name": "brm",
    "title": "Binary Regression Model",
    "description": "Fits novel models for the conditional relative risk, risk difference and odds ratio <doi:10.1080/01621459.2016.1192546>.",
    "version": "1.1.1",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "url": "http://github.com/mclements/brm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9669,
    "package_name": "brmsmargins",
    "title": "Bayesian Marginal Effects for 'brms' Models",
    "description": "Calculate Bayesian marginal effects, average marginal effects, and marginal coefficients (also called population averaged coefficients) for models fit using the 'brms' package including fixed effects, mixed effects, and location scale models. These are based on marginal predictions that integrate out random effects if necessary (see for example <doi:10.1186/s12874-015-0046-6> and <doi:10.1111/biom.12707>).",
    "version": "0.2.1",
    "maintainer": "Joshua F. Wiley <jwiley.psych@gmail.com>",
    "url": "https://joshuawiley.com/brmsmargins/,\nhttps://github.com/JWiley/brmsmargins",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9674,
    "package_name": "brokenstick",
    "title": "Broken Stick Model for Irregular Longitudinal Data",
    "description": "Data on multiple individuals through time are often sampled at \n    times that differ between persons. Irregular observation times can severely \n    complicate the statistical analysis of the data. The broken stick model \n    approximates each subject’s trajectory by one or more connected line segments. \n    The times at which segments connect (breakpoints) are identical for all \n    subjects and under control of the user. A well-fitting broken stick model \n    effectively transforms individual measurements made at irregular times into \n    regular trajectories with common observation times. Specification of the \n    model requires three variables: time, measurement and subject. The \n    model is a special case of the linear mixed model, with time as a linear \n    B-spline and subject as the grouping factor. The main assumptions are: \n    subjects are exchangeable, trajectories between consecutive breakpoints are \n    straight, random effects follow a multivariate normal distribution, and \n    unobserved data are missing at random. The package contains functions for \n    fitting the broken stick model to data, for predicting curves in new data \n    and for plotting broken stick estimates. The package supports two \n    optimization methods, and includes options to structure the \n    variance-covariance matrix of the random effects. The analyst may use the \n    software to smooth growth curves by a series of connected straight lines, to \n    align irregularly observed curves to a common time grid, to create synthetic \n    curves at a user-specified set of breakpoints, to estimate the time-to-time \n    correlation matrix and to predict future observations. See \n    <doi:10.18637/jss.v106.i07> for additional documentation on background, \n    methodology and applications.",
    "version": "2.6.0",
    "maintainer": "Stef van Buuren <stef.vanbuuren@tno.nl>",
    "url": "doi:10.18637/jss.v106.i07, https://growthcharts.org/brokenstick/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9675,
    "package_name": "brolgar",
    "title": "Browse Over Longitudinal Data Graphically and Analytically in R",
    "description": "Provides a framework of tools to summarise, visualise, and explore \n  longitudinal data. It builds upon the tidy time series data frames used in the\n  'tsibble' package, and is designed to integrate within the 'tidyverse', and\n  'tidyverts' (for time series) ecosystems. The methods implemented include \n  calculating features for understanding longitudinal data, including \n  calculating summary statistics such as quantiles, medians, and numeric ranges,\n  sampling individual series, identifying individual series representative of a \n  group, and extending the facet system  in 'ggplot2' to facilitate exploration of samples of data. These methods are\n  fully described in the paper \"brolgar: An R package to Browse Over \n  Longitudinal Data Graphically and Analytically in R\", Nicholas Tierney, \n  Dianne Cook, Tania Prvan (2020) <doi:10.32614/RJ-2022-023>.",
    "version": "1.0.2",
    "maintainer": "Nicholas Tierney <nicholas.tierney@gmail.com>",
    "url": "https://github.com/njtierney/brolgar,\nhttps://brolgar.njtierney.com/, http://brolgar.njtierney.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9678,
    "package_name": "broom.helpers",
    "title": "Helpers for Model Coefficients Tibbles",
    "description": "Provides suite of functions to work with regression model\n    'broom::tidy()' tibbles.  The suite includes functions to group\n    regression model terms by variable, insert reference and header rows\n    for categorical variables, add variable labels, and more.",
    "version": "1.22.0",
    "maintainer": "Joseph Larmarange <joseph@larmarange.net>",
    "url": "https://larmarange.github.io/broom.helpers/,\nhttps://github.com/larmarange/broom.helpers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9679,
    "package_name": "broom.mixed",
    "title": "Tidying Methods for Mixed Models",
    "description": "Convert fitted objects from various R mixed-model packages\n    into tidy data frames along the lines of the 'broom' package.\n    The package provides three\n    S3 generics for each model: tidy(), which summarizes a model's statistical findings such as\n    coefficients of a regression; augment(), which adds columns to the original\n    data such as predictions, residuals and cluster assignments; and glance(), which\n    provides a one-row summary of model-level statistics.",
    "version": "0.2.9.6",
    "maintainer": "Ben Bolker <bolker@mcmaster.ca>",
    "url": "https://github.com/bbolker/broom.mixed",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9684,
    "package_name": "bruceR",
    "title": "Broadly Useful Convenient and Efficient R Functions",
    "description": "",
    "version": "2025.11",
    "maintainer": "Han Wu Shuang Bao <baohws@foxmail.com>",
    "url": "https://github.com/psychbruce/bruceR",
    "exports": [],
    "topics": ["anova", "cran", "data-analysis", "data-science", "linear-models", "linear-regression", "multilevel-models", "r", "r-package", "rstats", "statistics", "toolbox"],
    "score": "NA",
    "stars": 190
  },
  {
    "id": 9688,
    "package_name": "brxx",
    "title": "Bayesian Test Reliability Estimation",
    "description": "When samples contain missing data, are small, or are suspected of bias,\n    estimation of scale reliability may not be trustworthy.  A recommended solution\n    for this common problem has been Bayesian model estimation.  Bayesian methods\n    rely on user specified information from historical data or researcher intuition \n    to more accurately estimate the parameters.  This package provides a user friendly\n    interface for estimating test reliability.  Here, reliability is modeled as a beta\n    distributed random variable with shape parameters alpha=true score variance and\n    beta=error variance (Tanzer & Harlow, 2020) <doi:10.1080/00273171.2020.1854082>.",
    "version": "0.1.2",
    "maintainer": "Joshua Ray Tanzer <jtanzer@lifespan.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9692,
    "package_name": "bsam",
    "title": "Bayesian State-Space Models for Animal Movement",
    "description": "Tools to fit Bayesian state-space models to animal tracking data. Models are provided for location \n    filtering, location filtering and behavioural state estimation, and their hierarchical versions. \n    The models are primarily intended for fitting to ARGOS satellite tracking data but options exist to fit \n    to other tracking data types. For Global Positioning System data, consider the 'moveHMM' package. \n    Simplified Markov Chain Monte Carlo convergence diagnostic plotting is provided but users are encouraged \n    to explore tools available in packages such as 'coda' and 'boa'.",
    "version": "1.1.3",
    "maintainer": "Ian Jonsen <ian.jonsen@mq.edu.au>",
    "url": "<https://github.com/ianjonsen/bsam>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9693,
    "package_name": "bsamGP",
    "title": "Bayesian Spectral Analysis Models using Gaussian Process Priors",
    "description": "Contains functions to perform Bayesian inference\n    using a spectral analysis of Gaussian process priors.\n    Gaussian processes are represented with a Fourier series \n    based on cosine basis functions. Currently the package\n    includes parametric linear models, partial linear additive\n    models with/without shape restrictions, generalized linear\n    additive models with/without shape restrictions, and  \n    density estimation model. To maximize computational \n    efficiency, the actual Markov chain Monte Carlo sampling \n    for each model is done using codes written in FORTRAN 90.\n    This software has been developed using funding supported by\n    Basic Science Research Program through the National Research\n    Foundation of Korea (NRF) funded by the Ministry of Education\n    (no. NRF-2016R1D1A1B03932178 and no. NRF-2017R1D1A3B03035235).",
    "version": "1.2.7",
    "maintainer": "Beomjo Park <beomjop@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9697,
    "package_name": "bshazard",
    "title": "Nonparametric Smoothing of the Hazard Function",
    "description": "The function estimates the hazard function non parametrically from a survival object (possibly adjusted for covariates). \n\t     The smoothed estimate is based on B-splines from the perspective of generalized linear mixed models. Left truncated and right censoring data are allowed. \n\t     The package is based on the work in Rebora P (2014) <doi:10.32614/RJ-2014-028>.",
    "version": "1.2",
    "maintainer": "Paola Rebora <paola.rebora@unimib.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9703,
    "package_name": "bspcov",
    "title": "Bayesian Sparse Estimation of a Covariance Matrix",
    "description": "Bayesian estimations of a covariance matrix for multivariate \n        normal data. Assumes that the covariance matrix is sparse or band \n        matrix and positive-definite. Methods implemented include the beta-mixture \n        shrinkage prior (Lee et al. (2022) <doi:10.1016/j.jmva.2022.105067>), \n        screened beta-mixture prior (Lee et al. (2024) <doi:10.1214/24-BA1495>), \n        and post-processed posteriors for banded and sparse covariances \n        (Lee et al. (2023) <doi:10.1214/22-BA1333>; Lee and Lee (2023) \n        <doi:10.1016/j.jeconom.2023.105475>). This software has been developed using \n        funding supported by Basic Science Research Program through the National \n        Research Foundation of Korea ('NRF') funded by the Ministry of Education\n        ('RS-2023-00211979', 'NRF-2022R1A5A7033499', 'NRF-2020R1A4A1018207' \n        and 'NRF-2020R1C1C1A01013338').",
    "version": "1.0.3",
    "maintainer": "Kyeongwon Lee <kwlee1718@gmail.com>",
    "url": "https://github.com/statjs/bspcov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9704,
    "package_name": "bspec",
    "title": "Bayesian Spectral Inference",
    "description": "Bayesian inference on the (discrete) power spectrum of time series.",
    "version": "1.6",
    "maintainer": "Christian Roever <christian.roever@med.uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9705,
    "package_name": "bspline",
    "title": "B-Spline Interpolation and Regression",
    "description": "Build and use B-splines for interpolation and regression.\n  In case of regression, equality constraints as well as monotonicity\n  and/or positivity of B-spline weights can be imposed. Moreover, \n  knot positions can be on regular grid or be part of \n  optimized parameters too (in addition to the spline weights).\n  For this end, 'bspline' is able to calculate\n  Jacobian of basis vectors as function of knot positions. User is provided with \n  functions calculating spline values at arbitrary points. These \n  functions can be differentiated and integrated to obtain B-splines calculating \n  derivatives/integrals at any point. B-splines of this package can \n  simultaneously operate on a series of curves sharing the same set of \n  knots. 'bspline' is written with concern about computing \n  performance that's why the basis and Jacobian calculation is implemented in C++.\n  The rest is implemented in R but without notable impact on computing speed.",
    "version": "2.5.0",
    "maintainer": "Serguei Sokol <sokol@insa-toulouse.fr>",
    "url": "https://github.com/MathsCell/bspline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9708,
    "package_name": "bspmma",
    "title": "Bayesian Semiparametric Models for Meta-Analysis",
    "description": "The main functions carry out Gibbs' sampler routines for nonparametric and semiparametric Bayesian models for random effects meta-analysis.",
    "version": "0.1-2",
    "maintainer": "Deborah Burr <burr@stat.ufl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9710,
    "package_name": "bssbinom",
    "title": "Bayesian Sample Size for Binomial Proportions",
    "description": "Computation of the minimum sample size using the Average\n    Coverage Criterion or the Average Length Criterion for estimating\n    binomial proportions using beta prior distributions. For more details\n    see Costa (2025) <DOI:10.1007/978-3-031-72215-8_14>.",
    "version": "1.0.0",
    "maintainer": "Eliardo Costa <eliardo.costa@ufrn.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9714,
    "package_name": "bstrl",
    "title": "Bayesian Streaming Record Linkage",
    "description": "Perform record linkage on streaming files using recursive Bayesian updating.",
    "version": "1.0.2",
    "maintainer": "Ian Taylor <ian.taylor@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9715,
    "package_name": "bsts",
    "title": "Bayesian Structural Time Series",
    "description": "Time series regression using dynamic linear models fit using\n  MCMC. See Scott and Varian (2014) <DOI:10.1504/IJMMNO.2014.059942>, among many\n  other sources.",
    "version": "0.9.11",
    "maintainer": "Steven L. Scott <steve.the.bayesian@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9717,
    "package_name": "bsvarSIGNs",
    "title": "Bayesian SVARs with Sign, Zero, and Narrative Restrictions",
    "description": "Implements state-of-the-art algorithms for the Bayesian analysis of Structural Vector Autoregressions (SVARs) identified by sign, zero, and narrative restrictions. The core model is based on a flexible Vector Autoregression with estimated hyper-parameters of the Minnesota prior and the dummy observation priors as in Giannone, Lenza, Primiceri (2015) <doi:10.1162/REST_a_00483>. The sign restrictions are implemented employing the methods proposed by Rubio-Ramírez, Waggoner & Zha (2010) <doi:10.1111/j.1467-937X.2009.00578.x>, while identification through sign and zero restrictions follows the approach developed by Arias, Rubio-Ramírez, & Waggoner (2018) <doi:10.3982/ECTA14468>. Furthermore, our tool provides algorithms for identification via sign and narrative restrictions, in line with the methods introduced by Antolín-Díaz and Rubio-Ramírez (2018) <doi:10.1257/aer.20161852>. Users can also estimate a model with sign, zero, and narrative restrictions imposed at once. The package facilitates predictive and structural analyses using impulse responses, forecast error variance and historical decompositions, forecasting and conditional forecasting, as well as analyses of structural shocks and fitted values. All this is complemented by colourful plots, user-friendly summary functions, and comprehensive documentation including the vignette by Wang & Woźniak (2024) <doi:10.48550/arXiv.2501.16711>. The 'bsvarSIGNs' package is aligned regarding objects, workflows, and code structure with the R package 'bsvars' by Woźniak (2024) <doi:10.32614/CRAN.package.bsvars>, and they constitute an integrated toolset. It was granted the Di Cook Open-Source Statistical Software Award by the Statistical Society of Australia in 2024.",
    "version": "2.0",
    "maintainer": "Xiaolei Wang <adamwang15@gmail.com>",
    "url": "https://bsvars.org/bsvarSIGNs/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9718,
    "package_name": "bsvars",
    "title": "Bayesian Estimation of Structural Vector Autoregressive Models",
    "description": "Provides fast and efficient procedures for Bayesian analysis of Structural Vector Autoregressions. This package estimates a wide range of models, including homo-, heteroskedastic, and non-normal specifications. Structural models can be identified by adjustable exclusion restrictions, time-varying volatility, or non-normality. They all include a flexible three-level equation-specific local-global hierarchical prior distribution for the estimated level of shrinkage for autoregressive and structural parameters. Additionally, the package facilitates predictive and structural analyses such as impulse responses, forecast error variance and historical decompositions, forecasting, verification of heteroskedasticity, non-normality, and hypotheses on autoregressive parameters, as well as analyses of structural shocks, volatilities, and fitted values. Beautiful plots, informative summary functions, and extensive documentation including the vignette by Woźniak (2024) <doi:10.48550/arXiv.2410.15090> complement all this. The implemented techniques align closely with those presented in Lütkepohl, Shang, Uzeda, & Woźniak (2024) <doi:10.48550/arXiv.2404.11057>, Lütkepohl & Woźniak (2020) <doi:10.1016/j.jedc.2020.103862>, and Song & Woźniak (2021) <doi:10.1093/acrefore/9780190625979.013.174>. The 'bsvars' package is aligned regarding objects, workflows, and code structure with the R package 'bsvarSIGNs' by Wang & Woźniak (2024) <doi:10.32614/CRAN.package.bsvarSIGNs>, and they constitute an integrated toolset.",
    "version": "3.2",
    "maintainer": "Tomasz Woźniak <wozniak.tom@pm.me>",
    "url": "https://bsvars.org/bsvars/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9719,
    "package_name": "bsynth",
    "title": "Bayesian Synthetic Control",
    "description": "Implements the Bayesian Synthetic Control method for causal inference\n    in comparative case studies. This package provides tools for estimating\n    treatment effects in settings with a single treated unit and multiple control\n    units, allowing for uncertainty quantification and flexible modeling of\n    time-varying effects. The methodology is based on the paper by Vives and\n    Martinez (2022) <doi:10.48550/arXiv.2206.01779>.",
    "version": "1.0",
    "maintainer": "Ignacio Martinez <ignacio@martinez.fyi>",
    "url": "https://github.com/google/bsynth, https://arxiv.org/abs/2206.01779",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9729,
    "package_name": "buildmer",
    "title": "Stepwise Elimination and Term Reordering for Mixed-Effects\nRegression",
    "description": "Finds the largest possible regression model that will still converge\n    for various types of regression analyses (including mixed models and generalized\n    additive models) and then optionally performs stepwise elimination similar to the\n    forward and backward effect-selection methods in SAS, based on the change in\n    log-likelihood or its significance, Akaike's Information Criterion, the Bayesian\n    Information Criterion, the explained deviance, or the F-test of the change in R².",
    "version": "2.12",
    "maintainer": "Cesko C. Voeten <cvoeten@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9731,
    "package_name": "bujar",
    "title": "Buckley-James Regression for Survival Data with High-Dimensional\nCovariates",
    "description": "Buckley-James regression for right-censoring survival data with high-dimensional covariates. Implementations for survival data include boosting with componentwise linear least squares, componentwise smoothing splines, regression trees and MARS. Other high-dimensional tools include penalized regression for survival data. See Wang and Wang (2010) <doi:10.2202/1544-6115.1550>.",
    "version": "0.2-11",
    "maintainer": "Zhu Wang <zwang145@uthsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9734,
    "package_name": "bulletcp",
    "title": "Automatic Groove Identification via Bayesian Changepoint\nDetection",
    "description": "Provides functionality to automatically detect groove locations via a Bayesian changepoint detection method to be used in the data preprocessing step\n    of forensic bullet matching algorithms. The methods in this package are based on those in Stephens (1994) <doi:10.2307/2986119>. Bayesian changepoint detection will simply be an option\n    in the function from the package 'bulletxtrctr' which identifies the groove locations.",
    "version": "1.0.0",
    "maintainer": "Nathaniel Garton <nate.garton13@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9743,
    "package_name": "bundesbank",
    "title": "Download Data from Bundesbank",
    "description": "Download data from the time-series\n  databases of the Bundesbank, the German central\n  bank. See the overview at the Bundesbank website\n  (<https://www.bundesbank.de/en/statistics/time-series-databases>)\n  for available series. The package provides only a\n  single function, getSeries(), which supports both\n  traditional and real-time datasets; it will also\n  download meta data if available. Downloaded data\n  can automatically be arranged in various formats,\n  such as data frames or 'zoo' series. The data\n  may optionally be cached, so as to avoid repeated\n  downloads of the same series.",
    "version": "0.1-12",
    "maintainer": "Enrico Schumann <es@enricoschumann.net>",
    "url": "http://enricoschumann.net/R/packages/bundesbank/index.htm,\nhttps://github.com/enricoschumann/bundesbank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9746,
    "package_name": "bunsen",
    "title": "Marginal Survival Estimation with Covariate Adjustment",
    "description": "Provides an efficient and robust implementation for estimating marginal Hazard Ratio (HR) and \n  Restricted Mean Survival Time (RMST) with covariate adjustment using Daniel et al. (2021) <doi:10.1002/bimj.201900297> and \n  Karrison et al. (2018) <doi:10.1177/1740774518759281>.",
    "version": "0.1.0",
    "maintainer": "Xinlei Deng <xinlei.deng@novartis.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9759,
    "package_name": "bvarsv",
    "title": "Bayesian Analysis of a Vector Autoregressive Model with\nStochastic Volatility and Time-Varying Parameters",
    "description": "R/C++ implementation of the model proposed by Primiceri (\"Time Varying Structural Vector Autoregressions and Monetary Policy\", Review of Economic Studies, 2005), with functionality for computing posterior predictive distributions and impulse responses.",
    "version": "1.1",
    "maintainer": "Fabian Krueger <Fabian.Krueger83@gmail.com>",
    "url": "https://sites.google.com/site/fk83research/code",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9766,
    "package_name": "bws",
    "title": "Bayesian Weighted Sums",
    "description": "An interface to the Bayesian Weighted Sums model implemented in 'RStan'.\n    It estimates the summed effect of multiple, often moderately to highly correlated,\n    continuous predictors. Its applications can be found in analysis of exposure mixtures.\n    The model was proposed by Hamra, Maclehose, Croen, Kauffman, and\n    Newschaffer (2021) <doi:10.3390/ijerph18041373>.\n    This implementation includes an extension to model binary outcome.",
    "version": "0.1.0",
    "maintainer": "Phuc H. Nguyen <phuc.nguyen.rcran@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9771,
    "package_name": "c060",
    "title": "Extended Inference for Lasso and Elastic-Net Regularized Cox and\nGeneralized Linear Models",
    "description": "The c060 package provides additional functions to perform stability selection, model validation and parameter tuning for glmnet models.",
    "version": "0.4-0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://github.com/fbertran/c060/,\nhttps://fbertran.github.io/c060/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9786,
    "package_name": "cSEM",
    "title": "Composite-Based Structural Equation Modeling",
    "description": "Estimate, assess, test, and study linear, nonlinear, hierarchical \n  and multigroup structural equation models using composite-based approaches \n  and procedures, including estimation techniques such as partial least squares \n  path modeling (PLS-PM) and its derivatives (PLSc, ordPLSc, robustPLSc), \n  generalized structured component analysis (GSCA), generalized structured \n  component analysis with uniqueness terms (GSCAm), generalized canonical \n  correlation analysis (GCCA), principal component analysis (PCA), \n  factor score regression (FSR) using sum score, regression or \n  Bartlett scores (including bias correction using Croon’s approach), \n  as well as several tests and typical postestimation procedures \n  (e.g., verify admissibility of the estimates, assess the model fit, \n  test the model fit etc.).",
    "version": "0.6.1",
    "maintainer": "Florian Schuberth <f.schuberth@utwente.nl>",
    "url": "https://github.com/FloSchuberth/cSEM/,\nhttps://floschuberth.github.io/cSEM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9818,
    "package_name": "calibrator",
    "title": "Bayesian Calibration of Complex Computer Codes",
    "description": "Performs Bayesian calibration of computer models as per\n Kennedy and O'Hagan 2001.  The package includes routines to find the\n hyperparameters and parameters; see the help page for stage1() for a\n worked example using the toy dataset.  A tutorial is provided in the\n calex.Rnw vignette; and a suite of especially simple one dimensional\n examples appears in inst/doc/one.dim/.",
    "version": "1.2-8",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "url": "https://github.com/RobinHankin/calibrator.git",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9866,
    "package_name": "carSurv",
    "title": "Correlation-Adjusted Regression Survival (CARS) Scores",
    "description": "Contains functions to estimate the Correlation-Adjusted Regression Survival (CARS) Scores. The method is described in Welchowski, T. and Zuber, V. and Schmid, M., (2018), Correlation-Adjusted Regression Survival Scores for High-Dimensional Variable Selection, <arXiv:1802.08178>.",
    "version": "1.0.0",
    "maintainer": "Thomas Welchowski <welchow@imbie.meb.uni-bonn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9868,
    "package_name": "caradpt",
    "title": "Covariate-Adjusted Response-Adaptive Designs for Clinical Trials",
    "description": "Tools for implementing covariate-adjusted response-adaptive procedures for binary, continuous and survival responses. Users can flexibly choose between two functions based on their specific needs for each procedure: use real patient data from clinical trials to compute allocation probabilities directly, or use built-in simulation functions to generate synthetic patient data. Detailed methodologies and algorithms used in this package are described in the following references:\n    Zhang, L. X., Hu, F., Cheung, S. H., & Chan, W. S. (2007)<doi:10.1214/009053606000001424>\n    Zhang, L. X. & Hu, F. (2009) <doi:10.1007/s11766-009-0001-6>\n    Hu, J., Zhu, H., & Hu, F. (2015) <doi:10.1080/01621459.2014.903846>\n    Zhao, W., Ma, W., Wang, F., & Hu, F. (2022) <doi:10.1002/pst.2160>\n    Mukherjee, A., Jana, S., & Coad, S. (2024) <doi:10.1177/09622802241287704>.",
    "version": "0.1.0",
    "maintainer": "Renjie Luo <jerry.luorj@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9870,
    "package_name": "carbondate",
    "title": "Calibration and Summarisation of Radiocarbon Dates",
    "description": "\n  Performs Bayesian non-parametric calibration of multiple related radiocarbon determinations, and summarises the calendar age information to plot their joint calendar age density (see Heaton (2022) <doi:10.1111/rssc.12599>). \n  Also models the occurrence of radiocarbon samples as a variable-rate (inhomogeneous) Poisson process, plotting the posterior estimate for the occurrence rate of the samples over calendar time, and providing information about potential change points.",
    "version": "1.1.0",
    "maintainer": "Timothy J Heaton <T.Heaton@leeds.ac.uk>",
    "url": "https://github.com/TJHeaton/carbondate,\nhttps://tjheaton.github.io/carbondate/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9873,
    "package_name": "carcass",
    "title": "Estimation of the Number of Fatalities from Carcass Searches",
    "description": "The number of bird or bat fatalities from collisions with buildings, towers or wind energy turbines can be estimated based on carcass searches and experimentally assessed carcass persistence times and searcher efficiency. Functions for estimating the probability that a bird or bat that died is found by a searcher are provided. Further functions calculate the posterior distribution of the number of fatalities based on the number of carcasses found and the estimated detection probability.",
    "version": "1.9",
    "maintainer": "Fraenzi Korner-Nievergelt <fraenzi.korner@oikostat.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9881,
    "package_name": "care",
    "title": "High-Dimensional Regression and CAR Score Variable Selection",
    "description": "Implements the regression approach \n  of Zuber and Strimmer (2011) \"High-dimensional regression and variable \n  selection using CAR scores\" SAGMB 10: 34, <DOI:10.2202/1544-6115.1730>.\n  CAR scores measure the correlation between the response and the \n  Mahalanobis-decorrelated  predictors.  The squared CAR score is a \n  natural measure of variable importance and provides a canonical \n  ordering of variables. This package provides functions for estimating \n  CAR scores, for variable selection using CAR scores, and for estimating \n  corresponding regression coefficients. Both shrinkage as well as \n  empirical estimators are available.",
    "version": "1.1.11",
    "maintainer": "Korbinian Strimmer <strimmerlab@gmail.com>",
    "url": "https://strimmerlab.github.io/software/care/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9889,
    "package_name": "carfima",
    "title": "Continuous-Time Fractionally Integrated ARMA Process for\nIrregularly Spaced Long-Memory Time Series Data",
    "description": "We provide a toolbox to fit a continuous-time fractionally integrated ARMA process (CARFIMA) on univariate and irregularly spaced time series data via both frequentist and Bayesian machinery. A general-order CARFIMA(p, H, q) model for p>q is specified in Tsai and Chan (2005) <doi:10.1111/j.1467-9868.2005.00522.x> and it involves p+q+2 unknown model parameters, i.e., p AR parameters, q MA parameters, Hurst parameter H, and process uncertainty (standard deviation) sigma. Also, the model can account for heteroscedastic measurement errors, if the information about measurement error standard deviations is known. The package produces their maximum likelihood estimates and asymptotic uncertainties using a global optimizer called the differential evolution algorithm. It also produces posterior samples of the model parameters via Metropolis-Hastings within a Gibbs sampler equipped with adaptive Markov chain Monte Carlo. These fitting procedures, however, may produce numerical errors if p>2. The toolbox also contains a function to simulate discrete time series data from CARFIMA(p, H, q) process given the model parameters and observation times. ",
    "version": "2.0.2",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9903,
    "package_name": "carts",
    "title": "Simulation-Based Assessment of Covariate Adjustment in\nRandomized Trials",
    "description": "Monte Carlo simulation framework for different randomized clinical\n    trial designs with a special emphasis on estimators based on covariate\n    adjustment. The package implements regression-based covariate adjustment\n    (Rosenblum & van der Laan (2010) <doi:10.2202/1557-4679.1138>) and a\n    one-step estimator (Van Lancker et al (2024)\n    <doi:10.48550/arXiv.2404.11150>) for trials with continuous, binary and\n    count outcomes. The estimation of the minimum sample-size required to reach\n    a specified statistical power for a given estimator uses bisection to find\n    an initial rough estimate, followed by stochastic approximation\n    (Robbins-Monro (1951) <doi:10.1214/aoms/1177729586>) to improve the\n    estimate, and finally, a grid search to refine the estimate in the\n    neighborhood of the current best solution.",
    "version": "0.1.0",
    "maintainer": "Benedikt Sommer <benediktsommer92@gmail.com>",
    "url": "https://novonordisk-opensource.github.io/carts/,\nhttps://github.com/NovoNordisk-OpenSource/carts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9929,
    "package_name": "catdata",
    "title": "Categorical Data",
    "description": "This R-package contains examples from the book \"Regression for Categorical Data\", Tutz 2012, Cambridge University Press. The names of the examples refer to the chapter and the data set that is used. ",
    "version": "1.2.5",
    "maintainer": "Gunther Schauberger <gunther.schauberger@tum.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9946,
    "package_name": "causact",
    "title": "Fast, Easy, and Visual Bayesian Inference",
    "description": "Accelerate Bayesian analytics workflows in 'R' through interactive modelling,\n    visualization, and inference. Define probabilistic graphical models using directed\n    acyclic graphs (DAGs) as a unifying language for business stakeholders, statisticians, \n    and programmers. This package relies on interfacing with the 'numpyro' python package. ",
    "version": "0.6.0",
    "maintainer": "Adam Fleischhacker <ajf@udel.edu>",
    "url": "https://github.com/flyaflya/causact, https://www.causact.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9947,
    "package_name": "causal.decomp",
    "title": "Causal Decomposition Analysis",
    "description": "We implement causal decomposition analysis using methods proposed by Park, Lee, and Qin (2022) and Park, Kang, and Lee (2023), which provide researchers with multiple-mediator imputation, single-mediator imputation, and product-of-coefficients regression approaches to estimate the initial disparity, disparity reduction, and disparity remaining (<doi:10.1177/00491241211067516>; <doi:10.1177/00811750231183711>). We also implement sensitivity analysis for causal decomposition using R-squared values as sensitivity parameters (Park, Kang, Lee, and Ma, 2023 <doi:10.1515/jci-2022-0031>). Finally, we include individualized causal decomposition and sensitivity analyses proposed by Park, Kang, and Lee (2025+) <doi:10.48550/arXiv.2506.19010>.",
    "version": "0.2.0",
    "maintainer": "Suyeon Kang <suyeon.kang@ucf.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9955,
    "package_name": "causalQual",
    "title": "Causal Inference for Qualitative Outcomes",
    "description": "Implements the framework introduced in Di Francesco and Mellace (2025) <doi:10.48550/arXiv.2502.11691>, shifting the focus to well-defined and interpretable \n    estimands that quantify how treatment affects the probability distribution over outcome categories. It supports selection-on-observables, instrumental variables, \n    regression discontinuity, and difference-in-differences designs.",
    "version": "1.0.0",
    "maintainer": "Riccardo Di Francesco <difrancesco.riccardo96@gmail.com>",
    "url": "https://riccardo-df.github.io/causalQual/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9956,
    "package_name": "causalSLSE",
    "title": "Semiparametric Least Squares Inference for Causal Effects",
    "description": "Several causal effects are measured using least squares regressions and basis function approximations. Backward and forward selection methods based on different criteria are used to select the basis functions.",
    "version": "0.4-1",
    "maintainer": "Pierre Chausse Developer <pchausse@uwaterloo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9958,
    "package_name": "causaldata",
    "title": "Example Data Sets for Causal Inference Textbooks",
    "description": "Example data sets to run the example\n    problems from causal inference textbooks. Currently, contains data\n    sets for Huntington-Klein, Nick (2021 and 2025) \"The Effect\" <https://theeffectbook.net>, first and second edition,\n    Cunningham, Scott (2021 and 2025, ISBN-13: 978-0-300-25168-5) \"Causal Inference: The Mixtape\", \n    and Hernán, Miguel and James Robins (2020) \"Causal Inference: What If\" \n    <https://www.hsph.harvard.edu/miguel-hernan/causal-inference-book/>.",
    "version": "0.1.4",
    "maintainer": "Nick Huntington-Klein <nhuntington-klein@seattleu.edu>",
    "url": "https://github.com/NickCH-K/causaldata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9959,
    "package_name": "causaldrf",
    "title": "Estimating Causal Dose Response Functions",
    "description": "Functions and data to estimate causal dose response functions given continuous, ordinal, or binary treatments.  A description of the methods is given in Galagate (2016) <https://drum.lib.umd.edu/handle/1903/18170>.",
    "version": "0.4.2",
    "maintainer": "Douglas Galagate <galagated@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9962,
    "package_name": "causaloptim",
    "title": "An Interface to Specify Causal Graphs and Compute Bounds on\nCausal Effects",
    "description": "When causal quantities are not identifiable from the observed data, it still may be possible \n            to bound these quantities using the observed data. We outline a class of problems for which the \n            derivation of tight bounds is always a linear programming problem and can therefore, at least \n            theoretically, be solved using a symbolic linear optimizer. We extend and generalize the \n            approach of Balke and Pearl (1994) <doi:10.1016/B978-1-55860-332-5.50011-0> and we provide \n            a user friendly graphical interface for setting up such problems via directed acyclic \n            graphs (DAG), which only allow for problems within this class to be depicted. The user can \n            then define linear constraints to further refine their assumptions to meet their specific \n            problem, and then specify a causal query using a text interface. The program converts this \n            user defined DAG, query, and constraints, and returns tight bounds. The bounds can be \n            converted to R functions to evaluate them for specific datasets, and to latex code for \n            publication. The methods and proofs of tightness and validity of the bounds are described in\n            a paper by Sachs, Jonzon, Gabriel, and Sjölander (2022) \n            <doi:10.1080/10618600.2022.2071905>.",
    "version": "1.0.0",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "url": "https://sachsmc.github.io/causaloptim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9964,
    "package_name": "causalsens",
    "title": "Selection Bias Approach to Sensitivity Analysis for Causal\nEffects",
    "description": "The causalsens package provides functions to perform sensitivity analyses and to study how various assumptions about selection bias affects estimates of causal effects.",
    "version": "0.1.3",
    "maintainer": "Matthew Blackwell <mblackwell@gov.harvard.edu>",
    "url": "https://www.mattblackwell.org/software/causalsens/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9965,
    "package_name": "causalweight",
    "title": "Estimation Methods for Causal Inference Based on Inverse\nProbability Weighting and Doubly Robust Estimation",
    "description": "Various estimators of causal effects based on inverse probability weighting, doubly robust estimation, and double machine learning. Specifically, the package includes methods for estimating average treatment effects, direct and indirect effects in causal mediation analysis, and dynamic treatment effects. The models refer to studies of Froelich (2007) <doi:10.1016/j.jeconom.2006.06.004>, Huber (2012) <doi:10.3102/1076998611411917>, Huber (2014) <doi:10.1080/07474938.2013.806197>, Huber (2014) <doi:10.1002/jae.2341>, Froelich and Huber (2017) <doi:10.1111/rssb.12232>, Hsu, Huber, Lee, and Lettry (2020)  <doi:10.1002/jae.2765>, and others.",
    "version": "1.1.4",
    "maintainer": "Hugo Bodory <hugo.bodory@unisg.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9977,
    "package_name": "cbl",
    "title": "Causal Discovery under a Confounder Blanket",
    "description": "Methods for learning causal relationships among a set of\n    foreground variables X based on signals from a (potentially much\n    larger) set of background variables Z, which are known non-descendants\n    of X. The confounder blanket learner (CBL) uses sparse regression\n    techniques to simultaneously perform many conditional independence\n    tests, with complementary pairs stability selection to guarantee\n    finite sample error control. CBL is sound and complete with respect to\n    a so-called \"lazy oracle\", and works with both linear and nonlinear\n    systems. For details, see Watson & Silva (2022) <arXiv:2205.05715>.",
    "version": "0.1.3",
    "maintainer": "David Watson <david.s.watson11@gmail.com>",
    "url": "https://github.com/dswatson/cbl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9980,
    "package_name": "cbsREPS",
    "title": "Hedonic and Multilateral Index Methods for Real Estate Price\nStatistics",
    "description": "Compute price indices using various Hedonic and\n    multilateral methods, including Laspeyres, Paasche, Fisher, and HMTS (Hedonic\n    Multilateral Time series re-estimation with splicing). The central function\n    calculate_price_index() offers a unified interface for running these methods\n    on structured datasets. This package is designed to support index construction\n    workflows for real estate and other domains where quality-adjusted price\n    comparisons over time are essential. The development of this package was funded\n    by Eurostat and Statistics Netherlands (CBS), and carried out by Statistics Netherlands.\n    The HMTS method implemented here is described in Ishaak, Ouwehand and Remøy (2024)\n    <doi:10.1177/0282423X241246617>. For broader methodological context, see Eurostat\n    (2013, ISBN:978-92-79-25984-5, <doi:10.2785/34007>).",
    "version": "0.1.0",
    "maintainer": "Vivek Gajadhar <v.gajadhar@cbs.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9991,
    "package_name": "ccdR",
    "title": "Utilities for Interacting with the 'CTX' APIs",
    "description": "Access chemical, hazard, bioactivity, and exposure data from the \n   Computational Toxicology and Exposure ('CTX') APIs \n   <https://api-ccte.epa.gov/docs/>. 'ccdR' was developed to streamline the \n   process of accessing the information available through the 'CTX' APIs \n   without requiring prior knowledge of how to use APIs. Most data is also \n   available on the CompTox Chemical Dashboard ('CCD') \n   <https://comptox.epa.gov/dashboard/> and other resources found at the \n   EPA Computational Toxicology and Exposure Online Resources \n   <https://www.epa.gov/comptox-tools>. ",
    "version": "1.1.0",
    "maintainer": "Paul Kruse <kruse.paul@epa.gov>",
    "url": "https://github.com/USEPA/ccdR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9994,
    "package_name": "cchs",
    "title": "Cox Model for Case-Cohort Data with Stratified\nSubcohort-Selection",
    "description": "Contains a function, also called 'cchs', that calculates Estimator III of Borgan et al (2000), <DOI:10.1023/A:1009661900674>. This estimator is for fitting a Cox proportional hazards model to data from a case-cohort study where the subcohort was selected by stratified simple random sampling.",
    "version": "0.4.5",
    "maintainer": "E. Jones <edmundjones79@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 9996,
    "package_name": "ccid",
    "title": "Cross-Covariance Isolate Detect: a New Change-Point Method for\nEstimating Dynamic Functional Connectivity",
    "description": "Provides efficient implementation of the Cross-Covariance\n    Isolate Detect (CCID) methodology for the estimation of the number\n    and location of multiple change-points in the second-order\n    (cross-covariance or network) structure of multivariate, possibly\n    high-dimensional time series. The method is motivated by the detection\n    of change points in functional connectivity networks for functional\n    magnetic resonance imaging (fMRI), electroencephalography (EEG),\n    magentoencephalography (MEG) and electrocorticography (ECoG) data. The\n    main routines in the package have been extensively tested on fMRI data. \n    For details on the CCID methodology, please see Anastasiou et\n    al (2022), Cross-covariance isolate detect: A new change-point method for\n    estimating dynamic functional connectivity. Medical Image Analysis, Volume\n    75.",
    "version": "1.2.0",
    "maintainer": "Andreas Anastasiou <anastasiou.andreas@ucy.ac.cy>",
    "url": "https://github.com/Anastasiou-Andreas/ccid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10022,
    "package_name": "cdfquantreg",
    "title": "Quantile Regression for Random Variables on the Unit Interval",
    "description": "Employs a two-parameter family of\n    distributions for modelling random variables on the (0, 1) interval by\n    applying the cumulative distribution function (cdf) of one parent\n    distribution to the quantile function of another.",
    "version": "1.3.1-2",
    "maintainer": "Yiyun Shou <yiyun.shou@anu.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10053,
    "package_name": "cem",
    "title": "Coarsened Exact Matching",
    "description": "Implementation of the Coarsened Exact Matching algorithm discussed \n\talong with its properties in\n  Iacus, King, Porro (2011) <DOI:10.1198/jasa.2011.tm09599>;\n\tIacus, King, Porro (2012) <DOI:10.1093/pan/mpr013> and\n\tIacus, King, Porro (2019) <DOI:10.1017/pan.2018.29>.",
    "version": "1.1.31",
    "maintainer": "Stefano M. Iacus <siacus@iq.harvard.edu>",
    "url": "https://gking.harvard.edu/cem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10055,
    "package_name": "cenGAM",
    "title": "Censored Regression with Smooth Terms",
    "description": "Implementation of Tobit type I and type II families for censored regression using the 'mgcv' package, based on methods detailed in Woods (2016) <doi:10.1080/01621459.2016.1180986>.",
    "version": "0.5.4",
    "maintainer": "Zhou Fang <zhou.fang@bioss.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10056,
    "package_name": "cenROC",
    "title": "Estimating Time-Dependent ROC Curve and AUC for Censored Data",
    "description": "Contains functions to estimate a smoothed and a non-smoothed (empirical) time-dependent \n             receiver operating characteristic curve and the corresponding area under the receiver \n             operating characteristic curve and the optimal cutoff point for the right and interval \n             censored survival data. See Beyene and El Ghouch (2020)<doi:10.1002/sim.8671> and Beyene \n             and El Ghouch (2022) <doi:10.1002/bimj.202000382>.",
    "version": "2.0.0",
    "maintainer": "Kassu Mehari Beyene <kassu.mehari@wu.edu.et>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10058,
    "package_name": "censCov",
    "title": "Linear Regression with a Randomly Censored Covariate",
    "description": "Implementations of threshold regression approaches for linear\n\t     regression models with a covariate subject to random censoring,\n\t     including deletion threshold regression and completion threshold regression.\n\t     Reverse survival regression, which flip the role of response variable and the\n\t     covariate, is also considered.",
    "version": "1.0-0",
    "maintainer": "Sy Han (Steven) Chiou <schiou@hsph.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10079,
    "package_name": "cequre",
    "title": "Censored Quantile Regression & Monotonicity-Respecting Restoring",
    "description": "Perform censored quantile regression of Huang (2010) <doi:10.1214/09-AOS771>, and restore monotonicity respecting via adaptive interpolation for dynamic regression of Huang (2017) <doi:10.1080/01621459.2016.1149070>. The monotonicity-respecting restoration applies to general dynamic regression models including (uncensored or censored) quantile regression model, additive hazards model, and dynamic survival models of Peng and Huang (2007) <doi:10.1093/biomet/asm058>, among others.",
    "version": "1.5",
    "maintainer": "Yijian Huang <yhuang5@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10087,
    "package_name": "ceser",
    "title": "Cluster Estimated Standard Errors",
    "description": "Implementation of the Cluster Estimated Standard Errors (CESE) proposed in Jackson (2020) <DOI:10.1017/pan.2019.38> to compute clustered standard errors of linear coefficients in regression models with grouped data.",
    "version": "1.0.0",
    "maintainer": "Diogo Ferrari <diogoferrari@gmail.com>",
    "url": "https://github.com/DiogoFerrari/ceser",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10095,
    "package_name": "cfdecomp",
    "title": "Counterfactual Decomposition: MC Integration of the G-Formula",
    "description": "Provides a set of functions for counterfactual decomposition (cfdecomp). The functions available in this package decompose differences in an outcome attributable to a mediating variable (or sets of mediating variables) between groups based on counterfactual (causal inference) theory. By using Monte Carlo (MC) integration (simulations based on empirical estimates from multivariable models) we provide added flexibility compared to existing (analytical) approaches, at the cost of computational power or time. The added flexibility means that we can decompose difference between groups in any outcome or and with any mediator (any variable type and distribution). See Sudharsanan & Bijlsma (2019) <doi:10.4054/MPIDR-WP-2019-004> for more information.",
    "version": "0.4.0",
    "maintainer": "Maarten Jacob Bijlsma <maarten.bijlsma@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10100,
    "package_name": "cfmortality",
    "title": "Cystic Fibrosis Survival Prediction Model Based on Stanojevic\nModel",
    "description": "Allows clinicians to predict survival probabilities over the next two years for cystic fibrosis patients, based on the clinical prediction models published in Stanojevic et al. (2019) <doi:10.1183/13993003.00224-2019>.",
    "version": "0.3.0",
    "maintainer": "Amin Adibi <adibi@alumni.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10101,
    "package_name": "cforward",
    "title": "Forward Selection using Concordance/C-Index",
    "description": "Performs forward model selection, using the C-index/concordance\n  in survival analysis models. ",
    "version": "0.2.0",
    "maintainer": "John Muschelli <muschellij2@gmail.com>",
    "url": "https://github.com/muschellij2/cforward",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10109,
    "package_name": "cgam",
    "title": "Constrained Generalized Additive Model",
    "description": "A constrained generalized additive model is fitted by the cgam routine. Given a set of predictors, each of which may have a shape or order restrictions, the maximum likelihood estimator for the constrained generalized additive model is found using an iteratively re-weighted cone projection algorithm. The ShapeSelect routine chooses a subset of predictor variables and describes the component relationships with the response. For each predictor, the user needs only specify a set of possible shape or order restrictions. A model selection method chooses the shapes and orderings of the relationships as well as the variables. The cone information criterion (CIC) is used to select the best combination of variables and shapes. A genetic algorithm may be used when the set of possible models is large. In addition, the cgam routine implements a two-dimensional isotonic regression using warped-plane splines without additivity assumptions.  It can also fit a convex or concave regression surface with triangle splines without additivity assumptions. See Liao X, Meyer MC (2019)<doi:10.18637/jss.v089.i05> for more details.",
    "version": "1.29",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10112,
    "package_name": "cglm",
    "title": "Fits Conditional Generalized Linear Models",
    "description": "Estimates the ratio of the regression coefficients and the dispersion parameter in conditional generalized linear models for clustered data.",
    "version": "1.1",
    "maintainer": "Arvid Sjolander <arvid.sjolander@ki.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10120,
    "package_name": "changeS",
    "title": "S-Curve Fit for Changepoint Analysis",
    "description": "Estimation of changepoints using an \"S-curve\"\n   approximation. Formation of confidence intervals for changepoint\n   locations and magnitudes. Both abrupt and gradual changes can be\n   modeled.",
    "version": "1.0.1",
    "maintainer": "Norm Matloff <nsmatloff@ucdavis.edu>",
    "url": "https://github.com/matloff/changeS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10121,
    "package_name": "changepoint",
    "title": "Methods for Changepoint Detection",
    "description": "Implements various mainstream and specialised changepoint methods for finding single and multiple changepoints within data.  Many popular non-parametric and frequentist methods are included.  The cpt.mean(), cpt.var(), cpt.meanvar() functions should be your first point of call.",
    "version": "2.3",
    "maintainer": "Rebecca Killick <r.killick@lancs.ac.uk>",
    "url": "https://github.com/rkillick/changepoint/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10122,
    "package_name": "changepoint.geo",
    "title": "Geometrically Inspired Multivariate Changepoint Detection",
    "description": "Implements the high-dimensional changepoint detection method GeomCP and the related mappings used for changepoint detection. These methods view the changepoint problem from a geometrical viewpoint and aim to extract relevant geometrical features in order to detect changepoints. The geomcp() function should be your first point of call. References: Grundy et al. (2020) <doi:10.1007/s11222-020-09940-y>. ",
    "version": "1.0.3",
    "maintainer": "Rebecca Killick <r.killick@lancaster.ac.uk>",
    "url": "https://github.com/grundy95/changepoint.geo/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10124,
    "package_name": "changepoint.np",
    "title": "Methods for Nonparametric Changepoint Detection",
    "description": "Implements the multiple changepoint algorithm PELT with a nonparametric cost function based on the empirical distribution of the data. This package extends the changepoint package (see Killick, R and Eckley, I (2014) <doi:10.18637/jss.v058.i03> ).",
    "version": "1.0.5",
    "maintainer": "Daniel Grose <changepoints@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10125,
    "package_name": "changepointGA",
    "title": "Changepoint Detection via Modified Genetic Algorithm",
    "description": "The Genetic Algorithm (GA) is used to perform changepoint analysis in time series data. The package also includes an extended island version of GA, as described in Lu, Lund, and Lee (2010, <doi:10.1214/09-AOAS289>). By mimicking the principles of natural selection and evolution, GA provides a powerful stochastic search technique for solving combinatorial optimization problems. In 'changepointGA', each chromosome represents a changepoint configuration, including the number and locations of changepoints, hyperparameters, and model parameters. The package employs genetic operators—selection, crossover, and mutation—to iteratively improve solutions based on the given fitness (objective) function. Key features of 'changepointGA' include encoding changepoint configurations in an integer format, enabling dynamic and simultaneous estimation of model hyperparameters, changepoint configurations, and associated parameters. The detailed algorithmic implementation can be found in the package vignettes and in the paper of Li (2024, <doi:10.48550/arXiv.2410.15571>).",
    "version": "0.1.3",
    "maintainer": "Mo Li <mo.li@louisiana.edu>",
    "url": "https://github.com/mli171/changepointGA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10127,
    "package_name": "changepoints",
    "title": "A Collection of Change-Point Detection Methods",
    "description": "Performs a series of offline and/or online change-point detection algorithms for 1) univariate mean: <doi:10.1214/20-EJS1710>, <arXiv:2006.03283>; 2) univariate polynomials: <doi:10.1214/21-EJS1963>; 3) univariate and multivariate nonparametric settings: <doi:10.1214/21-EJS1809>, <doi:10.1109/TIT.2021.3130330>; 4) high-dimensional covariances: <doi:10.3150/20-BEJ1249>; 5) high-dimensional networks with and without missing values: <doi:10.1214/20-AOS1953>, <arXiv:2101.05477>, <arXiv:2110.06450>; 6) high-dimensional linear regression models: <arXiv:2010.10410>, <arXiv:2207.12453>; 7) high-dimensional vector autoregressive models: <arXiv:1909.06359>; 8) high-dimensional self exciting point processes: <arXiv:2006.03572>; 9) dependent dynamic nonparametric random dot product graphs: <arXiv:1911.07494>; 10) univariate mean against adversarial attacks: <arXiv:2105.10417>.",
    "version": "1.1.0",
    "maintainer": "Haotian Xu <haotian.xu@uclouvain.be>",
    "url": "https://github.com/HaotianXu/changepoints",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10137,
    "package_name": "chartreview",
    "title": "Adaptive Multi-Wave Sampling for Efficient Chart Validation",
    "description": "Functionality to perform adaptive multi-wave sampling for efficient chart validation. Code allows one to define strata, adaptively sample using several types of confidence bounds for the quantity of interest (Lai's confidence bands, Bayesian credible intervals, normal confidence intervals), and sampling strategies (random sampling, stratified random sampling, Neyman's sampling, see Neyman (1934) <doi:10.2307/2342192> and Neyman (1938) <doi:10.1080/01621459.1938.10503378>).",
    "version": "1.0",
    "maintainer": "Georg Hahn <ghahn@bwh.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10161,
    "package_name": "chemCal",
    "title": "Calibration Functions for Analytical Chemistry",
    "description": "Simple functions for plotting linear\n\tcalibration functions and estimating standard errors for measurements\n\taccording to the Handbook of Chemometrics and Qualimetrics: Part A\n\tby Massart et al. (1997) There are also functions estimating the limit\n\tof detection (LOD) and limit of quantification (LOQ).\n\tThe functions work on model objects from - optionally weighted - linear\n\tregression (lm) or robust linear regression ('rlm' from the 'MASS' package).",
    "version": "0.2.3",
    "maintainer": "Johannes Ranke <johannes.ranke@jrwb.de>",
    "url": "https://pkgdown.jrwb.de/chemCal/,\nhttps://cgit.jrwb.de/chemCal/about",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10173,
    "package_name": "chicane",
    "title": "Capture Hi-C Analysis Engine",
    "description": "Toolkit for processing and calling interactions in capture Hi-C data. Converts BAM files into counts of reads linking restriction fragments, and identifies pairs of fragments that interact more than expected by chance. Significant interactions are identified by comparing the observed read count to the expected background rate from a count regression model.",
    "version": "0.1.8",
    "maintainer": "Syed Haider <Syed.Haider@icr.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10191,
    "package_name": "chngpt",
    "title": "Estimation and Hypothesis Testing for Threshold Regression",
    "description": "Threshold regression models are also called two-phase regression, broken-stick regression, split-point regression, structural change models, and regression kink models, with and without interaction terms. Methods for both continuous and discontinuous threshold models are included, but the support for the former is much greater. This package is described in Fong, Huang, Gilbert and Permar (2017) <DOI:10.1186/s12859-017-1863-x> and the package vignette.",
    "version": "2024.11-15",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10199,
    "package_name": "chopper",
    "title": "Changepoint-Aware Ensemble for Probabilistic Modeling",
    "description": "Implements a changepoint-aware ensemble forecasting algorithm that combines Theta, TBATS (Trigonometric, Box-Cox transformation, ARMA errors, Trend, Seasonal components), and ARFIMA (AutoRegressive, Fractionally Integrated, Moving Average) using a product-of-experts approach for robust probabilistic prediction.",
    "version": "1.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/chopper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10220,
    "package_name": "chron",
    "title": "Chronological Objects which Can Handle Dates and Times",
    "description": "Provides chronological objects which can handle dates and times.",
    "version": "2.3-62",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10235,
    "package_name": "cif",
    "title": "Cointegrated ICU Forecasting",
    "description": "Set of forecasting tools to predict ICU beds using a Vector Error Correction model with a single cointegrating vector. Method described in  Berta, P. Lovaglio, P.G. Paruolo, P. Verzillo, S., 2020. \"Real Time Forecasting of Covid-19 Intensive Care Units demand\" Health, Econometrics and Data Group (HEDG) Working Papers 20/16, HEDG, Department of Economics, University of York, <https://www.york.ac.uk/media/economics/documents/hedg/workingpapers/2020/2016.pdf>. ",
    "version": "0.1.1",
    "maintainer": "Paolo Paruolo <Paolo.PARUOLO@ec.europa.eu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10247,
    "package_name": "cir",
    "title": "Centered Isotonic Regression and Dose-Response Utilities",
    "description": "Isotonic regression (IR) and its improvement: centered isotonic regression (CIR). CIR is recommended in particular with small samples. Also, interval estimates for both, and additional utilities such as plotting dose-response data. For dev version and change history, see GitHub assaforon/cir.",
    "version": "2.5.1",
    "maintainer": "Assaf P. Oron <assaf.oron@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10249,
    "package_name": "circacompare",
    "title": "Analyses of Circadian Data",
    "description": "Uses non-linear regression to statistically compare two circadian rhythms.\n    Groups are only compared if both are rhythmic (amplitude is non-zero).\n    Performs analyses regarding mesor, phase, and amplitude, reporting on estimates and statistical differences, for each, between groups.\n    Details can be found in Parsons et al (2020) <doi:10.1093/bioinformatics/btz730>.",
    "version": "0.2.0",
    "maintainer": "Rex Parsons <Rex.Parsons94@gmail.com>",
    "url": "https://rwparsons.github.io/circacompare/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10265,
    "package_name": "cit",
    "title": "Causal Inference Test",
    "description": "A likelihood-based hypothesis testing approach is implemented for\n  assessing causal mediation. Described in Millstein, Chen, and Breton (2016),\n  <DOI:10.1093/bioinformatics/btw135>, it could be used to test for mediation\n  of a known causal association between a DNA variant, the 'instrumental variable',\n  and a clinical outcome or phenotype by gene expression or DNA methylation, the\n  potential mediator. Another example would be testing mediation of the effect\n  of a drug on a clinical outcome by the molecular target. The hypothesis test\n  generates a p-value or permutation-based FDR value with confidence intervals\n  to quantify uncertainty in the causal inference. The outcome can be represented\n  by either a continuous or binary variable, the potential mediator is continuous,\n  and the instrumental variable can be continuous or binary and is not limited to\n  a single variable but may be a design matrix representing multiple variables.",
    "version": "2.3.2",
    "maintainer": "Joshua Millstein <joshua.millstein@usc.edu>",
    "url": "https://github.com/USCbiostats/cit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10274,
    "package_name": "ciuupi",
    "title": "Confidence Intervals Utilizing Uncertain Prior Information",
    "description": "Computes a confidence interval for a specified linear combination of the \n    regression parameters in a linear regression model with iid normal errors\n    with known variance when there is uncertain prior information that a distinct\n    specified linear combination of the regression parameters takes a given \n    value.  This confidence interval, found by numerical nonlinear constrained \n    optimization, has the required minimum coverage and utilizes this uncertain \n    prior information through desirable expected length properties.\n    This confidence interval has the following three practical applications. \n    Firstly, if the error variance has been accurately estimated from previous \n    data then it may be treated as being effectively known. Secondly, for \n    sufficiently large (dimension of the response vector) minus (dimension of \n    regression parameter vector), greater than or equal to 30 (say),\n    if we replace the assumed known value of the error variance by its usual \n    estimator in the formula for the confidence interval then the resulting \n    interval has, to a very good approximation, the same coverage probability \n    and expected length properties as when the error variance is known. Thirdly,\n    some more complicated models can be approximated by the linear regression \n    model with error variance known when certain unknown parameters are replaced \n    by estimates. This confidence interval is described in \n    Mainzer, R. and Kabaila, P. \n    (2019) <doi:10.32614/RJ-2019-026>, and is a member of the family of \n    confidence intervals proposed by Kabaila, P. and Giri, K. (2009) \n    <doi:10.1016/j.jspi.2009.03.018>.",
    "version": "1.2.3",
    "maintainer": "Paul Kabaila <P.Kabaila@latrobe.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10275,
    "package_name": "ciuupi2",
    "title": "Kabaila and Giri (2009) Confidence Interval",
    "description": "\n    Computes a confidence interval for a specified linear combination of \n    the regression parameters in a linear regression model with iid normal \n    errors with unknown variance when there is uncertain prior information \n    that a distinct specified linear combination of the regression \n    parameters takes a specified number. This confidence interval, found by \n    numerical nonlinear constrained optimization, has the required minimum coverage \n    and utilizes this uncertain prior information through desirable \n    expected length properties. This confidence interval is proposed by \n    Kabaila, P. and Giri, K. (2009) <doi:10.1016/j.jspi.2009.03.018>.",
    "version": "1.0.1",
    "maintainer": "Paul Kabaila <P.Kabaila@latrobe.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10281,
    "package_name": "cjoint",
    "title": "AMCE Estimator for Conjoint Experiments",
    "description": "An R implementation of the Average Marginal Component-specific\n    Effects (AMCE) estimator presented in Hainmueller, J., Hopkins, D., and Yamamoto\n    T. (2014) <DOI:10.1093/pan/mpt024> Causal Inference in Conjoint Analysis: Understanding Multi-Dimensional\n    Choices via Stated Preference Experiments. Political Analysis 22(1):1-30.",
    "version": "2.1.1",
    "maintainer": "Anton Strezhnev <astrezhnev@uchicago.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10285,
    "package_name": "clam",
    "title": "Classical Age-Depth Modelling of Cores from Deposits",
    "description": "Performs 'classical' age-depth modelling of dated sediment deposits - prior to applying more sophisticated techniques such as Bayesian age-depth modelling. Any radiocarbon dated depths are calibrated. Age-depth models are constructed by sampling repeatedly from the dated levels, each time drawing age-depth curves. Model types include linear interpolation, linear or polynomial regression, and a range of splines. See Blaauw (2010) <doi:10.1016/j.quageo.2010.01.002>.",
    "version": "2.6.3",
    "maintainer": "Maarten Blaauw <maarten.blaauw@qub.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10291,
    "package_name": "clarkeTest",
    "title": "Distribution-Free Tests of Non-Nested Models",
    "description": "Implementation of Clarke's distribution-free test of\n  non-nested models.  Currently supported model functions are: \n  lm(), glm() ('binomial', 'poisson', 'negative binomial' links), \n  polr() ('MASS'), clm() ('ordinal'), and multinom() ('nnet').  \n  For more information on the test, see Clarke (2007) \n  <doi:10.1093/pan/mpm004>. ",
    "version": "0.2.0",
    "maintainer": "Dave Armstrong <davearmstrong.ps@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10307,
    "package_name": "cleanTS",
    "title": "Testbench for Univariate Time Series Cleaning",
    "description": "A reliable and efficient tool for cleaning univariate time \n    series data. It implements reliable and efficient procedures for \n    automating the process of cleaning univariate time series data. \n    The package provides integration with already developed and deployed \n    tools for missing value imputation and outlier detection. It also \n    provides a way of visualizing large time-series data in different \n    resolutions.",
    "version": "0.1.2",
    "maintainer": "Mayur Shende <mayur.k.shende@gmail.com>",
    "url": "https://github.com/Mayur1009/cleanTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10332,
    "package_name": "climaemet",
    "title": "Climate AEMET Tools",
    "description": "Tools to download the climatic data of the Spanish\n    Meteorological Agency (AEMET) directly from R using their API and\n    create scientific graphs (climate charts, trend analysis of climate\n    time series, temperature and precipitation anomalies maps, warming\n    stripes graphics, climatograms, etc.).",
    "version": "1.4.2",
    "maintainer": "Diego Hernangómez <diego.hernangomezherrero@gmail.com>",
    "url": "https://ropenspain.github.io/climaemet/,\nhttps://github.com/rOpenSpain/climaemet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10343,
    "package_name": "clinDR",
    "title": "Simulation and Analysis Tools for Clinical Dose Response\nModeling",
    "description": "Bayesian and ML Emax model fitting, graphics and simulation for clinical dose\n    response.  The summary data from the dose response meta-analyses in  \n    Thomas, Sweeney, and Somayaji (2014) <doi:10.1080/19466315.2014.924876> and\n    Thomas and Roy (2016) <doi:10.1080/19466315.2016.1256229> \n    Wu, Banerjee, Jin, Menon, Martin, and Heatherington(2017) <doi:10.1177/0962280216684528> \n    are included \n    in the package.  The prior distributions for the Bayesian analyses default to\n    the posterior predictive distributions derived from these references.",
    "version": "2.5.2",
    "maintainer": "Neal Thomas <snthomas99@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10347,
    "package_name": "clinfun",
    "title": "Clinical Trial Design and Data Analysis Functions",
    "description": "Utilities to make your clinical collaborations easier if not\n          fun. It contains functions for designing studies such as Simon\n          2-stage and group sequential designs and for data analysis such\n          as Jonckheere-Terpstra test and estimating survival quantiles.",
    "version": "1.1.5",
    "maintainer": "Venkatraman E. Seshan <seshanv@mskcc.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10382,
    "package_name": "clr",
    "title": "Curve Linear Regression via Dimension Reduction",
    "description": "A new methodology for linear regression with both curve response \n    and curve regressors, which is described in Cho, Goude, Brossat and Yao \n    (2013) <doi:10.1080/01621459.2012.722900> and (2015) \n    <doi:10.1007/978-3-319-18732-7_3>. The key idea behind this methodology is \n    dimension reduction based on a singular value decomposition in a Hilbert \n    space, which reduces the curve regression problem to several scalar linear \n    regression problems. ",
    "version": "0.1.2",
    "maintainer": "Amandine Pierrot <amandine.m.pierrot@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10386,
    "package_name": "clubSandwich",
    "title": "Cluster-Robust (Sandwich) Variance Estimators with Small-Sample\nCorrections",
    "description": "Provides several cluster-robust variance estimators (i.e.,\n    sandwich estimators) for ordinary and weighted least squares linear regression\n    models, including the bias-reduced linearization estimator introduced by Bell\n    and McCaffrey (2002) \n    <https://www150.statcan.gc.ca/n1/pub/12-001-x/2002002/article/9058-eng.pdf> and \n    developed further by Pustejovsky and Tipton (2017) \n    <DOI:10.1080/07350015.2016.1247004>. The package includes functions for estimating\n    the variance- covariance matrix and for testing single- and multiple-\n    contrast hypotheses based on Wald test statistics. Tests of single regression\n    coefficients use Satterthwaite or saddle-point corrections. Tests of multiple-\n    contrast hypotheses use an approximation to Hotelling's T-squared distribution.\n    Methods are provided for a variety of fitted models, including lm() and mlm\n    objects, glm(), geeglm() (from package 'geepack'), lm_robust() and lm_lin() \n    (from package 'estimatr'), ivreg() (from package 'AER'), ivreg() (from package 'ivreg' when \n    estimated by ordinary least squares), plm() (from package 'plm'), gls() and \n    lme() (from 'nlme'), lmer() (from `lme4`), robu() (from 'robumeta'), and rma.uni() \n    and rma.mv() (from 'metafor').",
    "version": "0.6.1",
    "maintainer": "James E. Pustejovsky <jepusto@gmail.com>",
    "url": "http://jepusto.github.io/clubSandwich/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10392,
    "package_name": "cluscov",
    "title": "Clustered Covariate Regression",
    "description": "Clustered covariate regression enables estimation and inference in \n both linear and non-linear models with linear predictor functions even when the\n design matrix is column rank deficient. Routines in this package implement algorithms\n in Soale and Tsyawo (2019) <doi:10.13140/RG.2.2.32355.81441>.",
    "version": "1.1.0",
    "maintainer": "Emmanuel S Tsyawo <estsyawo@temple.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10412,
    "package_name": "clusterSEs",
    "title": "Calculate Cluster-Robust p-Values and Confidence Intervals",
    "description": "Calculate p-values and confidence intervals using cluster-adjusted\n    t-statistics (based on Ibragimov and Muller (2010) <DOI:10.1198/jbes.2009.08046>, pairs cluster bootstrapped t-statistics, and wild cluster bootstrapped t-statistics (the latter two techniques based on Cameron, Gelbach, and Miller (2008) <DOI:10.1162/rest.90.3.414>. Procedures are included for use with GLM, ivreg, plm (pooling or fixed effects), and mlogit models.",
    "version": "2.6.5",
    "maintainer": "Justin Esarey <justin@justinesarey.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10443,
    "package_name": "cmce",
    "title": "Computer Model Calibration for Deterministic and Stochastic\nSimulators",
    "description": "Implements the Bayesian calibration model described\n    in Pratola and Chkrebtii (2018) <DOI:10.5705/ss.202016.0403> for stochastic \n    and deterministic simulators.  Additive and multiplicative discrepancy models \n    are currently supported. See <http://www.matthewpratola.com/software> for \n    more information and examples.",
    "version": "0.1.0",
    "maintainer": "Matthew T. Pratola <mpratola@stat.osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10456,
    "package_name": "cmprskQR",
    "title": "Analysis of Competing Risks Using Quantile Regressions",
    "description": "Estimation, testing and regression modeling of\n subdistribution functions in competing risks using quantile regressions,\n as described in Peng and Fine (2009) <DOI:10.1198/jasa.2009.tm08228>.",
    "version": "0.9.2",
    "maintainer": "Stephan Dlugosz <stephan.dlugosz@googlemail.com>",
    "url": "https://bitbucket.org/sdlugosz/cmprskqr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10470,
    "package_name": "cna",
    "title": "Causal Modeling with Coincidence Analysis",
    "description": "Provides comprehensive functionalities for causal modeling with Coincidence Analysis (CNA), which is a configurational comparative method of causal data analysis that was first introduced in Baumgartner (2009) <doi:10.1177/0049124109339369>, and generalized in Baumgartner & Ambuehl (2020) <doi:10.1017/psrm.2018.45>. CNA is designed to recover INUS-causation from data, which is particularly relevant for analyzing processes featuring conjunctural causation (component causation) and equifinality (alternative causation). CNA is currently the only method for INUS-discovery that allows for multiple effects (outcomes/endogenous factors), meaning it can analyze common-cause and causal chain structures. Moreover, as of version 4.0, it is the only method of its kind that provides measures for model evaluation and selection that are custom-made for the problem of INUS-discovery.",
    "version": "4.0.3",
    "maintainer": "Mathias Ambuehl <mathias.ambuehl@consultag.ch>",
    "url": "https://CRAN.R-project.org/package=cna",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10485,
    "package_name": "coalescentMCMC",
    "title": "MCMC Algorithms for the Coalescent",
    "description": "Flexible framework for coalescent analyses in R. It includes a main function running the  MCMC algorithm, auxiliary functions for tree rearrangement, and some functions to compute population genetic parameters. Extended description can be found in Paradis (2020) <doi:10.1201/9780429466700>. For details on the MCMC algorithm, see Kuhner et al. (1995) <doi:10.1093/genetics/140.4.1421> and Drummond et al. (2002) <doi:10.1093/genetics/161.3.1307>.",
    "version": "0.4-4",
    "maintainer": "Emmanuel Paradis <Emmanuel.Paradis@ird.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10491,
    "package_name": "cobin",
    "title": "Cobin and Micobin Regression Models for Continuous Proportional\nData",
    "description": "Provides functions for cobin and micobin regression models, a new family of generalized linear models for continuous proportional data (Y in the closed unit interval [0, 1]). It also includes an exact, efficient sampler for the Kolmogorov-Gamma random variable. For details, see Lee et al. (2025+) <doi:10.48550/arXiv.2504.15269>.",
    "version": "1.0.1.3",
    "maintainer": "Changwoo Lee <changwoo.lee@duke.edu>",
    "url": "https://github.com/changwoo-lee/cobin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10495,
    "package_name": "coconots",
    "title": "Convolution-Closed Models for Count Time Series",
    "description": "Useful tools for fitting, validating, and forecasting of practical convolution-closed time series models for low counts are provided. Marginal distributions of the data can be modelled via Poisson and Generalized Poisson innovations. Regression effects can be incorporated through time varying innovation rates. The models are described in Jung and Tremayne (2011) <doi:10.1111/j.1467-9892.2010.00697.x> and the model assessment tools are presented in Czado et al. (2009) <doi:10.1111/j.1541-0420.2009.01191.x> and, Tsay (1992) <doi:10.2307/2347612>.",
    "version": "2.0.2",
    "maintainer": "Manuel Huth <manuel.huth@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10503,
    "package_name": "coda",
    "title": "Output Analysis and Diagnostics for MCMC",
    "description": "Provides functions for summarizing and plotting the\n\toutput from Markov Chain Monte Carlo (MCMC) simulations, as\n\twell as diagnostic tests of convergence to the equilibrium\n\tdistribution of the Markov chain.",
    "version": "0.19-4.1",
    "maintainer": "Martyn Plummer <martyn.plummer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10506,
    "package_name": "coda4microbiome",
    "title": "Compositional Data Analysis for Microbiome Studies",
    "description": "Functions for microbiome data analysis that take into account its compositional nature. Performs variable selection through penalized regression for both, cross-sectional and longitudinal studies, and for binary and continuous outcomes.    ",
    "version": "0.2.4",
    "maintainer": "Toni Susin <toni.susin@upc.edu>",
    "url": "https://malucalle.github.io/coda4microbiome/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10508,
    "package_name": "codalm",
    "title": "Transformation-Free Linear Regression for Compositional Outcomes\nand Predictors",
    "description": "Implements the expectation-maximization (EM) algorithm as described in Fiksel et al. (2022) <doi:10.1111/biom.13465>\n    for transformation-free linear regression for compositional outcomes and predictors.",
    "version": "0.1.3",
    "maintainer": "Sandipan Pramanik <sandy.pramanik@gmail.com>",
    "url": "https://github.com/jfiksel/codalm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10536,
    "package_name": "coffee",
    "title": "Chronological Ordering for Fossils and Environmental Events",
    "description": "While individual calibrated radiocarbon dates can span several centuries, combining multiple dates together with any chronological constraints can make a chronology much more robust and precise. This package uses Bayesian methods to enforce the chronological ordering of radiocarbon and other dates, for example for trees with multiple radiocarbon dates spaced at exactly known intervals (e.g., 10 annual rings). For methods see Christen 2003 <doi:10.11141/ia.13.2>. Another example is sites where the relative chronological position of the dates is taken into account - the ages of dates further down a site must be older than those of dates further up (Buck, Kenworthy, Litton and Smith 1991 <doi:10.1017/S0003598X00080534>; Nicholls and Jones 2001 <doi:10.1111/1467-9876.00250>). The paper accompanying this R package is Blaauw et al. 2024 <doi:10.1017/RDC.2024.56>.",
    "version": "0.4.3",
    "maintainer": "Maarten Blaauw <maarten.blaauw@qub.ac.uk>",
    "url": "https://github.com/Maarten14C/coffee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10549,
    "package_name": "cohorttools",
    "title": "Cohort Data Analyses",
    "description": "Functions to make lifetables and to calculate hazard function estimate using Poisson regression model with splines. Includes function to draw simple flowchart of cohort study. Function boxesLx() makes boxes of transition rates between states. It utilizes 'Epi' package 'Lexis' data.",
    "version": "0.1.7",
    "maintainer": "Jari Haukka <jari.haukka@helsinki.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10552,
    "package_name": "cointReg",
    "title": "Parameter Estimation and Inference in a Cointegrating Regression",
    "description": "Cointegration methods are widely used in empirical macroeconomics\n    and empirical finance. It is well known that in a cointegrating\n    regression the ordinary least squares (OLS) estimator of the\n    parameters is super-consistent, i.e. converges at rate equal to the\n    sample size T. When the regressors are endogenous, the limiting\n    distribution of the OLS estimator is contaminated by so-called second\n    order bias terms, see e.g. Phillips and Hansen (1990) <DOI:10.2307/2297545>.\n    The presence of these bias terms renders inference difficult. Consequently,\n    several modifications to OLS that lead to zero mean Gaussian mixture\n    limiting distributions have been proposed, which in turn make\n    standard asymptotic inference feasible. These methods include\n    the fully modified OLS (FM-OLS) approach of Phillips and Hansen\n    (1990) <DOI:10.2307/2297545>, the dynamic OLS (D-OLS) approach of Phillips\n    and Loretan (1991) <DOI:10.2307/2298004>, Saikkonen (1991)\n    <DOI:10.1017/S0266466600004217> and Stock and Watson (1993)\n    <DOI:10.2307/2951763> and the new estimation approach called integrated\n    modified OLS (IM-OLS) of Vogelsang and Wagner (2014)\n    <DOI:10.1016/j.jeconom.2013.10.015>. The latter is based on an augmented\n    partial sum (integration) transformation of the regression model. IM-OLS is\n    similar in spirit to the FM- and D-OLS approaches, with the key difference\n    that it does not require estimation of long run variance matrices and avoids\n    the need to choose tuning parameters (kernels, bandwidths, lags). However,\n    inference does require that a long run variance be scaled out.\n    This package provides functions for the parameter estimation and inference\n    with all three modified OLS approaches. That includes the automatic\n    bandwidth selection approaches of Andrews (1991) <DOI:10.2307/2938229> and\n    of Newey and West (1994) <DOI:10.2307/2297912> as well as the calculation of\n    the long run variance.",
    "version": "0.2.0",
    "maintainer": "Philipp Aschersleben <aschersleben@statistik.tu-dortmund.de>",
    "url": "https://github.com/aschersleben/cointReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10563,
    "package_name": "collett",
    "title": "Datasets from \"Modelling Survival Data in Medical Research\" by\nCollett",
    "description": "Datasets for the book entitled \"Modelling Survival Data in Medical Research\" by Collett (2023) <doi:10.1201/9781003282525>. The datasets provide extensive examples of time-to-event data.",
    "version": "0.1.0",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "url": "https://github.com/mclements/collett",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10566,
    "package_name": "collin",
    "title": "Visualization the Effects of Collinearity in Distributed Lag\nModels and Other Linear Models",
    "description": "Tool to assessing whether the results of a study could be influenced by\n    collinearity. Simulations under a given hypothesized truth regarding effects of an\n    exposure on the outcome are used and the resulting curves of lagged effects are\n    visualized. A user's manual is provided, which includes detailed examples (e.g. a\n    cohort study looking for windows of vulnerability to air pollution, a time series\n    study examining the linear association of air pollution with hospital admissions,\n    and a time series study examining the non-linear association between temperature and\n    mortality). The methods are described in Basagana and Barrera-Gomez (2021) <doi:10.1093/ije/dyab179>.",
    "version": "0.0.4",
    "maintainer": "Jose Barrera-Gomez <jose.barrera@isglobal.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10643,
    "package_name": "compareC",
    "title": "Compare Two Correlated C Indices with Right-Censored Survival\nOutcome",
    "description": "Proposed by Harrell, the C index or concordance C, is considered an overall measure of discrimination in survival analysis between a survival outcome that is possibly right censored and a predictive-score variable, which can represent a measured biomarker or a composite-score output from an algorithm that combines multiple biomarkers. This package aims to statistically compare two C indices with right-censored survival outcome, which commonly arise from a paired design and thus resulting two correlated C indices.",
    "version": "1.3.3",
    "maintainer": "Le Kang <lkang@vcu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10644,
    "package_name": "compareCstat",
    "title": "Compare C-Statistics (Concordance) Between Survival Models",
    "description": "Compare C-statistics (concordance statistics)\n    between two survival models, using either bootstrap resampling (Harrell's C)\n    or Uno's C with perturbation-resampling (from the survC1 package). Returns\n    confidence intervals and a p-value for the difference in C-statistics.\n    Useful for evaluating and comparing predictive performance of survival models.\n    Methods implemented for Uno's C are described in Uno et al. (2011) <doi:10.1002/sim.4154>.",
    "version": "0.1.0",
    "maintainer": "Ning Meng <nmeng2@jh.edu>",
    "url": "https://github.com/Lemonade0924/compareCstat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10647,
    "package_name": "compareMCMCs",
    "title": "Compare MCMC Efficiency from 'nimble' and/or Other MCMC Engines",
    "description": "Manages comparison of MCMC performance metrics from multiple MCMC algorithms. These may come from different MCMC configurations using the 'nimble' package or from other packages. Plug-ins for JAGS via 'rjags' and Stan via 'rstan' are provided. It is possible to write plug-ins for other packages. Performance metrics are held in an MCMCresult class along with samples and timing data. It is easy to apply new performance metrics. Reports are generated as html pages with figures comparing sets of runs. It is possible to configure the html pages, including providing new figure components.",
    "version": "0.6.0",
    "maintainer": "Perry de Valpine <pdevalpine@berkeley.edu>",
    "url": "https://github.com/nimble-dev/compareMCMCs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10662,
    "package_name": "complmrob",
    "title": "Robust Linear Regression with Compositional Data as Covariates",
    "description": "Robust regression methods for compositional data.\n    The distribution of the estimates can be approximated with various bootstrap\n    methods. These bootstrap methods are available for the compositional as well\n    as for standard robust regression estimates. This allows for direct\n    comparison between them.",
    "version": "0.7.1",
    "maintainer": "David Kepplinger <david.kepplinger@gmail.com>",
    "url": "https://github.com/dakep/complmrob",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10665,
    "package_name": "compound.Cox",
    "title": "Univariate Feature Selection and Compound Covariate for\nPredicting Survival, Including Copula-Based Analyses for\nDependent Censoring",
    "description": "Univariate feature selection and compound covariate methods under the Cox model with high-dimensional features (e.g., gene expressions).\n Available are survival data for non-small-cell lung cancer patients with gene expressions (Chen et al 2007 New Engl J Med) <DOI:10.1056/NEJMoa060096>,\n statistical methods in Emura et al (2012 PLoS ONE) <DOI:10.1371/journal.pone.0047627>,\n Emura & Chen (2016 Stat Methods Med Res) <DOI:10.1177/0962280214533378>, and Emura et al (2019)<DOI:10.1016/j.cmpb.2018.10.020>.\n Algorithms for generating correlated gene expressions are also available.\n Estimation of survival functions via copula-graphic (CG) estimators is also implemented, which is useful for\n sensitivity analyses under dependent censoring (Yeh et al 2023 Biomedicines) <DOI:10.3390/biomedicines11030797> and\n factorial survival analyses (Emura et al 2024 Stat Methods Med Res) <DOI:10.1177/09622802231215805>.",
    "version": "3.33",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10675,
    "package_name": "conText",
    "title": "'a la Carte' on Text (ConText) Embedding Regression",
    "description": "A fast, flexible and transparent framework to estimate context-specific word and short document embeddings using the 'a la carte' \n    embeddings approach developed by Khodak et al. (2018) <doi:10.48550/arXiv.1805.05388> and evaluate hypotheses about covariate effects on embeddings using \n    the regression framework developed by Rodriguez et al. (2021)<doi:10.1017/S0003055422001228>. New version of the package applies a new estimator to measure the distance between word embeddings as described in Green et al. (2025) <doi:10.1017/pan.2024.22>.",
    "version": "3.0.0",
    "maintainer": "Sofia Avila <sofiaavila@princeton.edu>",
    "url": "https://github.com/prodriguezsosa/conText",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10686,
    "package_name": "condGEE",
    "title": "Parameter Estimation in Conditional GEE for Recurrent Event Gap\nTimes",
    "description": "Solves for the mean parameters, the variance parameter, and their asymptotic variance in a conditional GEE for recurrent event gap times, as described by Clement and Strawderman (2009) in the journal Biostatistics. Makes a parametric assumption for the length of the censored gap time.",
    "version": "0.2.0",
    "maintainer": "David Clement <dyc24@cornell.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10688,
    "package_name": "condSURV",
    "title": "Estimation of the Conditional Survival Function for Ordered\nMultivariate Failure Time Data",
    "description": "Method to implement some newly developed methods for the\n    estimation of the conditional survival function. See Meira-Machado, \n    Sestelo and Goncalves (2016) <doi:10.1002/bimj.201500038>.",
    "version": "2.0.4",
    "maintainer": "Marta Sestelo <sestelo@uvigo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10702,
    "package_name": "coneproj",
    "title": "Primal or Dual Cone Projections with Routines for Constrained\nRegression",
    "description": "Routines doing cone projection and quadratic programming, as well as doing estimation and inference for constrained parametric regression and shape-restricted regression problems. See Mary C. Meyer (2013)<doi:10.1080/03610918.2012.659820> for more details.",
    "version": "1.23",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10714,
    "package_name": "confintROB",
    "title": "Confidence Intervals for Robust and Classical Linear Mixed Model\nEstimators",
    "description": "The main function calculates confidence intervals (CI) for Mixed Models, utilizing both classical estimators from the lmer() function in the 'lme4' package and robust estimators from the rlmer() function in the 'robustlmm' package, as well as the varComprob() function in the 'robustvarComp' package. Three methods are available: the classical Wald method, the wild bootstrap, and the parametric bootstrap. Bootstrap methods offer flexibility in obtaining lower and upper bounds through percentile or BCa methods. More details are given in Mason, F., Cantoni, E., & Ghisletta, P. (2021) <doi:10.5964/meth.6607> and Mason, F., Cantoni, E., & Ghisletta, P. (2024) <doi:10.1037/met0000643>.",
    "version": "1.0-2",
    "maintainer": "Fabio Mason <fabio.mason@unige.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10719,
    "package_name": "conformalForecast",
    "title": "Conformal Prediction Methods for Multistep-Ahead Time Series\nForecasting",
    "description": "Methods and tools for performing multistep-ahead time series\n    forecasting using conformal prediction methods including classical\n    conformal prediction, adaptive conformal prediction, conformal PID\n    (Proportional-Integral-Derivative) control, and autocorrelated\n    multistep-ahead conformal prediction.\n    The methods were described by Wang and Hyndman (2024) <doi:10.48550/arXiv.2410.13115>.",
    "version": "0.1.0",
    "maintainer": "Xiaoqian Wang <Xiaoqian.Wang@amss.ac.cn>",
    "url": "https://github.com/xqnwang/conformalForecast,\nhttps://xqnwang.github.io/conformalForecast/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10720,
    "package_name": "conformalInference.fd",
    "title": "Tools for Conformal Inference for Regression in Multivariate\nFunctional Setting",
    "description": "It computes full conformal, split conformal and multi split\n    conformal prediction regions when the response has functional nature.\n    Moreover, the package also contain a plot function to visualize the\n    output of the split conformal.\n    To guarantee consistency, the package structure mimics the univariate \n    'conformalInference' package of professor Ryan Tibshirani.\n    The main references for the code are: \n    Diquigiovanni, Fontana, and Vantini (2021) <arXiv:2102.06746>, \n    Diquigiovanni, Fontana, and Vantini (2021) <arXiv:2106.01792>,\n    Solari, and Djordjilovic (2021) <arXiv:2103.00627>.",
    "version": "1.1.1",
    "maintainer": "Paolo Vergottini <paolo.vergottini@gmail.com>",
    "url": "https://github.com/ryantibs/conformal ,\nhttps://github.com/paolo-vergo/conformalInference.fd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10721,
    "package_name": "conformalInference.multi",
    "title": "Conformal Inference Tools for Regression with Multivariate\nResponse",
    "description": "\n    It computes full conformal, split conformal and multi-split conformal\n    prediction regions when the response variable is multivariate (i.e.\n    dimension is greater than one). Moreover, the package also contains\n    plot functions to visualize the output of the full and split conformal\n    functions. To guarantee consistency, the package structure mimics the\n    univariate package 'conformalInference' by Ryan Tibshirani.\n    See Lei, G’sell, Rinaldo, Tibshirani, & Wasserman (2018) <doi:10.1080/01621459.2017.1307116>\n    for full and split conformal prediction in regression, and Barber, Candès,\n    Ramdas, & Tibshirani (2023) <doi:10.1214/23-AOS2276> for extensions beyond exchangeability.",
    "version": "1.1.2",
    "maintainer": "Paolo Vergottini <paolo.vergottini@gmail.com>",
    "url": "https://github.com/ryantibs/conformal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10722,
    "package_name": "conformalbayes",
    "title": "Jackknife(+) Predictive Intervals for Bayesian Models",
    "description": "Provides functions to construct finite-sample calibrated predictive",
    "version": "0.1.3",
    "maintainer": "",
    "url": "https://github.com/CoryMcCartan/conformalbayes",
    "exports": [],
    "topics": ["bayesian", "conformal-prediction", "prediction", "r"],
    "score": "NA",
    "stars": 9
  },
  {
    "id": 10743,
    "package_name": "conquer",
    "title": "Convolution-Type Smoothed Quantile Regression",
    "description": "Estimation and inference for conditional linear quantile regression models using a convolution smoothed approach. In the low-dimensional setting, efficient gradient-based methods are employed for fitting both a single model and a regression process over a quantile range. Normal-based and (multiplier) bootstrap confidence intervals for all slope coefficients are constructed. In high dimensions, the conquer method is complemented with flexible types of penalties (Lasso, elastic-net, group lasso, sparse group lasso, scad and mcp) to deal with complex low-dimensional structures.",
    "version": "1.3.3",
    "maintainer": "Xiaoou Pan <xip024@ucsd.edu>",
    "url": "https://github.com/XiaoouPan/conquer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10756,
    "package_name": "consrq",
    "title": "Constrained Quantile Regression",
    "description": "Constrained quantile regression is performed. One constraint is that all beta coefficients (including the constant) cannot be negative, they can be either 0 or strictly positive. Another constraint is that the beta coefficients lie within an interval. References: Koenker R. (2005) Quantile Regression, Cambridge University Press. <doi:10.1017/CBO9780511754098>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10758,
    "package_name": "constellation",
    "title": "Identify Event Sequences Using Time Series Joins",
    "description": "Examine any number of time series data frames to identify \n    instances in which various criteria are met within specified time\n    frames. In clinical medicine, these types of events are often\n    called \"constellations of signs and symptoms\", because a single \n    condition depends on a series of events occurring within a certain \n    amount of time of each other. This package was written to work with\n    any number of time series data frames and is optimized for speed \n    to work well with data frames with millions of rows.",
    "version": "0.2.0",
    "maintainer": "Mark Sendak <mark.sendak@gmail.com>",
    "url": "https://github.com/marksendak/constellation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10762,
    "package_name": "contTimeCausal",
    "title": "Continuous Time Causal Models",
    "description": "Implements the semiparametric efficient estimators of \n    continuous-time causal models for time-varying treatments and confounders \n    in the presence of dependent censoring (including structural failure time \n    model and Cox proportional hazards marginal structural model). \n    S. Yang, K. Pieper, and F. Cools (2019) <doi:10.1111/biom.12845>.",
    "version": "1.1",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10770,
    "package_name": "contextual",
    "title": "Simulation and Analysis of Contextual Multi-Armed Bandit Policies",
    "description": "Facilitates the simulation and evaluation of context-free",
    "version": "0.9.8.4",
    "maintainer": "Robin van Emden <robinvanemden@gmail.com>",
    "url": "https://github.com/Nth-iteration-labs/contextual",
    "exports": [],
    "topics": ["bandit", "bandit-experiments", "bandit-learning", "cmab", "contextual", "contextual-bandit-policies", "contextual-bandits", "cran", "evaluation", "exploitation", "exploration", "machine-learning", "multi-armed", "multi-armed-bandit", "multi-armed-bandits", "offline-bandit", "reinforcement", "reinforcement-learning", "simulation", "statistics"],
    "score": "NA",
    "stars": 81
  },
  {
    "id": 10778,
    "package_name": "contrast",
    "title": "A Collection of Contrast Methods",
    "description": "One degree of freedom contrasts for 'lm', 'glm', 'gls', and 'geese' objects.",
    "version": "0.24.2",
    "maintainer": "Alan O'Callaghan <alan.ocallaghan@outlook.com>",
    "url": "https://github.com/Alanocallaghan/contrast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10779,
    "package_name": "contrastable",
    "title": "Consistent Contrast Coding for Factors",
    "description": "Quickly set and summarize contrasts for factors prior to regression  analyses. Intended comparisons, baseline conditions, and intercepts can be explicitly set and documented without the user needing to directly manipulate matrices. Reviews and introductions for contrast coding are available in Brehm and Alday (2022)<doi:10.1016/j.jml.2022.104334> and Schad et al. (2020)<doi:10.1016/j.jml.2019.104038>.",
    "version": "1.1.0",
    "maintainer": "Thomas Sostarics <tsostarics@gmail.com>",
    "url": "https://github.com/tsostarics/contrastable,\nhttps://tsostarics.github.io/contrastable/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10782,
    "package_name": "controlTest",
    "title": "Quantile Comparison for Two-Sample Right-Censored Survival Data",
    "description": "Nonparametric two-sample procedure for comparing survival quantiles.",
    "version": "1.1.0",
    "maintainer": "Eric S. Kawaguchi <erickawaguchi@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10789,
    "package_name": "convergenceDFM",
    "title": "Convergence and Dynamic Factor Models",
    "description": "Tests convergence in macro-financial panels combining\n    Dynamic Factor Models (DFM) and mean-reverting Ornstein-Uhlenbeck (OU)\n    processes. Provides: (i) static/approximate DFMs for large panels with\n    VAR/VECM stability checks, Portmanteau tests and rolling out-of-sample R^2,\n    following Stock and Watson (2002) <doi:10.1198/073500102317351921> and the\n    Generalized Dynamic Factor Model of Forni, Hallin, Lippi and Reichlin (2000)\n    <doi:10.1162/003465300559037>; (ii) cointegration analysis à la Johansen\n    (1988) <doi:10.1016/0165-1889(88)90041-3>; (iii) OU-based convergence and\n    half-life summaries grounded in Uhlenbeck and Ornstein (1930)\n    <doi:10.1103/PhysRev.36.823> and Vasicek (1977) <doi:10.1016/0304-405X(77)90016-2>;\n    (iv) robust inference via 'sandwich' HC/HAC estimators (Zeileis (2004)\n    <doi:10.18637/jss.v011.i10>) and regression diagnostics ('lmtest'); and\n    (v) optional PLS-based factor preselection (Mevik and Wehrens (2007)\n    <doi:10.18637/jss.v018.i02>). Functions emphasize reproducibility and clear,\n    publication-ready summaries.",
    "version": "0.1.4",
    "maintainer": "José Mauricio Gómez Julián <isadorenabi@pm.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10804,
    "package_name": "copBasic",
    "title": "General Bivariate Copula Theory and Many Utility Functions",
    "description": "Extensive functions for bivariate copula (bicopula) computations and related operations\n for bicopula theory. The lower, upper, product, and select other bicopula are implemented along\n with operations including the diagonal, survival copula, dual of a copula, co-copula, and\n numerical bicopula density. Level sets, horizontal and vertical sections are supported. Numerical\n derivatives and inverses of a bicopula are provided through which simulation is implemented.\n Bicopula composition, convex combination, asymmetry extension, and products also are provided.\n Support extends to the Kendall Function as well as the Lmoments thereof. Kendall Tau,\n Spearman Rho and Footrule, Gini Gamma, Blomqvist Beta, Hoeffding Phi, Schweizer-\n Wolff Sigma, tail dependency, tail order, skewness, and bivariate Lmoments are implemented, and\n positive/negative quadrant dependency, left (right) increasing (decreasing) are available.\n Other features include Kullback-Leibler Divergence, Vuong Procedure, spectral measure, and\n Lcomoments for fit and inference, Lcomoment ratio diagrams, maximum likelihood,\n and AIC, BIC, and RMSE for goodness-of-fit.",
    "version": "2.2.11",
    "maintainer": "William Asquith <william.asquith@ttu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10805,
    "package_name": "copCAR",
    "title": "Fitting the copCAR Regression Model for Discrete Areal Data",
    "description": "Provides tools for fitting the copCAR (Hughes, 2015)\n    <DOI:10.1080/10618600.2014.948178> regression model for discrete\n\tareal data. Three types of estimation are supported (continuous\n\textension, composite marginal likelihood, and distributional transform),\n\tfor three types of outcomes (Bernoulli, negative binomial, and Poisson).",
    "version": "2.0-4",
    "maintainer": "John Hughes <drjphughesjr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10812,
    "package_name": "copre",
    "title": "Tools for Nonparametric Martingale Posterior Sampling",
    "description": "Performs Bayesian nonparametric density estimation using Martingale\n posterior distributions including the Copula Resampling (CopRe) algorithm. \n Also included are a Gibbs sampler for the marginal Gibbs-type mixture model and\n an extension to include full uncertainty quantification via a predictive \n sequence resampling (SeqRe) algorithm. The CopRe and SeqRe samplers generate \n random nonparametric distributions as output, leading to complete nonparametric \n inference on posterior summaries. Routines for calculating arbitrary \n functionals from the sampled distributions are included as well as an important \n algorithm for finding the number and location of modes, which can then be used \n to estimate the clusters in the data using, for example, k-means.\n Implements work developed in Moya B., Walker S. G. (2022).\n <doi:10.48550/arxiv.2206.08418>, Fong, E., Holmes, C., Walker, S. G. (2021) \n <doi:10.48550/arxiv.2103.15671>, and Escobar M. D., West, M. (1995) \n <doi:10.1080/01621459.1995.10476550>.",
    "version": "0.2.2",
    "maintainer": "Blake Moya <blakemoya@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10817,
    "package_name": "copulaboost",
    "title": "Fitting Additive Copula Regression Models for Binary Outcome\nRegression",
    "description": "Additive copula regression for regression\n    problems with binary outcome via gradient boosting \n    [Brant, Hobæk Haff (2022); <arXiv:2208.04669>]. The fitting process\n    includes a specialised model selection algorithm for each component, where\n    each component is found (by greedy optimisation) among all the D-vines with\n    only Gaussian pair-copulas of a fixed dimension, as specified by the user.\n    When the variables and structure have been selected, the algorithm then\n    re-fits the component where the pair-copula distributions can be different\n    from Gaussian, if specified.",
    "version": "0.1.0",
    "maintainer": "Simon Boge Brant <simbrant91@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10818,
    "package_name": "copulareg",
    "title": "Copula Regression",
    "description": "\n    Fits multivariate models in an R-vine pair copula construction framework,\n    in such a way that the conditional copula can be easily evaluated.\n    In addition, the package implements functionality to compute or approximate\n    the conditional expectation via the conditional copula.",
    "version": "0.1.0",
    "maintainer": "Simon Boge Brant <simonbb@math.uio.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10825,
    "package_name": "corbouli",
    "title": "Corbae-Ouliaris Frequency Domain Filtering",
    "description": "Corbae-Ouliaris frequency domain filtering. According to \n             Corbae and Ouliaris (2006) <doi:10.1017/CBO9781139164863.008>,\n             this is a solution for extracting cycles from time series, like\n             business cycles etc. when filtering. This method is valid for both\n             stationary and non-stationary time series.",
    "version": "0.1.5",
    "maintainer": "Christos Adam <econp266@econ.soc.uoc.gr>",
    "url": "https://github.com/cadam00/corbouli,\nhttps://cadam00.github.io/corbouli/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10831,
    "package_name": "coreSim",
    "title": "Core Functionality for Simulating Quantities of Interest from\nGeneralised Linear Models",
    "description": "Core functions for simulating quantities of interest\n    from generalised linear models (GLM). This package will form the backbone of\n    a series of other packages that improve the interpretation of GLM estimates.",
    "version": "0.2.4",
    "maintainer": "Christopher Gandrud <christopher.gandrud@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10836,
    "package_name": "corlink",
    "title": "Record Linkage, Incorporating Imputation for Missing Agreement\nPatterns, and Modeling Correlation Patterns Between Fields",
    "description": "A matrix of agreement patterns and counts for record pairs is the input for the procedure.  An EM algorithm is used to impute plausible values for missing record pairs.  A second EM algorithm, incorporating possible correlations between per-field agreement, is used to estimate posterior probabilities that each pair is a true match - i.e. constitutes the same individual.",
    "version": "1.0.0",
    "maintainer": "John Ferguson <john.ferguson@nuigalway.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10837,
    "package_name": "corncob",
    "title": "Count Regression for Correlated Observations with the\nBeta-Binomial",
    "description": "Statistical modeling for correlated count data using the beta-binomial distribution, described in Martin et al. (2020) <doi:10.1214/19-AOAS1283>. It allows for both mean and overdispersion covariates.",
    "version": "0.4.2",
    "maintainer": "Amy D Willis <adwillis@uw.edu>",
    "url": "https://github.com/statdivlab/corncob,\nhttps://statdivlab.github.io/corncob/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10838,
    "package_name": "cornet",
    "title": "Penalised Regression for Dichotomised Outcomes",
    "description": "Implements lasso and ridge regression for dichotomised outcomes (<doi:10.1080/02664763.2023.2233057>), i.e., numerical outcomes that were transformed to binary outcomes. Such artificial binary outcomes indicate whether an underlying measurement is greater than a threshold.",
    "version": "1.0.0",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@uni.lu>",
    "url": "https://github.com/rauschenberger/cornet,\nhttps://rauschenberger.github.io/cornet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10867,
    "package_name": "corrselect",
    "title": "Correlation-Based and Model-Based Predictor Pruning",
    "description": "Provides functions for predictor pruning using association-based and model-based approaches. Includes corrPrune() for fast correlation-based pruning, modelPrune() for VIF-based regression pruning, and exact graph-theoretic algorithms (Eppstein–Löffler–Strash, Bron–Kerbosch) for exhaustive subset enumeration. Supports linear models, GLMs, and mixed models ('lme4', 'glmmTMB').",
    "version": "3.1.0",
    "maintainer": "Gilles Colling <gilles.colling051@gmail.com>",
    "url": "https://gillescolling.com/corrselect/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10870,
    "package_name": "corset",
    "title": "Arbitrary Bounding of Series and Time Series Objects",
    "description": "Set of methods to constrain numerical series and time series within\n             arbitrary boundaries.",
    "version": "0.1-5",
    "maintainer": "Fran Urbano <viraltux@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10884,
    "package_name": "cossonet",
    "title": "Sparse Nonparametric Regression for High-Dimensional Data",
    "description": "Estimation of sparse nonlinear functions in nonparametric regression using component selection and smoothing. Designed for the analysis of high-dimensional data, the models support various data types, including exponential family models and Cox proportional hazards models. The methodology is based on Lin and Zhang (2006) <doi:10.1214/009053606000000722>.",
    "version": "1.0",
    "maintainer": "Jieun Shin <jieunstat@uos.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10885,
    "package_name": "costat",
    "title": "Time Series Costationarity Determination",
    "description": "Contains functions that can determine whether a time series\n\tis second-order stationary or not (and hence evidence for\n\tlocally stationarity). Given two non-stationary series (i.e.\n\tlocally stationary series) this package can then discover\n\ttime-varying linear combinations that are second-order stationary.\n\tCardinali, A. and Nason, G.P. (2013)\n\t<doi:10.18637/jss.v055.i01>.",
    "version": "2.4.1",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10890,
    "package_name": "countHMM",
    "title": "Penalized Estimation of Flexible Hidden Markov Models for Time\nSeries of Counts",
    "description": "Provides tools for penalized estimation of flexible hidden Markov models for time series of counts w/o the need to specify a (parametric) family of distributions. These include functions for model fitting, model checking, and state decoding. For details, see Adam, T., Langrock, R., and Weiß, C.H. (2019): Penalized Estimation of Flexible Hidden Markov Models for Time Series of Counts. <arXiv:1901.03275>.",
    "version": "0.1.0",
    "maintainer": "Timo Adam <timo.adam@uni-bielefeld.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10898,
    "package_name": "countgmifs",
    "title": "Discrete Response Regression for High-Dimensional Data",
    "description": "Provides a function for fitting Poisson and negative binomial regression models when the number of parameters exceeds the sample size, using the the generalized monotone incremental forward stagewise method.",
    "version": "0.0.2",
    "maintainer": "Kellie Archer <archer.43@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10913,
    "package_name": "covafillr",
    "title": "Local Polynomial Regression of State Dependent Covariates in\nState-Space Models",
    "description": "Facilitates local polynomial regression for state dependent covariates in state-space models. The functionality can also be used from 'C++' based model builder tools such as 'Rcpp'/'inline', 'TMB', or 'JAGS'.",
    "version": "0.4.4",
    "maintainer": "Christoffer Moesgaard Albertsen <cmoe@aqua.dtu.dk>",
    "url": "https://github.com/calbertsen/covafillr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10937,
    "package_name": "covidcast",
    "title": "Client for Delphi's 'COVIDcast Epidata' API",
    "description": "Tools for Delphi's 'COVIDcast Epidata' API: data access, maps and\n    time series plotting, and basic signal processing. The API includes a\n    collection of numerous indicators relevant to the COVID-19 pandemic in the\n    United States, including official reports, de-identified aggregated medical\n    claims data, large-scale surveys of symptoms and public behavior, and\n    mobility data, typically updated daily and at the county level. All data\n    sources are documented at\n    <https://cmu-delphi.github.io/delphi-epidata/api/covidcast.html>.",
    "version": "0.5.3",
    "maintainer": "Alex Reinhart <areinhar@stat.cmu.edu>",
    "url": "https://cmu-delphi.github.io/covidcast/covidcastR/,\nhttps://github.com/cmu-delphi/covidcast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10947,
    "package_name": "cowbell",
    "title": "Performs Segmented Linear Regression on Two Independent\nVariables",
    "description": "Implements a specific form of segmented linear regression\n    with two independent variables. The visualization of that function looks \n    like a quarter segment of a cowbell giving the package its name. \n    The package has been specifically constructed for the case where minimum \n    and maximum value of the dependent and two independent variables \n    are known a prior, which is usually the case\n    when those values are derived from Likert scales.",
    "version": "0.1.0",
    "maintainer": "Christoph Luerig <luerig@hochschule-trier.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10951,
    "package_name": "coxerr",
    "title": "Cox Regression with Dependent Error in Covariates",
    "description": "Perform the functional modeling methods of Huang and Wang (2018) <doi:10.1111/biom.12741> to accommodate dependent error in covariates of the proportional hazards model. The adopted measurement error model has minimal assumptions on the dependence structure, and an instrumental variable is supposed to be available.",
    "version": "1.1",
    "maintainer": "Yijian Huang <yhuang5@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10952,
    "package_name": "coxme",
    "title": "Mixed Effects Cox Models",
    "description": "Fit Cox proportional hazards models containing both \n fixed and random effects.  The random effects can have a general form, of which\n familial interactions (a \"kinship\" matrix) is a particular special case. \n Note that the simplest case of a mixed effects Cox model, i.e. a single random \n per-group intercept, is also called a \"frailty\" model.  The approach is based\n on Ripatti and Palmgren, Biometrics 2002.",
    "version": "2.2-22",
    "maintainer": "Terry M. Therneau <therneau.terry@mayo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10953,
    "package_name": "coxphSGD",
    "title": "Stochastic Gradient Descent log-Likelihood Estimation in Cox\nProportional Hazards Model",
    "description": "Estimate coefficients of Cox proportional hazards model using stochastic gradient descent algorithm for batch data.",
    "version": "0.2.1",
    "maintainer": "Marcin Kosinski <m.p.kosinski@gmail.com>",
    "url": "https://github.com/MarcinKosinski/coxphSGD/blob/master/README.md",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10954,
    "package_name": "coxphf",
    "title": "Cox Regression with Firth's Penalized Likelihood",
    "description": "Implements Firth's penalized maximum likelihood bias reduction method  for Cox regression\n  which has been shown to provide a solution in case of monotone likelihood (nonconvergence of likelihood function), see \n  Heinze and Schemper (2001) and Heinze and Dunkler (2008).\n  The program fits profile penalized likelihood confidence intervals which were proved to outperform\n  Wald confidence intervals.",
    "version": "1.13.4",
    "maintainer": "Georg Heinze <georg.heinze@meduniwien.ac.at>",
    "url": "https://cemsiis.meduniwien.ac.at/kb/wf/software/statistische-software/fccoxphf/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10955,
    "package_name": "coxphm",
    "title": "Time-to-Event Data Analysis with Missing Survival Times",
    "description": "Fits a pseudo Cox proprotional hazards model when survival times are missing for control groups.",
    "version": "0.2.1",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10957,
    "package_name": "coxrobust",
    "title": "Fit Robustly Proportional Hazards Regression Model",
    "description": "An implementation of robust estimation in Cox model. Functionality includes fitting efficiently and robustly Cox proportional hazards regression model in its basic form, where explanatory variables are time independent with one event per subject.  Method is based on a smooth modification of the partial likelihood.",
    "version": "1.0.2",
    "maintainer": "Shana Scogin <shanarscogin@gmail.com>",
    "url": "https://github.com/ShanaScogin/coxrobust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10958,
    "package_name": "coxsei",
    "title": "Fitting a CoxSEI Model",
    "description": "Fit a CoxSEI (Cox type Self-Exciting Intensity) model to right-censored counting process data.",
    "version": "0.4",
    "maintainer": "Feng Chen <feng.chen@unsw.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10961,
    "package_name": "cpam",
    "title": "Changepoint Additive Models for Time Series Omics Data",
    "description": "Provides a comprehensive framework for time series omics analysis,\n  integrating changepoint detection, smooth and shape-constrained trends,\n  and uncertainty quantification. It supports gene- and transcript-level inferences,\n  p-value aggregation for improved power, and both case-only and case-control designs.\n  It includes an interactive 'shiny' interface. The methods are described in Yates et al. (2024) <doi:10.1101/2024.12.22.630003>.",
    "version": "0.1.3",
    "maintainer": "Luke Yates <luke.yates@utas.edu.au>",
    "url": "https://l-a-yates.github.io/cpam/,\nhttps://github.com/l-a-yates/cpam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10966,
    "package_name": "cplm",
    "title": "Compound Poisson Linear Models",
    "description": "Likelihood-based and Bayesian methods for various compound Poisson linear models based on Zhang, Yanwei (2013) <doi:10.1007/s11222-012-9343-7>.",
    "version": "0.7-12.1",
    "maintainer": "Yanwei (Wayne) Zhang <actuary_zhang@hotmail.com>",
    "url": "https://github.com/actuaryzhang/cplm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 10988,
    "package_name": "cpsurvsim",
    "title": "Simulating Survival Data from Change-Point Hazard Distributions",
    "description": "Simulates time-to-event data\n    with type I right censoring using two methods: the inverse CDF\n    method and our proposed memoryless method. The latter method\n    takes advantage of the memoryless property of survival and\n    simulates a separate distribution between change-points. We\n    include two parametric distributions: exponential and Weibull.\n    Inverse CDF method draws on the work of Rainer Walke (2010), \n    <https://www.demogr.mpg.de/papers/technicalreports/tr-2010-003.pdf>.",
    "version": "1.2.2",
    "maintainer": "Camille Hochheimer <dochoch19@gmail.com>",
    "url": "https://github.com/camillejo/cpsurvsim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11045,
    "package_name": "crmReg",
    "title": "Cellwise Robust M-Regression and SPADIMO",
    "description": "Method for fitting a cellwise robust linear M-regression model (CRM, Filzmoser et al. (2020) <DOI:10.1016/j.csda.2020.106944>) that yields both a map of cellwise outliers consistent with the linear model, and a vector of regression coefficients that is robust against vertical outliers and leverage points. As a by-product, the method yields an imputed data set that contains estimates of what the values in cellwise outliers would need to amount to if they had fit the model. The package also provides diagnostic tools for analyzing casewise and cellwise outliers using sparse directions of maximal outlyingness (SPADIMO, Debruyne et al. (2019) <DOI:10.1007/s11222-018-9831-5>).",
    "version": "1.0.4",
    "maintainer": "Sebastiaan Hoppner <sebastiaan.hoppner@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11062,
    "package_name": "crosslag",
    "title": "Perform Linear or Nonlinear Cross Lag Analysis",
    "description": "Linear or nonlinear cross-lagged panel model can be built from input data. Users can choose the appropriate method from three methods for constructing nonlinear cross lagged models. These three methods include polynomial regression, generalized additive model and generalized linear mixed model.In addition, a function for determining linear relationships is provided. \n    Relevant knowledge of cross lagged models can be learned through the paper by Fredrik Falkenström (2024) <doi:10.1016/j.cpr.2024.102435> and the paper by A Gasparrini (2010) <doi:10.1002/sim.3940>.",
    "version": "0.1.0",
    "maintainer": "Yaxin Li <LYX010308@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11066,
    "package_name": "crossnma",
    "title": "Cross-Design & Cross-Format Network Meta-Analysis and Regression",
    "description": "Network meta-analysis and meta-regression (allows\n  including up to three covariates) for individual participant data,\n  aggregate data, and mixtures of both formats using the three-level\n  hierarchical model. Each format can come from randomized controlled\n  trials or non-randomized studies or mixtures of both. Estimates are\n  generated in a Bayesian framework using JAGS. The implemented models\n  are described by Hamza et al. 2023 <DOI:10.1002/jrsm.1619>.",
    "version": "1.3.0",
    "maintainer": "Guido Schwarzer <guido.schwarzer@uniklinik-freiburg.de>",
    "url": "https://github.com/htx-r/crossnma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11072,
    "package_name": "crossvalidationCP",
    "title": "Cross-Validation for Change-Point Regression",
    "description": "Implements the cross-validation methodology from Pein and Shah (2021) <arXiv:2112.03220>. Can be customised by providing different cross-validation criteria, estimators for the change-point locations and local parameters, and freely chosen folds. Pre-implemented estimators and criteria are available. It also includes our own implementation of the COPPS procedure <doi:10.1214/19-AOS1814>.",
    "version": "1.1",
    "maintainer": "Pein Florian <f.pein@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11074,
    "package_name": "crov",
    "title": "Constrained Regression Model for an Ordinal Response and Ordinal\nPredictors",
    "description": "Fits a constrained regression model for an ordinal response with ordinal predictors and possibly others, Espinosa and Hennig (2019) <DOI:10.1007/s11222-018-9842-2>. The parameter estimates associated with an ordinal predictor are constrained to be monotonic. If a monotonicity direction (isotonic or antitonic) is not specified for an ordinal predictor by the user, then one of the available methods will either establish it or drop the monotonicity assumption. Two monotonicity tests are also available to test the null hypothesis of monotonicity over a set of parameters associated with an ordinal predictor.",
    "version": "0.3.0",
    "maintainer": "Javier Espinosa <javier.espinosa.b@usach.cl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11077,
    "package_name": "crrSC",
    "title": "Competing Risks Regression for Stratified and Clustered Data",
    "description": "Extension of 'cmprsk' to Stratified and Clustered data.    \n      A goodness of fit test for Fine-Gray model is also provided.        \n      Methods are detailed in the following articles: Zhou et al. (2011) <doi:10.1111/j.1541-0420.2010.01493.x>,\n      Zhou et al. (2012) <doi:10.1093/biostatistics/kxr032>, \n      Zhou et al. (2013) <doi: 10.1002/sim.5815>.",
    "version": "1.1.2",
    "maintainer": "Aurelien Latouche <aurelien.latouche@cnam.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11078,
    "package_name": "crrcbcv",
    "title": "Bias-Corrected Variance for Competing Risks Regression with\nClustered Data",
    "description": "\n  A user friendly function 'crrcbcv' to compute bias-corrected variances for competing risks regression models using proportional subdistribution hazards with small-sample clustered data. Four types of bias correction are included: the MD-type bias correction by Mancl and DeRouen (2001) <doi:10.1111/j.0006-341X.2001.00126.x>, the KC-type bias correction by Kauermann and Carroll (2001) <doi:10.1198/016214501753382309>, the FG-type bias correction by Fay and Graubard (2001) <doi:10.1111/j.0006-341X.2001.01198.x>, and the MBN-type bias correction by Morel, Bokossa, and Neerchal (2003) <doi:10.1002/bimj.200390021>.",
    "version": "1.0",
    "maintainer": "Xinyuan Chen <xchen@math.msstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11079,
    "package_name": "crrstep",
    "title": "Stepwise Covariate Selection for the Fine & Gray Competing Risks\nRegression Model",
    "description": "Performs forward and backwards stepwise regression for the Proportional subdistribution hazards model in competing risks (Fine & Gray 1999). Procedure uses AIC, BIC and BICcr as selection criteria. BICcr has a penalty of k = log(n*), where n* is the number of primary events.",
    "version": "2024.1.1",
    "maintainer": "Ravi Varadhan <ravi.varadhan@jhu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11080,
    "package_name": "crs",
    "title": "Categorical Regression Splines",
    "description": "Regression splines that handle a mix of continuous and categorical (discrete) data often encountered in applied settings. I would like to gratefully acknowledge support from the Natural Sciences and Engineering Research Council of Canada (NSERC, <https://www.nserc-crsng.gc.ca>), the Social Sciences and Humanities Research Council of Canada (SSHRC, <https://www.sshrc-crsh.gc.ca>), and the Shared Hierarchical Academic Research Computing Network (SHARCNET, <https://www.sharcnet.ca>). We would also like to acknowledge the contributions of the GNU GSL authors. In particular, we adapt the GNU GSL B-spline routine gsl_bspline.c adding automated support for quantile knots (in addition to uniform knots), providing missing functionality for derivatives, and for extending the splines beyond their endpoints.",
    "version": "0.15-38",
    "maintainer": "Jeffrey S. Racine <racinej@mcmaster.ca>",
    "url": "https://github.com/JeffreyRacine/R-Package-crs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11083,
    "package_name": "crsnls",
    "title": "Nonlinear Regression Parameters Estimation by 'CRS4HC' and\n'CRS4HCe'",
    "description": "Functions for nonlinear regression parameters estimation by algorithms based on Controlled Random Search algorithm.\n  Both functions (crs4hc(), crs4hce()) adapt current search strategy by four heuristics competition. In addition, crs4hce() improves adaptability by adaptive stopping condition.",
    "version": "0.2",
    "maintainer": "Tomáš Goryl <tomas.goryl@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11104,
    "package_name": "csampling",
    "title": "Functions for Conditional Simulation in Regression-Scale Models",
    "description": "Implements Monte Carlo conditional inference for the parameters of a linear nonnormal regression model.",
    "version": "1.2-4.1",
    "maintainer": "Alessandra R. Brazzale <alessandra.brazzale@unipd.it>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11106,
    "package_name": "csci",
    "title": "Current Status Confidence Intervals",
    "description": "Calculates pointwise confidence intervals for the cumulative distribution function of the event time for current status data, data where each individual is assessed at one time to see if they had the event or not by the assessment time.",
    "version": "0.9.3",
    "maintainer": "Michael P. Fay <mfay@niaid.nih.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11116,
    "package_name": "cspec",
    "title": "Complete Discrete Fourier Transform (DFT) and Periodogram",
    "description": "Calculate the predictive discrete Fourier transform, complete discrete Fourier transform, complete periodogram, and tapered complete periodogram. This algorithm is based on the preprint \"Spectral methods for small sample time series: A complete periodogram approach\" (2020) by Sourav Das, Suhasini Subba Rao, and Junho Yang.",
    "version": "0.1.2",
    "maintainer": "Junho Yang <junhoyang@stat.tamu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11120,
    "package_name": "csranks",
    "title": "Statistical Tools for Ranks",
    "description": "Account for uncertainty when working with ranks. \n    Estimate standard errors consistently in linear regression with ranked variables.\n    Construct confidence sets of various kinds for positions of populations in a ranking \n    based on values of a certain feature and their estimation errors. \n    Theory based on Mogstad, Romano, Shaikh, and Wilhelm (2023)<doi:10.1093/restud/rdad006> and\n    Chetverikov and Wilhelm (2023) <doi:10.48550/arXiv.2310.15512>.",
    "version": "1.2.3",
    "maintainer": "Daniel Wilhelm <d.wilhelm@lmu.de>",
    "url": "https://github.com/danielwilhelm/R-CS-ranks,\nhttps://danielwilhelm.github.io/R-CS-ranks/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11125,
    "package_name": "csurvey",
    "title": "Constrained Regression for Survey Data",
    "description": "Domain mean estimation with monotonicity or block monotone constraints. See Xu X, Meyer MC and Opsomer JD (2021)<doi:10.1016/j.jspi.2021.02.004> for more details. ",
    "version": "1.14",
    "maintainer": "Xiyue Liao <xliao@sdsu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11133,
    "package_name": "ctbi",
    "title": "A Procedure to Clean, Decompose and Aggregate Timeseries",
    "description": "Clean, decompose and aggregate univariate time series following the procedure \"Cyclic/trend decomposition using bin interpolation\" and the Logbox method for flagging outliers, both detailed in Ritter, F.: Technical note: A procedure to clean, decompose, and aggregate time series, Hydrol. Earth Syst. Sci., 27, 349–361, <doi:10.5194/hess-27-349-2023>, 2023.",
    "version": "2.0.5",
    "maintainer": "Francois Ritter <ritter.francois@gmail.com>",
    "url": "https://github.com/fritte2/ctbi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11139,
    "package_name": "cthreshER",
    "title": "Continuous Threshold Expectile Regression",
    "description": "Estimation and inference methods for the continuous threshold expectile regression.\n    It can fit the continuous threshold expectile regression and test the existence of change point,\n    for the paper, \"Feipeng Zhang and Qunhua Li (2016). A continuous threshold expectile regression, submitted.\" ",
    "version": "1.1.0",
    "maintainer": "Feipeng Zhang <zhangfp108@gmail.com>",
    "url": "https://arxiv.org/abs/1611.02609",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11146,
    "package_name": "ctqr",
    "title": "Censored and Truncated Quantile Regression",
    "description": "Estimation of quantile regression models for survival data.",
    "version": "2.2",
    "maintainer": "Paolo Frumento <paolo.frumento@unipi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11152,
    "package_name": "ctsemOMX",
    "title": "Continuous Time SEM - 'OpenMx' Based Functions",
    "description": "Original 'ctsem' (continuous time structural equation modelling)\n    functionality, based on the 'OpenMx' software, as described in \n    Driver, Oud, Voelkle (2017) <doi:10.18637/jss.v077.i05>, with updated details in vignette. \n    Combines stochastic differential equations representing latent processes with \n    structural equation measurement models. These functions were split off from\n    the main package of 'ctsem', as the main package uses the 'rstan' package as a backend now --\n    offering estimation options from max likelihood to Bayesian.\n    There are nevertheless use cases for the wide format SEM style approach as offered here, \n    particularly when there are no individual differences in observation timing and the\n    number of individuals is large. For the main 'ctsem' package, see <https://cran.r-project.org/package=ctsem>.",
    "version": "1.0.7",
    "maintainer": "Charles Driver <charles.driver2@uzh.ch>",
    "url": "https://github.com/cdriveraus/ctsemOMX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11154,
    "package_name": "ctsmTMB",
    "title": "Continuous Time Stochastic Modelling using Template Model\nBuilder",
    "description": "Perform state and parameter inference, and forecasting, in\n    stochastic state-space systems using the 'ctsmTMB' class. This class,\n    built with the 'R6' package, provides a user-friendly interface for\n    defining and handling state-space models. Inference is based on\n    maximum likelihood estimation, with derivatives efficiently computed\n    through automatic differentiation enabled by the 'TMB'/'RTMB' packages\n    (Kristensen et al., 2016) <doi:10.18637/jss.v070.i05>. The available\n    inference methods include Kalman filters, in addition to a Laplace\n    approximation-based smoothing method. For further details of these\n    methods refer to the documentation of the 'CTSMR' package\n    <https://ctsm.info/ctsmr-reference.pdf> and Thygesen (2025)\n    <doi:10.48550/arXiv.2503.21358>. Forecasting capabilities include\n    moment predictions and stochastic path simulations, both implemented\n    in 'C++' using 'Rcpp' (Eddelbuettel et al., 2018)\n    <doi:10.1080/00031305.2017.1375990> for computational efficiency.",
    "version": "1.0.1",
    "maintainer": "Phillip Vetter <pbrve@dtu.dk>",
    "url": "https://github.com/phillipbvetter/ctsmTMB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11156,
    "package_name": "ctxR",
    "title": "Utilities for Interacting with the 'CTX' APIs",
    "description": "Access chemical, hazard, bioactivity, and exposure data from the \n   Computational Toxicology and Exposure ('CTX') APIs \n   <https://www.epa.gov/comptox-tools/computational-toxicology-and-exposure-apis>. \n   'ctxR' was developed to streamline the process of accessing the information \n   available through the 'CTX' APIs without requiring prior knowledge of how to \n   use APIs. Most data is also available on the CompTox Chemical Dashboard \n   ('CCD') <https://comptox.epa.gov/dashboard/> and other resources found at the \n   EPA Computational Toxicology and Exposure Online Resources \n   <https://www.epa.gov/comptox-tools>. ",
    "version": "1.1.3",
    "maintainer": "Madison Feshuk <feshuk.madison@epa.gov>",
    "url": "https://github.com/USEPA/ctxR, https://usepa.github.io/ctxR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11178,
    "package_name": "curesurv",
    "title": "Mixture and Non Mixture Parametric Cure Models to Estimate Cure\nIndicators",
    "description": "Fits a variety of cure models using excess hazard modeling methodology such as the mixture model proposed by Phillips et al. (2002) <doi:10.1002/sim.1101> The Weibull distribution is used to represent the survival function of the uncured patients; Fits also non-mixture cure model such as the time-to-null excess hazard model proposed by Boussari et al. (2020) <doi:10.1111/biom.13361>.",
    "version": "0.1.2",
    "maintainer": "Juste Goungounga <juste.goungounga@ehesp.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11181,
    "package_name": "currentSurvival",
    "title": "Estimation of CCI and CLFS Functions",
    "description": "The currentSurvival package contains functions for\n  the estimation of the current cumulative incidence (CCI) and\n  the current leukaemia-free survival (CLFS). The CCI is the \n  probability that a patient is alive and in any disease \n  remission (e.g. complete cytogenetic remission in chronic \n  myeloid leukaemia) after initiating his or her therapy (e.g. \n  tyrosine kinase therapy for chronic myeloid leukaemia). The \n  CLFS is the probability that a patient is alive and in any \n  disease remission after achieving the first disease remission.",
    "version": "1.1",
    "maintainer": "Eva Koritakova <koritakova@iba.muni.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11196,
    "package_name": "customizedTraining",
    "title": "Customized Training for Lasso and Elastic-Net Regularized\nGeneralized Linear Models",
    "description": "Customized training is a simple technique for transductive\n    learning, when the test covariates are known at the time of training. The\n    method identifies a subset of the training set to serve as the training set\n    for each of a few identified subsets in the training set. This package\n    implements customized training for the glmnet() and cv.glmnet() functions.",
    "version": "1.3",
    "maintainer": "Scott Powers <saberpowers@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11200,
    "package_name": "cutpoint",
    "title": "Estimate Cutpoints of Metric Variables in the Context of Cox\nRegression",
    "description": "Estimate one or two cutpoints of a metric or ordinal-scaled \n    variable in the multivariable context of survival data or \n    time-to-event data. Visualise the cutpoint estimation process using contour \n    plots, index plots, and spline plots. It is also possible to estimate \n    cutpoints based on the assumption of a U-shaped or inverted U-shaped \n    relationship between the predictor and the hazard ratio.\n    Govindarajulu, U., and Tarpey, T. (2022) <doi:10.1080/02664763.2020.1846690>.",
    "version": "1.0.0",
    "maintainer": "Jan Porthun <jan.porthun@ntnu.no>",
    "url": "https://github.com/jan-por/cutpoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11202,
    "package_name": "cv",
    "title": "Cross-Validating Regression Models",
    "description": "Cross-validation methods of regression models that exploit features of various\n    modeling functions to improve speed. Some of the methods implemented in the package are\n    novel, as described in the package vignettes; for general introductions to cross-validation,\n    see, for example, Gareth James, Daniela Witten, Trevor Hastie, and Robert Tibshirani \n    (2021, ISBN 978-1-0716-1417-4, Secs. 5.1, 5.3), \"An Introduction to Statistical Learning with \n    Applications in R, Second Edition\", and Trevor Hastie, Robert Tibshirani, \n    and Jerome Friedman (2009, ISBN 978-0-387-84857-0, Sec. 7.10), \"The Elements of Statistical \n    Learning, Second Edition\".",
    "version": "2.0.4",
    "maintainer": "Georges Monette <georges+cv@yorku.ca>",
    "url": "https://gmonette.github.io/cv/,\nhttps://CRAN.R-project.org/package=cv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11206,
    "package_name": "cvLM",
    "title": "Cross-Validation for Linear & Ridge Regression Models",
    "description": "Efficient implementations of cross-validation techniques for linear and ridge regression models, leveraging 'C++' code with 'Rcpp', 'RcppParallel', and 'Eigen' libraries. It supports leave-one-out, generalized, and K-fold cross-validation methods, utilizing 'Eigen' matrices for high performance. Methodology references: Hastie, Tibshirani, and Friedman (2009) <doi:10.1007/978-0-387-84858-7>.",
    "version": "1.0.4",
    "maintainer": "Philip Nye <phipnye@proton.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11207,
    "package_name": "cvTools",
    "title": "Cross-Validation Tools for Regression Models",
    "description": "Tools that allow developers to write functions for\n    cross-validation with minimal programming effort and assist\n    users with model selection.",
    "version": "0.3.3",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11210,
    "package_name": "cvasi",
    "title": "Calibration, Validation, and Simulation of TKTD Models",
    "description": "Eases the use of ecotoxicological effect models. Can simulate\n    common toxicokinetic-toxicodynamic (TK/TD) models such as \n    General Unified Threshold models of Survival (GUTS) and Lemna. It can\n    derive effects and effect profiles (EPx) from scenarios. It supports the\n    use of 'tidyr' workflows employing the pipe symbol. Time-consuming\n    tasks can be parallelized.",
    "version": "1.5.0",
    "maintainer": "Nils Kehrein <nils.kehrein@gmail.com>",
    "url": "https://github.com/cvasi-tktd/cvasi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11216,
    "package_name": "cvmgof",
    "title": "Cramer-von Mises Goodness-of-Fit Tests",
    "description": "It is devoted to Cramer-von Mises goodness-of-fit tests.\n    It implements three statistical methods based on Cramer-von Mises statistics to estimate and test a regression model.",
    "version": "1.0.3",
    "maintainer": "Romain Azais <romain.azais@inria.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11227,
    "package_name": "cyclomort",
    "title": "Survival Modeling with a Periodic Hazard Function",
    "description": "Modeling periodic mortality (or other time-to event) processes from right-censored data. Given observations of a process with a known period (e.g. 365 days, 24 hours), functions determine the number, intensity, timing, and duration of peaks of periods of elevated hazard within a period.  The underlying model is a mixed wrapped Cauchy function fitted using maximum likelihoods (details in Gurarie et al. (2020) <doi:10.1111/2041-210X.13305>). The development of these tools was motivated by the strongly seasonal mortality patterns observed in many wild animal populations. Thus, the respective periods of higher mortality can be identified as \"mortality seasons\". ",
    "version": "1.0.3",
    "maintainer": "Eliezer Gurarie <egurarie@esf.edu>",
    "url": "https://github.com/EliGurarie/cyclomort",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11257,
    "package_name": "dCovTS",
    "title": "Distance Covariance and Correlation for Time Series Analysis",
    "description": "Computing and plotting the distance covariance and correlation function of a univariate or a multivariate time series. Both versions of biased and unbiased estimators of distance covariance and correlation are provided. Test statistics for testing pairwise independence are also implemented. Some data sets are also included. References include: \n\t\t\t       a) Edelmann Dominic, Fokianos Konstantinos and Pitsillou Maria (2019). 'An Updated Literature Review of Distance Correlation and Its Applications to Time Series'. International Statistical Review, 87(2): 237--262. <doi:10.1111/insr.12294>.\n             b) Fokianos Konstantinos and Pitsillou Maria (2018). 'Testing independence for multivariate time series via the auto-distance correlation matrix'. Biometrika, 105(2): 337--352. <doi:10.1093/biomet/asx082>.\n             c) Fokianos Konstantinos and Pitsillou Maria (2017). 'Consistent testing for pairwise dependence in time series'. Technometrics, 59(2): 262--270. <doi:10.1080/00401706.2016.1156024>.\n             d) Pitsillou Maria and Fokianos Konstantinos (2016). 'dCovTS: Distance Covariance/Correlation for Time Series'. R Journal, 8(2):324-340. <doi:10.32614/RJ-2016-049>.",
    "version": "1.4",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11260,
    "package_name": "dLagM",
    "title": "Time Series Regression Models with Distributed Lag Models",
    "description": "Provides time series regression models with one predictor using finite distributed lag models, polynomial (Almon) distributed lag models, geometric distributed lag models with Koyck transformation, and autoregressive distributed lag models. It also consists of functions for computation of h-step ahead forecasts from these models. See Demirhan (2020)(<doi:10.1371/journal.pone.0228812>) and Baltagi (2011)(<doi:10.1007/978-3-642-20059-5>) for more information.",
    "version": "1.1.13",
    "maintainer": "Haydar Demirhan <haydar.demirhan@rmit.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11262,
    "package_name": "dMrs",
    "title": "Competing Risk in Dependent Net Survival Analysis",
    "description": "Provides statistical tools for \n\tanalyzing net and relative survival, with a key feature \n\tof relaxing the assumption of independent censoring and \n\tincorporating the effect of dependent competing risks. \n\tIt employs a copula-based methodology, specifically the \n\tArchimedean copula, to simulate data, conduct survival \n\tanalysis, and offer comparisons with other methods. \n\tThis approach is detailed in the work of \n\tAdatorwovor et al. (2022) <doi:10.1515/ijb-2021-0016>.",
    "version": "1.0.0",
    "maintainer": "Paul Little <pllittle321@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11291,
    "package_name": "dagitty",
    "title": "Graphical Analysis of Structural Causal Models",
    "description": "A port of the web-based software 'DAGitty', available at \n    <https://dagitty.net>, for analyzing structural causal models \n    (also known as directed acyclic graphs or DAGs).\n    This package computes covariate adjustment sets for estimating causal\n    effects, enumerates instrumental variables, derives testable\n    implications (d-separation and vanishing tetrads), generates equivalent\n    models, and includes a simple facility for data simulation. ",
    "version": "0.3-4",
    "maintainer": "Johannes Textor <johannes.textor@gmx.de>",
    "url": "https://www.dagitty.net, https://github.com/jtextor/dagitty",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11305,
    "package_name": "dapper",
    "title": "Data Augmentation for Private Posterior Estimation",
    "description": "A data augmentation based sampler for conducting privacy-aware Bayesian inference. The dapper_sample()\n             function takes an existing sampler as input and automatically constructs\n             a privacy-aware sampler. The process of constructing a sampler is simplified \n             through the specification of four independent modules, allowing for\n             easy comparison between different privacy mechanisms by only swapping\n             out the relevant modules. Probability mass functions\n             for the discrete Gaussian and discrete Laplacian are provided to facilitate\n             analyses dealing with privatized count data. The output of dapper_sample()\n             can be analyzed using many of the same tools from the 'rstan' ecosystem. For methodological details\n             on the sampler see Ju et al. (2022) <doi:10.48550/arXiv.2206.00710>,\n             and for details on the discrete Gaussian and discrete Laplacian distributions see\n             Canonne et al. (2020) <doi:10.48550/arXiv.2004.00010>.",
    "version": "1.0.1",
    "maintainer": "Kevin Eng <kevine1221@gmail.com>",
    "url": "https://github.com/mango-empire/dapper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11320,
    "package_name": "data.table",
    "title": "Extension of `data.frame`",
    "description": "Fast aggregation of large data (e.g. 100GB in RAM), fast ordered joins, fast add/modify/delete of columns by group using no copies at all, list columns, friendly and fast character-separated-value read/write. Offers a natural and flexible syntax, for faster development.",
    "version": "1.18.0",
    "maintainer": "Tyson Barrett <t.barrett88@gmail.com>",
    "url": "https://r-datatable.com, https://Rdatatable.gitlab.io/data.table,\nhttps://github.com/Rdatatable/data.table",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11356,
    "package_name": "dataprep",
    "title": "Efficient and Flexible Data Preprocessing Tools",
    "description": "Efficiently and flexibly preprocess data using a set of data filtering, deletion, and interpolation tools.\n    These data preprocessing methods are developed based on the principles of completeness, accuracy, threshold method, and linear interpolation and through the setting of constraint conditions, time completion & recovery, and fast & efficient calculation and grouping.\n    Key preprocessing steps include deletions of variables and observations, outlier removal, and missing values (NA) interpolation, which are dependent on the incomplete and dispersed degrees of raw data.\n    They clean data more accurately, keep more samples, and add no outliers after interpolation, compared with ordinary methods.\n    Auto-identification of consecutive NA via run-length based grouping is used in observation deletion, outlier removal, and NA interpolation;\n    thus, new outliers are not generated in interpolation. Conditional extremum is proposed to realize point-by-point weighed outlier removal that saves non-outliers from being removed.\n    Plus, time series interpolation with values to refer to within short periods further ensures reliable interpolation.\n    These methods are based on and improved from the reference: Liang, C.-S., Wu, H., Li, H.-Y., Zhang, Q., Li, Z. & He, K.-B. (2020) <doi:10.1016/j.scitotenv.2020.140923>.",
    "version": "0.1.5",
    "maintainer": "Chun-Sheng Liang <liangchunsheng@lzu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11361,
    "package_name": "datarium",
    "title": "Data Bank for Statistical Analysis and Visualization",
    "description": "Contains data organized by topics: categorical data, regression model, \n            means comparisons, independent and repeated measures ANOVA, mixed ANOVA and ANCOVA.",
    "version": "0.1.0",
    "maintainer": "Alboukadel Kassambara <alboukadel.kassambara@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11370,
    "package_name": "datastat",
    "title": "Dataset for Statistical Analysis",
    "description": "Data are essential in statistical analysis.\n    This data package consists of four datasets for descriptive statistics, two datasets for statistical hypothesis testing, and two datasets for regression analysis.\n    All of the datasets are based on Rattanalertnusorn, A. (2024) <https://www.researchgate.net/publication/371944275_porkaermxarlaeakarprayuktchingan_R_and_its_applications>.   ",
    "version": "0.1.0",
    "maintainer": "Atchanut Rattanalertnusorn <atchanut_r@rmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11382,
    "package_name": "datetimeoffset",
    "title": "Datetimes with Optional UTC Offsets and/or Heterogeneous Time\nZones",
    "description": "Supports import/export for a number of datetime string standards \n    and R datetime classes often including\n    lossless re-export of     \n    any original reduced precision including 'ISO 8601' <https://en.wikipedia.org/wiki/ISO_8601> and \n    'pdfmark' <https://opensource.adobe.com/dc-acrobat-sdk-docs/library/pdfmark/> datetime strings.\n    Supports local/global datetimes with optional UTC offsets and/or (possibly heterogeneous) time zones\n    with up to nanosecond precision.",
    "version": "1.0.0",
    "maintainer": "Trevor L. Davis <trevor.l.davis@gmail.com>",
    "url": "https://trevorldavis.com/R/datetimeoffset/,\nhttps://github.com/trevorld/r-datetimeoffset",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11401,
    "package_name": "dbacf",
    "title": "Autocovariance Estimation via Difference-Based Methods",
    "description": "Provides methods for (auto)covariance/correlation function estimation \n    in change point regression with stationary errors circumventing the pre-estimation\n    of the underlying signal of the observations. Generic, first-order, (m+1)-gapped,\n    difference-based autocovariance function estimator is based on M. Levine and I. Tecuapetla-Gómez (2023) <doi:10.48550/arXiv.1905.04578>. Bias-reducing, second-order, (m+1)-gapped, \n    difference-based estimator is based on I. Tecuapetla-Gómez and A. Munk (2017) \n    <doi:10.1111/sjos.12256>. Robust autocovariance estimator for change point regression with autoregressive errors is based on S. Chakar et al. (2017) <doi:10.3150/15-BEJ782>. \n    It also includes a general projection-based method for covariance matrix estimation.",
    "version": "0.2.8",
    "maintainer": "Inder Tecuapetla-Gómez\n<itecuapetla@conabio.gob.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11402,
    "package_name": "dbarts",
    "title": "Discrete Bayesian Additive Regression Trees Sampler",
    "description": "Fits Bayesian additive regression trees (BART; Chipman, George, and McCulloch (2010) <doi:10.1214/09-AOAS285>) while allowing the updating of predictors or response so that BART can be incorporated as a conditional model in a Gibbs/Metropolis-Hastings sampler. Also serves as a drop-in replacement for package 'BayesTree'.",
    "version": "0.9-32",
    "maintainer": "Vincent Dorie <vdorie@gmail.com>",
    "url": "https://github.com/vdorie/dbarts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11408,
    "package_name": "dblcens",
    "title": "Compute the NPMLE of Distribution Function from Doubly Censored\nData, Plus the Empirical Likelihood Ratio for F(T)",
    "description": "Doubly censored data, as described in Chang and Yang (1987) <doi: 10.1214/aos/1176350608>), are commonly seen in many fields. We use EM algorithm to compute the non-parametric MLE (NPMLE) of the cummulative probability function/survival function and the two censoring distributions. One can also specify a constraint F(T)=C, it will return the constrained NPMLE and the -2 log empirical likelihood ratio for this constraint. This can be used to test the hypothesis about the constraint and, by inverting the test, find confidence intervals for probability or quantile via empirical likelihood ratio theorem. Influence functions of hat F may also be calculated, but currently, the it may be slow.",
    "version": "1.1.9",
    "maintainer": "Yifan Yang <yfyang.86@hotmail.com>",
    "url": "https://github.com/yfyang86/dblcens/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11419,
    "package_name": "dbw",
    "title": "Doubly Robust Distribution Balancing Weighting Estimation",
    "description": "Implements the doubly robust distribution balancing weighting proposed by Katsumata (2024) <doi:10.1017/psrm.2024.23>, which improves the augmented inverse probability weighting (AIPW) by estimating propensity scores with estimating equations suitable for the pre-specified parameter of interest (e.g., the average treatment effects or the average treatment effects on the treated) and estimating outcome models with the estimated inverse probability weights. It also implements the covariate balancing propensity score proposed by Imai and Ratkovic (2014) <doi:10.1111/rssb.12027> and the entropy balancing weighting proposed by Hainmueller (2012) <doi:10.1093/pan/mpr025>, both of which use covariate balancing conditions in propensity score estimation. The point estimate of the parameter of interest and its uncertainty as well as coefficients for propensity score estimation and outcome regression are produced using the M-estimation. The same functions can be used to estimate average outcomes in missing outcome cases.",
    "version": "1.1.4",
    "maintainer": "Hiroto Katsumata <hrt.katsumata@gmail.com>",
    "url": "https://github.com/hirotokatsumata/dbw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11430,
    "package_name": "dclone",
    "title": "Data Cloning and MCMC Tools for Maximum Likelihood Methods",
    "description": "Low level functions for implementing\n    maximum likelihood estimating procedures for\n    complex models using data cloning and Bayesian\n    Markov chain Monte Carlo methods\n    as described in Solymos 2010 <doi:10.32614/RJ-2010-011>.\n    Sequential and parallel MCMC support\n    for 'JAGS', 'WinBUGS', 'OpenBUGS', and 'Stan'.",
    "version": "2.3-3",
    "maintainer": "Peter Solymos <psolymos@gmail.com>",
    "url": "https://groups.google.com/forum/#!forum/dclone-users,\nhttps://datacloning.org, https://github.com/datacloning/dclone",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11438,
    "package_name": "dcortools",
    "title": "Providing Fast and Flexible Functions for Distance Correlation\nAnalysis",
    "description": "Provides methods for distance covariance and distance correlation (Szekely, et al. (2007) <doi:10.1214/009053607000000505>), generalized version thereof (Sejdinovic, et al. (2013) <doi:10.1214/13-AOS1140>) and corresponding tests (Berschneider, Bottcher (2018) <doi:10.48550/arXiv.1808.07280>. Distance standard deviation methods (Edelmann, et al. (2020) <doi:10.1214/19-AOS1935>) and distance correlation methods for survival endpoints (Edelmann, et al. (2021) <doi:10.1111/biom.13470>) are also included.",
    "version": "0.1.7",
    "maintainer": "Dominic Edelmann <dominic.edelmann@dkfz-heidelberg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11451,
    "package_name": "ddecompose",
    "title": "Detailed Distributional Decomposition",
    "description": "Implements the Oaxaca-Blinder decomposition method and generalizations of it that decompose differences in distributional statistics beyond the mean.\n    The function ob_decompose() decomposes differences in the mean outcome between two groups into one part explained by different covariates (composition effect) and into another part due to differences in the way covariates are linked to the outcome variable (structure effect). The function further divides the two effects into the contribution of each covariate and allows for weighted doubly robust decompositions. For distributional statistics beyond the mean, the function performs the recentered influence function (RIF) decomposition proposed by Firpo, Fortin, and Lemieux (2018).\n    The function dfl_decompose() divides differences in distributional statistics into an composition effect and a structure effect using inverse probability weighting as introduced by DiNardo, Fortin, and Lemieux (1996). The function also allows to sequentially decompose the composition effect into the contribution of single covariates.\n    References: \n    Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2018) <doi:10.3390/econometrics6020028>. \"Decomposing Wage Distributions Using Recentered Influence Function Regressions.\"\n    Fortin, Nicole M., Thomas Lemieux, and Sergio Firpo. (2011) <doi:10.3386/w16045>. \"Decomposition Methods in Economics.\"\n    DiNardo, John, Nicole M. Fortin, and Thomas Lemieux. (1996) <doi:10.2307/2171954>. \"Labor Market Institutions and the Distribution of Wages, 1973-1992: A Semiparametric Approach.\"\n    Oaxaca, Ronald. (1973) <doi:10.2307/2525981>. \"Male-Female Wage Differentials in Urban Labor Markets.\"\n    Blinder, Alan S. (1973) <doi:10.2307/144855>. \"Wage Discrimination: Reduced Form and Structural Estimates.\"",
    "version": "1.0.0",
    "maintainer": "Samuel Meier <samuel.meier+ddecompose@immerda.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11453,
    "package_name": "ddiv",
    "title": "Data Driven I-v Feature Extraction",
    "description": "The Data Driven I-V Feature Extraction is used to extract \n    Current-Voltage (I-V) features from I-V curves. I-V curves indicate the \n    relationship between current and voltage for a solar cell or Photovoltaic (PV) \n    modules. The I-V features such as maximum power point (Pmp), shunt resistance (Rsh), \n    series resistance (Rs),short circuit current (Isc), open circuit voltage (Voc), \n    fill factor (FF), current at maximum power (Imp) and voltage at maximum power(Vmp) \n    contain important information of the performance for PV modules. The traditional \n    method uses the single diode model to model I-V curves and extract I-V features. \n    This package does not use the diode model, but uses data-driven a method which \n    select different linear parts of the I-V curves to extract I-V features. \n    This method also uses a sampling method to calculate uncertainties when extracting \n    I-V features. Also, because of the partially shaded array, \"steps\" occurs in \n    I-V curves. The \"Segmented Regression\" method is used to identify steps in I-V curves.\n    This material is based upon work supported by the U.S. Department of Energy’s \n    Office of Energy Efficiency and Renewable Energy (EERE) under Solar Energy \n    Technologies Office (SETO) Agreement Number DE-EE0007140. Further information can be \n    found in the following paper.\n    [1] Ma, X. et al, 2019. \n    <doi:10.1109/JPHOTOV.2019.2928477>.",
    "version": "0.1.1",
    "maintainer": "Megan M. Morbitzer <mmm308@case.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11461,
    "package_name": "ddtlcm",
    "title": "Latent Class Analysis with Dirichlet Diffusion Tree Process\nPrior",
    "description": "Implements a Bayesian algorithm for overcoming weak separation in Bayesian latent class analysis. \n    Reference: Li et al. (2023) <arXiv:2306.04700>.",
    "version": "0.2.1",
    "maintainer": "Mengbing Li <mengbing@umich.edu>",
    "url": "https://github.com/limengbinggz/ddtlcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11462,
    "package_name": "deBInfer",
    "title": "Bayesian Inference for Differential Equations",
    "description": "A Bayesian framework for parameter inference in differential equations.\n    This approach offers a rigorous methodology for parameter inference as well as\n    modeling the link between unobservable model states and parameters, and\n    observable quantities. Provides templates for the DE model, the\n    observation model and data likelihood, and the model parameters and their prior\n    distributions. A Markov chain Monte Carlo (MCMC) procedure processes these inputs\n    to estimate the posterior distributions of the parameters and any derived\n    quantities, including the model trajectories. Further functionality is provided\n    to facilitate MCMC diagnostics and the visualisation of the posterior distributions\n    of model parameters and trajectories.",
    "version": "0.4.4",
    "maintainer": "Philipp H Boersch-Supan <pboesu@gmail.com>",
    "url": "https://github.com/pboesu/debinfer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11464,
    "package_name": "deFit",
    "title": "Fitting Differential Equations to Time Series Data",
    "description": "Use numerical optimization to fit ordinary differential equations (ODEs) to time series data to examine the dynamic relationships between variables or the characteristics of a dynamical system. It can now be used to estimate the parameters of ODEs up to second order, and can also apply to multilevel systems. See <https://github.com/yueqinhu/defit> for details.",
    "version": "0.3.0",
    "maintainer": "Yueqin Hu <yueqinhu@bnu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11470,
    "package_name": "deal",
    "title": "Learning Bayesian Networks with Mixed Variables",
    "description": "Bayesian networks with continuous and/or discrete\n        variables can be learned and compared from data. The method is described in Boettcher and Dethlefsen (2003), <doi:10.18637/jss.v008.i20>.",
    "version": "1.2-42",
    "maintainer": "Claus Dethlefsen <rpackage.deal@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11475,
    "package_name": "debiasedTrialEmulation",
    "title": "Pipeline for Debiased Target Trial Emulation",
    "description": "Supports propensity score-based methods—including matching, stratification, and weighting—for estimating causal treatment effects. It also implements calibration using negative control outcomes to enhance robustness. 'debiasedTrialEmulation' facilitates effect estimation for both binary and time-to-event outcomes, supporting risk ratio (RR), odds ratio (OR), and hazard ratio (HR) as effect measures. It integrates statistical modeling and visualization tools to assess covariate balance, equipoise, and bias calibration. Additional methods—including approaches to address immortal time bias, information bias, selection bias, and informative censoring—are under development. Users interested in these extended features are encouraged to contact the package authors.",
    "version": "0.1.0",
    "maintainer": "Bingyu Zhang <bingyuz7@sas.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11489,
    "package_name": "decomposedPSF",
    "title": "Time Series Prediction with PSF and Decomposition Methods (EMD\nand EEMD)",
    "description": "Predict future values with hybrid combinations of Pattern Sequence based\n        Forecasting (PSF), Autoregressive Integrated Moving Average (ARIMA), Empirical Mode\n        Decomposition (EMD) and Ensemble Empirical Mode Decomposition (EEMD) methods based\n        hybrid methods.",
    "version": "0.2",
    "maintainer": "Neeraj Bokde <neerajdhanraj@gmail.com>",
    "url": "https://www.neerajbokde.in/software/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11501,
    "package_name": "decp",
    "title": "Complete Change Point Analysis",
    "description": "Provides a comprehensive approach for identifying and estimating change points in multivariate time series through various statistical methods. Implements the multiple change point detection methodology from Ryan & Killick (2023) <doi:10.1080/00401706.2023.2183261> and a novel estimation methodology from Fotopoulos et al. (2023) <doi:10.1007/s00362-023-01495-0> generalized to fit the detection methodologies. Performs both detection and estimation of change points, providing visualization and summary information of the estimation process for each detected change point.",
    "version": "0.1.2",
    "maintainer": "Vasileios Pavlopoulos <vasileios.pavlopoulos@uah.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11515,
    "package_name": "deepgp",
    "title": "Bayesian Deep Gaussian Processes using MCMC",
    "description": "Performs Bayesian posterior inference for deep Gaussian \n    processes following Sauer, Gramacy, and Higdon (2023, <doi:10.48550/arXiv.2012.08015>).  \n    See Sauer (2023, <http://hdl.handle.net/10919/114845>) for comprehensive \n    methodological details and <https://bitbucket.org/gramacylab/deepgp-ex/> for \n    a variety of coding examples. Models are trained through MCMC including \n    elliptical slice sampling of latent Gaussian layers and Metropolis-Hastings \n    sampling of kernel hyperparameters.  Gradient-enhancement and gradient\n    predictions are offered following Booth (2025, <doi:10.48550/arXiv.2512.18066>).\n    Vecchia approximation for faster \n    computation is implemented following Sauer, Cooper, and Gramacy \n    (2023, <doi:10.48550/arXiv.2204.02904>).  Optional monotonic warpings are \n    implemented following Barnett et al. (2025, <doi:10.48550/arXiv.2408.01540>).  \n    Downstream tasks include sequential design \n    through active learning Cohn/integrated mean squared error (ALC/IMSE; Sauer, \n    Gramacy, and Higdon, 2023), optimization through expected improvement \n    (EI; Gramacy, Sauer, and Wycoff, 2022, <doi:10.48550/arXiv.2112.07457>), \n    and contour location through entropy (Booth, Renganathan, and Gramacy, \n    2025, <doi:10.48550/arXiv.2308.04420>).  Models extend up to three layers deep; \n    a one layer model is equivalent to typical Gaussian process regression.  \n    Incorporates OpenMP and SNOW parallelization and utilizes C/C++ under the hood.",
    "version": "1.2.0",
    "maintainer": "Annie S. Booth <annie_booth@vt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11533,
    "package_name": "degradr",
    "title": "Estimating Remaining Useful Life with Linear Mixed Effects\nModels",
    "description": "Provides tools for estimating the Remaining Useful Life (RUL) of degrading systems using linear mixed-effects models and creating a health index. It supports both univariate and multivariate degradation signals. For multivariate inputs, the signals are merged into a univariate health index prior to modeling. Linear and exponential degradation trajectories are supported (the latter using a log transformation). Remaining Useful Life (RUL) distributions are estimated using Bayesian updating for new units, enabling on-site predictive maintenance. Based on the methodology of Liu and Huang (2016) <doi:10.1109/TASE.2014.2349733>.",
    "version": "1.0.1",
    "maintainer": "Pedro Abraham Montoya Calzada <pedroabraham.montoya@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11537,
    "package_name": "dejaVu",
    "title": "Multiple Imputation for Recurrent Events",
    "description": "Performs reference based multiple imputation of recurrent event data\n    based on a negative binomial regression model, as described\n    by Keene et al (2014) <doi:10.1002/pst.1624>.",
    "version": "0.3.1",
    "maintainer": "Jonathan Bartlett <jonathan.bartlett1@lshtm.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11549,
    "package_name": "deming",
    "title": "Deming, Theil-Sen, Passing-Bablock and Total Least Squares\nRegression",
    "description": "Generalized Deming regression, Theil-Sen regression and Passing-Bablock regression functions.",
    "version": "1.4-1",
    "maintainer": "Terry Therneau <therneau.terry@mayo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11563,
    "package_name": "dendroTools",
    "title": "Linear and Nonlinear Methods for Analyzing Daily and Monthly\nDendroclimatological Data",
    "description": "Provides novel dendroclimatological methods, primarily used by the\n    Tree-ring research community. There are four core functions. The first one is \n    daily_response(), which finds the optimal sequence of days that are related \n    to one or more tree-ring proxy records. Similar function is daily_response_seascorr(), \n    which implements partial correlations in the analysis of daily response functions.\n    For the enthusiast of monthly data, there is monthly_response() function.\n    The last core function is compare_methods(), which effectively compares several \n    linear and nonlinear regression algorithms on the task of climate reconstruction.   ",
    "version": "1.2.15",
    "maintainer": "Jernej Jevsenak <jernej.jevsenak@gmail.com>",
    "url": "https://github.com/jernejjevsenak/dendroTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11570,
    "package_name": "denoiSeq",
    "title": "Differential Expression Analysis Using a Bottom-Up Model",
    "description": "Given count data from two conditions, it determines which transcripts are differentially expressed across the two conditions using Bayesian inference of the parameters of  a bottom-up model for PCR amplification. This model is  developed in Ndifon Wilfred, Hilah Gal, Eric Shifrut, Rina Aharoni, Nissan Yissachar, Nir Waysbort, Shlomit Reich Zeliger, Ruth Arnon, and Nir Friedman (2012), <http://www.pnas.org/content/109/39/15865.full>, and results in a distribution for the counts that is a superposition of the binomial and negative binomial distribution.",
    "version": "0.1.1",
    "maintainer": "Gershom Buri <buri@aims.edu.gh>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11572,
    "package_name": "densEstBayes",
    "title": "Density Estimation via Bayesian Inference Engines",
    "description": "Bayesian density estimates for univariate continuous random samples are provided using the Bayesian inference engine paradigm. The engine options are: Hamiltonian Monte Carlo, the no U-turn sampler, semiparametric mean field variational Bayes and slice sampling. The methodology is described in Wand and Yu (2020) <arXiv:2009.06182>.",
    "version": "1.0-2.2",
    "maintainer": "Matt P. Wand <matt.wand@uts.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11573,
    "package_name": "denseFLMM",
    "title": "Functional Linear Mixed Models for Densely Sampled Data",
    "description": "Estimation of functional linear mixed models for densely sampled data based on functional principal component analysis.",
    "version": "0.1.3",
    "maintainer": "Jona Cederbaum <Jona.Cederbaum@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11585,
    "package_name": "depCensoring",
    "title": "Statistical Methods for Survival Data with Dependent Censoring",
    "description": "Several statistical methods for analyzing survival data under various forms of dependent\n    censoring are implemented in the package. In addition to accounting for dependent censoring, it \n    offers tools to adjust for unmeasured confounding factors. The implemented approaches allow \n    users to estimate the dependency between survival time and dependent censoring time, based \n    solely on observed survival data. For more details on the methods, refer to Deresa and Van \n    Keilegom (2021) <doi:10.1093/biomet/asaa095>, Czado and Van Keilegom (2023)\n    <doi:10.1093/biomet/asac067>, Crommen et al. (2024) <doi:10.1007/s11749-023-00903-9>,\n    Deresa and Van Keilegom (2024) <doi:10.1080/01621459.2022.2161387>, Willems et al. (2025) \n    <doi:10.48550/arXiv.2403.11860>, Ding and Van Keilegom (2025) and D'Haen et al. (2025)\n    <doi:10.1007/s10985-025-09647-0>.",
    "version": "0.1.8",
    "maintainer": "Negera Wakgari Deresa <negera.deresa@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11587,
    "package_name": "depcoeff",
    "title": "Dependency Coefficients",
    "description": "Functions to compute coefficients measuring the dependence of two or more than two variables. The functions can be deployed to gain information about functional dependencies of the variables with emphasis on monotone functions. The statistics describe how well one response variable can be approximated by a monotone function of other variables. In regression analysis the variable selection is an important issue. In this framework the functions could be useful tools in modeling the regression function. Detailed explanations on the subject can be found in papers Liebscher (2014) <doi:10.2478/demo-2014-0004>; Liebscher (2017) <doi:10.1515/demo-2017-0012>; Liebscher (2019, submitted).",
    "version": "0.0.1",
    "maintainer": "Eckhard Liebscher <eckhard.liebscher@hs-merseburg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11588,
    "package_name": "depend.truncation",
    "title": "Statistical Methods for the Analysis of Dependently Truncated\nData",
    "description": "Estimation and testing methods for dependently truncated data.\n Semi-parametric methods are based on Emura et al. (2011)<Stat Sinica 21:349-67>, Emura & Wang (2012)<doi:10.1016/j.jmva.2012.03.012>,\n and Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.\n Parametric approaches are based on Emura & Konno (2012)<doi:10.1007/s00362-014-0626-2> and Emura & Pan (2017)<doi:10.1007/s00362-017-0947-z>.\n A regression approach is based on Emura & Wang (2016)<doi:10.1007/s10463-015-0526-9>. Quasi-independence tests are based on Emura & Wang (2010)<doi:10.1016/j.jmva.2009.07.006>.\n Right-truncated data for Japanese male centenarians are given by Emura & Murotani (2015)<doi:10.1007/s11749-015-0432-8>.",
    "version": "3.0",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11590,
    "package_name": "depmixS4",
    "title": "Dependent Mixture Models - Hidden Markov Models of GLMs and\nOther Distributions in S4",
    "description": "Fits latent (hidden) Markov models on mixed categorical and continuous (time series) data, otherwise known as dependent mixture models, see Visser & Speekenbrink (2010, <DOI:10.18637/jss.v036.i07>).",
    "version": "1.5-1",
    "maintainer": "Ingmar Visser <i.visser@uva.nl>",
    "url": "https://depmix.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11604,
    "package_name": "descomponer",
    "title": "Seasonal Adjustment by Frequency Analysis",
    "description": "Decompose a time series into seasonal, trend and irregular components using transformations to amplitude-frequency domain.",
    "version": "1.6",
    "maintainer": "Francisco Parra  <parra_fj@cantabria.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11616,
    "package_name": "deseats",
    "title": "Data-Driven Locally Weighted Regression for Trend and\nSeasonality in TS",
    "description": "Various methods for the identification of trend and seasonal\n  components in time series (TS) are provided. Among them is a data-driven locally\n  weighted regression approach with automatically selected bandwidth for\n  equidistant short-memory time series. The approach is a\n  combination / extension of the algorithms by\n  Feng (2013) <doi:10.1080/02664763.2012.740626> and Feng, Y., Gries, T.,\n  and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598> and a brief\n  description of this new method is provided in the package documentation.\n  Furthermore, the package allows its users to apply the base model of the\n  Berlin procedure, version 4.1, as described in Speth (2004) <https://www.destatis.de/DE/Methoden/Saisonbereinigung/BV41-methodenbericht-Heft3_2004.pdf?__blob=publicationFile>.\n  Permission to include this procedure was kindly provided by the Federal\n  Statistical Office of Germany.",
    "version": "1.1.1",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11625,
    "package_name": "desla",
    "title": "Desparsified Lasso Inference for Time Series",
    "description": "Calculates the desparsified lasso as originally introduced in van de Geer et al. (2014) <doi:10.1214/14-AOS1221>, and provides inference suitable for high-dimensional time series, based on the long run covariance estimator in Adamek et al. (2020) <arXiv:2007.10952>. Also estimates high-dimensional local projections by the desparsified lasso, as described in Adamek et al. (2022) <arXiv:2209.03218>.",
    "version": "0.3.0",
    "maintainer": "Robert Adamek <robertadamek94@gmail.com>",
    "url": "https://github.com/RobertAdamek/desla",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11631,
    "package_name": "detectR",
    "title": "Change Point Detection",
    "description": "Time series analysis of network connectivity. Detects and visualizes change points between networks.\n    Methods included in the package are discussed in depth in Baek, C., Gates, K. M., Leinwand, B., Pipiras, V. (2021)\n    \"Two sample tests for high-dimensional auto-covariances\" <doi:10.1016/j.csda.2020.107067>\n    and Baek, C., Gampe, M., Leinwand B., Lindquist K., Hopfinger J. and Gates K. (2023)\n    “Detecting functional connectivity changes in fMRI data” <doi:10.1007/s11336-023-09908-7>.",
    "version": "0.3.0",
    "maintainer": "Changryong Baek <crbaek@skku.edu>",
    "url": "https://github.com/crbaek/detectR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11637,
    "package_name": "detectseparation",
    "title": "Detect and Check for Separation and Infinite Maximum Likelihood\nEstimates",
    "description": "Provides pre-fit and post-fit methods for detecting separation and infinite maximum likelihood estimates in generalized linear models with categorical responses. The pre-fit methods apply on binomial-response generalized liner models such as logit, probit and cloglog regression, and can be directly supplied as fitting methods to the glm() function. They solve the linear programming problems for the detection of separation developed in Konis (2007, <https://ora.ox.ac.uk/objects/uuid:8f9ee0d0-d78e-4101-9ab4-f9cbceed2a2a>) using 'ROI' <https://cran.r-project.org/package=ROI> or 'lpSolveAPI' <https://cran.r-project.org/package=lpSolveAPI>. The post-fit methods apply to models with categorical responses, including binomial-response generalized linear models and multinomial-response models, such as baseline category logits and adjacent category logits models; for example, the models implemented in the 'brglm2' <https://cran.r-project.org/package=brglm2> package. The post-fit methods successively refit the model with increasing number of iteratively reweighted least squares iterations, and monitor the ratio of the estimated standard error for each parameter to what it has been in the first iteration. According to the results in Lesaffre & Albert (1989, <https://www.jstor.org/stable/2345845>), divergence of those ratios indicates data separation.",
    "version": "0.3",
    "maintainer": "Ioannis Kosmidis <ioannis.kosmidis@warwick.ac.uk>",
    "url": "https://github.com/ikosmidis/detectseparation",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11654,
    "package_name": "dfadjust",
    "title": "Degrees of Freedom Adjustment for Robust Standard Errors",
    "description": "Computes small-sample degrees of freedom adjustment for\n    heteroskedasticity robust standard errors, and for clustered standard errors\n    in linear regression. See Imbens and Kolesár (2016)\n    <doi:10.1162/REST_a_00552> for a discussion of these adjustments.",
    "version": "1.1.0",
    "maintainer": "Michal Kolesár <kolesarmi@googlemail.com>",
    "url": "https://github.com/kolesarm/Robust-Small-Sample-Standard-Errors",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11669,
    "package_name": "dfrr",
    "title": "Dichotomized Functional Response Regression",
    "description": "Implementing Function-on-Scalar Regression model in which the response function is dichotomized and observed sparsely. This package provides smooth estimations of functional regression coefficients and principal components for the dichotomized functional response regression (dfrr) model.",
    "version": "0.1.5",
    "maintainer": "Fatemeh Asgari <fatemeh.asgari@medisin.uio.no>",
    "url": "https://github.com/asgari-fatemeh/dfrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11672,
    "package_name": "dga",
    "title": "Capture-Recapture Estimation using Bayesian Model Averaging",
    "description": "Performs Bayesian model averaging for capture-recapture. This includes code to stratify records, check the strata for suitable overlap to be used for capture-recapture, and some functions to plot the estimated population size. ",
    "version": "2.0.2",
    "maintainer": "Olivier Binette <olivier.binette@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11674,
    "package_name": "dglars",
    "title": "Differential Geometric Least Angle Regression",
    "description": "Differential geometric least angle regression method for fitting sparse generalized linear models. In this version of the package, the user can fit models specifying Gaussian, Poisson, Binomial, Gamma and Inverse Gaussian family. Furthermore, several link functions can be used to model the relationship between the conditional expected value of the response variable and the linear predictor. The solution curve can be computed using an efficient predictor-corrector or a cyclic coordinate descent algorithm, as described in the paper linked to via the URL below.",
    "version": "2.1.7",
    "maintainer": "Luigi Augugliaro <luigi.augugliaro@unipa.it>",
    "url": "https://www.jstatsoft.org/v59/i08/.",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11675,
    "package_name": "dglm",
    "title": "Double Generalized Linear Models",
    "description": "Model fitting and evaluation tools for double generalized linear\n    models (DGLMs). This class of models uses one generalized linear model (GLM)\n    to fit the specified response and a second GLM to fit the deviance of the first\n    model.",
    "version": "1.8.6",
    "maintainer": "Gordon Smyth <smyth@wehi.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11682,
    "package_name": "diagL1",
    "title": "Routines for Fit, Inference and Diagnostics in Linear L1 and LAD\nModels",
    "description": "Diagnostics for linear L1 regression (also known as LAD - Least Absolute Deviations), including: estimation, confidence intervals, tests of hypotheses, measures of leverage, methods of diagnostics for L1 regression, special diagnostics graphs and measures of leverage. The algorithms are based in Dielman (2005)  <doi:10.1080/0094965042000223680>, Elian et al. (2000) <doi:10.1080/03610920008832518> and Dodge (1997) <doi:10.1006/jmva.1997.1666>. This package builds on the 'quantreg' package, which is a well-established package for tuning quantile regression models. There are also tests to verify if the errors have a Laplace distribution based on the work of Puig and Stephens (2000) <doi:10.2307/1270952>.",
    "version": "1.0.1",
    "maintainer": "Kevin Allan Sales Rodrigues <kevin.asr@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11700,
    "package_name": "did",
    "title": "Treatment Effects with Multiple Periods and Groups",
    "description": "The standard Difference-in-Differences (DID) setup involves two periods and two groups -- a treated group and untreated group.  Many applications of DID methods involve more than two periods and have individuals that are treated at different points in time.  This package contains tools for computing average treatment effect parameters in Difference in Differences setups with more than two periods and with variation in treatment timing using the methods developed in Callaway and Sant'Anna (2021) <doi:10.1016/j.jeconom.2020.12.001>.  The main parameters are group-time average treatment effects which are the average treatment effect for a particular group at a a particular time.  These can be aggregated into a fewer number of treatment effect parameters, and the package deals with the cases where there is selective treatment timing, dynamic treatment effects, calendar time effects, or combinations of these.  There are also functions for testing the Difference in Differences assumption, and plotting group-time average treatment effects.",
    "version": "2.3.0",
    "maintainer": "Brantly Callaway <brantly.callaway@uga.edu>",
    "url": "https://bcallaway11.github.io/did/,\nhttps://github.com/bcallaway11/did/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11738,
    "package_name": "digitTests",
    "title": "Tests for Detecting Irregular Digit Patterns",
    "description": "Provides statistical tests and support functions for detecting irregular digit patterns in numerical data. The package includes tools for extracting digits at various locations in a number, tests for repeated values, and (Bayesian) tests of digit distributions.",
    "version": "0.1.2",
    "maintainer": "Koen Derks <k.derks@nyenrode.nl>",
    "url": "https://koenderks.github.io/digitTests/,\nhttps://github.com/koenderks/digitTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11747,
    "package_name": "dineq",
    "title": "Decomposition of (Income) Inequality",
    "description": "Decomposition of (income) inequality by population sub groups. \n    For a decomposition on a single variable the mean log deviation can be used\n      (see Mookherjee Shorrocks (1982) <DOI:10.2307/2232673>).\n    For a decomposition on multiple variables a regression based technique can be \n      used (see Fields (2003) <DOI:10.1016/s0147-9121(03)22001-x>).\n    Recentered influence function regression for marginal effects of the (income \n      or wealth) distribution  (see Firpo et al. (2009) <DOI:10.3982/ECTA6822>).\n    Some extensions to inequality functions to handle weights and/or missings.",
    "version": "0.1.0",
    "maintainer": "René Schulenberg <reneschulenberg@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11761,
    "package_name": "disagg2",
    "title": "Support Functions for Time Series Analysis Book",
    "description": "Contains the support functions for the Time Series Analysis book.\n\t     We present a function to calculate MSE and MAE for inputs\n\t     of actual and forecast values.  We also have the code for\n\t     disaggregation as found in Wei and Stram\n\t     (1990, <doi:10.1111/j.2517-6161.1990.tb01799.x>),\n\t     and Hodgess and Wei (1996, \"Temporal Disaggregation of Time\n\t     Series\").",
    "version": "0.1.0",
    "maintainer": "Erin Hodgess <erinm.hodgess@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11768,
    "package_name": "discSurv",
    "title": "Discrete Time Survival Analysis",
    "description": "Provides data transformations, estimation utilities,\n    predictive evaluation measures and simulation functions for discrete time\n    survival analysis.",
    "version": "2.0.0",
    "maintainer": "Thomas Welchowski <welchow@imbie.meb.uni-bonn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11769,
    "package_name": "discfrail",
    "title": "Cox Models for Time-to-Event Data with Nonparametric Discrete\nGroup-Specific Frailties",
    "description": "Functions for fitting Cox proportional hazards models for grouped time-to-event data, where the shared group-specific frailties have a discrete nonparametric distribution. The methods proposed in the package is described by Gasperoni, F., Ieva, F., Paganoni, A. M., Jackson, C. H., Sharples, L. (2018) <doi:10.1093/biostatistics/kxy071>. There are also functions for simulating from these models, with a nonparametric or a parametric baseline hazard function.",
    "version": "0.2",
    "maintainer": "Francesca Gasperoni <francesca.gasperoni@polimi.it>",
    "url": "https://github.com/fgaspe04/discfrail",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11781,
    "package_name": "discovr",
    "title": "Interactive Tutorials and Data for \"Discovering Statistics Using\nR and RStudio\"",
    "description": "Interactive 'R' tutorials and datasets for the textbook Field\n    (2026), \"Discovering Statistics Using R and RStudio\",\n    <https://www.discovr.rocks/>.  Interactive tutorials cover general\n    workflow in 'R' and 'RStudio', summarizing data, visualizing data,\n    fitting models and bias, correlation, the general linear model (GLM),\n    moderation, mediation, missing values, comparing means using the GLM\n    (analysis of variance), comparing adjusted means (analysis of covariance), factorial\n    designs, repeated measures designs, exploratory factor analysis (EFA).\n    There are no functions, only datasets and interactive tutorials.",
    "version": "0.2.2",
    "maintainer": "Andy Field <andyf@sussex.ac.uk>",
    "url": "https://www.discovr.rocks,\nhttps://github.com/profandyfield/discovr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11798,
    "package_name": "dispmod",
    "title": "Modelling Dispersion in GLM",
    "description": "Functions for estimating Gaussian dispersion regression models (Aitkin, 1987 <doi:10.2307/2347792>), overdispersed binomial logit models (Williams, 1987 <doi:10.2307/2347977>), and overdispersed Poisson log-linear models (Breslow, 1984 <doi:10.2307/2347661>), using a quasi-likelihood approach.",
    "version": "1.2",
    "maintainer": "Luca Scrucca <luca.scrucca@unipg.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11810,
    "package_name": "distcomp",
    "title": "Computations over Distributed Data without Aggregation",
    "description": "Implementing algorithms and fitting models when sites (possibly remote) share\n  computation summaries rather than actual data over HTTP with a master R process (using\n  'opencpu', for example). A stratified Cox model and a singular value decomposition are\n  provided. The former makes direct use of code from the R 'survival' package. (That is,\n  the underlying Cox model code is derived from that in the R 'survival' package.)\n  Sites may provide data via several means: CSV files, Redcap API, etc. An extensible\n  design allows for new methods to be added in the future and includes facilities\n  for local prototyping and testing. Web applications are provided (via 'shiny') for\n  the implemented methods to help in designing and deploying the computations.",
    "version": "1.3-4",
    "maintainer": "Balasubramanian Narasimhan <naras@stat.Stanford.EDU>",
    "url": "http://dx.doi.org/10.18637/jss.v077.i13",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11814,
    "package_name": "distfreereg",
    "title": "Distribution-Free Goodness-of-Fit Testing for Regression",
    "description": "Implements the distribution-free goodness-of-fit regression test\n  for the mean structure of parametric models introduced in Khmaladze (2021)\n  <doi:10.1007/s10463-021-00786-3>. The test is implemented for general\n  functions with minimal distributional assumptions as well as common models\n  (e.g., lm, glm) with the usual assumptions.",
    "version": "1.1",
    "maintainer": "Jesse Miller <mill9116@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11834,
    "package_name": "distributional",
    "title": "Vectorised Probability Distributions",
    "description": "Vectorised distribution objects with tools for manipulating,",
    "version": "0.5.0.9000",
    "maintainer": "",
    "url": "https://github.com/mitchelloharawild/distributional",
    "exports": [],
    "topics": ["probability-distribution", "r", "statistics", "vctrs"],
    "score": "NA",
    "stars": 108
  },
  {
    "id": 11845,
    "package_name": "divDyn",
    "title": "Diversity Dynamics using Fossil Sampling Data",
    "description": "Functions to describe sampling and diversity dynamics of fossil occurrence datasets (e.g. from the Paleobiology Database). The package includes methods to calculate range- and occurrence-based metrics of taxonomic richness, extinction and origination rates, along with traditional sampling measures. A powerful subsampling tool is also included that implements frequently used sampling standardization methods in a multiple bin-framework. The plotting of time series and the occurrence data can be simplified by the functions incorporated in the package, as well as other calculations, such as environmental affinities and extinction selectivity testing. Details can be found in: Kocsis, A.T.; Reddin, C.J.; Alroy, J. and Kiessling, W. (2019) <doi:10.1101/423780>.",
    "version": "0.8.3",
    "maintainer": "Adam T. Kocsis <adam.t.kocsis@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11864,
    "package_name": "dlbayes",
    "title": "Use Dirichlet Laplace Prior to Solve Linear Regression Problem\nand Do Variable Selection",
    "description": "The Dirichlet Laplace shrinkage prior in Bayesian linear regression and variable selection, featuring:\n    utility functions in implementing Dirichlet-Laplace priors such as visualization; \n    scalability in Bayesian linear regression; \n    penalized credible regions for variable selection.",
    "version": "0.1.0",
    "maintainer": "Shijia Zhang <zsj27@mail.ustc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11867,
    "package_name": "dlm",
    "title": "Bayesian and Likelihood Analysis of Dynamic Linear Models",
    "description": "Provides routines for Maximum likelihood,\n    Kalman filtering and smoothing, and Bayesian\n    analysis of Normal linear State Space models, also known as \n    Dynamic Linear Models. ",
    "version": "1.1-6.1",
    "maintainer": "Giovanni Petris <GPetris@uark.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11868,
    "package_name": "dlmtree",
    "title": "Bayesian Treed Distributed Lag Models",
    "description": "Estimation of distributed lag models (DLMs) based on a Bayesian additive regression trees framework. Includes several extensions of DLMs: treed DLMs and distributed lag mixture models (Mork and Wilson, 2023) <doi:10.1111/biom.13568>; treed distributed lag nonlinear models (Mork and Wilson, 2022) <doi:10.1093/biostatistics/kxaa051>; heterogeneous DLMs (Mork, et. al., 2024) <doi:10.1080/01621459.2023.2258595>; monotone DLMs (Mork and Wilson, 2024) <doi:10.1214/23-BA1412>. The package also includes visualization tools and a 'shiny' interface to check model convergence and to help interpret results.",
    "version": "1.1.0",
    "maintainer": "Daniel Mork <dmork@hsph.harvard.edu>",
    "url": "https://github.com/danielmork/dlmtree,\nhttps://danielmork.github.io/dlmtree/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11869,
    "package_name": "dlmwwbe",
    "title": "Dynamic Linear Model for Wastewater-Based Epidemiology",
    "description": "Implement dynamic linear models outlined in Shumway and Stoffer (2025) <doi:10.1007/978-3-031-70584-7>. Two model structures for data smoothing and forecasting are considered. The specific models proposed will be added once the manuscript is published. ",
    "version": "0.1.0",
    "maintainer": "Difan Ouyang <ouyan146@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11870,
    "package_name": "dlnm",
    "title": "Distributed Lag Non-Linear Models",
    "description": "Collection of functions for distributed lag linear and non-linear models.",
    "version": "2.4.10",
    "maintainer": "Antonio Gasparrini <antonio.gasparrini@lshtm.ac.uk>",
    "url": "https://github.com/gasparrini/dlnm,\nhttp://www.ag-myresearch.com/package-dlnm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11873,
    "package_name": "dlsem",
    "title": "Distributed-Lag Linear Structural Equation Models",
    "description": "Inference functionalities for distributed-lag linear structural equation models (DLSEMs). DLSEMs are Markovian structural causal models where each factor of the joint probability distribution is a distributed-lag linear regression with constrained lag shapes (Magrini, 2018 <doi:10.2478/bile-2018-0012>; Magrini et al., 2019 <doi:10.1007/s11135-019-00855-z>). DLSEMs account for temporal delays in the dependence relationships among the variables through a single parameter per covariate, thus allowing to perform dynamic causal inference in a feasible fashion. Endpoint-constrained quadratic, quadratic decreasing, linearly decreasing and gamma lag shapes are available.",
    "version": "2.4.6",
    "maintainer": "Alessandro Magrini <alessandro.magrini@unifi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11904,
    "package_name": "dobson",
    "title": "Data from the GLM Book by Dobson and Barnett",
    "description": "Example datasets from the book \"An Introduction to Generalised Linear Models\" (Year: 2018, ISBN:9781138741515) by Dobson and Barnett.",
    "version": "0.4",
    "maintainer": "Adrian Barnett <a.barnett@qut.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11930,
    "package_name": "domir",
    "title": "Tools to Support Relative Importance Analysis",
    "description": "Methods to apply decomposition-based relative importance \n  analysis for R functions. This package supports the application of \n  decomposition methods by providing 'lapply'- or 'Map'-like meta-functions that \n  compute dominance analysis (Azen, R., & Budescu, D. V. (2003) \n  <doi:10.1037/1082-989X.8.2.129>; Grömping, U. (2007) \n  <doi:10.1198/000313007X188252>) an extension of Shapley value regression \n  (Lipovetsky, S., & Conklin, M. (2001) <doi:10.1002/asmb.446>)\n  based on the values returned from other functions.",
    "version": "1.2.0",
    "maintainer": "Joseph Luchman <jluchman@gmail.com>",
    "url": "https://github.com/jluchman/domir,\nhttps://jluchman.github.io/domir/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11937,
    "package_name": "doremi",
    "title": "Dynamics of Return to Equilibrium During Multiple Inputs",
    "description": "Provides models to fit the dynamics of a regulated system experiencing exogenous inputs. \n    The underlying models use differential equations and linear mixed-effects regressions to estimate the \n    coefficients of the equation. With them, the functions can provide an estimated signal.\n    The package provides simulation and analysis functions and also print, summary, plot and predict methods,\n    adapted to the function outputs, for easy implementation and presentation of results.",
    "version": "1.0.0",
    "maintainer": "Mongin Denis <Denis.Mongin@unige.ch>",
    "url": "https://github.com/dcourvoisier/doremi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11952,
    "package_name": "dotwhisker",
    "title": "Dot-and-Whisker Plots of Regression Results",
    "description": "Create quick and easy dot-and-whisker plots of regression results. It takes as input either (1) a coefficient table in standard form or (2) one (or a list of) fitted model objects (of any type that has methods implemented in the 'parameters' package). It returns 'ggplot' objects that can be further customized using tools from the 'ggplot2' package. The package also includes helper functions for tasks such as rescaling coefficients or relabeling predictor variables. See more methodological discussion of the visualization and data management methods used in this package in Kastellec and Leoni (2007) <doi:10.1017/S1537592707072209> and Gelman (2008) <doi:10.1002/sim.3107>.",
    "version": "0.8.4",
    "maintainer": "Yue Hu <yuehu@tsinghua.edu.cn>",
    "url": "https://fsolt.org/dotwhisker/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11958,
    "package_name": "douconca",
    "title": "Double Constrained Correspondence Analysis for Trait-Environment\nAnalysis in Ecology",
    "description": "Double constrained correspondence analysis (dc-CA) analyzes \n   (multi-)trait (multi-)environment ecological data by using the 'vegan' \n   package and native R code. Throughout the two step algorithm of ter Braak \n   et al. (2018) is used. This algorithm combines and extends community- \n   (sample-) and species-level analyses, i.e. the usual community weighted \n   means (CWM)-based regression analysis and the species-level analysis of \n   species-niche centroids (SNC)-based regression analysis. The two steps use \n   canonical correspondence analysis to regress the abundance data on to the \n   traits and (weighted) redundancy analysis to regress the CWM of the \n   orthonormalized traits on to the environmental predictors. The function \n   dc_CA() has an option to divide the abundance data of a site by the site \n   total, giving equal site weights. This division has the advantage that the \n   multivariate analysis corresponds with an unweighted (multi-trait) \n   community-level analysis, instead of being weighted. The first step of \n   the algorithm uses vegan::cca(). The second step uses wrda() but \n   vegan::rda() if the site weights are equal. This version has a predict() \n   function. For details see ter Braak et al. 2018 \n   <doi:10.1007/s10651-017-0395-x>.\n   and ter Braak & van Rossum 2025 <doi:10.1016/j.ecoinf.2025.103143>.",
    "version": "1.2.4",
    "maintainer": "Bart-Jan van Rossum <bart-jan.vanrossum@wur.nl>",
    "url": "https://zenodo.org/records/13970152,\nhttps://github.com/Biometris/douconca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11967,
    "package_name": "dpasurv",
    "title": "Dynamic Path Analysis of Survival Data via Aalen's Additive\nHazards Model",
    "description": "Dynamic path analysis with estimation of the corresponding direct, indirect, and total effects, based on Fosen et al., (2006) <doi:10.1007/s10985-006-9004-2>. The main outcome of interest is a counting process from survival analysis (or recurrent events) data. At each time of event, ordinary linear regression is used to estimate the relation between the covariates, while Aalen's additive hazard model is used for the regression of the counting process on the covariates.",
    "version": "0.1.0",
    "maintainer": "Matthias Kormaksson <mk375@cornell.edu>",
    "url": "http://opensource.nibr.com/dpasurv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11982,
    "package_name": "dr",
    "title": "Methods for Dimension Reduction for Regression",
    "description": "Functions, methods, and datasets for fitting dimension\n reduction regression, using slicing (methods SAVE and SIR), Principal\n Hessian Directions (phd, using residuals and the response), and an\n iterative IRE.  Partial methods, that condition on categorical\n predictors are also available.  A variety of tests, and stepwise\n deletion of predictors, is also included.  Also included is\n code for computing permutation tests of dimension.  Adding additional\n methods of estimating dimension is straightforward.\n For documentation, see the vignette in the package.   With version 3.0.4,\n the arguments for dr.step have been modified.",
    "version": "3.0.11",
    "maintainer": "\"Sanford Weisberg,\" <sandy@umn.edu>",
    "url": "https://CRAN.R-project.org/package=dr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 11989,
    "package_name": "drape",
    "title": "Doubly Robust Average Partial Effects",
    "description": "Doubly robust average partial effect estimation. This implementation contains methods for adding additional smoothness to plug-in regression procedures and for estimating score functions using smoothing splines. Details of the method can be found in Harvey Klyne and Rajen D. Shah (2023) <doi:10.48550/arXiv.2308.09207>.",
    "version": "0.0.2",
    "maintainer": "Harvey Klyne <hck33@cantab.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12017,
    "package_name": "dropR",
    "title": "Dropout Analysis by Condition",
    "description": "Analysis and visualization of dropout between conditions in surveys and (online) experiments. Features include computation of dropout statistics, comparing dropout between conditions (e.g. Chi square), analyzing survival (e.g. Kaplan-Meier estimation), comparing conditions with the most different rates of dropout (Kolmogorov-Smirnov) and visualizing the result of each in designated plotting functions. Sources: Andrea Frick, Marie-Terese Baechtiger & Ulf-Dietrich Reips (2001) <https://www.researchgate.net/publication/223956222_Financial_incentives_personal_information_and_drop-out_in_online_studies>; Ulf-Dietrich Reips (2002) \"Standards for Internet-Based Experimenting\" <doi:10.1027//1618-3169.49.4.243>.",
    "version": "1.0.3",
    "maintainer": "Annika Tave Overlander <annika-tave.overlander@uni.kn>",
    "url": "https://iscience-kn.github.io/dropR/,\nhttps://github.com/iscience-kn/dropR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12036,
    "package_name": "dsa",
    "title": "Seasonal Adjustment of Daily Time Series",
    "description": "Seasonal- and calendar adjustment of time series\n    with daily frequency using the DSA approach developed by Ollech,\n    Daniel (2018): Seasonal adjustment of daily time series. Bundesbank\n    Discussion Paper 41/2018.",
    "version": "1.0.12",
    "maintainer": "Daniel Ollech <daniel.ollech@bundesbank.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12037,
    "package_name": "dsample",
    "title": "Discretization-Based Direct Random Sample Generation",
    "description": "Discretization-based random sampling algorithm that is useful for a complex model in high dimension is implemented. The normalizing constant of a target distribution is not needed. Posterior summaries are compared with those by 'OpenBUGS'. The method is described: Wang and Lee (2014) <doi:10.1016/j.csda.2013.06.011> and exercised in Lee (2009) <http://hdl.handle.net/1993/21352>.",
    "version": "0.91.3.4",
    "maintainer": "Chel Hee Lee <chelhee.lee@ucalgary.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12040,
    "package_name": "dscoreMSM",
    "title": "Survival Proximity Score Matching in Multi-State Survival Model",
    "description": "Implements survival proximity score matching in multi-state survival models. Includes tools for simulating survival data and estimating transition-specific coxph models with frailty terms. The primary methodological work on multistate censored data modeling using propensity score matching has been published by Bhattacharjee et al.(2024) <doi:10.1038/s41598-024-54149-y>.",
    "version": "0.1.0",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12050,
    "package_name": "dsp",
    "title": "Dynamic Shrinkage Process and Change Point Detection",
    "description": "Provides efficient Markov chain Monte Carlo (MCMC) algorithms for \n    dynamic shrinkage processes, which extend global-local shrinkage priors to \n    the time series setting by allowing shrinkage to depend on its own past. \n    These priors yield locally adaptive estimates, useful for time series and \n    regression functions with irregular features. The package includes full MCMC \n    implementations for trend filtering using dynamic shrinkage on signal differences, \n    producing locally constant or linear fits with adaptive credible bands. \n    Also included are models with static shrinkage and normal-inverse-Gamma priors for comparison. \n    Additional tools cover dynamic regression with time-varying coefficients and \n    B-spline models with shrinkage on basis differences, allowing for flexible \n    curve-fitting with unequally spaced data. Some support for heteroscedastic errors, \n    outlier detection, and change point estimation.\n    Methods in this package are described in Kowal et al. (2019) <doi:10.1111/rssb.12325>, \n    Wu et al. (2024) <doi:10.1080/07350015.2024.2362269>, Schafer and Matteson (2024)\n    <doi:10.1080/00401706.2024.2407316>, and Cho and Matteson (2024) <doi:10.48550/arXiv.2408.11315>.",
    "version": "1.2.0",
    "maintainer": "Toryn Schafer <toryn27@gmail.com>",
    "url": "https://github.com/schafert/dsp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12056,
    "package_name": "dstat",
    "title": "Conditional Sensitivity Analysis for Matched Observational\nStudies",
    "description": "A d-statistic tests the null hypothesis of no treatment effect in a matched, nonrandomized study of the effects caused by treatments.  A d-statistic focuses on subsets of matched pairs that demonstrate insensitivity to unmeasured bias in such an observational study, correcting for double-use of the data by conditional inference. This conditional inference can, in favorable circumstances, substantially increase the power of a sensitivity analysis (Rosenbaum (2010) <doi:10.1007/978-1-4419-1213-8_14>).  There are two examples, one concerning unemployment from Lalive et al. (2006) <doi:10.1111/j.1467-937X.2006.00406.x>, the other concerning smoking and periodontal disease from Rosenbaum (2017) <doi:10.1214/17-STS621>.  ",
    "version": "1.0.4",
    "maintainer": "Paul R. Rosenbaum <rosenbaum@wharton.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12068,
    "package_name": "dtrSurv",
    "title": "Dynamic Treatment Regimes for Survival Analysis",
    "description": "Provides methods for estimating multi-stage optimal \n    dynamic treatment regimes for survival outcomes with dependent censoring.\n    Cho, H., Holloway, S. T., and Kosorok, M. R. (2022) <doi:10.1093/biomet/asac047>.",
    "version": "1.5",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12073,
    "package_name": "dtts",
    "title": "'data.table' Time-Series",
    "description": "High-frequency time-series support via 'nanotime' and 'data.table'.",
    "version": "0.1.3",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12074,
    "package_name": "dtw",
    "title": "Dynamic Time Warping Algorithms",
    "description": "A comprehensive implementation of dynamic time warping\n    (DTW) algorithms in R.  DTW computes the optimal (least cumulative\n    distance) alignment between points of two time series.  Common DTW\n    variants covered include local (slope) and global (window)\n    constraints, subsequence matches, arbitrary distance definitions,\n    normalizations, minimum variance matching, and so on.  Provides\n    cumulative distances, alignments, specialized plot styles, etc.,\n    as described in Giorgino (2009) <doi:10.18637/jss.v031.i07>.",
    "version": "1.23-1",
    "maintainer": "Toni Giorgino <toni.giorgino@gmail.com>",
    "url": "https://dynamictimewarping.github.io/,\nhttp://dtw.r-forge.r-project.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12105,
    "package_name": "dwp",
    "title": "Density-Weighted Proportion",
    "description": "Fit a Poisson regression to carcass distance data and integrate over the searched area at a wind farm to estimate the fraction of carcasses falling in the searched area and format the output for use as the dwp parameter in the 'GenEst' or 'eoa' package for estimating bird and bat mortality, following Dalthorp, et al. (2022) <arXiv:2201.10064>.",
    "version": "1.1",
    "maintainer": "Daniel Dalthorp <ddalthorp@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12107,
    "package_name": "dyads",
    "title": "Dyadic Network Analysis",
    "description": "Contains functions for the MCMC simulation of dyadic network models j2 (Zijlstra, 2017, <doi:10.1080/0022250X.2017.1387858>) and p2 (Van Duijn, Snijders & Zijlstra, 2004, <doi: 10.1046/j.0039-0402.2003.00258.x>), the multilevel p2 model (Zijlstra, Van Duijn & Snijders (2009) <doi: 10.1348/000711007X255336>), and the bidirectional (multilevel) counterpart of the the multilevel p2 model as described in Zijlstra, Van Duijn & Snijders (2009) <doi: 10.1348/000711007X255336>, the (multilevel) b2 model. ",
    "version": "1.2.1",
    "maintainer": "Bonne J.H. Zijlstra <B.J.H.Zijlstra@uva.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12112,
    "package_name": "dymo",
    "title": "Dynamic Mode Decomposition Forecasting with Conformal Predictive\nSampling",
    "description": "The DYMO package provides tools for multi-feature time-series forecasting using a Dynamic Mode Decomposition (DMD) model combined with conformal predictive sampling for uncertainty quantification.",
    "version": "2.0.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/dymo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12113,
    "package_name": "dyn",
    "title": "Time Series Regression",
    "description": "Time series regression.  The dyn class interfaces ts,\n        irts(), zoo() and zooreg() time series classes to lm(), glm(),\n        loess(), quantreg::rq(), MASS::rlm(), MCMCpack::MCMCregress(),\n        quantreg::rq(), randomForest::randomForest() and other regression\n        functions allowing those functions to be used with time series\n        including specifications that may contain lags, diffs and\n        missing values.",
    "version": "0.2-9.6",
    "maintainer": "M. Leeds <markleeds2@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12132,
    "package_name": "dynlm",
    "title": "Dynamic Linear Regression",
    "description": "Dynamic linear models and time series regression.",
    "version": "0.3-6",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12136,
    "package_name": "dynpred",
    "title": "Companion Package to \"Dynamic Prediction in Clinical Survival\nAnalysis\"",
    "description": "The dynpred package contains functions for dynamic prediction in survival analysis.",
    "version": "0.1.2",
    "maintainer": "Hein Putter <H.Putter@lumc.nl>",
    "url": "http://www.msbi.nl/putter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12164,
    "package_name": "ePCR",
    "title": "Ensemble Penalized Cox Regression for Survival Prediction",
    "description": "The top-performing ensemble-based Penalized Cox Regression (ePCR) framework developed during the DREAM 9.5 mCRPC Prostate Cancer Challenge <https://www.synapse.org/ProstateCancerChallenge> presented in Guinney J, Wang T, Laajala TD, et al. (2017) <doi:10.1016/S1470-2045(16)30560-5> is provided here-in, together with the corresponding follow-up work. While initially aimed at modeling the most advanced stage of prostate cancer, metastatic Castration-Resistant Prostate Cancer (mCRPC), the modeling framework has subsequently been extended to cover also the non-metastatic form of advanced prostate cancer (CRPC). Readily fitted ensemble-based model S4-objects are provided, and a simulated example dataset based on a real-life cohort is provided from the Turku University Hospital, to illustrate the use of the package. Functionality of the ePCR methodology relies on constructing ensembles of strata in patient cohorts and averaging over them, with each ensemble member consisting of a highly optimized penalized/regularized Cox regression model. Various cross-validation and other modeling schema are provided for constructing novel model objects.",
    "version": "0.11.0",
    "maintainer": "Teemu Daniel Laajala <teelaa@utu.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12173,
    "package_name": "earlygating",
    "title": "Properties of Bayesian Early Gating Designs",
    "description": "Computes the most important properties of four 'Bayesian' early gating \n             designs (two single arm and two randomized controlled designs), such \n             as minimum required number of successes in the experimental group to \n             make a GO decision, operating characteristics and average operating \n             characteristics with respect to the sample size. \n             These might aid in deciding what design to use for the early phase trial.",
    "version": "1.1",
    "maintainer": "Elias Laurin Meyer <elias.meyer@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12195,
    "package_name": "easyViz",
    "title": "Easy Visualization of Conditional Effects from Regression Models",
    "description": "Offers a flexible and user-friendly interface for visualizing conditional \n effects from a broad range of regression models, including mixed-effects and generalized \n additive (mixed) models. Compatible model types include lm(), rlm(), glm(), glm.nb(),\n betareg(), and gam() (from 'mgcv'); nonlinear models via nls(); generalized least \n squares via gls(); and survival models via coxph() (from 'survival'). \n Mixed-effects models with random intercepts and/or slopes can be fitted using lmer(), \n glmer(), glmer.nb(), glmmTMB(), or gam() (from 'mgcv', via smooth terms). \n Plots are rendered using base R graphics with extensive customization options. \n Approximate confidence intervals for nls() and betareg() models are computed using \n the delta method. Robust standard errors for rlm() are computed using the sandwich \n estimator (Zeileis 2004) <doi:10.18637/jss.v011.i10>. For beta regression using \n 'betareg', see Cribari-Neto and Zeileis (2010) <doi:10.18637/jss.v034.i02>. For \n mixed-effects models with 'lme4', see Bates et al. (2015) <doi:10.18637/jss.v067.i01>. \n For models using 'glmmTMB', see Brooks et al. (2017) <doi:10.32614/RJ-2017-066>. \n Methods for generalized additive models using 'mgcv' follow Wood (2017)\n <doi:10.1201/9781315370279>.",
    "version": "2.0.0",
    "maintainer": "Luca Corlatti <lucac1980@yahoo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12198,
    "package_name": "easybgm",
    "title": "Extracting and Visualizing Bayesian Graphical Models",
    "description": "Fit and visualize the results of a Bayesian analysis of networks commonly found in psychology. \n    The package supports fitting cross-sectional network models fitted using the packages 'BDgraph', 'bgms' and 'BGGM', \n    as well as network comparison fitted using the 'bgms' and 'BBGM'. \n    The package provides the parameter estimates, posterior inclusion probabilities, inclusion Bayes factor, and the \n    posterior density of the parameters. In addition, for 'BDgraph' and 'bgms' it allows to assess the posterior \n    structure space. Furthermore, the package comes with an extensive suite for visualizing results.",
    "version": "0.3.1",
    "maintainer": "Karoline Huth <k.huth@uva.nl>",
    "url": "https://github.com/KarolineHuth/easybgm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12210,
    "package_name": "easyreg",
    "title": "Easy Regression",
    "description": "Performs analysis of regression in simple designs with quantitative treatments, \n             including mixed models and non linear models. ",
    "version": "4.0",
    "maintainer": "Emmanuel Arnhold <emmanuelarnhold@yahoo.com.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12213,
    "package_name": "easysurv",
    "title": "Simplify Survival Data Analysis and Model Fitting",
    "description": "Inspect survival data, plot Kaplan-Meier curves, assess the\n    proportional hazards assumption, fit parametric survival models,\n    predict and plot survival and hazards, and export the outputs to\n    'Excel'.  A simple interface for fitting survival models using \n    flexsurv::flexsurvreg(), flexsurv::flexsurvspline(), \n    flexsurvcure::flexsurvcure(), and survival::survreg().",
    "version": "2.0.2",
    "maintainer": "Niall Davison <niall.davison@maplehealthgroup.com>",
    "url": "https://github.com/Maple-Health-Group/easysurv,\nhttps://maple-health-group.github.io/easysurv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12221,
    "package_name": "ebTobit",
    "title": "Empirical Bayesian Tobit Matrix Estimation",
    "description": "Estimation tools for multidimensional Gaussian means using\n    empirical Bayesian g-modeling. Methods are able to handle fully observed data as\n    well as left-, right-, and interval-censored observations (Tobit\n    likelihood); descriptions of these methods can be found in Barbehenn and\n    Zhao (2023) <doi:10.48550/arXiv.2306.07239>. Additional, lower-level functionality based\n    on Kiefer and Wolfowitz (1956) <doi:10.1214/aoms/1177728066> and Jiang and\n    Zhang (2009) <doi:10.1214/08-AOS638> is provided that can be used to\n    accelerate many empirical Bayes and nonparametric maximum likelihood\n    problems.",
    "version": "1.0.2",
    "maintainer": "Alton Barbehenn <altonbarbehenn@gmail.com>",
    "url": "https://github.com/barbehenna/ebTobit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12223,
    "package_name": "ebal",
    "title": "Entropy Reweighting to Create Balanced Samples",
    "description": "Package implements entropy balancing, a data preprocessing procedure described in Hainmueller (2008, <doi:10.1093/pan/mpr025>) that allows users to reweight a dataset such that the covariate distributions in the reweighted data satisfy a set of user specified moment conditions. This can be useful to create balanced samples in observational studies with a binary treatment where the control group data can be reweighted to match the covariate moments in the treatment group. Entropy balancing can also be used to reweight a survey sample to known characteristics from a target population.",
    "version": "0.1-8",
    "maintainer": "Jens Hainmueller <jhain@stanford.edu>",
    "url": "https://web.stanford.edu/~jhain/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12225,
    "package_name": "ebdbNet",
    "title": "Empirical Bayes Estimation of Dynamic Bayesian Networks",
    "description": "Infer the adjacency matrix of a\n\tnetwork from time course data using an empirical Bayes\n\testimation procedure based on Dynamic Bayesian Networks.",
    "version": "1.2.8",
    "maintainer": "Andrea Rau <andrea.rau@inra.fr>",
    "url": "https://github.com/andreamrau/ebdbNet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12230,
    "package_name": "ebmstate",
    "title": "Empirical Bayes Multi-State Cox Model",
    "description": "Implements an empirical Bayes, multi-state Cox model for survival analysis. Run \"?'ebmstate-package'\" for details. See also Schall (1991) <doi:10.1093/biomet/78.4.719>.",
    "version": "0.1.5",
    "maintainer": "Rui Costa <ruibarrigana@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12236,
    "package_name": "ecb",
    "title": "Programmatic Access to the European Central Bank's Data Portal",
    "description": "Provides an interface to the European Central Bank's Data\n    Portal API, allowing for programmatic retrieval of a vast quantity\n    of statistical data.",
    "version": "0.4.3",
    "maintainer": "Eric Persson <expersso5@gmail.com>",
    "url": "https://github.com/expersso/ecb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12246,
    "package_name": "echoice2",
    "title": "Choice Models with Economic Foundation",
    "description": "Implements choice models based on economic theory, including\n    estimation using Markov chain Monte Carlo (MCMC), prediction, and\n    more. Its usability is inspired by ideas from 'tidyverse'. Models\n    include versions of the Hierarchical Multinomial Logit and Multiple\n    Discrete-Continous (Volumetric) models with and without screening. The\n    foundations of these models are described in Allenby, Hardt and Rossi\n    (2019) <doi:10.1016/bs.hem.2019.04.002>. Models with conjunctive\n    screening are described in Kim, Hardt, Kim and Allenby (2022)\n    <doi:10.1016/j.ijresmar.2022.04.001>. Models with set-size variation\n    are described in Hardt and Kurz (2020) <doi:10.2139/ssrn.3418383>.",
    "version": "0.2.5",
    "maintainer": "Nino Hardt <me@ninohardt.com>",
    "url": "https://github.com/ninohardt/echoice2,\nhttp://ninohardt.de/echoice2/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12252,
    "package_name": "ecm",
    "title": "Build Error Correction Models",
    "description": "Functions for easy building of error correction models (ECM) for time series regression. ",
    "version": "7.2.0",
    "maintainer": "Gaurav Bansal <gaurbans@gmail.com>",
    "url": "https://github.com/gaurbans/ecm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12253,
    "package_name": "ecmwfr",
    "title": "Interface to 'ECMWF' and 'CDS' Data Web Services",
    "description": "Programmatic interface to the European Centre for Medium-Range\n    Weather Forecasts dataset web services (ECMWF; <https://www.ecmwf.int/>)\n    and Copernicus's Data Stores. Allows for easy downloads of weather \n    forecasts and climate reanalysis data in R. Data stores covered include the Climate Data Store (CDS; \n    <https://cds.climate.copernicus.eu>), Atmosphere Data Store (ADS; \n    <https://ads.atmosphere.copernicus.eu>) and Early Warning Data Store (CEMS; \n    <https://ewds.climate.copernicus.eu>).",
    "version": "2.0.3",
    "maintainer": "Koen Hufkens <koen.hufkens@gmail.com>",
    "url": "https://github.com/bluegreen-labs/ecmwfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12254,
    "package_name": "eco",
    "title": "Ecological Inference in 2x2 Tables",
    "description": "Implements the Bayesian and likelihood methods proposed in Imai, Lu, and Strauss (2008 <doi:10.1093/pan/mpm017>) and (2011 <doi:10.18637/jss.v042.i05>) for ecological inference in 2 by 2 tables as well as the method of bounds introduced by Duncan and Davis (1953).  The package fits both parametric and nonparametric models using either the Expectation-Maximization algorithms (for likelihood models) or the Markov chain Monte Carlo algorithms (for Bayesian models).  For all models, the individual-level data can be directly incorporated into the estimation whenever such data are available. Along with in-sample and out-of-sample predictions, the package also provides a functionality which allows one to quantify the effect of data aggregation on parameter estimation and hypothesis testing under the parametric likelihood models.",
    "version": "4.0-6",
    "maintainer": "Kosuke Imai <imai@Harvard.Edu>",
    "url": "https://github.com/kosukeimai/eco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12266,
    "package_name": "ecolottery",
    "title": "Coalescent-Based Simulation of Ecological Communities",
    "description": "Coalescent-Based Simulation of Ecological Communities as proposed\n    by Munoz et al. (2018) <doi:10.1111/2041-210X.12918>. The package includes\n    a tool for estimating parameters of community assembly by using Approximate \n    Bayesian Computation.",
    "version": "1.0.2",
    "maintainer": "François Munoz <francois.munoz@hotmail.fr>",
    "url": "https://github.com/frmunoz/ecolottery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12277,
    "package_name": "ecoreg",
    "title": "Ecological Regression using Aggregate and Individual Data",
    "description": "Estimating individual-level covariate-outcome associations \n using aggregate data (\"ecological inference\") or a combination of \n aggregate and individual-level data (\"hierarchical related regression\").",
    "version": "0.2.5",
    "maintainer": "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12285,
    "package_name": "ecostate",
    "title": "State-Space Mass-Balance Model for Marine Ecosystems",
    "description": "\n\tFits a state-space mass-balance model for marine ecosystems,\n\twhich implements dynamics derived from \n\t'Ecopath with Ecosim' ('EwE') <https://ecopath.org/>\n\twhile fitting to time-series of fishery catch, biomass indices,\n\tage-composition samples, and weight-at-age data.  \n\t'Ecostate' fits biological parameters (e.g., equilibrium mass)\n\tand measurement parameters (e.g., catchability coefficients)\n\tjointly with residual variation in process errors, and can include\n\tBayesian priors for parameters.  ",
    "version": "0.3.0",
    "maintainer": "James T. Thorson <James.Thorson@noaa.gov>",
    "url": "https://james-thorson-noaa.github.io/ecostate/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12290,
    "package_name": "ecotox",
    "title": "Analysis of Ecotoxicology",
    "description": "A simple approach to using a probit or logit analysis to calculate\n        lethal concentration (LC) or time (LT) and the appropriate fiducial \n        confidence limits desired for selected LC or LT for\n        ecotoxicology studies (Finney 1971; Wheeler et al. 2006; \n        Robertson et al. 2007). The simplicity of 'ecotox' comes from the \n        syntax it implies within its functions which are similar to functions \n        like glm() and lm(). In addition to the simplicity of the syntax, \n        a comprehensive data frame is produced which gives the user a \n        predicted LC or LT value for the desired level and a suite of important \n        parameters such as fiducial confidence limits and slope.   \n        Finney, D.J. (1971, ISBN: 052108041X);\n        Wheeler, M.W., Park, R.M., and Bailer, A.J. (2006) <doi:10.1897/05-320R.1>;\n        Robertson, J.L., Savin, N.E., Russell, R.M., and Preisler, H.K.\n        (2007, ISBN: 0849323312).",
    "version": "1.4.4",
    "maintainer": "Benjamin L Hlina <benjamin.hlina@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12295,
    "package_name": "ecp",
    "title": "Non-Parametric Multiple Change-Point Analysis of Multivariate\nData",
    "description": "Implements various procedures for finding \n\t     multiple change-points from Matteson D. et al (2013)\n\t     <doi:10.1080/01621459.2013.849605>, Zhang W. et al (2017) \n\t     <doi:10.1109/ICDMW.2017.44>, Arlot S. et al (2019).\n\t     Two methods make use of dynamic  \n\t     programming and pruning, with no distributional \n\t     assumptions other than the existence of certain absolute \n\t     moments in one method. Hierarchical and exact search methods \n\t     are included. All methods return the set of estimated change-\n\t     points as well as other summary information.",
    "version": "3.1.6",
    "maintainer": "Wenyu Zhang <wz258@cornell.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12296,
    "package_name": "ecpdist",
    "title": "Extended Chen-Poisson Lifetime Distribution",
    "description": "Computes the Extended Chen-Poisson (ecp) distribution, survival, \n    density, hazard, cumulative hazard and quantile functions. It also allows \n    to generate a pseudo-random sample from this distribution. The corresponding \n    graphics are available. Functions to obtain measures of skewness and \n    kurtosis, k-th raw moments, conditional k-th moments and mean \n    residual life function were added. For details about ecp distribution, see \n    Sousa-Ferreira, I., Abreu, A.M. & Rocha, C. (2023). \n    <doi:10.57805/revstat.v21i2.405>.",
    "version": "0.2.1",
    "maintainer": "Ana Abreu <abreu@staff.uma.pt>",
    "url": "<https://github.com/abreu-uma/ecpdist>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12306,
    "package_name": "edecob",
    "title": "Event Detection Using Confidence Bounds",
    "description": "Detects sustained change in digital bio-marker data using\n    simultaneous confidence bands. Accounts for noise using an auto-regressive\n    model. Based on Buehlmann (1998) \"Sieve bootstrap for smoothing in\n    nonstationary time series\" <doi:10.1214/aos/1030563978>.",
    "version": "1.2.2",
    "maintainer": "Zheng Chen Man <zheng.chen.man@alumni.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12318,
    "package_name": "edgedata",
    "title": "Datasets that Support the EDGE Server DIY Logic",
    "description": "Datasets from most recent CCIIO DIY entry\n  in a tidy format. These support the Centers for Medicare and Medicaid\n  Services' (CMS) risk adjustment Do-It-Yourself (DIY) process, which allows\n  health insurance issuers to calculate member risk profiles under the Health\n  and Human Services-Hierarchical Condition Categories (HHS-HCC) regression\n  model. This regression model is used to calculate risk adjustment transfers.\n  Risk adjustment is a selection mitigation program implemented under the\n  Patient Protection and Affordable Care Act (ACA or Obamacare) in the USA.\n  Under the ACA, health insurance issuers submit claims data to CMS\tin order\n  for CMS to calculate a risk score under the HHS-HCC regression model.\n  However, CMS does not inform issuers of their average risk score until after\n  the data submission deadline. These data sets can be used by issuers to\n  calculate their average risk score mid-year. More information about risk\n  adjustment and the HHS-HCC model can be found here:\n  <https://www.cms.gov/mmrr/Articles/A2014/MMRR2014_004_03_a03.html>.",
    "version": "0.2.0",
    "maintainer": "Ethan Brockmann <ethanbrockmann@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12321,
    "package_name": "edina",
    "title": "Bayesian Estimation of an Exploratory Deterministic Input, Noisy\nand Gate Model",
    "description": "Perform a Bayesian estimation of the exploratory \n    deterministic input, noisy and gate (EDINA)\n    cognitive diagnostic model described by Chen et al. (2018)\n    <doi:10.1007/s11336-017-9579-4>.",
    "version": "0.1.2",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "url": "https://github.com/tmsalab/edina, https://tmsalab.github.io/edina/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12329,
    "package_name": "edstan",
    "title": "Stan Models for Item Response Theory",
    "description": "Streamlines the fitting of common Bayesian item response models \n    using Stan.",
    "version": "1.1.0",
    "maintainer": "Daniel C. Furr <danielcfurr@berkeley.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12334,
    "package_name": "eefAnalytics",
    "title": "Robust Analytical Methods for Evaluating Educational\nInterventions using Randomised Controlled Trials Designs",
    "description": "Analysing data from evaluations of educational interventions using a randomised controlled trial design. Various analytical tools to perform sensitivity analysis using different methods are supported (e.g. frequentist models with bootstrapping and permutations options, Bayesian models). The included commands can be used for simple randomised trials, cluster randomised trials and multisite trials. The methods can also be used more widely beyond education trials. This package can be used to evaluate other intervention designs using Frequentist and Bayesian multilevel models.",
    "version": "1.1.5",
    "maintainer": "Germaine Uwimpuhwe <germaine.uwimpuhwe@durham.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12339,
    "package_name": "eemdARIMA",
    "title": "EEMD Based Auto Regressive Integrated Moving Average Model",
    "description": "Forecasting time series with different decomposition based ARIMA models. For method details see Yu L, Wang S, Lai KK (2008). <doi:10.1016/j.eneco.2008.05.003>. ",
    "version": "0.1.0",
    "maintainer": "Rajeev Ranjan Kumar <rrk.uasd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12342,
    "package_name": "eesectors",
    "title": "Functions to create DCMS Economic Estimates statistics",
    "description": "Provides functions to support production of the Economic Estimates",
    "version": "0.1.0",
    "maintainer": "Matthew Upson <matthew.upson@digital.cabinet-office.gov.uk>",
    "url": "https://github.com/DCMSstats/eesectors",
    "exports": [],
    "topics": ["dataops", "government", "reproducible", "statistics"],
    "score": "NA",
    "stars": 30
  },
  {
    "id": 12347,
    "package_name": "effClust",
    "title": "Calculate Effective Number of Clusters for a Linear Model",
    "description": "Calculates the (approximate) effective number of clusters for a regression model, as described in Carter, Schnepel, and Steigerwald (2017) <doi:10.1162/REST_a_00639>.  The effective number of clusters is a statistic to assess the reliability of asymptotic inference when sampling or treatment assignment is clustered. Methods are implemented for stats::lm(), plm::plm(), and fixest::feols(). There is also a formula method.",
    "version": "0.8.0",
    "maintainer": "Joe Ritter <R-effclust@proton.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12355,
    "package_name": "eggCounts",
    "title": "Hierarchical Modelling of Faecal Egg Counts",
    "description": "An implementation of Bayesian hierarchical models\n    for faecal egg count data to assess anthelmintic\n    efficacy. Bayesian inference is done via MCMC sampling using 'Stan' <https://mc-stan.org/>.",
    "version": "2.5-1",
    "maintainer": "Craig Wang <craigwang247@gmail.com>",
    "url": "https://www.math.uzh.ch/pages/eggcount/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12358,
    "package_name": "eha",
    "title": "Event History Analysis",
    "description": "Parametric proportional hazards fitting with left truncation and\n        right censoring for common families of distributions, piecewise constant \n        hazards, and discrete models. Parametric accelerated failure time models\n        for left truncated and right censored data. Proportional hazards\n        models for tabular and register data. Sampling of risk sets in Cox \n        regression, selections in the Lexis diagram, bootstrapping. \n        Broström (2022) <doi:10.1201/9780429503764>.",
    "version": "2.11.5",
    "maintainer": "Göran Broström <goran.brostrom@umu.se>",
    "url": "https://ehar.se/r/eha/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12359,
    "package_name": "ehaGoF",
    "title": "Calculates Goodness of Fit Statistics",
    "description": "Calculates 15 different goodness of fit criteria. These are; standard deviation ratio (SDR), coefficient of variation (CV), relative root mean square error (RRMSE), Pearson's correlation coefficients (PC), root mean square error (RMSE), performance index (PI), mean error (ME), global relative approximation error (RAE), mean relative approximation error (MRAE), mean absolute percentage error (MAPE), mean absolute deviation (MAD), coefficient of determination (R-squared), adjusted coefficient of determination (adjusted R-squared), Akaike's information criterion (AIC), corrected Akaike's information criterion (CAIC), Mean Square Error (MSE), Bayesian Information Criterion (BIC) and Normalized Mean Square Error (NMSE).",
    "version": "0.1.1",
    "maintainer": "Alper Gulbe <alper.gulbe@igdir.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12366,
    "package_name": "eiPack",
    "title": "Ecological Inference and Higher-Dimension Data Management",
    "description": "Provides methods for analyzing R by C ecological contingency\n        tables using the extreme case analysis, ecological regression,\n        and Multinomial-Dirichlet ecological inference models.  Also\n        provides tools for manipulating higher-dimension data objects.",
    "version": "0.2-2",
    "maintainer": "Michael Kellermann <mrkellermann@gmail.com>",
    "url": "http://www.olivialau.org/software/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12371,
    "package_name": "eigenmodel",
    "title": "Semiparametric Factor and Regression Models for Symmetric\nRelational Data",
    "description": "Estimation of the parameters in a model for\n        symmetric relational data (e.g., the above-diagonal part of a\n        square matrix), using a model-based eigenvalue decomposition\n        and regression. Missing data is accommodated, and a posterior\n        mean for missing data is calculated under the assumption that\n        the data are missing at random. The marginal distribution of\n        the relational data can be arbitrary, and is fit with an\n        ordered probit specification. See Hoff (2007) <arXiv:0711.1146> \n        for details on the model.  ",
    "version": "1.11",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "https://pdhoff.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12379,
    "package_name": "eirm",
    "title": "Explanatory Item Response Modeling for Dichotomous and\nPolytomous Items",
    "description": "Analysis of dichotomous and polytomous response data using the explanatory item response modeling framework, as described in Bulut, Gorgun, & Yildirim-Erbasli (2021) <doi:10.3390/psych3030023>, Stanke & Bulut (2019) <doi:10.21449/ijate.515085>, and De Boeck & Wilson (2004) <doi:10.1007/978-1-4757-3990-9>. Generalized linear mixed modeling is used for estimating the effects of item-related and person-related variables on dichotomous and polytomous item responses. ",
    "version": "0.5",
    "maintainer": "Okan Bulut <bulut@ualberta.ca>",
    "url": "https://github.com/okanbulut/eirm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12381,
    "package_name": "eive",
    "title": "An Algorithm for Reducing Errors-in-Variable Bias in Simple and\nMultiple Linear Regressions",
    "description": "Performs a compact genetic algorithm search to reduce errors-in-variables bias in linear regression. The algorithm estimates the regression parameters with lower biases and higher variances but mean-square errors (MSEs) are reduced.  ",
    "version": "3.1.3",
    "maintainer": "Mehmet Hakan Satman <mhsatman@istanbul.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12386,
    "package_name": "elasso",
    "title": "Enhanced Least Absolute Shrinkage and Selection Operator\nRegression Model",
    "description": "Performs some enhanced variable selection algorithms \n  based on the least absolute shrinkage and selection operator for regression model.",
    "version": "1.1",
    "maintainer": "Pi Guo <guopi.01@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12402,
    "package_name": "elhmc",
    "title": "Sampling from a Empirical Likelihood Bayesian Posterior of\nParameters Using Hamiltonian Monte Carlo",
    "description": "A tool to draw samples from a Empirical Likelihood Bayesian posterior\n   of parameters using Hamiltonian Monte Carlo.",
    "version": "1.2.1",
    "maintainer": "Sanjay Chaudhuri <schaudhuri2@unl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12445,
    "package_name": "emplik",
    "title": "Empirical Likelihood Ratio for Censored/Truncated Data",
    "description": "Empirical likelihood ratio tests and confidence intervals for means/quantiles/hazards\n \tfrom possibly censored and/or truncated data. In particular, the empirical likelihood\n        for the Kaplan-Meier/Nelson-Aalen estimator. Now does AFT regression too.",
    "version": "1.3-2",
    "maintainer": "Mai Zhou <maizhou@gmail.com>",
    "url": "https://www.ms.uky.edu/~mai/EmpLik.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12446,
    "package_name": "emplik2",
    "title": "Empirical Likelihood Ratio Test for Two-Sample U-Statistics with\nCensored Data",
    "description": "Calculates the empirical likelihood ratio and p-value for a mean-type hypothesis \n            (or multiple mean-type hypotheses) based on two samples with possible censored data.",
    "version": "1.33",
    "maintainer": "Mai Zhou <maizhou@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12452,
    "package_name": "emulator",
    "title": "Bayesian Emulation of Computer Programs",
    "description": "\n Allows one to estimate the output of a computer program,\n as a function of the input parameters, without actually running it.\n The computer program is assumed to be a Gaussian process, whose\n parameters are estimated using Bayesian techniques that give a PDF of\n expected program output.  This PDF is conditional on a training set\n of runs, each consisting of a point in parameter space and the model\n output at that point.  The emphasis is on complex codes that take\n weeks or months to run, and that have a large number of undetermined\n input parameters; many climate prediction models fall into this\n class.  The emulator essentially determines Bayesian posterior\n estimates of the PDF of the output of a model, conditioned on results\n from previous runs and a user-specified prior linear model.  The\n package includes functionality to evaluate quadratic forms \n efficiently. ",
    "version": "1.2-24",
    "maintainer": "Robin K. S. Hankin <hankin.robin@gmail.com>",
    "url": "https://github.com/RobinHankin/emulator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12461,
    "package_name": "endogenous",
    "title": "Classical Simultaneous Equation Models",
    "description": "Likelihood-based approaches to estimate linear regression parameters and treatment effects in the presence of endogeneity. Specifically, this package includes James Heckman's classical simultaneous equation models-the sample selection model for outcome selection bias and hybrid model with structural shift for endogenous treatment. For more information, see the seminal paper of Heckman (1978) <DOI:10.3386/w0177> in which the details of these models are provided. This package accommodates repeated measures on subjects with a working independence approach. The hybrid model further accommodates treatment effect modification.",
    "version": "1.0",
    "maintainer": "Andrew J. Spieker <aspieker@upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12462,
    "package_name": "endorse",
    "title": "Bayesian Measurement Models for Analyzing Endorsement\nExperiments",
    "description": "Fit the hierarchical and non-hierarchical Bayesian measurement models proposed by Bullock, Imai, and Shapiro (2011) <DOI:10.1093/pan/mpr031> to analyze endorsement experiments.  Endorsement experiments are a survey methodology for eliciting truthful responses to sensitive questions.  This methodology is helpful when measuring support for socially sensitive political actors such as militant groups.  The model is fitted with a Markov chain Monte Carlo algorithm and produces the output containing draws from the posterior distribution. ",
    "version": "1.6.2",
    "maintainer": "Yuki Shiraito <shiraito@umich.edu>",
    "url": "https://github.com/SensitiveQuestions/endorse/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12464,
    "package_name": "energy",
    "title": "E-Statistics: Multivariate Inference via the Energy of Data",
    "description": "E-statistics (energy) tests and statistics for multivariate and univariate inference,",
    "version": "1.7-12",
    "maintainer": "",
    "url": "https://github.com/mariarizzo/energy",
    "exports": [],
    "topics": ["distance-correlation", "energy", "multivariate-analysis", "r", "r-package", "statistics"],
    "score": "NA",
    "stars": 48
  },
  {
    "id": 12469,
    "package_name": "enetLTS",
    "title": "Robust and Sparse Methods for High Dimensional Linear and Binary\nand Multinomial Regression",
    "description": "Fully robust versions of the elastic net estimator are introduced for linear and binary and multinomial regression, in particular high dimensional data. The algorithm searches for outlier free subsets on which the classical elastic net estimators can be applied. A reweighting step is added to improve the statistical efficiency of the proposed estimators. Selecting appropriate tuning parameters for elastic net penalties are done via cross-validation. ",
    "version": "1.1.0",
    "maintainer": "Fatma Sevinc Kurnaz <fatmasevinckurnaz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12482,
    "package_name": "enrichwith",
    "title": "Methods to Enrich R Objects with Extra Components",
    "description": "Provides the \"enrich\" method to enrich list-like R objects with new, relevant components. The current version has methods for enriching objects of class 'family', 'link-glm', 'lm', 'glm' and 'betareg'. The resulting objects preserve their class, so all methods associated with them still apply. The package also provides the 'enriched_glm' function that has the same interface as 'glm' but results in objects of class 'enriched_glm'. In addition to the usual components in a `glm` object, 'enriched_glm' objects carry an object-specific simulate method and functions to compute the scores, the observed and expected information matrix, the first-order bias, as well as model densities, probabilities, and quantiles at arbitrary parameter values. The package can also be used to produce customizable source code templates for the structured implementation of methods to compute new components and enrich arbitrary objects.",
    "version": "0.4.0",
    "maintainer": "Ioannis Kosmidis <ioannis.kosmidis@warwick.ac.uk>",
    "url": "https://github.com/ikosmidis/enrichwith",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12484,
    "package_name": "ensembleBMA",
    "title": "Probabilistic Forecasting using Ensembles and Bayesian Model\nAveraging",
    "description": "Bayesian Model Averaging to create probabilistic forecasts\n        from ensemble forecasts and weather observations\n <https://stat.uw.edu/sites/default/files/files/reports/2007/tr516.pdf>.",
    "version": "5.1.8",
    "maintainer": "Chris Fraley <fraley@u.washington.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12485,
    "package_name": "ensembleMOS",
    "title": "Ensemble Model Output Statistics",
    "description": "Ensemble Model Output Statistics to create probabilistic\n        forecasts from ensemble forecasts and weather observations.",
    "version": "0.8.2",
    "maintainer": "Sandor Baran <baran.sandor@inf.unideb.hu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12487,
    "package_name": "ensemblepp",
    "title": "Ensemble Postprocessing Data Sets",
    "description": "Data sets for the chapter \"Ensemble Postprocessing with R\" of the book Stephane Vannitsem, Daniel S. Wilks, and Jakob W. Messner (2018) \"Statistical Postprocessing of Ensemble Forecasts\", Elsevier, 362pp. These data sets contain temperature and precipitation ensemble weather forecasts and corresponding observations at Innsbruck/Austria. Additionally, a demo with the full code of the book chapter is provided.",
    "version": "1.0-0",
    "maintainer": "Jakob Messner <jakob.messner@posteo.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12489,
    "package_name": "ensr",
    "title": "Elastic Net SearcheR",
    "description": "Elastic net regression models are controlled by two parameters,\n  lambda, a measure of shrinkage, and alpha, a metric defining the model's\n  location on the spectrum between ridge and lasso regression.  \n  glmnet provides tools for selecting lambda via cross\n  validation but no automated methods for selection of alpha.  Elastic Net\n  SearcheR automates the simultaneous selection of both lambda and alpha.\n  Developed, in part, with support by NICHD R03 HD094912.",
    "version": "0.1.0",
    "maintainer": "Peter DeWitt <peter.dewitt@ucdenver.edu>",
    "url": "https://github.com/dewittpe/ensr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12491,
    "package_name": "entropy",
    "title": "Estimation of Entropy, Mutual Information and Related Quantities",
    "description": "Implements various estimators of entropy for discrete random \n  variables, including the shrinkage estimator by Hausser and Strimmer (2009),\n  the maximum likelihood and the Millow-Madow estimator, various Bayesian\n  estimators, and the Chao-Shen estimator.  It also offers an R interface to the\n  NSB estimator.  Furthermore, the package provides functions for estimating the\n  Kullback-Leibler divergence, the chi-squared divergence, mutual information,\n  and the chi-squared divergence of independence.  It also computes the\n  G statistic and the chi-squared statistic and corresponding p-values.\n  Furthermore, there are functions for discretizing continuous random variables.",
    "version": "1.3.2",
    "maintainer": "Korbinian Strimmer <strimmerlab@gmail.com>",
    "url": "https://strimmerlab.github.io/software/entropy/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12502,
    "package_name": "envoutliers",
    "title": "Methods for Identification of Outliers in Environmental Data",
    "description": "Three semi-parametric methods for detection of outliers in environmental data based on kernel regression and subsequent analysis of smoothing residuals. The first method (Campulova, Michalek, Mikuska and Bokal (2018) <DOI: 10.1002/cem.2997>) analyzes the residuals using changepoint analysis, the second method is based on control charts (Campulova, Veselik and Michalek (2017) <DOI: 10.1016/j.apr.2017.01.004>) and the third method (Holesovsky, Campulova and Michalek (2018) <DOI: 10.1016/j.apr.2017.06.005>) analyzes the residuals using extreme value theory (Holesovsky, Campulova and Michalek (2018) <DOI: 10.1016/j.apr.2017.06.005>).",
    "version": "1.1.0",
    "maintainer": "Martina Campulova <martina.campulova@mendelu.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12516,
    "package_name": "epe4md",
    "title": "EPE's 4MD Model to Forecast the Adoption of Distributed\nGeneration",
    "description": "EPE's (Empresa de Pesquisa Energética) 4MD (Modelo de Mercado da Micro e Minigeração Distribuída - Micro and Mini Distributed Generation Market Model) model to forecast the adoption of Distributed Generation. Given the user's assumptions, it is possible to estimate how many consumer units will have distributed generation in Brazil over the next 10 years, for example. In addition, it is possible to estimate the installed capacity, the amount of investments that will be made in the country and the monthly energy contribution of this type of generation. <https://www.epe.gov.br/sites-pt/publicacoes-dados-abertos/publicacoes/PublicacoesArquivos/publicacao-689/topico-639/NT_Metodologia_4MD_PDE_2032_VF.pdf>.",
    "version": "0.1.4",
    "maintainer": "Gabriel Konzen <gabriel.konzen@epe.gov.br>",
    "url": "https://epe-gov-br.github.io/epe4md/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12588,
    "package_name": "epr",
    "title": "Easy Polynomial Regression",
    "description": "Performs analysis of polynomial regression in simple designs with quantitative treatments.",
    "version": "3.0",
    "maintainer": "Emmanuel Arnhold <emmanuelarnhold@yahoo.com.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12591,
    "package_name": "epts",
    "title": "Educational Platform Trials Simulator",
    "description": "Simulating multi-arm cluster-randomized, multi-site, and simple randomized trials. Includes functions for conducting multilevel analyses using both Bayesian and Frequentist methods. Supports futility and superiority analyses through Bayesian approaches, along with visualization tools to aid interpretation and presentation of results.",
    "version": "1.2.2",
    "maintainer": "Mohammad Sayari <md.sayari13@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12596,
    "package_name": "eqtesting",
    "title": "Equivalence Testing Functions",
    "description": "Contains several functions for equivalence testing and practical significance testing. First, the tsti() command provides an automatic computation of three-sided testing results for a given estimate, standard error, and region of practical equivalence. For details, see Goeman, Solari, & Stijnen (2010) <doi:10.1002/sim.4002> and Isager & Fitzgerald (2024) <doi:10.31234/osf.io/8y925>. Second, the lddtest() command performs logarithmic density discontinuity equivalence testing for regression discontinuity designs. For reference, see Fitzgerald (2025) <doi:10.31222/osf.io/2dgrp_v1>. ",
    "version": "0.1.1",
    "maintainer": "Jack Fitzgerald <j.f.fitzgerald@vu.nl>",
    "url": "https://github.com/jack-fitzgerald/eqtesting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12609,
    "package_name": "era",
    "title": "Year-Based Time Scales",
    "description": "Provides a consistent representation of year-based time scales as a\n    numeric vector with an associated 'era'. There are built-in era definitions\n    for many year numbering systems used in contemporary and historic calendars \n    (e.g. Common Era, Islamic 'Hijri' years); year-based time scales used in \n    archaeology, astronomy, geology, and other palaeosciences (e.g. \n    Before Present, SI-prefixed 'annus'); and support for arbitrary user-defined\n    eras. Years can converted from any one era to another using a generalised \n    transformation function. Methods are also provided for robust casting and \n    coercion between years and other numeric types, type-stable arithmetic with \n    years, and pretty-printing in tables.",
    "version": "0.5.0",
    "maintainer": "Joe Roe <joe@joeroe.io>",
    "url": "https://era.joeroe.io, https://github.com/joeroe/era",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12612,
    "package_name": "erboost",
    "title": "Nonparametric Multiple Expectile Regression via ER-Boost",
    "description": "Expectile regression is a nice tool for estimating the conditional expectiles of a response variable given a set of covariates. This package implements a regression tree based gradient boosting estimator for nonparametric multiple expectile regression, proposed by Yang, Y., Qian, W. and Zou, H. (2018) <doi:10.1080/00949655.2013.876024>. The code is based on the 'gbm' package originally developed by Greg Ridgeway.",
    "version": "1.5",
    "maintainer": "Yi Yang <yi.yang6@mcgill.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12623,
    "package_name": "ergmgp",
    "title": "Tools for Modeling ERGM Generating Processes",
    "description": "Provides tools for simulating draws from continuous time processes with well-defined exponential family random graph (ERGM) equilibria, i.e. ERGM generating processes (EGPs).  A number of EGPs are supported, including the families identified in Butts (2023) <doi:10.1080/0022250X.2023.2180001>, as are functions for hazard calculation and timing calibration.",
    "version": "0.1-2",
    "maintainer": "Carter T. Butts <buttsc@uci.edu>",
    "url": "https://statnet.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12634,
    "package_name": "errum",
    "title": "Exploratory Reduced Reparameterized Unified Model Estimation",
    "description": "Perform a Bayesian estimation of the exploratory reduced\n    reparameterized unified model (ErRUM) described by Culpepper and Chen (2018)\n    <doi:10.3102/1076998618791306>.",
    "version": "0.0.4",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "url": "https://github.com/tmsalab/errum, https://tmsalab.github.io/errum/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12640,
    "package_name": "esback",
    "title": "Expected Shortfall Backtesting",
    "description": "Implementations of the expected shortfall backtests of Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    as well as other well known backtests from the literature. Can be used to assess the correctness of forecasts of the \n    expected shortfall risk measure which is e.g. used in the banking and finance industry for quantifying the market risk \n    of investments. A special feature of the backtests of  Bayer and Dimitriadis (2020) <doi:10.1093/jjfinec/nbaa013> \n    is that they only require forecasts of  the expected shortfall, which is in striking contrast to all other existing \n    backtests, making them particularly attractive for practitioners.",
    "version": "0.3.1",
    "maintainer": "Sebastian Bayer <sebastian.bayer@uni-konstanz.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12649,
    "package_name": "esemifar",
    "title": "Smoothing Long-Memory Time Series",
    "description": "The nonparametric trend and its derivatives in equidistant time \n    series (TS) with long-memory errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user.\n    The smoothing methods of the package are described in Letmathe, S., Beran,\n    J. and Feng, Y., (2023) <doi:10.1080/03610926.2023.2276049>.",
    "version": "2.0.1",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12651,
    "package_name": "eshrink",
    "title": "Shrinkage for Effect Estimation",
    "description": "Computes shrinkage estimators for regression problems. Selects penalty parameter by minimizing bias and variance in the effect estimate, where bias and variance are estimated from the posterior predictive distribution. See Keller and Rice (2017) <doi:10.1093/aje/kwx225> for more details.",
    "version": "0.2.0",
    "maintainer": "Kayleigh Keller <kayleigh.keller@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12657,
    "package_name": "esreg",
    "title": "Joint Quantile and Expected Shortfall Regression",
    "description": "\n    Simultaneous modeling of the quantile and the expected shortfall of a response variable given \n    a set of covariates, see Dimitriadis and Bayer (2019) <doi:10.1214/19-EJS1560>.",
    "version": "0.6.2",
    "maintainer": "Sebastian Bayer <sebastian.bayer@uni-konstanz.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12663,
    "package_name": "estar",
    "title": "Ecological Stability Metrics",
    "description": "Standardises and facilitates the use of eleven established stability properties that have been used to assess systems’ responses to press or pulse disturbances at different ecological levels (e.g. population, community). There are two sets of functions. The first set corresponds to functions that measure stability at any level of organisation, from individual to community and can be applied to a time series of a system’s state variables (e.g., body mass, population abundance, or species diversity). The properties included in this set are: invariability, resistance, extent and rate of recovery, persistence, and overall ecological vulnerability. The second set of functions can be applied to Jacobian matrices. The functions in this set measure the stability of a community at short and long time scales. In the short term, the community’s response is measured by maximal amplification, reactivity and initial resilience (i.e. initial rate of return to equilibrium). In the long term, stability can be measured as asymptotic resilience and intrinsic stochastic invariability. Figueiredo et al. (2025) <doi:10.32942/X2M053>.",
    "version": "1.0-1",
    "maintainer": "Ludmilla Figueiredo <ludmilla.figueiredo@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12666,
    "package_name": "estimability",
    "title": "Tools for Assessing Estimability of Linear Predictions",
    "description": "Provides tools for determining estimability of linear functions \n  of regression coefficients, and 'epredict' methods that handle \n  non-estimable cases correctly. Estimability theory is discussed in\n  many linear-models textbooks including Chapter 3 of Monahan, JF (2008), \n  \"A Primer on Linear Models\", Chapman and Hall (ISBN 978-1-4200-6201-4).",
    "version": "1.5.1",
    "maintainer": "Russell Lenth <russell-lenth@uiowa.edu>",
    "url": "https://github.com/rvlenth/estimability,\nhttps://rvlenth.github.io/estimability/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12670,
    "package_name": "estimatr",
    "title": "Fast Estimators for Design-Based Inference",
    "description": "Fast procedures for small set of commonly-used, design-appropriate estimators with robust standard errors and confidence intervals. Includes estimators for linear regression, instrumental variables regression, difference-in-means, Horvitz-Thompson estimation, and regression improving precision of experimental estimates by interacting treatment with centered pre-treatment covariates introduced by Lin (2013) <doi:10.1214/12-AOAS583>.",
    "version": "1.0.6",
    "maintainer": "Graeme Blair <graeme.blair@gmail.com>",
    "url": "https://declaredesign.org/r/estimatr/,\nhttps://github.com/DeclareDesign/estimatr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12675,
    "package_name": "esviz",
    "title": "Plotting Functions for Climate Science and Services",
    "description": "A plotting package for climate science and services. Provides a set\n    of functions for visualizing climate data, including maps, time series, \n    scorecards and other diagnostics. Some functions are adapted and extended \n    from the 's2dv' and 'CSTools' packages (Manubens et al. (2018)\n    <doi:10.1016/j.envsoft.2018.01.018>; Pérez-Zanón et al. (2022)\n    <doi:10.5194/gmd-15-6115-2022>), with more consistent and integrated \n    functionalities.",
    "version": "0.0.1",
    "maintainer": "Ariadna Batalla <ariadna.batalla@bsc.es>",
    "url": "https://gitlab.earth.bsc.es/es/esviz/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12688,
    "package_name": "etwfe",
    "title": "Extended Two-Way Fixed Effects",
    "description": "Convenience functions for implementing extended two-way \n  fixed effect regressions a la Wooldridge (2021, 2023)\n  <doi:10.2139/ssrn.3906345>, <doi:10.1093/ectj/utad016>.",
    "version": "0.6.0",
    "maintainer": "Grant McDermott <gmcd@amazon.com>",
    "url": "https://grantmcdermott.com/etwfe/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12705,
    "package_name": "evalITR",
    "title": "Evaluating Individualized Treatment Rules",
    "description": "Provides various statistical methods for evaluating\n    Individualized Treatment Rules under randomized data. The provided\n    metrics include Population Average Value (PAV), Population Average\n    Prescription Effect (PAPE), Area Under Prescription Effect Curve\n    (AUPEC). It also provides the tools to analyze Individualized\n    Treatment Rules under budget constraints. Detailed reference in Imai\n    and Li (2019) <arXiv:1905.05389>.",
    "version": "1.0.0",
    "maintainer": "Michael Lingzhi Li <mili@hbs.edu>",
    "url": "https://github.com/MichaelLLi/evalITR,\nhttps://michaellli.github.io/evalITR/,\nhttps://jialul.github.io/causal-ml/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12713,
    "package_name": "evdbayes",
    "title": "Bayesian Analysis in Extreme Value Theory",
    "description": "Provides functions for the Bayesian analysis of extreme\n        value models, using Markov chain Monte Carlo methods. Allows\n\t\tthe construction of both uninformative and informed prior \n\t\tdistributions for common statistical models applied to extreme\n        event data, including the generalized extreme value distribution.\t\t",
    "version": "1.1-3",
    "maintainer": "Alec Stephenson <alec_stephenson@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12719,
    "package_name": "eventTrack",
    "title": "Event Prediction for Time-to-Event Endpoints",
    "description": "Implements the hybrid framework for event prediction described in Fang & Zheng (2011, <doi:10.1016/j.cct.2011.05.013>). To estimate the survival function the event prediction is based on, a piecewise exponential hazard function is fit to the time-to-event data to infer the potential change points. Prior to the last identified change point, the survival function is estimated using Kaplan-Meier, and the tail after the change point is fit using piecewise exponential. ",
    "version": "1.0.4",
    "maintainer": "Kaspar Rufibach <kaspar.rufibach@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12727,
    "package_name": "evian",
    "title": "Evidential Analysis of Genetic Association Data",
    "description": "Evidential regression analysis for dichotomous and quantitative outcome data. The following references described the methods in this package:\n        Strug, L. J., Hodge, S. E., Chiang, T., Pal, D. K., Corey, P. N., & Rohde, C. (2010) <doi:10.1038/ejhg.2010.47>.\n        Strug, L. J., & Hodge, S. E. (2006) <doi:10.1159/000094709>.\n        Royall, R. (1997) <ISBN:0-412-04411-0>.",
    "version": "2.1.0",
    "maintainer": "Jiafen Gong <jiafen.gong@sickkids.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12735,
    "package_name": "evmissing",
    "title": "Extreme Value Analyses with Missing Data",
    "description": "Performs likelihood-based extreme value inferences with \n    adjustment for the presence of missing values based on Simpson and \n    Northrop (2025) <doi:10.48550/arXiv.2512.15429>. A Generalised Extreme Value\n    distribution is fitted to block maxima using maximum likelihood estimation, \n    with the location and scale parameters reflecting the numbers of \n    non-missing raw values in each block. A Bayesian version is also provided. \n    For the purposes of comparison, there are options to make no adjustment for \n    missing values or to discard any block maximum for which greater than a \n    percentage of the underlying raw values are missing. Example datasets \n    containing missing values are provided. ",
    "version": "1.0.0",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "url": "https://paulnorthrop.github.io/evmissing/,\nhttps://github.com/paulnorthrop/evmissing",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12752,
    "package_name": "ewp",
    "title": "An Empirical Model for Underdispersed Count Data",
    "description": "Count regression models for underdispersed small counts (lambda < 20) based on the three-parameter exponentially weighted Poisson distribution of Ridout & Besbeas (2004) <DOI:10.1191/1471082X04st064oa>.",
    "version": "0.1.2",
    "maintainer": "Philipp Boersch-Supan <pboesu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12757,
    "package_name": "exactRankTests",
    "title": "Exact Distributions for Rank and Permutation Tests",
    "description": "Computes exact conditional p-values and quantiles using an\n implementation of the Shift-Algorithm by Streitberg & Roehmel.",
    "version": "0.8-35",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12765,
    "package_name": "exams.forge",
    "title": "Support for Compiling Examination Tasks using the 'exams'\nPackage",
    "description": "The main aim is to further facilitate the creation of exercises based on the package 'exams' \n    by Grün, B., and Zeileis, A. (2009) <doi:10.18637/jss.v029.i10>. Creating effective student exercises \n    involves challenges such as creating appropriate data sets and ensuring access to intermediate values \n    for accurate explanation of solutions. The functionality includes the generation of univariate and \n    bivariate data including simple time series, functions for theoretical distributions and their approximation, \n    statistical and mathematical calculations for tasks in basic statistics courses as well as general tasks \n    such as string manipulation, LaTeX/HTML formatting and the editing of XML task files for 'Moodle'.",
    "version": "1.0.12",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12766,
    "package_name": "exams.forge.data",
    "title": "Sample and Precomputed Data for Use with 'exams.forge'",
    "description": "Provides a small collection of datasets supporting Pearson correlation \n    and linear regression analysis. It includes the precomputed dataset 'sos100', \n    with integer values summing to zero and squared sum equal to 100. For other \n    values of 'n' and user-defined parameters, the 'sos()' function from the \n    'exams.forge' package can be used to generate datasets on the fly. In addition, \n    the package contains around 500 german R Markdown exercises that illustrate the usage \n    of 'exams.forge' commands.",
    "version": "0.1.3",
    "maintainer": "Sigbert Klinke <sigbert@hu-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12779,
    "package_name": "exdex",
    "title": "Estimation of the Extremal Index",
    "description": "Performs frequentist inference for the extremal index of a \n    stationary time series.  Two types of methodology are used.  One type is\n    based on a model that relates the distribution of block maxima to the \n    marginal distribution of series and leads to the semiparametric maxima \n    estimators described in Northrop (2015) <doi:10.1007/s10687-015-0221-5> and \n    Berghaus and Bucher (2018) <doi:10.1214/17-AOS1621>.  Sliding block maxima\n    are used to increase precision of estimation. A graphical block size \n    diagnostic is provided.  The other type of methodology uses a model for the \n    distribution of threshold inter-exceedance times (Ferro and Segers (2003) \n    <doi:10.1111/1467-9868.00401>). Three versions of this type of approach are \n    provided: the iterated weight least squares approach of Suveges (2007) \n    <doi:10.1007/s10687-007-0034-2>, the K-gaps model of \n    Suveges and Davison (2010) <doi:10.1214/09-AOAS292> and a similar approach\n    of Holesovsky and Fusek (2020) <doi:10.1007/s10687-020-00374-3> \n    that we refer to as D-gaps. For the K-gaps and D-gaps models this package \n    allows missing values in the data, can accommodate independent subsets of \n    data, such as monthly or seasonal time series from different years, and can \n    incorporate information from right-censored inter-exceedance times.  \n    Graphical diagnostics for the threshold level and the respective tuning\n    parameters K and D are provided.",
    "version": "1.2.3",
    "maintainer": "Paul J. Northrop <p.northrop@ucl.ac.uk>",
    "url": "https://github.com/paulnorthrop/exdex,\nhttps://paulnorthrop.github.io/exdex/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12780,
    "package_name": "exdqlm",
    "title": "Extended Dynamic Quantile Linear Models",
    "description": "Routines for Bayesian estimation and analysis of dynamic quantile linear models utilizing the extended asymmetric Laplace error distribution, also known as extended dynamic quantile linear models (exDQLM) described in Barata et al (2020) <doi:10.1214/21-AOAS1497>. ",
    "version": "0.1.4",
    "maintainer": "Raquel Barata <raquel.a.barata@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12794,
    "package_name": "experiment",
    "title": "R Package for Designing and Analyzing Randomized Experiments",
    "description": "Provides various statistical methods for\n  designing and analyzing randomized experiments. One functionality\n  of the package is the implementation of randomized-block and\n  matched-pair designs based on possibly multivariate pre-treatment\n  covariates. The package also provides the tools to analyze various\n  randomized experiments including cluster randomized experiments,\n  two-stage randomized experiments, randomized experiments with \n  noncompliance, and randomized experiments with missing data.",
    "version": "1.2.1",
    "maintainer": "Kosuke Imai <imai@harvard.edu>",
    "url": "https://github.com/kosukeimai/experiment",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12796,
    "package_name": "expertsurv",
    "title": "Incorporate Expert Opinion with Parametric Survival Models",
    "description": "Enables users to incorporate expert opinion with parametric survival analysis using a Bayesian or frequentist approach. Expert Opinion can be provided on the survival probabilities at certain time-point(s) or for the difference in mean survival between two treatment arms. Please reference it's use as Cooney, P., White, A. (2023) <doi:10.1177/0272989X221150212>.",
    "version": "1.4.0",
    "maintainer": "Philip Cooney <phcooney@tcd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12805,
    "package_name": "export",
    "title": "Streamlined Export of Graphs and Data Tables",
    "description": "Easily export 'R' graphs and statistical output to 'Microsoft\n    Office' / 'LibreOffice', 'Latex' and 'HTML' Documents, using sensible defaults\n    that result in publication-quality output with simple, straightforward commands.\n    Output to 'Microsoft Office' is in editable 'DrawingML' vector format for\n    graphs, and can use corporate template documents for styling. This enables\n    the production of standardized reports and also allows for manual tidy-up\n    of the layout of 'R' graphs in 'Powerpoint' before final publication. Export\n    of graphs is flexible, and functions enable the currently showing R graph\n    or the currently showing 'R' stats object to be exported, but also allow the\n    graphical or tabular output to be passed as objects. The package relies on package\n    'officer' for export to 'Office' documents,and output files are also fully compatible\n    with 'LibreOffice'. Base 'R', 'ggplot2' and 'lattice' plots are supported, as\n    well as a wide variety of 'R' stats objects, via wrappers to xtable(), broom::tidy() \n    and stargazer(), including aov(), lm(), glm(), lme(), glmnet() and coxph() as\n    well as matrices and data frames and many more...",
    "version": "0.3.2",
    "maintainer": "Tom Wenseleers <tom.wenseleers@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12807,
    "package_name": "expsmooth",
    "title": "Data Sets from \"Forecasting with Exponential Smoothing\"",
    "description": "Data sets from the book \"Forecasting with exponential smoothing: the state space approach\" by \n\tHyndman, Koehler, Ord and Snyder (Springer, 2008).",
    "version": "2.3",
    "maintainer": "Rob J Hyndman <Rob.Hyndman@monash.edu>",
    "url": "https://github.com/robjhyndman/expsmooth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12811,
    "package_name": "extBatchMarking",
    "title": "Extended Batch Marking Models",
    "description": "A system for batch-marking data analysis to estimate survival probabilities, capture probabilities, and enumerate the population abundance for both marked and unmarked individuals. The estimation of only marked individuals can be achieved through the batchMarkOptim() function. Similarly, the combined marked and unmarked can be achieved through the batchMarkUnmarkOptim() function. The algorithm was also implemented for the hidden Markov model encapsulated in batchMarkUnmarkOptim() to estimate the abundance of both marked and unmarked individuals in the population. The package is based on the paper: \"Hidden Markov Models for Extended Batch Data\" of Cowen et al. (2017) <doi:10.1111/biom.12701>.",
    "version": "1.1.0",
    "maintainer": "Kehinde Olobatuyi <olobatuyikenny@uvic.ca>",
    "url": "https://github.com/Olobatuyi/extBatchMarking_cov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12817,
    "package_name": "extlasso",
    "title": "Maximum Penalized Likelihood Estimation with Extended Lasso\nPenalty",
    "description": "Estimates coefficients of extended LASSO penalized linear regression and generalized linear models. Currently lasso and elastic net penalized linear regression and generalized linear models are considered. This package currently utilizes an accurate approximation of L1 penalty and then a modified Jacobi algorithm to estimate the coefficients. There is provision for plotting of the solutions and predictions of coefficients at given values of lambda. This package also contains functions for cross validation to select a suitable lambda value given the data. Also provides a function for estimation in fused lasso penalized linear regression. For more details, see Mandal, B. N.(2014). Computational methods for L1 penalized GLM model fitting, unpublished report submitted to Macquarie University, NSW, Australia.",
    "version": "0.3",
    "maintainer": "B N Mandal <mandal.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12818,
    "package_name": "extr",
    "title": "Extinction Risk Estimation",
    "description": "Estimates extinction risk from population time series under a\n    drifted Wiener process using the w-z method for accurate confidence\n    intervals.",
    "version": "1.0.0",
    "maintainer": "Hiroshi Hakoyama <hiroshi.hakoyama@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12827,
    "package_name": "extras",
    "title": "Helper Functions for Bayesian Analyses",
    "description": "Functions to 'numericise' 'R' objects (coerce to numeric\n    objects), summarise 'MCMC' (Monte Carlo Markov Chain) samples and\n    calculate deviance residuals as well as 'R' translations of some\n    'BUGS' (Bayesian Using Gibbs Sampling), 'JAGS' (Just Another Gibbs\n    Sampler), 'STAN' and 'TMB' (Template Model Builder) functions.",
    "version": "0.8.0",
    "maintainer": "Nicole Hill <nicole@poissonconsulting.ca>",
    "url": "https://poissonconsulting.github.io/extras/,\nhttps://github.com/poissonconsulting/extras",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12830,
    "package_name": "extremeIndex",
    "title": "Forecast Verification for Extreme Events",
    "description": "An index measuring the amount of information brought by forecasts for extreme events, subject to calibration, is computed. This index is originally designed for weather or climate forecasts, but it may be used in other forecasting contexts. This is the implementation of the index in Taillardat et al. (2019) <arXiv:1905.04022>.",
    "version": "0.0.3",
    "maintainer": "Maxime Taillardat <maxime.taillardat@meteo.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12832,
    "package_name": "extremefit",
    "title": "Estimation of Extreme Conditional Quantiles and Probabilities",
    "description": "Extreme value theory, nonparametric kernel estimation, tail\n    conditional probabilities, extreme conditional quantile, adaptive estimation,\n    quantile regression, survival probabilities.",
    "version": "1.1.0",
    "maintainer": "Kevin Jaunatre <kevin.jaunatre@hotmail.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12833,
    "package_name": "extrememix",
    "title": "Bayesian Estimation of Extreme Value Mixture Models",
    "description": "Fits extreme value mixture models, which are models for tails not requiring selection of a threshold, for continuous data. It includes functions for model comparison, estimation of quantity of interest in extreme value analysis and plotting. Reference: CN Behrens, HF Lopes, D Gamerman (2004) <doi:10.1191/1471082X04st075oa>. FF do Nascimento, D. Gamerman, HF Lopes <doi:10.1007/s11222-011-9270-z>.",
    "version": "0.0.1",
    "maintainer": "Manuele Leonelli <manuele.leonelli@ie.edu>",
    "url": "https://github.com/manueleleonelli/extrememix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12836,
    "package_name": "extremogram",
    "title": "Estimation of Extreme Value Dependence for Time Series Data",
    "description": "Estimation of the sample univariate, cross and return time extremograms. The package can also adds empirical confidence bands to each of the extremogram plots via a permutation procedure under the assumption that the data are independent. Finally, the stationary bootstrap allows us to construct credible confidence bands for the extremograms.  ",
    "version": "1.0.2",
    "maintainer": "Nadezda Frolova <nfrolova@ualberta.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12849,
    "package_name": "ez.combat",
    "title": "Easy ComBat Harmonization",
    "description": "A dataframe-friendly implementation of ComBat Harmonization which uses an empirical Bayesian framework to remove batch effects.\n    Johnson WE & Li C (2007)  <doi:10.1093/biostatistics/kxj037> \"Adjusting batch effects in microarray expression data using empirical Bayes methods.\"\n    Fortin J-P, Cullen N, Sheline YI, Taylor WD, Aselcioglu I, Cook PA, Adams P, Cooper C, Fava M, McGrath PJ, McInnes M, Phillips ML, Trivedi MH, Weissman MM, & Shinohara RT (2017) <doi:10.1016/j.neuroimage.2017.11.024> \"Harmonization of cortical thickness measurements across scanners and sites.\"\n    Fortin J-P, Parker D, Tun<e7> B, Watanabe T, Elliott MA, Ruparel K, Roalf DR, Satterthwaite TD, Gur RC, Gur RE, Schultz RT, Verma R, & Shinohara RT (2017) <doi:10.1016/j.neuroimage.2017.08.047> \"Harmonization of multi-site diffusion tensor imaging data.\"",
    "version": "1.0.0",
    "maintainer": "Timothy Koscik <timothy-koscik@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12862,
    "package_name": "fANCOVA",
    "title": "Nonparametric Analysis of Covariance",
    "description": "A collection of R functions to perform nonparametric \n    analysis of covariance for regression curves or surfaces. \n    Testing the equality or parallelism of nonparametric curves \n    or surfaces is equivalent to analysis of variance (ANOVA) or \n    analysis of covariance (ANCOVA) for one-sample functional data. \n    Three different testing methods are available in the package, \n    including one based on L-2 distance, one based on an ANOVA \n    statistic, and one based on variance estimators.",
    "version": "0.6-1",
    "maintainer": "Xiaofeng Wang <wangx6@ccf.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12871,
    "package_name": "fEGarch",
    "title": "SM/LM EGARCH & GARCH, VaR/ES Backtesting & Dual LM Extensions",
    "description": "Implement and fit a variety of short-memory (SM) and long-memory\n  (LM) models from a very broad family of exponential generalized autoregressive\n  conditional heteroskedasticity (EGARCH) models, such as a MEGARCH (modified\n  EGARCH), FIEGARCH (fractionally integrated EGARCH), FIMLog-GARCH (fractionally\n  integrated modulus Log-GARCH), and more. The FIMLog-GARCH as part of the\n  EGARCH family is discussed in Feng et al. (2023)\n  <https://econpapers.repec.org/paper/pdnciepap/156.htm>. For convenience and\n  the purpose of comparison, a variety of other popular SM and LM GARCH-type\n  models, like an APARCH model, a fractionally integrated\n  APARCH (FIAPARCH) model, standard GARCH and fractionally integrated GARCH\n  (FIGARCH) models, GJR-GARCH and FIGJR-GARCH models, TGARCH and FITGARCH\n  models, are implemented as well as dual models with simultaneous modelling of\n  the mean, including dual long-memory models with a fractionally integrated\n  autoregressive moving average (FARIMA) model in the mean and a long-memory\n  model in the variance, and semiparametric volatility model extensions.\n  Parametric models and parametric model parts are fitted through\n  quasi-maximum-likelihood estimation.\n  Furthermore, common forecasting and backtesting functions for value-at-risk\n  (VaR) and expected shortfall (ES) based on the package's models are provided.",
    "version": "1.0.4",
    "maintainer": "Dominik Schulz <dominik.schulz@uni-paderborn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12873,
    "package_name": "fGarch",
    "title": "Rmetrics - Autoregressive Conditional Heteroskedastic Modelling",
    "description": "Analyze and model heteroskedastic behavior in financial time series.",
    "version": "4052.93",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/fGarchDoc/ (doc),\nhttps://CRAN.R-project.org/package=fGarch,\nhttps://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12877,
    "package_name": "fMRIscrub",
    "title": "Scrubbing and Other Data Cleaning Routines for fMRI",
    "description": "Data-driven fMRI denoising with projection scrubbing (Pham et al \n    (2022) <doi:10.1016/j.neuroimage.2023.119972>). Also includes routines for \n    DVARS (Derivatives VARianceS) (Afyouni and Nichols (2018) \n    <doi:10.1016/j.neuroimage.2017.12.098>), motion scrubbing (Power et al \n    (2012) <doi:10.1016/j.neuroimage.2011.10.018>), aCompCor (anatomical \n    Components Correction) (Muschelli et al (2014)\n    <doi:10.1016/j.neuroimage.2014.03.028>), detrending, and nuisance\n    regression. Projection scrubbing is also applicable to other\n    outlier detection tasks involving high-dimensional data.",
    "version": "0.14.5",
    "maintainer": "Amanda Mejia <mandy.mejia@gmail.com>",
    "url": "https://github.com/mandymejia/fMRIscrub",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12878,
    "package_name": "fMRItools",
    "title": "Routines for Common fMRI Processing Tasks",
    "description": "Supports fMRI (functional magnetic resonance imaging) \n    analysis tasks including reading in 'CIFTI', 'GIFTI' and \n    'NIFTI' data, temporal filtering, nuisance regression, and \n    aCompCor (anatomical Components Correction) (Muschelli et al.\n    (2014) <doi:10.1016/j.neuroimage.2014.03.028>).",
    "version": "0.7.2",
    "maintainer": "Amanda Mejia <mandy.mejia@gmail.com>",
    "url": "https://github.com/mandymejia/fMRItools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12880,
    "package_name": "fNonlinear",
    "title": "Rmetrics - Nonlinear and Chaotic Time Series Modelling",
    "description": "Provides a collection of functions for testing various aspects of\n\tunivariate time series including independence and neglected\n\tnonlinearities. Further provides functions to investigate the chaotic\n\tbehavior of time series processes and to simulate different types of chaotic\n\ttime series maps.",
    "version": "4041.82",
    "maintainer": "Paul Smith <paul@waternumbers.co.uk>",
    "url": "https://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12886,
    "package_name": "fTrading",
    "title": "Rmetrics - Trading and Rebalancing Financial Instruments",
    "description": "A collection of functions for trading and rebalancing financial\n\tinstruments. It implements various technical indicators to analyse time series such\n\tas moving averages or stochastic oscillators.",
    "version": "3042.79",
    "maintainer": "Tobias Setz <tobias.setz@live.com>",
    "url": "http://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12887,
    "package_name": "fUnitRoots",
    "title": "Rmetrics - Modelling Trends and Unit Roots",
    "description": "Provides four addons for analyzing trends and\n    unit roots in financial time series: (i) functions for the density\n    and probability of the augmented Dickey-Fuller Test, (ii) functions \n    for the density and probability of MacKinnon's unit root test \n    statistics, (iii) reimplementations for the ADF and MacKinnon\n    Test, and (iv) an 'urca' Unit Root Test Interface for Pfaff's\n    unit root test suite.",
    "version": "4052.82",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/fUnitRootsDoc/ (doc),\nhttps://CRAN.R-project.org/package=fUnitRoots,\nhttps://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12888,
    "package_name": "fabCI",
    "title": "FAB Confidence Intervals",
    "description": "Frequentist assisted by Bayes (FAB) confidence interval\n    construction. See 'Adaptive multigroup confidence intervals with constant\n    coverage' by Yu and Hoff <DOI:10.1093/biomet/asy009> and  \n    'Exact adaptive confidence intervals for linear regression coefficients'\n    by Hoff and Yu <DOI:10.1214/18-EJS1517>.",
    "version": "0.2",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12896,
    "package_name": "fable.prophet",
    "title": "Prophet Modelling Interface for 'fable'",
    "description": "Allows prophet models from the 'prophet' package to be used in a tidy workflow with the modelling interface of 'fabletools'. This extends 'prophet' to provide enhanced model specification and management, performance evaluation methods, and model combination tools.",
    "version": "0.1.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "url": "https://pkg.mitchelloharawild.com/fable.prophet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12897,
    "package_name": "fableCount",
    "title": "INGARCH and GLARMA Models for Count Time Series in Fable\nFramework",
    "description": "Provides a tidy R interface for count time series analysis. It includes implementation of the INGARCH (Integer Generalized Autoregressive Conditional Heteroskedasticity) model from the 'tscount' package and the GLARMA (Generalized Linear Autoregressive Moving Averages) model from the 'glarma' package. Additionally, it offers automated parameter selection algorithms based on the minimization of a penalized likelihood.",
    "version": "0.1.0",
    "maintainer": "Gustavo Almeida <gustavoalmeidasilva@ice.ufjf.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12898,
    "package_name": "fabletools",
    "title": "Core Tools for Packages in the 'fable' Framework",
    "description": "Provides tools, helpers and data structures for\n    developing models and time series functions for 'fable' and extension\n    packages. These tools support a consistent and tidy interface for time\n    series modelling and analysis.",
    "version": "0.5.1",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "url": "https://fabletools.tidyverts.org/,\nhttps://github.com/tidyverts/fabletools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12910,
    "package_name": "facmodTS",
    "title": "Time Series Factor Models for Asset Returns",
    "description": "Supports teaching methods of estimating and testing time series\n    factor models for use in robust portfolio construction and analysis. Unique\n    in providing not only classical least squares, but also modern robust model\n    fitting methods which are not much influenced by outliers. Includes\n    returns and risk decompositions, with user choice of  standard deviation,\n    value-at-risk, and expected shortfall risk measures. \"Robust Statistics\n    Theory and Methods (with R)\", R. A. Maronna, R. D. Martin, V. J. Yohai, \n    M. Salibian-Barrera (2019) <doi:10.1002/9781119214656>.",
    "version": "1.0",
    "maintainer": "Doug Martin <martinrd3d@gmail.com>",
    "url": "https://github.com/robustport/facmodTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12913,
    "package_name": "factReg",
    "title": "Multi-Environment Genomic Prediction with Penalized Factorial\nRegression",
    "description": "Multi-environment genomic prediction for training and test \n    environments using penalized factorial regression. Predictions are made \n    using genotype-specific environmental sensitivities as in Millet et al. \n    (2019) <doi:10.1038/s41588-019-0414-y>.",
    "version": "1.0.0",
    "maintainer": "Bart-Jan van Rossum <bart-jan.vanrossum@wur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12917,
    "package_name": "factor.switching",
    "title": "Post-Processing MCMC Outputs of Bayesian Factor Analytic Models",
    "description": "A well known identifiability issue in factor analytic models is the invariance with respect to orthogonal transformations. This problem burdens the inference under a Bayesian setup, where Markov chain Monte Carlo (MCMC) methods are used to generate samples from the posterior distribution. The package applies a series of rotation, sign and permutation transformations (Papastamoulis and Ntzoufras (2022) <DOI:10.1007/s11222-022-10084-4>) into raw MCMC samples of factor loadings, which are provided by the user. The post-processed output is identifiable and can be used for MCMC inference on any parametric function of factor loadings. Comparison of multiple MCMC chains is also possible.  ",
    "version": "1.4",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12921,
    "package_name": "factorial2x2",
    "title": "Design and Analysis of a 2x2 Factorial Trial",
    "description": "Used for the design and analysis of a 2x2 factorial trial for\n    a time-to-event endpoint.  It performs power calculations and significance\n    testing as well as providing estimates of the relevant hazard ratios and the \n    corresponding 95% confidence intervals.  Important reference papers include\n    Slud EV. (1994) <https://www.ncbi.nlm.nih.gov/pubmed/8086609>\n    Lin DY, Gong J, Gallo P, Bunn PH, Couper D. (2016) <DOI:10.1111/biom.12507>\n    Leifer ES, Troendle JF, Kolecki A, Follmann DA. (2020)\n    <https://github.com/EricSLeifer/factorial2x2/blob/master/Leifer%20et%20al.%20paper.pdf>.",
    "version": "0.2.0",
    "maintainer": "Eric Leifer <Eric.Leifer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12924,
    "package_name": "factorstochvol",
    "title": "Bayesian Estimation of (Sparse) Latent Factor Stochastic\nVolatility Models",
    "description": "Markov chain Monte Carlo (MCMC) sampler for fully Bayesian estimation of latent factor stochastic volatility models with interweaving <doi:10.1080/10618600.2017.1322091>. Sparsity can be achieved through the usage of Normal-Gamma priors on the factor loading matrix <doi:10.1016/j.jeconom.2018.11.007>.",
    "version": "1.1.0",
    "maintainer": "Gregor Kastner <gregor.kastner@aau.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12932,
    "package_name": "faintr",
    "title": "Factor Interpreter for Bayesian Regression Models",
    "description": "The FActorINTerpreteR provides convenience functions for",
    "version": "0.0.0.9010",
    "maintainer": "",
    "url": "https://github.com/michael-franke/faintr",
    "exports": [],
    "topics": ["bayesian", "brms", "contrast-coding", "factorial-design", "r-package", "regression", "rstats", "stan"],
    "score": "NA",
    "stars": 4
  },
  {
    "id": 12938,
    "package_name": "fairmodels",
    "title": "Flexible Tool for Bias Detection, Visualization, and Mitigation",
    "description": "Measure fairness metrics in one place for many models. Check how big is model's bias towards different races, sex, nationalities etc. Use measures such as Statistical Parity, Equal odds to detect the discrimination against unprivileged groups. Visualize the bias using heatmap, radar plot, biplot, bar chart (and more!). There are various pre-processing and post-processing bias mitigation algorithms implemented. Package also supports calculating fairness metrics for regression models. Find more details in (Wiśniewski, Biecek (2021)) <doi:10.48550/arXiv.2104.00507>.  ",
    "version": "1.2.2",
    "maintainer": "Jakub Wiśniewski <jakwisn@gmail.com>",
    "url": "https://fairmodels.drwhy.ai/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12951,
    "package_name": "familial",
    "title": "Statistical Tests of Familial Hypotheses",
    "description": "Provides functionality for testing familial hypotheses. Supports testing centers \n    belonging to the Huber family. Testing is carried out using the Bayesian bootstrap. One- and \n    two-sample tests are supported, as are directional tests. Methods for visualizing output are \n    provided.",
    "version": "1.0.7",
    "maintainer": "Ryan Thompson <ryan.thompson-1@uts.edu.au>",
    "url": "https://github.com/ryan-thompson/familial",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12955,
    "package_name": "fangs",
    "title": "Feature Allocation Neighborhood Greedy Search Algorithm",
    "description": "A neighborhood-based, greedy search algorithm is performed to estimate a feature allocation by minimizing the expected loss based on posterior samples from the feature allocation distribution. The method is described in Dahl, Johnson, and Andros (2023) \"Comparison and Bayesian Estimation of Feature Allocations\" <doi:10.1080/10618600.2023.2204136>.",
    "version": "0.2.21",
    "maintainer": "David B. Dahl <dahl@stat.byu.edu>",
    "url": "https://github.com/dbdahl/fangs-package",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12962,
    "package_name": "faraway",
    "title": "Datasets and Functions for Books by Julian Faraway",
    "description": "Books are \"Linear Models with R\" published 1st Ed. August 2004, 2nd Ed. July 2014, 3rd Ed. February 2025 by CRC press, ISBN 9781439887332, and \"Extending the Linear Model with R\" published by CRC press in 1st Ed. December 2005 and 2nd Ed. March 2016, ISBN 9781584884248 and \"Practical Regression and ANOVA in R\" contributed documentation on CRAN (now very dated).",
    "version": "1.0.9",
    "maintainer": "Julian Faraway <jjf23@bath.ac.uk>",
    "url": "https://github.com/julianfaraway/faraway",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12970,
    "package_name": "fastAFT",
    "title": "Fast Regression for the Accelerated Failure Time (AFT) Model",
    "description": "Fast censored linear regression for the accelerated failure time (AFT) model of Huang (2013) <doi:10.1111/sjos.12031>.",
    "version": "1.4",
    "maintainer": "Yijian Huang <yhuang5@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12972,
    "package_name": "fastFMM",
    "title": "Fast Functional Mixed Models using Fast Univariate Inference",
    "description": "Implementation of the fast univariate inference approach (Cui et al. (2022) <doi:10.1080/10618600.2021.1950006>, Loewinger et al. (2024) <doi:10.7554/eLife.95802.2>, Xin et al. (2025)) for fitting functional mixed models. User guides and Python package information can be found at <https://github.com/gloewing/photometry_FLMM>.",
    "version": "1.0.0",
    "maintainer": "Al Xin <axin@andrew.cmu.edu>",
    "url": "https://github.com/gloewing/fastFMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12975,
    "package_name": "fastGraph",
    "title": "Fast Drawing and Shading of Graphs of Statistical Distributions",
    "description": "Provides functionality to produce graphs of probability density functions and cumulative distribution functions with few keystrokes, allows shading under the curve of the probability density function to illustrate concepts such as p-values and critical values, and fits a simple linear regression line on a scatter plot with the equation as the main title.",
    "version": "2.1",
    "maintainer": "Steven T. Garren <garrenst@jmu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12990,
    "package_name": "fastTS",
    "title": "Fast Time Series Modeling for Seasonal Series with Exogenous\nVariables",
    "description": "An implementation of sparsity-ranked lasso and related methods \n    for time series data. This methodology is especially useful for \n    large time series with exogenous features and/or complex \n    seasonality. Originally described in Peterson and Cavanaugh \n    (2022) <doi:10.1007/s10182-021-00431-7> in the context of variable \n    selection with interactions and/or polynomials, ranked sparsity is \n    a philosophy with methods useful for variable selection in the \n    presence of prior informational asymmetry. This situation exists for time \n    series data with complex seasonality, as shown in Peterson and Cavanaugh \n    (2024) <doi:10.1177/1471082X231225307>, which also describes this package\n    in greater detail. The sparsity-ranked penalization methods for time series\n    implemented in 'fastTS' can fit large/complex/high-frequency time series\n    quickly, even with a high-dimensional exogenous feature set. The method is\n    considerably faster than its competitors, while often producing more \n    accurate predictions. Also included is a long hourly series of arrivals \n    into the University of Iowa Emergency Department with concurrent local \n    temperature.",
    "version": "1.0.3",
    "maintainer": "Ryan Andrew Peterson <ryan-peterson@uiowa.edu>",
    "url": "https://petersonr.github.io/fastTS/,\nhttps://github.com/petersonR/fastTS/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 12995,
    "package_name": "fastWavelets",
    "title": "Compute Maximal Overlap Discrete Wavelet Transform (MODWT) and À\nTrous Discrete Wavelet Transform",
    "description": "A lightweight package to compute Maximal Overlap Discrete Wavelet \n    Transform (MODWT) and À Trous Discrete Wavelet Transform by leveraging the \n    power of 'Rcpp' to make these operations fast. This package was designed for use in forecasting, and\n    allows users avoid the inclusion of future data when performing wavelet decomposition of time series.\n    See Quilty and Adamowski (2018) <doi:10.1016/j.jhydrol.2018.05.003>.",
    "version": "1.0.1",
    "maintainer": "John You <johnswyou@gmail.com>",
    "url": "https://github.com/johnswyou/fastWavelets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13000,
    "package_name": "fastbaps",
    "title": "A fast genetic clustering algorithm that approximates a Dirichlet Process Mixture model",
    "description": "Takes a multiple sequence alignment as input and clusters according to the 'no-admixture' model.",
    "version": "1.0.8",
    "maintainer": "",
    "url": "https://github.com/gtonkinhill/fastbaps",
    "exports": [],
    "topics": ["bayesian", "genetics", "hierarchical-clustering"],
    "score": "NA",
    "stars": 68
  },
  {
    "id": 13005,
    "package_name": "fastcox",
    "title": "Lasso and Elastic-Net Penalized Cox's Regression in High\nDimensions Models using the Cocktail Algorithm",
    "description": "We implement a cocktail algorithm, a good mixture of coordinate decent, the majorization-minimization principle and the strong rule, for computing the solution paths of the elastic net penalized Cox's proportional hazards model. The package is an implementation of Yang, Y. and Zou, H. (2013) <doi:10.4310/SII.2013.v6.n2.a1>.",
    "version": "1.1.4",
    "maintainer": "Yi Yang <yi.yang6@mcgill.ca>",
    "url": "https://github.com/archer-yang-lab/fastcox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13015,
    "package_name": "fastkqr",
    "title": "A Fast Algorithm for Kernel Quantile Regression",
    "description": "An efficient algorithm to fit and tune kernel quantile regression models based on the majorization-minimization (MM) method. It can also fit multiple quantile curves simultaneously without crossing.",
    "version": "1.0.0",
    "maintainer": "Qian Tang <qian-tang@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13016,
    "package_name": "fastliu",
    "title": "Fast Functions for Liu Regression with Regularization Parameter\nand Statistics",
    "description": "Efficient computation of the Liu regression coefficient paths, Liu-related statistics and \n    information criteria for a grid of the regularization parameter. \n    The computations are based on the 'C++' library 'Armadillo' through the 'R' package 'Rcpp'.",
    "version": "1.0",
    "maintainer": "Murat Genç <muratgenc@tarsus.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13030,
    "package_name": "fastqrs",
    "title": "Fast Algorithms for Quantile Regression with Selection",
    "description": "Fast estimation algorithms to implement the Quantile Regression with Selection estimator and the multiplicative Bootstrap for inference. This estimator can be used to estimate models that feature sample selection and heterogeneous effects in cross-sectional data. For more details, see Arellano and Bonhomme (2017) <doi:10.3982/ECTA14030> and Pereda-Fernández (2024) <doi:10.48550/arXiv.2402.16693>.",
    "version": "1.0.0",
    "maintainer": "Santiago Pereda-Fernandez <santiagopereda@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13053,
    "package_name": "fbglm",
    "title": "Fractional Binomial Regression Model",
    "description": "Fit a fractional binomial regression model and\n extended zero-inflated negative binomial regression \n model to count data with excess zeros using maximum\n likelihood estimation.\n Compare zero-inflated regression models via Vuong \n closeness test. ",
    "version": "1.5.0",
    "maintainer": "Jeonghwa Lee <leejb@uncw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13054,
    "package_name": "fbi",
    "title": "Factor-Based Imputation and FRED-MD/QD Data Set",
    "description": "Factor-Based imputation of missing values in panel data and",
    "version": "0.7.0",
    "maintainer": "Yankang (Bennie) Chen <yankang.chen@yale.edu>",
    "url": "https://github.com/cykbennie/fbi",
    "exports": [],
    "topics": ["econometrics", "matrix-completion", "missing-data", "r", "r-packages", "statistics", "synthetic-control"],
    "score": "NA",
    "stars": 60
  },
  {
    "id": 13055,
    "package_name": "fbnet",
    "title": "Forensic Bayesian Networks",
    "description": "Open-source package for computing likelihood ratios in kinship testing and human identification cases. It has the core function of the software GENis, developed by Fundación Sadosky. It relies on a Bayesian Networks framework and is particularly well suited to efficiently perform large-size queries against databases of missing individuals.",
    "version": "1.0.4",
    "maintainer": "Franco Marsico <franco.lmarsico@gmail.com>",
    "url": "https://marsicofl.github.io/fbnet/,\nhttps://github.com/MarsicoFL/fbnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13065,
    "package_name": "fcirt",
    "title": "Forced Choice in Item Response Theory",
    "description": "Bayesian estimation of forced choice models in Item Response Theory using 'rstan' (See Stan Development Team (2020) <https://mc-stan.org/>).",
    "version": "0.2.1",
    "maintainer": "Naidan Tu <naidantu1031@gmail.com>",
    "url": "https://github.com/Naidantu/fcirt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13077,
    "package_name": "fdaACF",
    "title": "Autocorrelation Function for Functional Time Series",
    "description": "Quantify the serial correlation across lags of a given functional \n    time series using the autocorrelation function and a partial autocorrelation\n    function for functional time series proposed in \n    Mestre et al. (2021) <doi:10.1016/j.csda.2020.107108>.\n    The autocorrelation functions are based on the L2 norm of the lagged covariance \n    operators of the series. Functions are available for estimating the \n    distribution of the autocorrelation functions under the assumption \n    of strong functional white noise.",
    "version": "1.0.0",
    "maintainer": "Guillermo Mestre Marcos <guillermo.mestre@comillas.edu>",
    "url": "https://github.com/GMestreM/fdaACF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13078,
    "package_name": "fdaMixed",
    "title": "Functional Data Analysis in a Mixed Model Framework",
    "description": "Likelihood based analysis of 1-dimension functional data\n        in a mixed-effects model framework. Matrix computation are\n        approximated by semi-explicit operator equivalents with linear\n        computational complexity. Markussen (2013) <doi:10.3150/11-BEJ389>.",
    "version": "0.6.1",
    "maintainer": "Bo Markussen <bomar@math.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13083,
    "package_name": "fdaconcur",
    "title": "Concurrent Regression and History Index Models for Functional\nData",
    "description": "Provides an implementation of concurrent or varying coefficient regression methods for \n    functional data. The implementations are done for both dense and sparsely observed functional\n    data. Pointwise confidence bands can be constructed for each case. Further, the influence of\n    past predictor values are modeled by a smooth history index function, \n    while the effects on the response are described by smooth varying coefficient functions, \n    which are very useful in analyzing real data such as COVID data.\n    References: Yao, F., Müller, H.G., Wang, J.L. (2005) <doi:10.1214/009053605000000660>.\n                Sentürk, D., Müller, H.G. (2010) <doi:10.1198/jasa.2010.tm09228>.",
    "version": "0.1.3",
    "maintainer": "Su I Iao <siao@ucdavis.edu>",
    "url": "https://github.com/functionaldata/tFDAconcur",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13099,
    "package_name": "fdrtool",
    "title": "Estimation of (Local) False Discovery Rates and Higher Criticism",
    "description": "Estimates both tail area-based false \n   discovery rates (Fdr) as well as local false discovery rates (fdr) for a \n   variety of null models (p-values, z-scores, correlation coefficients,\n   t-scores).  The proportion of null values and the parameters of the null \n   distribution are adaptively estimated from the data.  In addition, the package \n   contains functions for non-parametric density estimation (Grenander estimator), \n   for monotone regression (isotonic regression and antitonic regression with weights),\n   for computing the greatest convex minorant (GCM) and the least concave majorant (LCM), \n   for the half-normal and correlation distributions, and for computing\n   empirical higher criticism (HC) scores and the corresponding decision threshold.",
    "version": "1.2.18",
    "maintainer": "Korbinian Strimmer <strimmerlab@gmail.com>",
    "url": "https://strimmerlab.github.io/software/fdrtool/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13102,
    "package_name": "feasts",
    "title": "Feature Extraction and Statistics for Time Series",
    "description": "Provides a collection of features, decomposition methods, \n    statistical summaries and graphics functions for the analysing tidy time\n    series data. The package name 'feasts' is an acronym comprising of its key\n    features: Feature Extraction And Statistics for Time Series.",
    "version": "0.4.2",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "url": "http://feasts.tidyverts.org/, https://github.com/tidyverts/feasts/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13121,
    "package_name": "feisr",
    "title": "Estimating Fixed Effects Individual Slope Models",
    "description": "Provides the function feis() to estimate fixed effects individual \n    slope (FEIS) models. The FEIS model constitutes a more general version of \n    the often-used fixed effects (FE) panel model, as implemented in the \n    package 'plm' by Croissant and Millo (2008) <doi:10.18637/jss.v027.i02>. \n    In FEIS models, data are not only person demeaned like in conventional \n    FE models, but detrended by the predicted individual slope of each \n    person or group. Estimation is performed by applying least squares lm() \n    to the transformed data. For more details on FEIS models see Bruederl and \n    Ludwig (2015, ISBN:1446252442); Frees (2001) <doi:10.2307/3316008>; \n    Polachek and Kim (1994) <doi:10.1016/0304-4076(94)90075-2>; \n\tRuettenauer and Ludwig (2020) <doi:10.1177/0049124120926211>;\n    Wooldridge (2010, ISBN:0262294354). To test consistency of conventional FE \n    and random effects estimators against heterogeneous slopes, the package \n    also provides the functions feistest() for an artificial regression test \n    and bsfeistest() for a bootstrapped version of the Hausman test.",
    "version": "1.3.0",
    "maintainer": "Tobias Ruettenauer <ruettenauer@sowi.uni-kl.de>",
    "url": "https://github.com/ruettenauer/feisr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13136,
    "package_name": "fetwfe",
    "title": "Fused Extended Two-Way Fixed Effects",
    "description": "Calculates the fused extended two-way fixed effects (FETWFE) estimator for unbiased and efficient estimation of difference-in-differences in panel data with staggered treatment adoption. This estimator eliminates bias inherent in conventional two-way fixed effects estimators, while also employing a novel bridge regression regularization approach to improve efficiency and yield valid standard errors. Also implements extended TWFE (etwfe) and bridge-penalized ETWFE (betwfe). Provides S3 classes for streamlined workflow and supports flexible tuning (ridge and rank-condition guarantees), automatic covariate centering/scaling, and detailed overall and cohort-specific effect estimates with valid standard errors. Includes simulation and formatting utilities, extensive diagnostic tools, vignettes, and examples. See Faletto (2025) (<doi:10.48550/arXiv.2312.05985>).",
    "version": "1.5.0",
    "maintainer": "Gregory Faletto <gfaletto@gmail.com>",
    "url": "https://github.com/gregfaletto/fetwfePackage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13145,
    "package_name": "fftab",
    "title": "Tidy Manipulation of Fourier Transformed Data",
    "description": "The 'fftab' package stores Fourier coefficients in a tibble and \n  allows their manipulation in various ways. Functions are available for converting \n  between complex, rectangular ('re', 'im'), and polar ('mod', 'arg') representations, \n  as well as for extracting components as vectors or matrices. Inputs can include \n  vectors, time series, and arrays of arbitrary dimensions, which are restored \n  to their original form when inverting the transform. Since 'fftab' stores Fourier \n  frequencies as columns in the tibble, many standard operations on spectral data \n  can be easily performed using tidy packages like 'dplyr'.",
    "version": "0.1.0",
    "maintainer": "Timothy Keitt <tkeitt@gmail.com>",
    "url": "https://github.com/thk686/fftab, https://thk686.github.io/fftab/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13156,
    "package_name": "fglsnet",
    "title": "A Feasible Generalized Least Squares Estimator for Regression\nAnalysis of Outcomes with Network Dependence",
    "description": "The function estimates a multivariate regression model for outcomes with network dependence.",
    "version": "1.1",
    "maintainer": "Weihua An <weihua.an@emory.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13162,
    "package_name": "fic",
    "title": "Focused Information Criteria for Model Comparison",
    "description": "Compares how well different models estimate a quantity of interest (the \"focus\") so that different models may be preferred for different purposes.  Comparisons within any class of models fitted by maximum likelihood are supported, with shortcuts for commonly-used classes such as generalised linear models and parametric survival models.  The methods originate from Claeskens and Hjort (2003) <doi:10.1198/016214503000000819> and Claeskens and Hjort (2008, ISBN:9780521852258).",
    "version": "1.0.1",
    "maintainer": "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>",
    "url": "https://github.com/chjackson/fic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13189,
    "package_name": "finalfit",
    "title": "Quickly Create Elegant Regression Results Tables and Plots when\nModelling",
    "description": "Generate regression results tables and plots in final \n    format for publication. Explore models and export directly to PDF \n    and 'Word' using 'RMarkdown'. ",
    "version": "1.1.0",
    "maintainer": "Ewen Harrison <ewen.harrison@ed.ac.uk>",
    "url": "https://github.com/ewenharrison/finalfit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13209,
    "package_name": "finmix",
    "title": "An R package for Bayesian estimation of finite mixture distributions",
    "description": "An R package for Bayesian estimation of finite mixture",
    "version": "0.1.0",
    "maintainer": "Lars Simon Zehnder <simon.zehnder@neway.ai>",
    "url": "https://github.com/simonsays1980/finmix",
    "exports": [],
    "topics": ["bayesian", "bayesian-inference", "finite-mixture-distributions", "r", "r-package", "statistics"],
    "score": "NA",
    "stars": 5
  },
  {
    "id": 13220,
    "package_name": "firebehavioR",
    "title": "Prediction of Wildland Fire Behavior and Hazard",
    "description": "Fire behavior prediction models, including the Scott & Reinhardt's (2001) Rothermel Wildland Fire Modelling System <DOI:10.2737/RMRS-RP-29> and Alexander et al.'s (2006) Crown Fire Initiation & Spread model <DOI:10.1016/j.foreco.2006.08.174>. Also contains sample datasets, estimation of fire behavior prediction model inputs (e.g., fuel moisture, canopy characteristics, wind adjustment factor), results visualization, and methods to estimate fire weather hazard.",
    "version": "0.1.2",
    "maintainer": "Justin Ziegler <Justin.Ziegler@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13228,
    "package_name": "fishMod",
    "title": "Fits Poisson-Sum-of-Gammas GLMs, Tweedie GLMs, and Delta\nLog-Normal Models",
    "description": "Fits models to catch and effort data. Single-species models are 1) delta log-normal, 2) Tweedie, or 3) Poisson-gamma (G)LMs.",
    "version": "0.29.2",
    "maintainer": "Scott D. Foster <scott.foster@csiro.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13241,
    "package_name": "fit.models",
    "title": "Compare Fitted Models",
    "description": "The fit.models function and its associated methods (coefficients, print,\n  summary, plot, etc.) were originally provided in the robust package to compare robustly\n  and classically fitted model objects. See chapters 2, 3, and 5 in Insightful (2002)\n  'Robust Library User's Guide' <http://robust.r-forge.r-project.org/Robust.pdf>). The aim\n  of the fit.models package is to separate this fitted model object comparison functionality\n  from the robust package and to extend it to support fitting methods (e.g., classical,\n  robust, Bayesian, regularized, etc.) more generally.",
    "version": "0.64",
    "maintainer": "Kjell Konis <kjellk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13247,
    "package_name": "fitPlotR",
    "title": "Plotting Probability Distributions",
    "description": "Provides functions for plotting probability density functions, distribution functions, survival functions, hazard functions and computing distribution moments. The implementation is inspired by Delignette-Muller and Dutang (2015) <doi:10.18637/jss.v064.i04>.",
    "version": "0.1.0",
    "maintainer": "Muhammad Osama <muhammadosama0846@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13252,
    "package_name": "fitdistcp",
    "title": "Distribution Fitting with Calibrating Priors for Commonly Used\nDistributions",
    "description": "Generates predictive distributions based on calibrating priors\n    for various commonly used statistical models, including models with \n    predictors. Routines for densities, probabilities, quantiles, random\n    deviates and the parameter posterior are provided. The predictions are\n    generated from the Bayesian prediction integral, with priors chosen to\n    give good reliability (also known as calibration). For homogeneous models,\n    the prior is set to the right Haar prior, giving predictions which are\n    exactly reliable. As a result, in repeated testing, the frequencies of\n    out-of-sample outcomes and the probabilities from the predictions agree.\n    For other models, the prior is chosen to give good reliability. Where\n    possible, the Bayesian prediction integral is solved exactly. Where exact\n    solutions are not possible, the Bayesian prediction integral is solved\n    using the Datta-Mukerjee-Ghosh-Sweeting (DMGS) asymptotic expansion.\n    Optionally, the prediction integral can also be solved using posterior\n    samples generated using Paul Northrop's ratio of uniforms sampling package\n    ('rust'). Results are also generated based on maximum likelihood, for\n    comparison purposes. Various model selection diagnostics and testing\n    routines are included. Based on \"Reducing reliability bias in assessments\n    of extreme weather risk using calibrating priors\", Jewson, S., Sweeting, T.\n    and Jewson, L. (2024); <doi:10.5194/ascmo-11-1-2025>.",
    "version": "0.2.3",
    "maintainer": "Stephen Jewson <stephen.jewson@gmail.com>",
    "url": "https://www.fitdistcp.info",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13253,
    "package_name": "fitdistrplus",
    "title": "Help to Fit of a Parametric Distribution to Non-Censored or\nCensored Data",
    "description": "Extends the fitdistr() function (of the MASS package) with several functions \n  to help the fit of a parametric distribution to non-censored or censored data. \n  Censored data may contain left censored, right censored and interval censored values, \n  with several lower and upper bounds. In addition to maximum likelihood estimation (MLE), \n  the package provides moment matching (MME), quantile matching (QME), maximum goodness-of-fit \n  estimation (MGE) and maximum spacing estimation (MSE) methods (available only for \n  non-censored data). Weighted versions of MLE, MME, QME and MSE are available. See e.g. \n  Casella & Berger (2002), Statistical inference, Pacific Grove, for a general introduction \n  to parametric estimation.",
    "version": "1.2-4",
    "maintainer": "Aurélie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "url": "https://lbbe-software.github.io/fitdistrplus/,\nhttps://lbbe.univ-lyon1.fr/fr/fitdistrplus,\nhttps://github.com/lbbe-software/fitdistrplus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13261,
    "package_name": "fivethirtyeight",
    "title": "Data and Code Behind the Stories and Interactives at 'FiveThirtyEight'",
    "description": "Datasets and code published by the data journalism website",
    "version": "0.6.2.9000",
    "maintainer": "Albert Y. Kim <albert.ys.kim@gmail.com>",
    "url": "https://github.com/rudeboybert/fivethirtyeight",
    "exports": [],
    "topics": ["cran", "data-science", "datajournalism", "fivethirtyeight", "r", "rpackage", "statistics"],
    "score": "NA",
    "stars": 455
  },
  {
    "id": 13265,
    "package_name": "fixest",
    "title": "Fast Fixed-Effects Estimations",
    "description": "Fast and user-friendly estimation of econometric models with multiple fixed-effects. Includes ordinary least squares (OLS), generalized linear models (GLM) and the negative binomial.\n    The core of the package is based on optimized parallel C++ code, scaling especially well for large data sets. The method to obtain the fixed-effects coefficients is based on Berge (2018) <https://github.com/lrberge/fixest/blob/master/_DOCS/FENmlm_paper.pdf>.\n    Further provides tools to export and view the results of several estimations with intuitive design to cluster the standard-errors.",
    "version": "0.13.2",
    "maintainer": "Laurent Berge <laurent.berge@u-bordeaux.fr>",
    "url": "https://lrberge.github.io/fixest/,\nhttps://github.com/lrberge/fixest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13270,
    "package_name": "fkbma",
    "title": "Free Knot-Bayesian Model Averaging",
    "description": "Analysis of Bayesian adaptive enrichment clinical trial using Free-Knot Bayesian Model Averaging (FK-BMA) method of Maleyeff et al. (2024) for Gaussian data. Maleyeff, L., Golchi, S., Moodie, E. E. M., & Hudson, M. (2024) \"An adaptive enrichment design using Bayesian model averaging for selection and threshold-identification of predictive variables\" <doi:10.1093/biomtc/ujae141>.",
    "version": "0.2.0",
    "maintainer": "Lara Maleyeff <lara.maleyeff@mcgill.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13278,
    "package_name": "flap",
    "title": "Forecast Linear Augmented Projection",
    "description": "The Forecast Linear Augmented Projection (flap) method reduces \n    forecast variance by adjusting the forecasts of multivariate time series to \n    be consistent with the forecasts of linear combinations (components) of the \n    series by projecting all forecasts onto the space where the linear \n    constraints are satisfied. The forecast variance can be reduced \n    monotonically by including more components. For a given number of \n    components, the flap method achieves maximum forecast variance reduction \n    among linear projections. ",
    "version": "0.2.0",
    "maintainer": "Yangzhuoran Fin Yang <yangyangzhuoran@gmail.com>",
    "url": "https://github.com/FinYang/flap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13291,
    "package_name": "flex",
    "title": "Fuzzy Linear Squares Estimation with Explicit Formula (FLEX)",
    "description": "The FLEX method, developed by Yoon and Choi (2013) <doi:10.1007/978-3-642-33042-1_21>, performs least squares estimation for fuzzy predictors and outcomes, generating crisp regression coefficients by minimizing the distance between observed and predicted outcomes. It also provides functions for fuzzifying data and inference tasks, including significance testing, fit indices, and confidence interval estimation.",
    "version": "0.1.0",
    "maintainer": "Chaewon Lee <chaewon.lee@unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13292,
    "package_name": "flexBCF",
    "title": "Fast & Flexible Implementation of Bayesian Causal Forests",
    "description": "A faster implementation of Bayesian Causal Forests (BCF; Hahn et al. (2020) <doi:10.1214/19-BA1195>), which uses regression tree ensembles to estimate the conditional average treatment effect of a binary treatment on a scalar output as a function of many covariates. This implementation avoids many redundant computations and memory allocations present in the original BCF implementation, allowing the model to be fit to larger datasets. The implementation was originally developed for the 2022 American Causal Inference Conference's Data Challenge. See Kokandakar et al. (2023) <doi:10.1353/obs.2023.0024> for more details.",
    "version": "1.0.2",
    "maintainer": "Sameer K. Deshpande <sameer.deshpande@wisc.edu>",
    "url": "https://github.com/skdeshpande91/flexBCF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13293,
    "package_name": "flexCWM",
    "title": "Flexible Cluster-Weighted Modeling",
    "description": "Allows maximum likelihood fitting of cluster-weighted models, a class of mixtures of regression models with random covariates. \n            Methods are described in Angelo Mazza, Antonio Punzo, Salvatore Ingrassia (2018) <doi:10.18637/jss.v086.i02>.",
    "version": "1.92",
    "maintainer": "Angelo Mazza <a.mazza@unict.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13303,
    "package_name": "flexmsm",
    "title": "A General Framework for Flexible Multi-State Survival Modelling",
    "description": "A general estimation framework for multi-state Markov processes with flexible specification of the transition intensities.\n    The log-transition intensities can be specified through Generalised Additive Models which allow for virtually any type of covariate\n    effect. Elementary specifications such as time-homogeneous processes and simple parametric forms are also supported. There are \n    no limitations on the type of process one can assume, with both forward and backward transitions allowed and virtually any number\n    of states.",
    "version": "0.1.2",
    "maintainer": "Alessia Eletti <alessia.eletti.19@ucl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13307,
    "package_name": "flexrsurv",
    "title": "Flexible Relative Survival Analysis",
    "description": "Package for parametric relative survival analyses. It allows to model non-linear and \n        non-proportional effects and both non proportional and non linear effects, using splines (B-spline and truncated power basis), Weighted Cumulative Index of Exposure effect, with correction model for \n        the life table. Both non proportional and non linear effects are described in \n\t\t\tRemontet, L. et al. (2007) <doi:10.1002/sim.2656> and \n\t\t\tMahboubi, A. et al. (2011) <doi:10.1002/sim.4208>. ",
    "version": "2.0.18",
    "maintainer": "Michel Grzebyk <michel.grzebyk@inrs.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13309,
    "package_name": "flexsurv",
    "title": "Flexible Parametric Survival and Multi-State Models",
    "description": "Flexible parametric models for time-to-event data,\n    including the Royston-Parmar spline model, generalized gamma and\n    generalized F distributions.  Any user-defined parametric\n    distribution can be fitted, given at least an R function defining\n    the probability density or hazard. There are also tools for\n    fitting and predicting from fully parametric multi-state models,\n    based on either cause-specific hazards or mixture models.",
    "version": "2.3.2",
    "maintainer": "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>",
    "url": "https://github.com/chjackson/flexsurv,\nhttp://chjackson.github.io/flexsurv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13325,
    "package_name": "flipscores",
    "title": "Robust Score Testing in GLMs, by Sign-Flip Contributions",
    "description": "Provides robust tests for testing in GLMs, by sign-flipping score contributions. The tests are robust against overdispersion, heteroscedasticity and, in some cases, ignored nuisance variables. See Hemerik, Goeman and Finos (2020) <doi:10.1111/rssb.12369>.",
    "version": "1.3.2",
    "maintainer": "Livio Finos <livio.finos@unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13371,
    "package_name": "fluoSurv",
    "title": "Estimate Insect Survival from Fluorescence Data",
    "description": "Use spectrophotometry measurements performed on insects as a way to infer pathogens \n      virulence. Insect movements cause fluctuations in fluorescence signal, and functions are \n      provided to estimate when the insect has died as the moment when variance in autofluorescence \n      signal drops to zero. The package provides functions to obtain this estimate together with \n      functions to import spectrophotometry data from a Biotek microplate reader. Details of the method \n      are given in Parthuisot et al. (2018) <doi:10.1101/297929>.",
    "version": "1.0.0",
    "maintainer": "Jean-Baptiste Ferdy <jean-baptiste.ferdy@univ-tlse3.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13379,
    "package_name": "fma",
    "title": "Data Sets from \"Forecasting: Methods and Applications\" by\nMakridakis, Wheelwright & Hyndman (1998)",
    "description": "All data sets from \"Forecasting: methods and applications\" by Makridakis, Wheelwright & Hyndman (Wiley, 3rd ed., 1998) <https://robjhyndman.com/forecasting/>.",
    "version": "2.5",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "url": "https://pkg.robjhyndman.com/fma/,\nhttps://github.com/robjhyndman/fma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13380,
    "package_name": "fmcmc",
    "title": "A friendly MCMC framework",
    "description": "Provides a friendly (flexible) Markov Chain Monte Carlo (MCMC)\n         framework for implementing Metropolis-Hastings algorithm in a modular way\n         allowing users to specify automatic convergence checker, personalized\n         transition kernels, and out-of-the-box multiple MCMC chains using\n         parallel computing. Most of the methods implemented in this package can\n         be found in Brooks et al. (2011, ISBN 9781420079425). Among the methods\n         included, we have: Haario (2001) <doi:10.1007/s11222-011-9269-5>\n         Adaptive Metropolis, Vihola (2012) <doi:10.1007/s11222-011-9269-5>\n         Robust Adaptive Metropolis, and Thawornwattana et\n         al. (2018) <doi:10.1214/17-BA1084> Mirror transition kernels.",
    "version": "0.5-2",
    "maintainer": "George Vega Yon <g.vegayon@gmail.com>",
    "url": "https://github.com/USCbiostats/fmcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13415,
    "package_name": "footBayes",
    "title": "Fitting Bayesian and MLE Football Models",
    "description": "This is the first package allowing for the estimation,\n             visualization and prediction of the most well-known \n             football models: double Poisson, bivariate Poisson,\n             Skellam, student_t, diagonal-inflated bivariate Poisson, and\n             zero-inflated Skellam. It supports both maximum likelihood estimation (MLE, for \n             'static' models only) and Bayesian inference.\n             For Bayesian methods, it incorporates several techniques:\n             MCMC sampling with Hamiltonian Monte Carlo, variational inference using\n             either the Pathfinder algorithm or Automatic Differentiation Variational\n             Inference (ADVI), and the Laplace approximation.\n             The package compiles all the 'CmdStan' models once during installation\n             using the 'instantiate' package.\n             The model construction relies on the most well-known football references, such as \n             Dixon and Coles (1997) <doi:10.1111/1467-9876.00065>,\n             Karlis and Ntzoufras (2003) <doi:10.1111/1467-9884.00366> and\n             Egidi, Pauli and Torelli (2018) <doi:10.1177/1471082X18798414>.",
    "version": "2.0.0",
    "maintainer": "Leonardo Egidi <legidi@units.it>",
    "url": "https://github.com/leoegidi/footbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13421,
    "package_name": "forceR",
    "title": "Force Measurement Analyses",
    "description": "For cleaning and analysis of graphs, such as animal closing force \n    measurements. \n    'forceR' was initially written and optimized to deal with insect bite force \n    measurements, but can be used for any time series. Includes a full workflow \n    to load, plot and crop data, correct amplifier and baseline drifts, \n    identify individual peak shapes (bites), rescale (normalize) peak curves, \n    and find best polynomial fits to describe and analyze force curve shapes.",
    "version": "1.0.20",
    "maintainer": "Peter T. Rühr <peter.ruehr@gmail.com>",
    "url": "https://github.com/Peter-T-Ruehr/forceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13426,
    "package_name": "forecTheta",
    "title": "Forecasting Time Series by Theta Models",
    "description": "Routines for forecasting univariate time series using Theta Models.",
    "version": "3.0",
    "maintainer": "Jose Augusto Fiorucci <jafiorucci@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13429,
    "package_name": "forecastHybrid",
    "title": "Convenient Functions for Ensemble Time Series Forecasts",
    "description": "Convenient functions for ensemble forecasts in R combining\n    approaches from the 'forecast' package. Forecasts generated from auto.arima(), ets(),\n    thetaf(), nnetar(), stlm(), tbats(), snaive() and arfima() can be combined with equal weights, weights\n    based on in-sample errors (introduced by Bates & Granger (1969) <doi:10.1057/jors.1969.103>),\n    or cross-validated weights. Cross validation for time series data with user-supplied models\n    and forecasting functions is also supported to evaluate model accuracy.",
    "version": "5.1.20",
    "maintainer": "David Shaub <davidshaub@alumni.harvard.edu>",
    "url": "https://gitlab.com/dashaub/forecastHybrid,\nhttps://github.com/ellisp/forecastHybrid",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13430,
    "package_name": "forecastLSW",
    "title": "Forecasting Routines for Locally Stationary Wavelet Processes",
    "description": "Implementation to perform forecasting of locally stationary wavelet processes by examining the local second order structure of the time series. ",
    "version": "1.1.1",
    "maintainer": "Rebecca Killick <r.killick@lancs.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13432,
    "package_name": "forecastSNSTS",
    "title": "Forecasting for Stationary and Non-Stationary Time Series",
    "description": "Methods to compute linear h-step ahead prediction coefficients based\n    on localised and iterated Yule-Walker estimates and empirical mean squared\n    and absolute prediction errors for the resulting predictors. Also, functions\n    to compute autocovariances for AR(p) processes, to simulate tvARMA(p,q) time\n    series, and to verify an assumption from Kley et al. (2019), Electronic of Statistics,\n    forthcoming. Preprint <arXiv:1611.04460>.",
    "version": "1.3-0",
    "maintainer": "Tobias Kley <tobias.kley@bristol.ac.uk>",
    "url": "http://github.com/tobiaskley/forecastSNSTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13434,
    "package_name": "forecasteR",
    "title": "Time Series Forecast System",
    "description": "A web application for displaying, analysing and forecasting univariate time series. Includes basic methods such as mean, naïve, seasonal naïve and drift, as well as more complex methods such as Holt-Winters Box,G and Jenkins, G (1976) <doi:10.1111/jtsa.12194> and ARIMA Brockwell, P.J. and R.A.Davis (1991) <doi:10.1007/978-1-4419-0320-4>.",
    "version": "3.0.2",
    "maintainer": "Oldemar Rodriguez <oldemar.rodriguez@ucr.ac.cr>",
    "url": "https://promidat.website, https://github.com/PROMiDAT/forecasteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13451,
    "package_name": "forestmodel",
    "title": "Forest Plots from Regression Models",
    "description": "Produces forest plots using 'ggplot2' from models produced by functions\n    such as stats::lm(), stats::glm() and survival::coxph().",
    "version": "0.6.2",
    "maintainer": "Nick Kennedy <r@nick-kennedy.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13470,
    "package_name": "forsearch",
    "title": "Diagnostic Analysis Using Forward Search Procedure for Various\nModels",
    "description": "Identifies potential data outliers and their impact on estimates and \n         analyses. Tool for evaluation of study credibility. Uses the forward \n         search approach of Atkinson and Riani, \"Robust Diagnostic Regression \n         Analysis\", 2000,<ISBN: o-387-95017-6> to prepare descriptive statistics \n         of a dataset that is to be analyzed by functions lm {stats}, glm {stats}, \n         nls {stats}, lme {nlme}, or coxph {survival}, or their equivalent in\n         another language.  Includes graphics functions to display the \n         descriptive statistics.",
    "version": "6.4.0",
    "maintainer": "William Fairweather <wrf343@flowervalleyconsulting.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13474,
    "package_name": "forward",
    "title": "Robust Analysis using Forward Search",
    "description": "Robust analysis using forward search in linear and generalized linear regression models, as described in Atkinson, A.C. and Riani, M. (2000), Robust Diagnostic Regression Analysis, First Edition. New York: Springer.",
    "version": "1.0.7",
    "maintainer": "Ken Beath <ken@kjbeath.id.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13490,
    "package_name": "fpop",
    "title": "Segmentation using Optimal Partitioning and Function Pruning",
    "description": "A dynamic programming algorithm for the fast segmentation of univariate signals into piecewise constant profiles.\n  The 'fpop'  package is a wrapper to a C++ implementation of the fpop (Functional Pruning Optimal Partioning) algorithm described in Maidstone et al. 2017\n  <doi:10.1007/s11222-016-9636-3>. The problem of detecting changepoints in an univariate sequence is formulated \n  in terms of minimising the mean squared error over segmentations. The fpop algorithm exactly minimizes the mean squared error \n  for a penalty linear in the number of changepoints.",
    "version": "2019.08.26",
    "maintainer": "Guillem Rigaill <guillem.rigaill@inra.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13502,
    "package_name": "fracARMA",
    "title": "Fractionally Integrated ARMA Model",
    "description": "Implements fractional differencing with Autoregressive Moving Average models to analyse long-memory \ttime series data. Traditional ARIMA models typically use integer values for differencing, which are \tsuitable for time series with short memory or anti-persistent behaviour. In contrast, the Fractional ARIMA \tmodel allows fractional differencing, enabling it to effectively capture long memory characteristics in \ttime series data. The ‘fracARMA’ package is user-friendly and allows users to manually input the \tfractional differencing parameter, which can be obtained using various estimators such as the GPH \testimator, Sperio method, or Wavelet method and many. Additionally, the package enables users to directly \tfeed the time series data, AR order, MA order, fractional differencing parameter, and the proportion of \ttraining data as a split ratio, all in a single command. The package is based on the reference from the \tpaper of Irshad and others (2024, <doi:10.22271/maths.2024.v9.i6b.1906>).",
    "version": "0.1.0",
    "maintainer": "Muhammed Irshad M <irshadmiitm@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13503,
    "package_name": "fracdiff",
    "title": "Fractionally Differenced ARIMA aka ARFIMA(P,d,q) Models",
    "description": "Maximum likelihood estimation of the parameters of a fractionally\n   differenced ARIMA(p,d,q) model (Haslett and Raftery, Appl.Statistics, 1989);\n   including inference and basic methods.  Some alternative algorithms to estimate \"H\".",
    "version": "1.5-3",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "url": "https://github.com/mmaechler/fracdiff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13504,
    "package_name": "fracdist",
    "title": "Numerical CDFs for Fractional Unit Root and Cointegration Tests",
    "description": "Calculate numerical asymptotic distribution functions of likelihood ratio \n    statistics for fractional unit root tests and tests of cointegration rank. \n    For these distributions, the included functions calculate critical values \n    and P-values used in unit root tests, cointegration tests, and rank tests \n    in the Fractionally Cointegrated Vector Autoregression (FCVAR) model.\n    The functions implement procedures for tests described in the following articles:\n    Johansen, S. and M. Ø. Nielsen (2012) <doi:10.3982/ECTA9299>,\n    MacKinnon, J. G. and M. Ø. Nielsen (2014) <doi:10.1002/jae.2295>.",
    "version": "0.1.1",
    "maintainer": "Lealand Morin <lealand.morin@ucf.edu>",
    "url": "https://github.com/LeeMorinUCF/fracdist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13507,
    "package_name": "fractaldim",
    "title": "Estimation of Fractal Dimensions",
    "description": "Implements various methods for estimating fractal dimension of time series and 2-dimensional data <doi:10.1214/11-STS370>.",
    "version": "0.8-5",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13512,
    "package_name": "frailtyEM",
    "title": "Fitting Frailty Models with the EM Algorithm",
    "description": "Contains functions for fitting shared frailty models with a semi-parametric\n    baseline hazard with the Expectation-Maximization algorithm. Supported data formats \n    include clustered failures with left truncation and recurrent events in gap-time\n    or Andersen-Gill format. Several frailty distributions, such as the the gamma, positive stable\n    and the Power Variance Family are supported. ",
    "version": "1.0.1",
    "maintainer": "Theodor Adrian Balan <hello@tbalan.com>",
    "url": "https://github.com/tbalan/frailtyEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13513,
    "package_name": "frailtyHL",
    "title": "Frailty Models via Hierarchical Likelihood",
    "description": "Implements the h-likelihood estimation procedures for general frailty models including competing-risk models and joint models.",
    "version": "2.3",
    "maintainer": "Maengseok Noh <msnoh@pknu.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13515,
    "package_name": "frailtyROC",
    "title": "Time-Dependent ROC Curve Estimation for Correlated\nRight-Censored Survival Data",
    "description": "This contains functions that can be used to estimate a smoothed and \n              a non-smoothed (empirical) time-dependent receiver operating characteristic\n              curve and the corresponding area under the receiver operating characteristic\n              curve for correlated right-censored time-to-event data. See\n              Beyene and Chen (2024) <doi:10.1177/09622802231220496>.",
    "version": "1.0.0",
    "maintainer": "Kassu Mehari Beyene <m2kassu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13517,
    "package_name": "frailtypack",
    "title": "Shared, Joint (Generalized) Frailty Models; Surrogate Endpoints",
    "description": "The following several classes of frailty models using a\n    penalized likelihood estimation on the hazard function but also a\n    parametric estimation can be fit using this R package: 1) A shared\n    frailty model (with gamma or log-normal frailty distribution) and Cox\n    proportional hazard model. Clustered and recurrent survival times can\n    be studied.  2) Additive frailty models for proportional hazard models\n    with two correlated random effects (intercept random effect with\n    random slope).  3) Nested frailty models for hierarchically clustered\n    data (with 2 levels of clustering) by including two iid gamma random\n    effects.  4) Joint frailty models in the context of the joint\n    modelling for recurrent events with terminal event for clustered data\n    or not. A joint frailty model for two semi-competing risks and\n    clustered data is also proposed.  5) Joint general frailty models in\n    the context of the joint modelling for recurrent events with terminal\n    event data with two independent frailty terms.  6) Joint Nested\n    frailty models in the context of the joint modelling for recurrent\n    events with terminal event, for hierarchically clustered data (with\n    two levels of clustering) by including two iid gamma random effects.\n    7) Multivariate joint frailty models for two types of recurrent events\n    and a terminal event.  8) Joint models for longitudinal data and a\n    terminal event.  9) Trivariate joint models for longitudinal data,\n    recurrent events and a terminal event.  10) Joint frailty models for\n    the validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time and/or longitudinal endpoints with the\n    possibility to use a mediation analysis model.  11) Conditional and\n    Marginal two-part joint models for longitudinal semicontinuous data\n    and a terminal event.  12) Joint frailty-copula models for the\n    validation of surrogate endpoints in multiple randomized clinical\n    trials with failure-time endpoints.  13) Generalized shared and joint\n    frailty models for recurrent and terminal events. Proportional hazards\n    (PH), additive hazard (AH), proportional odds (PO) and probit models\n    are available in a fully parametric framework. For PH and AH models,\n    it is possible to consider type-varying coefficients and flexible\n    semiparametric hazard function.  Prediction values are available (for\n    a terminal event or for a new recurrent event). Left-truncated (not\n    for Joint model), right-censored data, interval-censored data (only\n    for Cox proportional hazard and shared frailty model) and strata are\n    allowed. In each model, the random effects have the gamma or normal\n    distribution. Now, you can also consider time-varying covariates\n    effects in Cox, shared and joint frailty models (1-5). The package\n    includes concordance measures for Cox proportional hazards models and\n    for shared frailty models.  14) Competing Joint Frailty Model: A\n    single type of recurrent event and two terminal events.  15) functions \n    to compute power and sample size for four Gamma-frailty-based designs: \n    Shared Frailty Models, Nested Frailty Models, Joint Frailty Models, and \n    General Joint Frailty Models. Each design includes two primary functions: a \n    power function, which computes power given a specified sample size; \n    and a sample size function, which computes the required sample size to achieve \n    a specified power. 16) Weibull Illness-Death model with or without shared frailty\n    between transitions. Left-truncated and right-censored data are allowed. \n    17) Weibull Competing risks model with or without shared frailty between the \n    transitions. Left-truncated and right-censored data are allowed. Moreover, the package can be used with its shiny\n    application, in a local mode or by following the link below.",
    "version": "3.8.0",
    "maintainer": "Virginie Rondeau <virginie.rondeau@u-bordeaux.fr>",
    "url": "https://virginie1rondeau.wixsite.com/virginierondeau/software-frailtypack\nhttps://frailtypack-pkg.shinyapps.io/shiny_frailtypack/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13524,
    "package_name": "frechet",
    "title": "Statistical Analysis for Random Objects and Non-Euclidean Data",
    "description": "Provides implementation of statistical methods for random objects \n    lying in various metric spaces, which are not necessarily linear spaces. \n    The core of this package is Fréchet regression for random objects with \n    Euclidean predictors, which allows one to perform regression analysis \n    for non-Euclidean responses under some mild conditions. \n    Examples include distributions in 2-Wasserstein space, \n    covariance matrices endowed with power metric (with Frobenius metric \n    as a special case), Cholesky and log-Cholesky metrics, spherical data.  \n    References: Petersen, A., & Müller, H.-G. (2019) <doi:10.1214/17-AOS1624>.",
    "version": "0.3.0",
    "maintainer": "Yaqing Chen <yqchen@stat.rutgers.edu>",
    "url": "https://github.com/functionaldata/tFrechet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13525,
    "package_name": "fredr",
    "title": "An R Client for the 'FRED' API",
    "description": "An R client for the 'Federal Reserve Economic Data'\n    ('FRED') API <https://research.stlouisfed.org/docs/api/>.  Functions\n    to retrieve economic time series and other data from 'FRED'.",
    "version": "2.1.0",
    "maintainer": "Sam Boysel <sboysel@gmail.com>",
    "url": "https://github.com/sboysel/fredr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13540,
    "package_name": "freqdom",
    "title": "Frequency Domain Based Analysis: Dynamic PCA",
    "description": "Implementation of dynamic principal component\n    analysis (DPCA), simulation of VAR and VMA processes and frequency domain tools. \n    These frequency domain methods for dimensionality reduction of multivariate time series\n    were introduced by David Brillinger in his book Time Series (1974). We follow implementation\n    guidelines as described in Hormann, Kidzinski and Hallin (2016),\n    Dynamic Functional Principal Component <doi:10.1111/rssb.12076>.",
    "version": "2.0.5",
    "maintainer": "Kidzinski L. <lukasz.kidzinski@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13547,
    "package_name": "frequentistSSD",
    "title": "Screened Selection Design with Survival Endpoints",
    "description": "A study based on the screened selection design (SSD) is an exploratory phase II randomized trial with two or more arms but without concurrent control. The primary aim of the SSD trial is to pick a desirable treatment arm (e.g., in terms of the median survival time) to recommend to the subsequent randomized phase IIb (with the concurrent control) or phase III. Though The survival endpoint is often encountered in phase II trials, the existing SSD methods cannot deal with the survival endpoint. Furthermore, the existing SSD won’t control the type I error rate.  The proposed designs can “partially” control or provide the empirical type I error/false positive rate by an optimal algorithm (implemented by the optimal() function) for each arm. All the design needed components (sample size, operating characteristics) are supported.",
    "version": "0.1.1",
    "maintainer": "Chia-Wei Hsu <Chia-Wei.Hsu@stjude.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13570,
    "package_name": "fsdaR",
    "title": "Robust Data Analysis Through Monitoring and Dynamic\nVisualization",
    "description": "Provides interface to the 'MATLAB' toolbox 'Flexible Statistical Data Analysis\n    (FSDA)' which is comprehensive and computationally efficient\n    software package for robust statistics in regression, multivariate\n    and categorical data analysis. The current R version implements tools\n    for regression: (forward search, S- and MM-estimation, least trimmed\n    squares (LTS) and least median of squares (LMS)), for multivariate analysis\n    (forward search, S- and MM-estimation), for cluster analysis and cluster-wise regression.\n    The distinctive feature of our package is the possibility of\n    monitoring the statistics of interest as a function of breakdown point,\n    efficiency or subset size, depending on the estimator. This is\n    accompanied by a rich set of graphical features, such as dynamic\n    brushing, linking, particularly useful for exploratory data analysis.",
    "version": "0.9-0",
    "maintainer": "Valentin Todorov <valentin.todorov@chello.at>",
    "url": "https://github.com/UniprJRC/fsdaR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13586,
    "package_name": "ftsa",
    "title": "Functional Time Series Analysis",
    "description": "Functions for visualizing, modeling, forecasting and hypothesis testing of functional time series.",
    "version": "6.6",
    "maintainer": "Han Lin Shang <hanlin.shang@mq.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13587,
    "package_name": "ftsspec",
    "title": "Spectral Density Estimation and Comparison for Functional Time\nSeries",
    "description": "Functions for estimating spectral density operator of functional\n    time series (FTS) and comparing the spectral density operator of two\n    functional time series, in a way that allows detection of differences of\n    the spectral density operator in frequencies and along the curve length.",
    "version": "1.0.0",
    "maintainer": "Shahin Tavakoli <s.tavakoli@statslab.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13602,
    "package_name": "funGp",
    "title": "Gaussian Process Models for Scalar and Functional Inputs",
    "description": "Construction and smart selection of Gaussian process models\n\tfor analysis of computer experiments\n\twith emphasis on treatment of functional inputs that are regularly sampled. This package\n\toffers: (i) flexible modeling of functional-input regression\n\tproblems through the fairly general Gaussian process model; (ii)\n\tbuilt-in dimension reduction for functional inputs; (iii)\n\theuristic optimization of the structural parameters of the model\n\t(e.g., active inputs, kernel function, type of distance).\n\tAn in-depth tutorial in the use of funGp is provided in\n\tBetancourt et al. (2024) <doi:10.18637/jss.v109.i05> and\n\tMetamodeling background is provided in\n\tBetancourt et al. (2020) <doi:10.1016/j.ress.2020.106870>.\n\tThe algorithm for structural parameter optimization is described\n\tin <https://hal.science/hal-02532713>.",
    "version": "1.0.0",
    "maintainer": "Jose Betancourt <fungp.rpack@gmail.com>",
    "url": "https://djbetancourt-gh.github.io/funGp/,\nhttps://github.com/djbetancourt-gh/funGp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13608,
    "package_name": "funbootband",
    "title": "Simultaneous Prediction and Confidence Bands for Time Series\nData",
    "description": "Provides methods to compute simultaneous prediction and confidence\n    bands for dense time series data. The implementation builds on the\n    functional bootstrap approach proposed by Lenhoff et al. (1999)\n    <doi:10.1016/S0966-6362(98)00043-5> and extended by Koska et al. (2023)\n    <doi:10.1016/j.jbiomech.2023.111506> to support both independent and\n    clustered (hierarchical) data. Includes a simple API (see band()) and an\n    'Rcpp' backend for performance.",
    "version": "0.2.0",
    "maintainer": "Daniel Koska <dkoska@proton.me>",
    "url": "https://github.com/koda86/funbootband-cran",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13617,
    "package_name": "fungible",
    "title": "Psychometric Functions from the Waller Lab",
    "description": "Computes fungible coefficients and Monte Carlo data. Underlying theory for these functions is described in the following publications:\n    Waller, N. (2008). Fungible Weights in Multiple Regression. Psychometrika, 73(4), 691-703, <DOI:10.1007/s11336-008-9066-z>.\n    Waller, N. & Jones, J. (2009). Locating the Extrema of Fungible Regression Weights.\n    Psychometrika, 74(4), 589-602, <DOI:10.1007/s11336-008-9087-7>.\n    Waller, N. G. (2016). Fungible Correlation Matrices:\n    A Method for Generating Nonsingular, Singular, and Improper Correlation Matrices for\n    Monte Carlo Research. Multivariate Behavioral Research, 51(4), 554-568.\n    Jones, J. A. & Waller, N. G. (2015). The normal-theory and asymptotic distribution-free (ADF)\n    covariance matrix of standardized regression coefficients: theoretical extensions\n    and finite sample behavior. Psychometrika, 80, 365-378, <DOI:10.1007/s11336-013-9380-y>.\n    Waller, N. G.  (2018).  Direct Schmid-Leiman transformations and rank-deficient loadings matrices.  Psychometrika, 83, 858-870. <DOI:10.1007/s11336-017-9599-0>.",
    "version": "2.4.4.1",
    "maintainer": "Niels Waller <nwaller@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13624,
    "package_name": "funpca",
    "title": "Functional Principal Component Analysis",
    "description": "Functional principal component analysis under the Linear Mixed Models representation of smoothing splines. The method utilizes the Demmler-Reinsch basis and assumes error independence. For more details see: F. Rosales (2016) <https://ediss.uni-goettingen.de/handle/11858/00-1735-0000-0028-87F9-6>.",
    "version": "9.0",
    "maintainer": "Francisco Rosales <francisco.rosales-marticorena@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13628,
    "package_name": "funreg",
    "title": "Functional Regression for Irregularly Timed Data",
    "description": "Performs functional regression, and some related\n    approaches, for intensive longitudinal data (see the book by Walls & Schafer, \n    2006, Models for Intensive Longitudinal Data, Oxford) when such data is not\n    necessarily observed on an equally spaced grid of times.  The\n    approach generally follows the ideas of Goldsmith, Bobb, Crainiceanu,\n    Caffo, and Reich (2011)<DOI:10.1198/jcgs.2010.10007> and the approach taken in their sample code, but\n    with some modifications to make it more feasible to use with long rather\n    than wide, non-rectangular longitudinal datasets with unequal and\n    potentially random measurement times.  It also allows easy plotting of the\n    correlation between the smoothed covariate and the outcome as a function of\n    time, which can add additional insights on how to interpret a functional\n    regression.  Additionally, it also provides several permutation tests for\n    the significance of the functional predictor.  The heuristic interpretation\n    of ``time'' is used to describe the index of the functional predictor, but\n    the same methods can equally be used for another unidimensional continuous\n    index, such as space along a north-south axis.  Note that most of the functionality\n\tof this package has been superseded by added features after 2016 in the 'pfr' function by\n    Jonathan Gellar, Mathew W. McLean, Jeff Goldsmith, and Fabian Scheipl, in the \n\t'refund' package built by Jeff Goldsmith and co-authors and maintained by Julia Wrobel.\n\tThe development of the funreg package in 2015 and 2016 was part of a research \n\tproject supported by Award R03 CA171809-01 from the National Cancer Institute and Award P50 \n\tDA010075 from the National Institute on Drug Abuse. The content is solely the responsibility of the\n    authors and does not necessarily represent the official views of the\n    National Institute on Drug Abuse, the National Cancer Institute, or the\n    National Institutes of Health.",
    "version": "1.2.2",
    "maintainer": "John Dziak <dziakj1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13631,
    "package_name": "funtimes",
    "title": "Functions for Time Series Analysis",
    "description": "Nonparametric estimators and tests for time series analysis. The functions use bootstrap techniques and robust nonparametric difference-based estimators to test for the presence of possibly non-monotonic trends and for synchronicity of trends in multiple time series.",
    "version": "10.0",
    "maintainer": "Vyacheslav Lyubchich <lyubchich@umces.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13638,
    "package_name": "fusedTree",
    "title": "Fused Partitioned Regression for Clinical and Omics Data",
    "description": "Fit (generalized) linear regression models in each leaf node of a tree.\n    The tree is constructed using clinical variables only. The linear regression\n    models are constructed using (high-dimensional) omics variables only. The\n    leaf-node-specific regression models are estimated using the penalized likelihood\n    including a standard ridge (L2) penalty and a fusion penalty that links the\n    leaf-node-specific regression models to one another. The intercepts of the\n    leaf nodes reflect the effects of the clinical variables and are left\n    unpenalized. The tree, fitted with the clinical variables only,\n    should be constructed outside of the package with the 'rpart' 'R' package.\n    See Goedhart and others (2024) <doi:10.48550/arXiv.2411.02396> for details\n    on the method.",
    "version": "1.1.0",
    "maintainer": "Jeroen M. Goedhart <jeroengoed@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13640,
    "package_name": "fuser",
    "title": "Fused Lasso for High-Dimensional Regression over Groups",
    "description": "Enables high-dimensional penalized regression across heterogeneous \n    subgroups. Fusion penalties are used to share information about the linear \n    parameters across subgroups. The underlying model is described \n    in detail in Dondelinger and Mukherjee (2017) <arXiv:1611.00953>.",
    "version": "1.0.1",
    "maintainer": "Frank Dondelinger <fdondelinger.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13645,
    "package_name": "futility",
    "title": "Interim Analysis of Operational Futility in Randomized Trials\nwith Time-to-Event Endpoints and Fixed Follow-Up",
    "description": "Randomized clinical trials commonly follow participants for a time-to-event efficacy endpoint for a fixed period of time. Consequently, at the time when the last enrolled participant completes their follow-up, the number of observed endpoints is a random variable. Assuming data collected through an interim timepoint, simulation-based estimation and inferential procedures in the standard right-censored failure time analysis framework are conducted for the distribution of the number of endpoints--in total as well as by treatment arm--at the end of the follow-up period. The future (i.e., yet unobserved) enrollment, endpoint, and dropout times are generated according to mechanisms specified in the simTrial() function in the 'seqDesign' package. A Bayesian model for the endpoint rate, offering the option to specify a robust mixture prior distribution, is used for generating future data (see the vignette for details). Inference can be restricted to participants who received treatment according to the protocol and are observed to be at risk for the endpoint at a specified timepoint. Plotting functions are provided for graphical display of results.",
    "version": "0.4",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "url": "https://github.com/mjuraska/futility",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13660,
    "package_name": "fwb",
    "title": "Fractional Weighted Bootstrap",
    "description": "An implementation of the fractional weighted bootstrap to be used as a drop-in for functions in\n   the 'boot' package. The fractional weighted bootstrap (also known as the Bayesian bootstrap) involves drawing\n   weights randomly that are applied to the data rather than resampling units from the data. See Xu et al. (2020)\n   <doi:10.1080/00031305.2020.1731599> for details.",
    "version": "0.5.1",
    "maintainer": "Noah Greifer <noah.greifer@gmail.com>",
    "url": "https://ngreifer.github.io/fwb/, https://github.com/ngreifer/fwb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13665,
    "package_name": "fxregime",
    "title": "Exchange Rate Regime Analysis",
    "description": "Exchange rate regression and structural change tools\n             for estimating, testing, dating, and monitoring\n\t     (de facto) exchange rate regimes.",
    "version": "1.0-4",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13668,
    "package_name": "g.ridge",
    "title": "Generalized Ridge Regression for Linear Models",
    "description": "Ridge regression due to Hoerl and Kennard (1970)<DOI:10.1080/00401706.1970.10488634> and generalized ridge regression due to Yang and Emura (2017)<DOI:10.1080/03610918.2016.1193195> with optimized tuning parameters.\n These ridge regression estimators (the HK estimator and the YE estimator) are computed by minimizing the cross-validated mean squared errors.\n Both the ridge and generalized ridge estimators are applicable for high-dimensional regressors (p>n), where p is the number of regressors, and n is the sample size.",
    "version": "1.0",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13679,
    "package_name": "gFormulaMI",
    "title": "G-Formula for Causal Inference via Multiple Imputation",
    "description": "Implements the G-Formula method for causal inference with time-varying treatments and\n  confounders using Bayesian multiple imputation methods, as described by\n  Bartlett et al (2025) <doi:10.1177/09622802251316971>. It creates multiple synthetic imputed\n  datasets under treatment regimes of interest using the 'mice' package. These can then be analysed\n  using rules developed for analysing multiple synthetic datasets.",
    "version": "1.0.3",
    "maintainer": "Jonathan Bartlett <jonathan.bartlett1@lshtm.ac.uk>",
    "url": "https://jwb133.github.io/gFormulaMI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13690,
    "package_name": "gRain",
    "title": "Bayesian Networks",
    "description": "Probability propagation in Bayesian networks, also known as graphical independence networks. Documentation\n    of the package is provided in vignettes included in the package and in\n    the paper by Højsgaard (2012, <doi:10.18637/jss.v046.i10>).\n    See 'citation(\"gRain\")' for details. ",
    "version": "1.4.5",
    "maintainer": "Søren Højsgaard <sorenh@math.aau.dk>",
    "url": "https://people.math.aau.dk/~sorenh/software/gR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13691,
    "package_name": "gRaven",
    "title": "Bayes Nets: 'RHugin' Emulation with 'gRain'",
    "description": "Wrappers for functions in the 'gRain' package to emulate some 'RHugin' \n  functionality, allowing the building of Bayesian networks consisting on discrete \n  chance nodes incrementally, through   adding nodes, edges and conditional probability \n  tables, the setting of evidence,   both 'hard' (boolean) or 'soft' (likelihoods), \n  querying marginal probabilities   and normalizing constants, and generating sets of\n  high-probability configurations. Computations will typically not be so fast as they are\n  with 'RHugin', but this package should assist users without access to 'Hugin' to use\n  code written to use 'RHugin'.",
    "version": "1.1.10",
    "maintainer": "Peter Green <P.J.Green@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13700,
    "package_name": "gWQS",
    "title": "Generalized Weighted Quantile Sum Regression",
    "description": "Fits Weighted Quantile Sum (WQS) regression  (Carrico et al. (2014) <doi:10.1007/s13253-014-0180-3>), a random subset implementation of WQS (Curtin et al. (2019) <doi:10.1080/03610918.2019.1577971>), a repeated holdout validation WQS (Tanner et al. (2019) <doi:10.1016/j.mex.2019.11.008>) and a WQS with 2 indices (Renzetti et al. (2023) <doi:10.3389/fpubh.2023.1289579>) for continuous, binomial, multinomial, Poisson, quasi-Poisson and negative binomial outcomes.",
    "version": "3.0.5",
    "maintainer": "Stefano Renzetti <stefano.renzetti88@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13710,
    "package_name": "gaiah",
    "title": "Genetic and Isotopic Assignment Accounting for Habitat\nSuitability",
    "description": "Tools for using genetic markers, stable isotope data, and habitat\n    suitability data to calculate posterior probabilities of breeding origin of\n    migrating birds.",
    "version": "0.0.5",
    "maintainer": "Eric C. Anderson <eric.anderson@noaa.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13714,
    "package_name": "galamm",
    "title": "Generalized Additive Latent and Mixed Models",
    "description": "Estimates generalized additive latent and\n    mixed models using maximum marginal likelihood, \n    as defined in Sorensen et al. (2023) \n    <doi:10.1007/s11336-023-09910-z>, which is an extension of Rabe-Hesketh and\n    Skrondal (2004)'s unifying framework for multilevel latent variable \n    modeling <doi:10.1007/BF02295939>. Efficient computation is done using sparse \n    matrix methods, Laplace approximation, and automatic differentiation. The \n    framework includes generalized multilevel models with heteroscedastic \n    residuals, mixed response types, factor loadings, smoothing splines, \n    crossed random effects, and combinations thereof. Syntax for model \n    formulation is close to 'lme4' (Bates et al. (2015) \n    <doi:10.18637/jss.v067.i01>) and 'PLmixed' (Rockwood and Jeon (2019) \n    <doi:10.1080/00273171.2018.1516541>).",
    "version": "0.4.0",
    "maintainer": "Øystein Sørensen <oystein.sorensen@psykologi.uio.no>",
    "url": "https://github.com/LCBC-UiO/galamm,\nhttps://lcbc-uio.github.io/galamm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13719,
    "package_name": "galts",
    "title": "Genetic Algorithms and C-Steps Based LTS (Least Trimmed Squares)\nEstimation",
    "description": "Includes the ga.lts() function that estimates\n        LTS (Least Trimmed Squares) parameters using genetic algorithms\n        and C-steps. ga.lts() constructs a genetic algorithm to form a\n        basic subset and iterates C-steps as defined in Rousseeuw and\n        van-Driessen (2006) to calculate the cost value of the LTS\n        criterion. OLS (Ordinary Least Squares) regression is known to\n        be sensitive to outliers. A single outlying observation can\n        change the values of estimated parameters. LTS is a resistant\n        estimator even the number of outliers is up to half of the\n        data. This package is for estimating the LTS parameters with\n        lower bias and variance in a reasonable time. Version >=1.3 \n        includes the function medmad for fast outlier detection in\n        linear regression.",
    "version": "1.3.2",
    "maintainer": "Mehmet Hakan Satman <mhsatman@istanbul.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13731,
    "package_name": "gamlr",
    "title": "Gamma Lasso Regression",
    "description": "The gamma lasso algorithm provides regularization paths corresponding to a range of non-convex cost functions between L0 and L1 norms.  As much as possible, usage for this package is analogous to that for the glmnet package (which does the same thing for penalization between L1 and L2 norms).  For details see: Taddy (2017 JCGS), 'One-Step Estimator Paths for Concave Regularization', <arXiv:1308.5623>.",
    "version": "1.13-8",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "url": "https://github.com/TaddyLab/gamlr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13732,
    "package_name": "gamlss",
    "title": "Generalized Additive Models for Location Scale and Shape",
    "description": "Functions for fitting the Generalized Additive Models for Location Scale and Shape introduced by Rigby and Stasinopoulos (2005), <doi:10.1111/j.1467-9876.2005.00510.x>. The models use a distributional regression approach where all the parameters of the conditional distribution of the response variable are modelled using explanatory variables.",
    "version": "5.5-0",
    "maintainer": "Mikis Stasinopoulos <d.stasinopoulos@gre.ac.uk>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13734,
    "package_name": "gamlss.cens",
    "title": "Fitting an Interval Response Variable Using `gamlss.family'\nDistributions",
    "description": "This is an add-on package to GAMLSS. The purpose of this\n        package is to allow users to fit interval response variables in\n        GAMLSS models. The main function gen.cens() generates a\n        censored version of an existing GAMLSS family distribution.",
    "version": "5.0-7",
    "maintainer": "Mikis Stasinopoulos <d.stasinopoulos@gre.ac.uk>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13735,
    "package_name": "gamlss.data",
    "title": "Data for Generalized Additive Models for Location Scale and\nShape",
    "description": "Data used as examples in the books on Generalized Additive Models for Location Scale and Shape:\n  Stasinopoulos, Rigby, Heller, Voudouris, De Bastiani (2017). Flexible Regression and Smoothing: Using GAMLSS in R, <doi:10.1201/b21973>.\n  Rigby, Stasinopoulos, Heller, De Bastiani (2019). Distributions for Modeling Location, Scale, and Shape Using GAMLSS in R, <doi:10.1201/9780429298547>.\n  Stasinopoulos, Kneib, Klein, Mayr, Heller (2024). Generalized Additive Models for Location, Scale and Shape: A Distributional Regression Approach, with Applications, <doi:10.1017/9781009410076>.",
    "version": "6.0-7",
    "maintainer": "Mikis Stasinopoulos <d.stasinopoulos@gre.ac.uk>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13738,
    "package_name": "gamlss.foreach",
    "title": "Parallel Computations for Distributional Regression",
    "description": "Computational intensive calculations for Generalized Additive Models for Location Scale and Shape, <doi:10.1111/j.1467-9876.2005.00510.x>. ",
    "version": "1.1-6",
    "maintainer": "Mikis Stasinopoulos <d.stasinopoulos@londonmet.ac.uk>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13740,
    "package_name": "gamlss.lasso",
    "title": "Extra Lasso-Type Additive Terms for GAMLSS",
    "description": "Interface for extra high-dimensional smooth functions for Generalized Additive Models for Location Scale and Shape (GAMLSS) including (adaptive) lasso, ridge, elastic net and least angle regression.",
    "version": "1.0-1",
    "maintainer": "Florian Ziel <florian.ziel@uni-due.de>",
    "url": "https://www.gamlss.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13745,
    "package_name": "gamm4",
    "title": "Generalized Additive Mixed Models using 'mgcv' and 'lme4'",
    "description": "Estimate generalized additive mixed models via a version of\n         function gamm() from 'mgcv', using 'lme4' for estimation.",
    "version": "0.2-7",
    "maintainer": "Simon Wood <simon.wood@r-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13749,
    "package_name": "gammi",
    "title": "Generalized Additive Mixed Model Interface",
    "description": "An interface for fitting generalized additive models (GAMs) and generalized additive mixed models (GAMMs) using the 'lme4' package as the computational engine, as described in Helwig (2024) <doi:10.3390/stats7010003>. Supports default and formula methods for model specification, additive and tensor product splines for capturing nonlinear effects, and automatic determination of spline type based on the class of each predictor. Includes an S3 plot method for visualizing the (nonlinear) model terms, an S3 predict method for forming predictions from a fit model, and an S3 summary method for conducting significance testing using the Bayesian interpretation of a smoothing spline.",
    "version": "0.2",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13750,
    "package_name": "gamreg",
    "title": "Robust and Sparse Regression via Gamma-Divergence",
    "description": "Robust regression via gamma-divergence with L1, elastic net and ridge.",
    "version": "0.3",
    "maintainer": "Takayuki Kawashima <t-kawa@ism.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13752,
    "package_name": "gamselBayes",
    "title": "Bayesian Generalized Additive Model Selection",
    "description": "Generalized additive model selection via approximate Bayesian inference is provided. Bayesian mixed model-based penalized splines with spike-and-slab-type coefficient prior distributions are used to facilitate fitting and selection. The approximate Bayesian inference engine options are: (1) Markov chain Monte Carlo and (2) mean field variational Bayes. Markov chain Monte Carlo has better Bayesian inferential accuracy, but requires a longer run-time. Mean field variational Bayes is faster, but less accurate. The methodology is described in He and Wand (2024) <doi:10.1007/s10182-023-00490-y>. ",
    "version": "2.0-3",
    "maintainer": "Matt P. Wand <matt.wand@uts.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13768,
    "package_name": "garma",
    "title": "Fitting and Forecasting Gegenbauer ARMA Time Series Models",
    "description": "Methods for estimating univariate long memory-seasonal/cyclical\n             Gegenbauer time series processes. See for example (2022) <doi:10.1007/s00362-022-01290-3>.\n             Refer to the vignette for details of fitting these processes.",
    "version": "0.9.24",
    "maintainer": "Richard Hunt <maint@huntemail.id.au>",
    "url": "https://github.com/rlph50/garma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13771,
    "package_name": "gaselect",
    "title": "Genetic Algorithm (GA) for Variable Selection from\nHigh-Dimensional Data",
    "description": "Provides a genetic algorithm for finding variable\n    subsets in high dimensional data with high prediction performance. The\n    genetic algorithm can use ordinary least squares (OLS) regression models or\n    partial least squares (PLS) regression models to evaluate the prediction\n    power of variable subsets. By supporting different cross-validation\n    schemes, the user can fine-tune the tradeoff between speed and quality of\n    the solution.",
    "version": "1.0.25",
    "maintainer": "David Kepplinger <david.kepplinger@gmail.com>",
    "url": "https://github.com/dakep/gaselect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13775,
    "package_name": "gastempt",
    "title": "Analyzing Gastric Emptying from MRI or Scintigraphy",
    "description": "Fits gastric emptying time series from MRI or 'scintigraphic' measurements\n   using nonlinear mixed-model population fits with 'nlme' and Bayesian methods with \n   Stan; computes derived parameters such as t50 and AUC.",
    "version": "0.7.0",
    "maintainer": "Dieter Menne <dieter.menne@menne-biomed.de>",
    "url": "https://github.com/dmenne/gastempt,\nhttp://dmenne.github.io/gastempt/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13776,
    "package_name": "gaston",
    "title": "Genetic Data Handling (QC, GRM, LD, PCA) & Linear Mixed Models",
    "description": "Manipulation of genetic data (SNPs). Computation of GRM and dominance matrix, LD, heritability with efficient algorithms for linear mixed model (AIREML). Dandine et al <doi:10.1159/000488519>.",
    "version": "1.6",
    "maintainer": "Hervé Perdry <herve.perdry@universite-paris-saclay.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13784,
    "package_name": "gausscov",
    "title": "The Gaussian Covariate Method for Variable Selection",
    "description": "The standard linear regression theory whether frequentist or Bayesian is based on an 'assumed (revealed?) truth' (John Tukey) attitude to models. This is reflected in the language of statistical inference which involves a concept of truth, for example confidence intervals, hypothesis testing and consistency. The motivation behind this package was to remove the word true from the theory and practice of linear regression and to replace it by approximation. The approximations considered are the least squares approximations. An approximation is called valid if it contains no irrelevant covariates. This is operationalized using the concept of a Gaussian P-value which is the probability that pure Gaussian noise is better in term of least squares than the covariate. The precise definition given in the paper  \"An Approximation Based Theory of Linear Regression\".  Only four simple equations are required. Moreover the Gaussian P-values can be simply derived from standard F P-values. Furthermore they are exact and valid whatever the data in contrast F P-values are only valid for specially designed simulations. A valid approximation is one where all the Gaussian P-values are less than a threshold p0 specified by the statistician, in this package with the default value 0.01. This approximations approach is not only much simpler it is overwhelmingly better than the standard model based approach. The will be demonstrated using high dimensional regression and vector autoregression real data sets. The goal is to find valid approximations. The search function is f1st which is a greedy forward selection procedure which results in either just one or no approximations which may however not be valid. If the size is less than than a threshold with default value 21 then an all subset procedure is called which returns the best valid subset. A good default start is f1st(y,x,kmn=15) The best function for returning multiple approximations is f3st which repeatedly calls f1st. For more information see the papers: L. Davies and L. Duembgen, \"Covariate Selection Based on a Model-free Approach to Linear Regression with Exact Probabilities\", <doi:10.48550/arXiv.2202.01553>, L. Davies, \"An Approximation Based Theory of Linear Regression\", 2024, <doi:10.48550/arXiv.2402.09858>.",
    "version": "1.1.8",
    "maintainer": "Laurie Davies <pldavies44@cantab.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13792,
    "package_name": "gbeta",
    "title": "Generalized Beta and Beta Prime Distributions",
    "description": "Density, distribution function, quantile function, and random generation for the generalized Beta and Beta prime distributions. The family of generalized Beta distributions is conjugate for the Bayesian binomial model, and the generalized Beta prime distribution is the posterior distribution of the relative risk in the Bayesian 'two Poisson samples' model when a Gamma prior is assigned to the Poisson rate of the reference group and a Beta prime prior is assigned to the relative risk. References: Laurent (2012) <doi:10.1214/11-BJPS139>, Hamza & Vallois (2016) <doi:10.1016/j.spl.2016.03.014>, Chen & Novick (1984) <doi:10.3102/10769986009002163>.",
    "version": "0.1.0",
    "maintainer": "Stéphane Laurent <laurent_step@outlook.fr>",
    "url": "https://github.com/stla/gbeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13795,
    "package_name": "gbm",
    "title": "Generalized Boosted Regression Models",
    "description": "An implementation of extensions to Freund and Schapire's AdaBoost \n  algorithm and Friedman's gradient boosting machine. Includes regression \n  methods for least squares, absolute loss, t-distribution loss, quantile \n  regression, logistic, multinomial logistic, Poisson, Cox proportional hazards \n  partial likelihood, AdaBoost exponential loss, Huberized hinge loss, and \n  Learning to Rank measures (LambdaMart). Originally developed by Greg Ridgeway.\n  Newer version available at github.com/gbm-developers/gbm3.",
    "version": "2.2.2",
    "maintainer": "Greg Ridgeway <gridge@upenn.edu>",
    "url": "https://github.com/gbm-developers/gbm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13808,
    "package_name": "gcerisk",
    "title": "Generalized Competing Event Model",
    "description": "Generalized competing event model based on Cox PH model and Fine-Gray model.\n    This function is designed to develop optimized risk-stratification methods for competing\n    risks data, such as described in:\n    1. Carmona R, Gulaya S, Murphy JD, Rose BS, Wu J, Noticewala S,McHale MT, Yashar CM, Vaida F,\n    and Mell LK (2014) <DOI:10.1016/j.ijrobp.2014.03.047>.\n    2. Carmona R, Zakeri K, Green G, Hwang L, Gulaya S, Xu B, Verma R, Williamson CW, Triplett DP, Rose\n    BS, Shen H, Vaida F, Murphy JD, and Mell LK (2016) <DOI:10.1200/JCO.2015.65.0739>.\n    3. Lunn, Mary, and Don McNeil (1995) <DOI:10.2307/2532940>.",
    "version": "19.05.24",
    "maintainer": "Hanjie Shen <shenhanjie0418@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13814,
    "package_name": "gcmr",
    "title": "Gaussian Copula Marginal Regression",
    "description": "Likelihood inference in Gaussian copula marginal\n        regression models.",
    "version": "1.0.4",
    "maintainer": "Cristiano Varin <cristiano.varin@unive.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13819,
    "package_name": "gctsc",
    "title": "Modeling Count Time Series Data via Gaussian Copula Models",
    "description": "Gaussian copula models for count time series. Includes simulation utilities, likelihood approximation, maximum-likelihood estimation, residual diagnostics, and predictive inference. Implements the Time Series Minimax Exponential Tilting (TMET) method, an adaptation of Minimax Exponential Tilting (Botev, 2017) <doi:10.1111/rssb.12162> and the Vecchia-based tilting framework of Cao and Katzfuss (2025) <doi:10.1080/01621459.2025.2546586>. Also provides a linear-cost implementation of the Geweke–Hajivassiliou–Keane (GHK) simulator inspired by Masarotto and Varin (2012) <doi:10.1214/12-EJS721>, and the Continuous Extension (CE) approximation of Nguyen and De Oliveira (2025) <doi:10.1080/02664763.2025.2498502>. The package follows the S3 structure of 'gcmr', but all code in 'gctsc' was developed independently.",
    "version": "0.1.3",
    "maintainer": "Quynh Nguyen <nqnhu2209@gmail.com>",
    "url": "https://github.com/QNNHU/gctsc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13832,
    "package_name": "gdpc",
    "title": "Generalized Dynamic Principal Components",
    "description": "Functions to compute the Generalized Dynamic Principal Components\n    introduced in Peña and Yohai (2016) <DOI:10.1080/01621459.2015.1072542>. The implementation\n    includes an automatic procedure proposed in Peña, Smucler and Yohai (2020) <DOI:10.18637/jss.v092.c02>\n    for the identification of both the number of lags to be used\n    in the generalized dynamic principal components as well as the number of components required\n    for a given reconstruction accuracy.",
    "version": "1.1.4",
    "maintainer": "Ezequiel Smucler <ezequiels.90@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13846,
    "package_name": "geeVerse",
    "title": "A Comprehensive Analysis of High Dimensional Longitudinal Data",
    "description": "To provide a comprehensive analysis of high dimensional longitudinal\n    data,this package provides analysis for any combination of 1) simultaneous\n    variable selection and estimation, 2) mean regression or quantile regression\n    for heterogeneous data, 3) cross-sectional or longitudinal data, 4) balanced\n    or imbalanced data, 5) moderate, high or even ultra-high dimensional data,\n    via computationally efficient implementations of penalized generalized\n    estimating equations.",
    "version": "0.3.1",
    "maintainer": "Tianhai Zu <zuti@mail.uc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13848,
    "package_name": "geecure",
    "title": "Marginal Proportional Hazards Mixture Cure Models with\nGeneralized Estimating Equations",
    "description": "Features the marginal parametric and semi-parametric proportional hazards mixture cure models for analyzing clustered survival data with a possible cure fraction. A reference is Yi Niu and Yingwei Peng (2014) <doi:10.1016/j.jmva.2013.09.003>.",
    "version": "1.0-6",
    "maintainer": "Yi Niu <niuyi@dlut.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13864,
    "package_name": "gems",
    "title": "Generalized Multistate Simulation Model",
    "description": "Simulate and analyze multistate models with general hazard\n    functions. gems provides functionality for the preparation of hazard functions\n    and parameters, simulation from a general multistate model and predicting future\n    events. The multistate model is not required to be a Markov model and may take\n    the history of previous events into account. In the basic version, it allows\n    to simulate from transition-specific hazard function, whose parameters are\n    multivariable normally distributed.",
    "version": "1.1.1",
    "maintainer": "Luisa Salazar Vizcaya <luisapaola.salazarvizcaya@insel.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13865,
    "package_name": "gemtc",
    "title": "Network Meta-Analysis Using Bayesian Methods",
    "description": "Network meta-analyses (mixed treatment comparisons) in the Bayesian\n    framework using JAGS. Includes methods to assess heterogeneity and\n    inconsistency, and a number of standard visualizations.\n    van Valkenhoef et al. (2012) <doi:10.1002/jrsm.1054>;\n    van Valkenhoef et al. (2015) <doi:10.1002/jrsm.1167>.",
    "version": "1.1-0",
    "maintainer": "Gert van Valkenhoef <gert@gertvv.nl>",
    "url": "https://github.com/gertvv/gemtc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13873,
    "package_name": "genMCMCDiag",
    "title": "Generalized Convergence Diagnostics for Difficult MCMC\nAlgorithms",
    "description": "Trace plots and convergence diagnostics for Markov Chain Monte Carlo (MCMC) algorithms on highly multivariate or unordered spaces. Methods outlined in a forthcoming paper.",
    "version": "0.2.3",
    "maintainer": "Luke Duttweiler <lduttweiler@hsph.harvard.edu>",
    "url": "https://github.com/LukeDuttweiler/genMCMCDiag",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13875,
    "package_name": "genSurv",
    "title": "Generating Multi-State Survival Data",
    "description": "Generation of survival data with one (binary)\n  time-dependent covariate.  Generation of survival data arising\n  from a progressive illness-death model.",
    "version": "1.0.4",
    "maintainer": "Artur Araujo <artur.stat@gmail.com>",
    "url": "https://github.com/arturstat/genSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13903,
    "package_name": "generalCorr",
    "title": "Generalized Correlations, Causal Paths and Portfolio Selection",
    "description": "Function gmcmtx0() computes a more reliable (general) \n    correlation matrix. Since causal paths from data are important for all sciences, the\n    package provides many sophisticated functions. causeSummBlk() and causeSum2Blk()\n    give easy-to-interpret causal paths.  Let Z denote control variables and compare \n    two flipped kernel regressions: X=f(Y, Z)+e1 and Y=g(X, Z)+e2. Our criterion Cr1 \n    says that if |e1*Y|>|e2*X| then variation in X is more \"exogenous or independent\"\n    than in Y, and the causal path is X to Y. Criterion Cr2 requires |e2|<|e1|. These\n    inequalities between many absolute values are quantified by four orders of \n    stochastic dominance. Our third criterion Cr3, for the causal path X to Y,\n    requires new generalized partial correlations to satisfy |r*(x|y,z)|< |r*(y|x,z)|.\n    The function parcorVec() reports generalized partials between the first\n    variable and all others.  The package provides several R functions including\n    get0outliers() for outlier detection, bigfp() for numerical integration by the\n    trapezoidal rule, stochdom2() for stochastic dominance, pillar3D() for 3D charts,\n    canonRho() for generalized canonical correlations, depMeas() measures nonlinear\n    dependence, and causeSummary(mtx) reports summary of causal paths among matrix \n    columns. Portfolio selection: decileVote(), momentVote(), dif4mtx(), exactSdMtx()\n    can rank several stocks. Functions whose names begin with 'boot' provide bootstrap\n    statistical inference, including a new bootGcRsq() test for \"Granger-causality\" \n    allowing nonlinear relations. A new tool for evaluation of out-of-sample\n    portfolio performance is outOFsamp(). Panel data implementation is now included.\n    See eight vignettes of the package for theory, examples, and\n    usage tips. See Vinod (2019) \\doi{10.1080/03610918.2015.1122048}.",
    "version": "1.2.6",
    "maintainer": "H. D. Vinod <vinod@fordham.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13912,
    "package_name": "geneticae",
    "title": "Statistical Tools for the Analysis of Multi Environment\nAgronomic Trials",
    "description": "Data from multi environment agronomic trials, which are often \n  carried out by plant breeders, can be analyzed with the tools offered by this \n  package such as the Additive Main effects and Multiplicative Interaction model \n  or 'AMMI' ('Gauch' 1992, ISBN:9780444892409) \n  and the Site Regression model or 'SREG' ('Cornelius' 1996, \n  <doi:10.1201/9780367802226>). Since these methods present a poor performance \n  under the presence of outliers and missing values, this package includes \n  robust versions of the 'AMMI' model ('Rodrigues' 2016, \n  <doi:10.1093/bioinformatics/btv533>), and also imputation techniques \n  specifically developed for this kind of data ('Arciniegas-Alarcón' 2014, \n  <doi:10.2478/bile-2014-0006>).",
    "version": "0.4.0",
    "maintainer": "Julia Angelini <jangelini_93@hotmail.com>",
    "url": "https://jangelini.github.io/geneticae/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13915,
    "package_name": "genfrn",
    "title": "Generating Triangular and Trapezoidal Fuzzy Random Numbers via\nUniform Distribution",
    "description": "Triangular and trapezoidal fuzzy numbers are used to study fuzzy logic,\n    fuzzy reasoning and approximating, fuzzy regression models, etc.\n    This package builds the generating function for triangular and trapezoidal fuzzy numbers based on Souliotis et al. (2022)<doi:10.3390/math10183350>. \n    They proposed a method for the construction of fuzzy numbers via a cumulative distribution function based on the possibility theory.  ",
    "version": "0.1.4",
    "maintainer": "Atchanut Rattanalertnusorn <atchanut_r@rmutt.ac.th>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13936,
    "package_name": "genridge",
    "title": "Generalized Ridge Trace Plots for Ridge Regression",
    "description": "The genridge package introduces generalizations of the standard univariate\n  ridge trace plot used in ridge regression and related methods.  These graphical methods\n  show both bias (actually, shrinkage) and precision, by plotting the covariance ellipsoids of the estimated\n  coefficients, rather than just the estimates themselves.  2D and 3D plotting methods are provided,\n  both in the space of the predictor variables and in the transformed space of the PCA/SVD of the\n  predictors.  ",
    "version": "0.8.0",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "url": "https://github.com/friendly/genridge,\nhttps://friendly.github.io/genridge/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13938,
    "package_name": "genset",
    "title": "Generates Multiple Data Sets",
    "description": "Generate multiple data sets for educational purposes to demonstrate the importance of multiple regression. The genset function generates a data set from an initial data set to have the same summary statistics (mean, median, and standard deviation) but opposing regression results.",
    "version": "0.1.1",
    "maintainer": "Lori Murray <lori.murray@uwo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13940,
    "package_name": "genstab",
    "title": "Resampling Based Yield Stability Analyses",
    "description": "Several yield stability analyses are mentioned in this package: variation and regression based yield stability analyses. Resampling techniques are integrated with these stability analyses. The function stab.mean() provides the genotypic means and ranks including their corresponding confidence intervals. The function stab.var() provides the genotypic variances over environments including their corresponding confidence intervals. The function stab.fw() is an extended method from the Finlay-Wilkinson method (1963). This method can include several other factors that might impact yield stability. Resampling technique is integrated into this method. A few missing data points or unbalanced data are allowed too. The function stab.fw.check() is an extended method from the Finlay-Wilkinson method (1963). The yield stability is evaluated via common check line(s). Resampling technique is integrated.",
    "version": "1.0.0",
    "maintainer": "Jixiang Wu <jixiang.wu@sdstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13944,
    "package_name": "geoBayes",
    "title": "Analysis of Geostatistical Data using Bayes and Empirical Bayes\nMethods",
    "description": "Functions to fit geostatistical data. The data can be\n        continuous, binary or count data and the models implemented are\n        flexible. Conjugate priors are assumed on some parameters while\n        inference on the other parameters can be done through a full\n        Bayesian analysis of by empirical Bayes methods.",
    "version": "0.7.4",
    "maintainer": "Evangelos Evangelou <e.evangelou@maths.bath.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 13988,
    "package_name": "geommc",
    "title": "Geometric Markov Chain Sampling",
    "description": "Simulates from discrete and continuous target distributions \n        using geometric Metropolis-Hastings (MH) \n        algorithms. Users specify the target distribution \n        by an R function that evaluates the log un-normalized pdf or pmf. The package \n        also contains a function implementing a specific geometric MH algorithm for \n        performing high dimensional Bayesian variable selection. ",
    "version": "0.1.1",
    "maintainer": "Vivekananda Roy <vroy@iastate.edu>",
    "url": "https://github.com/vroys/geommc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14026,
    "package_name": "gerbil",
    "title": "Generalized Efficient Regression-Based Imputation with Latent\nProcesses",
    "description": "Implements a new multiple imputation method that draws\n    imputations from a latent joint multivariate normal model which\n    underpins generally structured data. This model is constructed using a\n    sequence of flexible conditional linear models that enables the\n    resulting procedure to be efficiently implemented on high dimensional\n    datasets in practice. See Robbins (2021) <arXiv:2008.02243>.",
    "version": "0.1.9",
    "maintainer": "Michael Robbins <mrobbins@rand.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14036,
    "package_name": "gestate",
    "title": "Generalised Survival Trial Assessment Tool Environment",
    "description": "Provides tools to assist planning and monitoring of time-to-event trials under complicated censoring assumptions and/or non-proportional hazards. There are three main components: The first is analytic calculation of predicted time-to-event trial properties, providing estimates of expected hazard ratio, event numbers and power under different analysis methods. The second is simulation, allowing stochastic estimation of these same properties. Thirdly, it provides parametric event prediction using blinded trial data, including creation of prediction intervals. Methods are based upon numerical integration and a flexible object-orientated structure for defining event, censoring and recruitment distributions (Curves).",
    "version": "1.6.0",
    "maintainer": "James Bell <james.bell.ext@boehringer-ingelheim.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14048,
    "package_name": "gets",
    "title": "General-to-Specific (GETS) Modelling and Indicator Saturation\nMethods",
    "description": "Automated General-to-Specific (GETS) modelling of the mean and variance of a regression, and indicator saturation methods for detecting and testing for structural breaks in the mean, see Pretis, Reade and Sucarrat (2018) <doi:10.18637/jss.v086.i03> for an overview of the package. In advanced use, the estimator and diagnostics tests can be fully user-specified, see Sucarrat (2021) <doi:10.32614/RJ-2021-024>.",
    "version": "0.38",
    "maintainer": "Genaro Sucarrat <genaro.sucarrat@bi.no>",
    "url": "https://CRAN.R-project.org/package=gets,\nhttp://www.sucarrat.net/R/gets/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14059,
    "package_name": "gfiUltra",
    "title": "Generalized Fiducial Inference for Ultrahigh-Dimensional\nRegression",
    "description": "Variable selection for ultrahigh-dimensional (\"large p small n\") linear Gaussian models using a fiducial framework allowing to draw inference on the parameters. Reference: Lai, Hannig & Lee (2015) <doi:10.1080/01621459.2014.931237>.",
    "version": "1.0.0",
    "maintainer": "Stéphane Laurent <laurent_step@outlook.fr>",
    "url": "https://github.com/stla/gfiUltra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14060,
    "package_name": "gfoRmula",
    "title": "Parametric G-Formula",
    "description": "Implements the non-iterative conditional expectation (NICE) \n    algorithm of the g-formula algorithm (Robins (1986) \n    <doi:10.1016/0270-0255(86)90088-6>, Hernán and Robins (2024, ISBN:9781420076165)). \n    The g-formula can estimate an outcome's counterfactual mean or risk under \n    hypothetical treatment strategies (interventions) when there is sufficient \n    information on time-varying treatments and confounders. \n    This package can be used for discrete or continuous time-varying treatments \n    and for failure time outcomes or continuous/binary end of follow-up \n    outcomes. The package can handle a random measurement/visit process and a \n    priori knowledge of the data structure, as well as censoring (e.g., by loss \n    to follow-up) and two options for handling competing events for failure time\n    outcomes. Interventions can be flexibly specified, both as interventions on \n    a single treatment or as joint interventions on multiple treatments.\n    See McGrath et al. (2020) <doi:10.1016/j.patter.2020.100008> for a guide on \n    how to use the package.",
    "version": "1.1.1",
    "maintainer": "Sean McGrath <sean.mcgrath514@gmail.com>",
    "url": "https://github.com/CausalInference/gfoRmula,\nhttps://doi.org/10.1016/j.patter.2020.100008",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14061,
    "package_name": "gfoRmulaICE",
    "title": "Parametric Iterative Conditional Expectation G-Formula",
    "description": "Implements iterative conditional expectation (ICE) estimators of the plug-in g-formula (Wen, Young, Robins, and Hernán (2020) <doi: 10.1111/biom.13321>). \n    Both singly robust and doubly robust ICE estimators based on parametric models are available. \n    The package can be used to estimate survival curves under sustained treatment strategies (interventions) using longitudinal data with time-varying treatments, time-varying confounders, censoring, and competing events.\n    The interventions can be static or dynamic, and deterministic or stochastic (including threshold interventions). Both prespecified and user-defined interventions are available.",
    "version": "1.1.1",
    "maintainer": "Jing Li <jing_li@hsph.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14074,
    "package_name": "ggDoE",
    "title": "Modern Graphs for Design of Experiments with 'ggplot2'",
    "description": "Generate commonly used plots in the field of design of experiments using 'ggplot2'.\n  'ggDoE' currently supports the following plots: alias matrix, box cox transformation, boxplots, lambda plot,\n  regression diagnostic plots, half normal plots, main and interaction effect plots for factorial designs,\n  contour plots for response surface methodology, Pareto plot, and two dimensional projections of a latin hypercube design.",
    "version": "0.8",
    "maintainer": "Jose Toledo Luna <toledo60@protonmail.com>",
    "url": "https://ggdoe.netlify.app",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14087,
    "package_name": "ggTimeSeries",
    "title": "Time Series Visualisations Using the Grammar of Graphics",
    "description": "Provides additional display mediums for time series visualisations.",
    "version": "1.0.2",
    "maintainer": "Aditya Kothari <mail.thecomeonman@gmail.com>",
    "url": "https://github.com/thecomeonman/ggTimeSeries",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14137,
    "package_name": "ggdmc",
    "title": "Cognitive Models",
    "description": "Hierarchical Bayesian models. The package provides tools to fit two response time models, using the population-based Markov Chain Monte Carlo. ",
    "version": "0.2.6.2",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "url": "https://github.com/yxlin/ggdmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14138,
    "package_name": "ggdmcHeaders",
    "title": "'C++' Headers for 'ggdmc' Package",
    "description": "A fast 'C++' implementation of the design-based, Diffusion Decision Model (DDM) and the Linear Ballistic Accumulation (LBA) model. It enables the user to optimise the choice response time model by connecting with the Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampler implemented in the 'ggdmc' package. The package fuses the hierarchical modelling, Bayesian inference, choice response time models and factorial designs, allowing users to build their own design-based models. For more information on the underlying models, see the works by Voss, Rothermund, and Voss (2004) <doi:10.3758/BF03196893>, Ratcliff and McKoon (2008) <doi:10.1162/neco.2008.12-06-420>, and Brown and Heathcote (2008) <doi:10.1016/j.cogpsych.2007.12.002>.",
    "version": "0.2.9.1",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "url": "https://github.com/yxlin/ggdmcHeaders",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14140,
    "package_name": "ggdmcModel",
    "title": "Model Builders for 'ggdmc' Package",
    "description": "A suite of tools for specifying and examining experimental designs related to choice response time models (e.g., the Diffusion Decision Model). This package allows users to define how experimental factors influence one or more model parameters using R-style formula syntax, while also checking the logical consistency of these associations. Additionally, it integrates with the 'ggdmc' package, which employs Differential Evolution Markov Chain Monte Carlo (DE-MCMC) sampling to optimise model parameters. For further details on the model-building approach, see Heathcote, Lin, Reynolds, Strickland, Gretton, and Matzke (2019) <doi:10.3758/s13428-018-1067-y>.",
    "version": "0.2.9.0",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "url": "https://github.com/yxlin/ggdmcModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14141,
    "package_name": "ggdmcPrior",
    "title": "Prior Probability Functions of the Standard and Truncated\nDistribution",
    "description": "Provides tools for specifying and evaluating standard and truncated probability distributions, with support for log-space computation and joint distribution specification. It enables Bayesian computation for cognition models and includes utilities for density calculation, sampling, and visualisation, facilitating prior distribution specification and model assessment in hierarchical Bayesian frameworks.",
    "version": "0.2.9.0",
    "maintainer": "Yi-Shin Lin <yishinlin001@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14186,
    "package_name": "ggisotonic",
    "title": "'ggplot2' Friendly Isotonic or Monotonic Regression Curves",
    "description": "Provides stat_isotonic() to add weighted univariate isotonic regression curves.",
    "version": "0.1.2",
    "maintainer": "Komala Sheshachala Srikanth <sri.teach@gmail.com>",
    "url": "https://github.com/talegari/ggisotonic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14206,
    "package_name": "ggmr",
    "title": "Generalized Gauss Markov Regression",
    "description": "Implements the generalized Gauss Markov regression, this is useful when both predictor and response have uncertainty attached to them and also when covariance within the predictor, within the response and between the predictor and the response is present. Base on the results published in guide ISO/TS 28037 (2010) <https://www.iso.org/standard/44473.html>. ",
    "version": "0.1.1",
    "maintainer": "Hugo Gasca-Aragon <hugo_gasca_aragon@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14240,
    "package_name": "ggpmisc",
    "title": "Miscellaneous Extensions to 'ggplot2'",
    "description": "Extensions to 'ggplot2' respecting the grammar of graphics\n    paradigm. Statistics: locate and tag peaks and valleys; label plot with the\n    equation of a fitted polynomial or other types of models including major\n    axis, quantile and robust and resistant regression. Labels for P-value, \n    R^2 or adjusted R^2 or information criteria for fitted models; parametric\n    and non-parametric correlation; label with ANOVA table for fitted models; \n    label with summary table for fitted models; annotations for multiple \n    comparisons with adjusted P-values. Model fit classes for which suitable \n    methods are provided by package 'broom' and 'broom.mixed' are supported as \n    well as user-defined wrappers on model fit functions. Scales and stats to \n    build volcano and quadrant plots based on\n    outcomes, fold changes, p-values and false discovery rates.",
    "version": "0.6.3",
    "maintainer": "Pedro J. Aphalo <pedro.aphalo@helsinki.fi>",
    "url": "https://docs.r4photobiology.info/ggpmisc/,\nhttps://github.com/aphalo/ggpmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14267,
    "package_name": "ggrisk",
    "title": "Risk Score Plot for Cox Regression",
    "description": "The risk plot may be one of the most commonly used figures in \n    tumor genetic data analysis. We can conclude the following two points: \n    Comparing the prediction results of the model with the real survival situation \n    to see whether the survival rate of the high-risk group is lower than that of the \n    low-level group, and whether the survival time of the high-risk group is \n    shorter than that of the low-risk group. The other is to compare the heat \n    map and scatter plot to see the correlation between the predictors and the \n    outcome.",
    "version": "1.3",
    "maintainer": "Jing Zhang <zj391120@163.com>",
    "url": "https://github.com/yikeshu0611/ggrisk",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14286,
    "package_name": "ggsmc",
    "title": "Visualising Output from Sequential Monte Carlo and\nEnsemble-Based Methods",
    "description": "Functions for plotting, and animating, the output of importance samplers, sequential Monte Carlo samplers (SMC) and ensemble-based methods. The package can be used to plot and animate histograms, densities, scatter plots and time series, and to plot the genealogy of an SMC or ensemble-based algorithm. These functions all rely on algorithm output to be supplied in tidy format. A function is provided to transform algorithm output from matrix format (one Monte Carlo point per row) to the tidy format required by the plotting and animating functions.",
    "version": "0.2.0",
    "maintainer": "Richard G Everitt <richard.g.everitt@gmail.com>",
    "url": "https://github.com/richardgeveritt/ggsmc,\nhttps://richardgeveritt.github.io/ggsmc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14298,
    "package_name": "ggstatsplot",
    "title": "'ggplot2' Based Plots with Statistical Details",
    "description": "Extension of 'ggplot2', 'ggstatsplot' creates graphics with\n    details from statistical tests included in the plots themselves. It\n    provides an easier syntax to generate information-rich plots for\n    statistical analysis of continuous (violin plots, scatterplots,\n    histograms, dot plots, dot-and-whisker plots) or categorical (pie and\n    bar charts) data. Currently, it supports the most common types of\n    statistical approaches and tests: parametric, nonparametric, robust,\n    and Bayesian versions of t-test/ANOVA, correlation analyses,\n    contingency table analysis, meta-analysis, and regression analyses.\n    References: Patil (2021) <doi:10.21105/joss.03236>.",
    "version": "0.13.4",
    "maintainer": "Indrajeet Patil <patilindrajeet.science@gmail.com>",
    "url": "https://indrajeetpatil.github.io/ggstatsplot/,\nhttps://github.com/IndrajeetPatil/ggstatsplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14315,
    "package_name": "ggtime",
    "title": "Grammar of Graphics and Plot Helpers for Time Series\nVisualization",
    "description": "Extends the capabilities of 'ggplot2' by providing grammatical\n    elements and plot helpers designed for visualizing temporal patterns. The \n    package implements a grammar of temporal graphics, which leverages calendar\n    structures to highlight changes over time. The package also provides plot \n    helper functions to quickly produce commonly used time series graphics,\n    including time plots, season plots, and seasonal sub-series plots.",
    "version": "0.1.0",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14323,
    "package_name": "ggtrendline",
    "title": "Add Trendline and Confidence Interval to 'ggplot'",
    "description": "Add trendline and confidence interval of linear or nonlinear regression model and\n    show equation to 'ggplot' as simple as possible. For a general overview of the methods used in\n\t  this package, see Ritz and Streibig (2008) <doi:10.1007/978-0-387-09616-2> and \n\t  Greenwell and Schubert Kabban (2014) <doi:10.32614/RJ-2014-009>.",
    "version": "1.0.3",
    "maintainer": "Weiping Mei <meiweipingg@163.com>",
    "url": "https://github.com/PhDMeiwp/ggtrendline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14341,
    "package_name": "ghcm",
    "title": "Functional Conditional Independence Testing with the GHCM",
    "description": "A statistical hypothesis test for conditional independence.\n    Given residuals from a sufficiently powerful regression, it tests whether \n    the covariance of the residuals is vanishing. It can be applied to both\n    discretely-observed functional data and multivariate data. \n    Details of the method can be found in Anton Rask Lundborg, Rajen D. Shah and Jonas\n    Peters (2022) <doi:10.1111/rssb.12544>.",
    "version": "3.0.1",
    "maintainer": "Anton Rask Lundborg <arl@math.ku.dk>",
    "url": "https://github.com/arlundborg/ghcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14355,
    "package_name": "gigg",
    "title": "Group Inverse-Gamma Gamma Shrinkage for Sparse Regression with\nGrouping Structure",
    "description": "A Gibbs sampler corresponding to a Group \n    Inverse-Gamma Gamma (GIGG) regression model with adjustment covariates. \n    Hyperparameters in the GIGG prior specification can either be fixed by the \n    user or can be estimated via Marginal Maximum Likelihood Estimation.\n    Jonathan Boss, Jyotishka Datta, Xin Wang, Sung Kyun Park, Jian Kang, Bhramar \n    Mukherjee (2021) <arXiv:2102.10670>.",
    "version": "0.2.1",
    "maintainer": "Michael Kleinsasser <mkleinsa@umich.edu>",
    "url": "https://github.com/umich-cphds/gigg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14357,
    "package_name": "gilmour",
    "title": "The Interpretation of Adjusted Cp Statistic",
    "description": "Several methods may be found for selecting a subset of regressors from a set of k candidate variables in multiple linear regression.\n        One possibility is to evaluate all possible regression models and comparing them using Mallows's Cp statistic (Cp) according to Gilmour original study.\n        Full model is calculated, all possible combinations of regressors are generated, adjusted Cp for each submodel are computed, and the submodel with the minimum adjusted value Cp (ModelMin) is calculated.\n        To identify the final model, the package applies a sequence of hypothesis tests on submodels nested within ModelMin, following the approach outlined in Gilmour's original paper.\n        For more details see the help of the function final_model() and the original study (1996) <doi:10.2307/2348411>.",
    "version": "0.1.1",
    "maintainer": "Josef Dolejs <josef.dolejs@uhk.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14359,
    "package_name": "gimme",
    "title": "Group Iterative Multiple Model Estimation",
    "description": "Data-driven approach for arriving at person-specific time series models. The method first identifies which relations replicate across the majority of individuals to detect signal from noise. These group-level relations are then used as a foundation for starting the search for person-specific (or individual-level) relations. See Gates & Molenaar (2012) <doi:10.1016/j.neuroimage.2012.06.026>. ",
    "version": "0.9.3",
    "maintainer": "Kathleen M Gates <gateskm@email.unc.edu>",
    "url": "https://github.com/GatesLab/gimme/,\nhttps://tarheels.live/gimme/tutorials/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14391,
    "package_name": "gkwreg",
    "title": "Generalized Kumaraswamy Regression Models for Bounded Data",
    "description": "\n    Implements regression models for bounded continuous data in the open interval (0,1) \n    using the five-parameter Generalized 'Kumaraswamy' distribution. Supports modeling \n    all distribution parameters (alpha, beta, gamma, delta, lambda) as functions of \n    predictors through various link functions. Provides efficient maximum likelihood \n    estimation via Template Model Builder ('TMB'), offering comprehensive diagnostics,\n    model comparison tools, and simulation methods. Particularly useful for analyzing \n    proportions, rates, indices, and other bounded response data with complex \n    distributional features not adequately captured by simpler models.",
    "version": "2.1.14",
    "maintainer": "José Evandeilton Lopes <evandeilton@gmail.com>",
    "url": "https://github.com/evandeilton/gkwreg,\nhttps://evandeilton.github.io/gkwreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14392,
    "package_name": "glam",
    "title": "Generalized Additive and Linear Models (GLAM)",
    "description": "Contains methods for fitting Generalized Linear Models (GLMs) \n  and Generalized Additive Models (GAMs). Generalized regression models are\n  common methods for handling data for which assuming Gaussian-distributed\n  errors is not appropriate. For instance, if the response of interest is\n  binary, count, or proportion data, one can instead model the expectation of\n  the response based on an appropriate data-generating distribution.\n  This package provides methods for fitting GLMs and GAMs under\n  Beta regression, Poisson regression, Gamma regression, and Binomial regression\n  (currently GLM only) settings. Models are fit using local scoring algorithms\n  described in Hastie and Tibshirani (1990) <doi:10.1214/ss/1177013604>.",
    "version": "1.0.2",
    "maintainer": "Andrew Cooper <ahcooper@vt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14393,
    "package_name": "glarma",
    "title": "Generalized Linear Autoregressive Moving Average Models",
    "description": "Functions are provided for estimation, testing, diagnostic checking and forecasting of generalized linear autoregressive moving average (GLARMA) models for discrete valued time series with regression variables.  These are a class of observation driven non-linear non-Gaussian state space models. The state vector consists of a linear regression component plus an observation driven component consisting of an autoregressive-moving average (ARMA) filter of past predictive residuals. Currently three distributions (Poisson, negative binomial and binomial) can be used for the response series. Three options (Pearson, score-type and unscaled) for the residuals in the observation driven component are available. Estimation is via maximum likelihood (conditional on initializing values for the ARMA process) optimized using Fisher scoring or Newton Raphson iterative methods. Likelihood ratio and Wald tests for the observation driven component allow testing for serial dependence in generalized linear model settings. Graphical diagnostics including model fits, autocorrelation functions and probability integral transform residuals are included in the package. Several standard data sets are included in the package.",
    "version": "1.7-1",
    "maintainer": "William T.M. Dunsmuir <w.dunsmuir@unsw.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14401,
    "package_name": "gldrm",
    "title": "Generalized Linear Density Ratio Models",
    "description": "Fits a generalized linear density ratio model (GLDRM).\n    A GLDRM is a semiparametric generalized linear model.\n    In contrast to a GLM, which assumes a particular exponential family distribution, \n    the GLDRM uses a semiparametric likelihood to estimate the reference distribution. \n    The reference distribution may be any discrete, continuous, or mixed exponential \n    family distribution. The model parameters, which include both the regression \n    coefficients and the cdf of the unspecified reference distribution, are estimated \n    by maximizing a semiparametric likelihood. Regression coefficients are estimated \n    with no loss of efficiency, i.e. the asymptotic variance is the same as if the \n    true exponential family distribution were known.\n    Huang (2014) <doi:10.1080/01621459.2013.824892>.\n    Huang and Rathouz (2012) <doi:10.1093/biomet/asr075>.\n    Rathouz and Gao (2008) <doi:10.1093/biostatistics/kxn030>.",
    "version": "1.6",
    "maintainer": "Michael Wurm <wurm@uwalumni.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14407,
    "package_name": "glm.deploy",
    "title": "'C' and 'Java' Source Code Generator for Fitted Glm Objects",
    "description": "Provides two functions that generate source code implementing the predict \n  function of fitted glm objects. In this version, code can be generated for either 'C' \n  or 'Java'. The idea is to provide a tool for the easy and fast deployment of glm \n  predictive models into production. The source code generated by this package implements \n  two function/methods. One of such functions implements the equivalent to \n  predict(type=\"response\"), while the second implements predict(type=\"link\"). Source code \n  is written to disk as a .c or .java file in the specified path. In the case of c, an .h \n  file is also generated.",
    "version": "1.0.4",
    "maintainer": "Oscar Castro-Lopez <castroloj@gmail.com>",
    "url": "https://github.com/oscarcastrolopez/glm.deploy",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14409,
    "package_name": "glm2",
    "title": "Fitting Generalized Linear Models",
    "description": "Fits generalized linear models using the same model specification as glm in the stats package, but with a modified default fitting method that provides greater stability for models that may fail to converge using glm.",
    "version": "1.2.1",
    "maintainer": "Mark W. Donoghoe <markdonoghoe@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14415,
    "package_name": "glme",
    "title": "Generalized Linear Mixed Effects Models",
    "description": "Provides Generalized Inferences based on exact distributions and exact probability statements for mixed effect models, provided by such papers as Weerahandi and Yu (2020) <doi:10.1186/s40488-020-00105-w> under the widely used Compound Symmetric Covariance structure. The package returns the estimation of the coefficients in random and fixed part of the mixed models by generalized inference.",
    "version": "0.1.0",
    "maintainer": "Mustafa Cavus <mustafacavus@eskisehir.edu.tr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14417,
    "package_name": "glmfitmiss",
    "title": "Fitting GLMs with Missing Data in Both Responses and Covariates",
    "description": "Fits generalized linear models (GLMs) when there is missing data in both the response and categorical covariates. The functions implement likelihood-based methods using the Expectation and Maximization (EM) algorithm and optionally apply Firth’s bias correction for improved inference. See Pradhan, Nychka, and Bandyopadhyay (2025) <https:>, Maiti and Pradhan (2009) <doi:10.1111/j.1541-0420.2008.01186.x>, Maity, Pradhan, and Das (2019) <doi:10.1080/00031305.2017.1407359> for further methodological details.",
    "version": "2.1.0",
    "maintainer": "Vivek Pradhan <vpradhan2009@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14419,
    "package_name": "glmm",
    "title": "Generalized Linear Mixed Models via Monte Carlo Likelihood\nApproximation",
    "description": "Approximates the likelihood of a generalized linear mixed model using Monte Carlo likelihood approximation. Then maximizes the likelihood approximation to return maximum likelihood estimates, observed Fisher information, and other model information.",
    "version": "1.4.5",
    "maintainer": "Christina Knudson <drchristinaknudson@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14421,
    "package_name": "glmmEP",
    "title": "Generalized Linear Mixed Model Analysis via Expectation\nPropagation",
    "description": "Approximate frequentist inference for generalized linear mixed model analysis with expectation propagation used to circumvent the need for multivariate integration. In this version, the random effects can be any reasonable dimension. However, only probit mixed models with one level of nesting are supported. The methodology is described in Hall, Johnstone, Ormerod, Wand and Yu (2018) <arXiv:1805.08423v1>.",
    "version": "1.0-3.1",
    "maintainer": "Matt P. Wand <matt.wand@uts.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14422,
    "package_name": "glmmFEL",
    "title": "Generalized Linear Mixed Models via Fully Exponential Laplace in\nEM",
    "description": "Fit generalized linear mixed models (GLMMs) with normal random\n    effects using first-order Laplace, fully exponential Laplace (FEL) with\n    mean-only corrections, and FEL with mean and covariance corrections in\n    the E-step of an expectation-maximization (EM) algorithm. The current\n    development version provides a matrix-based interface (y, X, Z) and\n    supports binary logit and probit, and Poisson log-link models. An EM\n    framework is used to update fixed effects, random effects, and a single\n    variance component tau^2 for G = tau^2 I, with staged approximations\n    (Laplace -> FEL mean-only -> FEL full) for efficiency and stability. A\n    pseudo-likelihood engine glmmFEL_pl() implements the working-response /\n    working-weights linearization approach of Wolfinger and O'Connell (1993)\n    <doi:10.1080/00949659308811554>, and is adapted from the implementation\n    used in the 'RealVAMS' package (Broatch, Green, and Karl (2018))\n    <doi:10.32614/RJ-2018-033>. The FEL implementation follows Karl, Yang,\n    and Lohr (2014) <doi:10.1016/j.csda.2013.11.019> and related work (e.g.,\n    Tierney, Kass, and Kadane (1989) <doi:10.1080/01621459.1989.10478824>;\n    Rizopoulos, Verbeke, and Lesaffre (2009)\n    <doi:10.1111/j.1467-9868.2008.00704.x>; Steele (1996)\n    <doi:10.2307/2532845>). Package code was drafted with assistance from\n    generative AI tools.",
    "version": "1.0.5",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14423,
    "package_name": "glmmLasso",
    "title": "Variable Selection for Generalized Linear Mixed Models by\nL1-Penalized Estimation",
    "description": "A variable selection approach for generalized linear mixed models by L1-penalized estimation is provided, \n\tsee Groll and Tutz (2014) <doi:10.1007/s11222-012-9359-z>.\n\tSee also Groll and Tutz (2017) <doi:10.1007/s10985-016-9359-y> for discrete survival models including heterogeneity.",
    "version": "1.6.3",
    "maintainer": "Andreas Groll <groll@statistik.tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14425,
    "package_name": "glmmPen",
    "title": "High Dimensional Penalized Generalized Linear Mixed Models\n(pGLMM)",
    "description": "Fits high dimensional penalized generalized linear \n    mixed models using \n    the Monte Carlo Expectation Conditional Minimization (MCECM) algorithm. \n    The purpose of the package is to perform variable selection on both the fixed and \n    random effects simultaneously for generalized linear mixed models.\n    The package supports fitting of Binomial, Gaussian, and Poisson data with canonical links, and\n    supports penalization using the MCP, SCAD, or LASSO penalties. The MCECM algorithm\n    is described in Rashid et al. (2020) <doi:10.1080/01621459.2019.1671197>.\n    The techniques used in the minimization portion of the procedure (the M-step) are\n    derived from the procedures of the 'ncvreg' package (Breheny and Huang (2011) \n    <doi:10.1214/10-AOAS388>) and 'grpreg' package (Breheny and Huang (2015)\n    <doi:10.1007/s11222-013-9424-2>), with\n    appropriate modifications to account for the estimation and penalization of\n    the random effects. The 'ncvreg' and 'grpreg' packages also describe the MCP, SCAD, \n    and LASSO penalties.",
    "version": "1.5.4.8",
    "maintainer": "Hillary Heiling <hmheiling@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14429,
    "package_name": "glmmrBase",
    "title": "Generalised Linear Mixed Models in R",
    "description": "Specification, analysis, simulation, and fitting of generalised linear mixed models. \n  Includes Markov Chain Monte Carlo Maximum likelihood and Laplace approximation model fitting for a range of models, \n  non-linear fixed effect specifications, a wide range of flexible covariance functions that can be combined arbitrarily,\n  robust and bias-corrected standard error estimation, power calculation, data simulation, and more. \n  See <https://samuel-watson.github.io/glmmr-web/> for a detailed manual.",
    "version": "1.1.0",
    "maintainer": "Sam Watson <S.I.Watson@bham.ac.uk>",
    "url": "https://github.com/samuel-watson/glmmrBase",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14431,
    "package_name": "glmmsel",
    "title": "Generalised Linear Mixed Model Selection",
    "description": "Provides tools for fitting sparse generalised linear mixed models with l0 \n  regularisation. Selects fixed and random effects under the hierarchy constraint that fixed effects \n  must precede random effects. Uses coordinate descent and local search algorithms to rapidly \n  deliver near-optimal estimates. Gaussian and binomial response families are currently supported.\n  For more details see Thompson, Wand, and Wang (2025) <doi:10.48550/arXiv.2506.20425>.",
    "version": "1.0.3",
    "maintainer": "Ryan Thompson <ryan.thompson-1@uts.edu.au>",
    "url": "https://github.com/ryan-thompson/glmmsel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14432,
    "package_name": "glmnet",
    "title": "Lasso and Elastic-Net Regularized Generalized Linear Models",
    "description": "Extremely efficient procedures for fitting the entire lasso or elastic-net regularization path for linear regression, logistic and multinomial regression models, Poisson regression, Cox model,  multiple-response Gaussian, and the grouped multinomial regression; see <doi:10.18637/jss.v033.i01> and <doi:10.18637/jss.v039.i05>. There are two new and important additions. The family argument can be a GLM family object, which opens the door to any programmed family (<doi:10.18637/jss.v106.i01>). This comes with a modest computational cost, so when the built-in families suffice, they should be used instead.  The other novelty is the relax option, which refits each of the active sets in the path unpenalized. The algorithm uses cyclical coordinate descent in a path-wise fashion, as described in the papers cited.",
    "version": "4.1-10",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "https://glmnet.stanford.edu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14433,
    "package_name": "glmnetSE",
    "title": "Add Nonparametric Bootstrap SE to 'glmnet' for Selected\nCoefficients (No Shrinkage)",
    "description": "Builds a LASSO, Ridge, or Elastic Net model with 'glmnet' or\n    'cv.glmnet' with bootstrap inference statistics (SE, CI, and p-value)\n    for selected coefficients with no shrinkage applied for them. Model\n    performance can be evaluated on test data and an automated alpha\n    selection is implemented for Elastic Net. Parallelized computation is\n    used to speed up the process. The methods are described in Friedman et\n    al. (2010) <doi:10.18637/jss.v033.i01> and Simon et al. (2011)\n    <doi:10.18637/jss.v039.i05>.",
    "version": "0.0.1",
    "maintainer": "Sebastian Bahr <sebastian.bahr@unibe.ch>",
    "url": "https://github.com/sebastianbahr/glmnetSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14434,
    "package_name": "glmnetUtils",
    "title": "Utilities for 'Glmnet'",
    "description": "Provides a formula interface for the 'glmnet' package for\n    elasticnet regression, a method for cross-validating the alpha parameter,\n    and other quality-of-life tools.",
    "version": "1.1.9",
    "maintainer": "Hong Ooi <hongooi73@gmail.com>",
    "url": "https://github.com/hongooi73/glmnetUtils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14435,
    "package_name": "glmnetcr",
    "title": "Fit a Penalized Constrained Continuation Ratio Model for\nPredicting an Ordinal Response",
    "description": "Penalized methods are useful for fitting over-parameterized models. This package includes functions for restructuring an ordinal\n response dataset for fitting continuation ratio models for datasets  where the number of covariates exceeds the sample size or when\n there is collinearity among the covariates. The 'glmnet' fitting algorithm is used to fit the continuation ratio model after data restructuring.",
    "version": "1.0.7",
    "maintainer": "Kellie J. Archer <archer.43@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14437,
    "package_name": "glmpath",
    "title": "L1 Regularization Path for Generalized Linear Models and Cox\nProportional Hazards Model",
    "description": "A path-following algorithm for L1 regularized generalized linear models and Cox proportional hazards model.",
    "version": "0.98",
    "maintainer": "Mee Young Park <meeyoung@google.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14438,
    "package_name": "glmpathcr",
    "title": "Fit a Penalized Continuation Ratio Model for Predicting an\nOrdinal Response",
    "description": "Provides a function for fitting a penalized constrained  continuation ratio model using the glmpath algorithm and methods for  extracting coefficient estimates, predicted class, class probabilities, and plots as described by Archer and Williams (2012) <doi:10.1002/sim.4484>. ",
    "version": "1.0.10",
    "maintainer": "Kellie J. Archer <archer.43@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14439,
    "package_name": "glmpca",
    "title": "Dimension Reduction of Non-Normally Distributed Data",
    "description": "Implements a generalized version of principal components analysis\n    (GLM-PCA) for dimension reduction of non-normally distributed data such as\n    counts or binary matrices.\n    Townes FW, Hicks SC, Aryee MJ, Irizarry RA (2019) <doi:10.1186/s13059-019-1861-6>.\n    Townes FW (2019) <arXiv:1907.02647>.",
    "version": "0.2.0",
    "maintainer": "F. William Townes <will.townes@gmail.com>",
    "url": "https://github.com/willtownes/glmpca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14444,
    "package_name": "glmulti",
    "title": "Model Selection and Multimodel Inference Made Easy",
    "description": "Automated model selection and model-averaging. Provides a\n        wrapper for glm and other functions, automatically generating\n        all possible models (under constraints set by the user) with\n        the specified response and explanatory variables, and finding\n        the best models in terms of some Information Criterion (AIC,\n        AICc or BIC). Can handle very large numbers of candidate\n        models. Features a Genetic Algorithm to find the best models\n        when an exhaustive screening of the candidates is not feasible.",
    "version": "1.0.8",
    "maintainer": "Vincent Calcagno <vincent.calcagno@inrae.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14446,
    "package_name": "glmx",
    "title": "Generalized Linear Models Extended",
    "description": "Extended techniques for generalized linear models (GLMs), especially for binary responses,\n             including parametric links and heteroscedastic latent variables.",
    "version": "0.2-1",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14447,
    "package_name": "glmxdiag",
    "title": "A Collection of Graphic Tools for GLM Diagnostics and some\nExtensions",
    "description": "Provides diagnostic graphic tools for GLMs, beta-binomial regression model (estimated by 'VGAM' package), beta regression model (estimated by 'betareg' package) and negative binomial regression model (estimated by 'MASS' package). Since most of functions implemented in 'glmxdiag' already exist in other packages, the aim is to provide the user unique functions that work on almost all regression models previously specified. Details about some of the implemented functions can be found in Brown (1992) <doi:10.2307/2347617>, Dunn and Smyth (1996) <doi:10.2307/1390802>, O'Hara Hines and Carter (1993) <doi:10.2307/2347405>, Wang (1985) <doi:10.2307/1269708>.",
    "version": "1.0.0",
    "maintainer": "Giuseppe Reale <giuseppe.reale97@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14458,
    "package_name": "glober",
    "title": "Estimating Functions with Multivariate B-Splines",
    "description": "Generalized LassO applied to knot selection in multivariate B-splinE Regression (GLOBER) implements a novel approach for estimating functions in a multivariate nonparametric regression model based on an adaptive knot selection for B-splines using the Generalized Lasso. For further details we refer the reader to the paper Savino, M. E. and Lévy-Leduc, C. (2023), <arXiv:2306.00686>.",
    "version": "1.0",
    "maintainer": "Mary E. Savino <mary.savino@outlook.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14483,
    "package_name": "gmfamm",
    "title": "Generalized Multivariate Functional Additive Models",
    "description": "Supply implementation to model generalized multivariate functional\n data using Bayesian additive mixed models of R package 'bamlss' via a latent\n Gaussian process (see Umlauf, Klein, Zeileis (2018) \n <doi:10.1080/10618600.2017.1407325>).",
    "version": "0.1.0",
    "maintainer": "Alexander Volkmann <alexandervolkmann8@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14485,
    "package_name": "gmgm",
    "title": "Gaussian Mixture Graphical Model Learning and Inference",
    "description": "Gaussian mixture graphical models include Bayesian networks and\n    dynamic Bayesian networks (their temporal extension) whose local probability\n    distributions are described by Gaussian mixture models. They are powerful\n    tools for graphically and quantitatively representing nonlinear dependencies\n    between continuous variables. This package provides a complete framework to\n    create, manipulate, learn the structure and the parameters, and perform\n    inference in these models. Most of the algorithms are described in the PhD\n    thesis of Roos (2018) <https://tel.archives-ouvertes.fr/tel-01943718>.",
    "version": "1.1.2",
    "maintainer": "Jérémy Roos <jeremy.roos@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14498,
    "package_name": "gmvarkit",
    "title": "Estimate Gaussian and Student's t Mixture Vector Autoregressive\nModels",
    "description": "Unconstrained and constrained maximum likelihood estimation of structural and reduced form \n    Gaussian mixture vector autoregressive, Student's t mixture vector autoregressive, and Gaussian and Student's t\n    mixture vector autoregressive models, quantile residual tests, graphical diagnostics,\n    simulations, forecasting, and estimation of generalized impulse response function and generalized \n    forecast error variance decomposition.\n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2016) <doi:10.1016/j.jeconom.2016.02.012>,\n    Savi Virolainen (2025) <doi:10.1080/07350015.2024.2322090>,\n    Savi Virolainen (in press) <doi:10.1016/j.ecosta.2025.09.003>.",
    "version": "2.2.1",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14500,
    "package_name": "gmwm",
    "title": "Generalized Method of Wavelet Moments",
    "description": "Generalized Method of Wavelet Moments (GMWM) is an estimation",
    "version": "3.0.0",
    "maintainer": "Stephane Guerrier <stef.guerrier@gmail.com>",
    "url": "https://github.com/SMAC-Group/gmwm",
    "exports": [],
    "topics": ["armadillo", "r", "rcpp", "robust", "time-series", "wavelet-variance"],
    "score": "NA",
    "stars": 31
  },
  {
    "id": 14504,
    "package_name": "gnlm",
    "title": "Generalized Nonlinear Regression Models",
    "description": "A variety of functions to fit linear and nonlinear\n    regression with a large selection of distributions.",
    "version": "1.1.2",
    "maintainer": "Bruce Swihart <bruce.swihart@gmail.com>",
    "url": "https://www.commanster.eu/rcode.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14525,
    "package_name": "gofar",
    "title": "Generalized Co-Sparse Factor Regression",
    "description": "Divide and conquer approach for estimating low-rank and sparse coefficient matrix in the generalized co-sparse factor regression. Please refer the manuscript 'Mishra, Aditya, Dipak K. Dey, Yong Chen, and Kun Chen. Generalized co-sparse factor regression. Computational Statistics & Data Analysis 157 (2021): 107127' for more details. ",
    "version": "0.1",
    "maintainer": "Aditya Mishra <amishra@flatironinstitute.org>",
    "url": "https://github.com/amishra-stats/gofar,\nhttps://www.sciencedirect.com/science/article/pii/S0167947320302188",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14531,
    "package_name": "gofreg",
    "title": "Bootstrap-Based Goodness-of-Fit Tests for Parametric Regression",
    "description": "Provides statistical methods to check if a parametric family of\n  conditional density functions fits to some given dataset of covariates and \n  response variables. Different test statistics can be used to determine the \n  goodness-of-fit of the assumed model, see Andrews (1997) \n  <doi:10.2307/2171880>, Bierens & Wang (2012) <doi:10.1017/S0266466611000168>,\n  Dikta & Scheer (2021) <doi:10.1007/978-3-030-73480-0> and Kremling & Dikta \n  (2024) <doi:10.48550/arXiv.2409.20262>. As proposed in these papers, the \n  corresponding p-values are approximated using a parametric bootstrap method.",
    "version": "1.0.0",
    "maintainer": "Gitte Kremling <gitte.kremling@web.de>",
    "url": "https://github.com/gkremling/gofreg,\nhttps://gkremling.github.io/gofreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14539,
    "package_name": "gomms",
    "title": "GLM-Based Ordination Method",
    "description": "A zero-inflated quasi-Poisson factor model to display similarity between samples visually in a low (2 or 3) dimensional space.",
    "version": "1.0",
    "maintainer": "Michael B. Sohn <msohn@mail.med.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14541,
    "package_name": "gompertztrunc",
    "title": "Conducting Maximum Likelihood Estimation with Truncated\nMortality Data",
    "description": "Estimates hazard ratios and mortality differentials for\n    doubly-truncated data without population denominators. This method is\n    described in Goldstein et al. (2023) <doi:10.1007/s11113-023-09785-z>.",
    "version": "0.1.2",
    "maintainer": "Maria Osborne <mariaosborne@berkeley.edu>",
    "url": "https://caseybreen.github.io/gompertztrunc/,\nhttps://github.com/caseybreen/gompertztrunc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14575,
    "package_name": "goweragreement",
    "title": "Bayesian Gower Agreement for Categorical Data",
    "description": "Provides tools for applying the Bayesian Gower agreement methodology (presented in the package vignette) to nominal or ordinal data. The framework can accommodate any number of units, any number of coders, and missingness; and can handle both one-way and two-way random study designs. Influential units and/or coders can be identified easily using leave-one-out statistics.",
    "version": "1.0-1",
    "maintainer": "John Hughes <drjphughesjr@gmail.com>",
    "url": "http://www.johnhughes.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14581,
    "package_name": "gpcp",
    "title": "Genomic Prediction of Cross Performance",
    "description": "This function performs genomic prediction of cross performance using genotype and phenotype data.\n  It processes data in several steps including loading necessary software, converting genotype data, processing phenotype data,\n  fitting mixed models, and predicting cross performance based on weighted marker effects.\n  For more information, see Labroo et al. (2023) <doi:10.1007/s00122-023-04377-z>.",
    "version": "0.1.0",
    "maintainer": "Christine Nyaga <cmn92@cornell.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14587,
    "package_name": "gplite",
    "title": "General Purpose Gaussian Process Modelling",
    "description": "\n  Implements the most common Gaussian process (GP) models using Laplace and\n  expectation propagation (EP) approximations, maximum marginal likelihood\n  (or posterior) inference for the hyperparameters, and sparse approximations\n  for larger datasets.",
    "version": "0.13.0",
    "maintainer": "Juho Piironen <juho.t.piironen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14589,
    "package_name": "gplots",
    "title": "Various R Programming Tools for Plotting Data",
    "description": "Various R programming tools for plotting data, including:\n  - calculating and plotting locally smoothed summary function as\n    ('bandplot', 'wapply'),\n  - enhanced versions of standard plots ('barplot2', 'boxplot2',\n    'heatmap.2', 'smartlegend'),\n  - manipulating colors ('col2hex', 'colorpanel', 'redgreen',\n    'greenred', 'bluered', 'redblue', 'rich.colors'),\n  - calculating and plotting two-dimensional data summaries ('ci2d',\n    'hist2d'),\n  - enhanced regression diagnostic plots ('lmplot2', 'residplot'),\n  - formula-enabled interface to 'stats::lowess' function ('lowess'),\n  - displaying textual data in plots ('textplot', 'sinkplot'),\n  - plotting dots whose size reflects the relative magnitude of the\n    elements ('balloonplot', 'bubbleplot'),\n  - plotting \"Venn\" diagrams ('venn'),\n  - displaying Open-Office style plots ('ooplot'),\n  - plotting multiple data on same region, with separate axes\n    ('overplot'),\n  - plotting means and confidence intervals ('plotCI', 'plotmeans'),\n  - spacing points in an x-y plot so they don't overlap ('space').",
    "version": "3.3.0",
    "maintainer": "Tal Galili <tal.galili@gmail.com>",
    "url": "https://github.com/talgalili/gplots,\nhttps://talgalili.github.io/gplots/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14611,
    "package_name": "gradLasso",
    "title": "Gradient Descent LASSO with Stability Selection and Bootstrapped\nConfidence Intervals",
    "description": "Implements LASSO regression using gradient descent with support for Gaussian, Binomial, Negative Binomial, and Zero-Inflated Negative Binomial (ZINB) families. Features cross-validation for determining lambda, stability selection, and bootstrapping for confidence intervals. Methods described in Tibshirani (1996) <doi:10.1111/j.2517-6161.1996.tb02080.x> and Meinshausen and Buhlmann (2010) <doi:10.1111/j.1467-9868.2010.00740.x>.",
    "version": "0.1.1",
    "maintainer": "David DeFranza <david.defranza@ucd.ie>",
    "url": "https://github.com/ddefranza/gradLasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14637,
    "package_name": "graphicalEvidence",
    "title": "Graphical Evidence",
    "description": "Computes marginal likelihood in Gaussian graphical models through a\n  novel telescoping block decomposition of the precision matrix which allows\n  estimation of model evidence. The top level function used to estimate marginal\n  likelihood is called evidence(), which expects the prior name, data, and\n  relevant prior specific parameters. This package also provides an MCMC prior\n  sampler using the same underlying approach, implemented in prior_sampling(),\n  which expects a prior name and prior specific parameters. Both functions also\n  expect the number of burn-in iterations and the number of sampling iterations\n  for the underlying MCMC sampler.",
    "version": "1.1",
    "maintainer": "David Rowe <david@rowe-stats.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14640,
    "package_name": "graphicalVAR",
    "title": "Graphical VAR for Experience Sampling Data",
    "description": "Estimates within and between time point interactions in experience sampling data, using the Graphical vector autoregression model in combination with regularization. See also Epskamp, Waldorp, Mottus & Borsboom (2018) <doi:10.1080/00273171.2018.1454823>.",
    "version": "0.3.4",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14657,
    "package_name": "gravitas",
    "title": "Explore Probability Distributions for Bivariate Temporal\nGranularities",
    "description": "Provides tools for systematically exploring large quantities of \n             temporal data across cyclic temporal granularities\n             (deconstructions of time) by visualizing probability distributions.\n             Cyclic time granularities can be circular, quasi-circular or \n             aperiodic. 'gravitas' computes cyclic\n             single-order-up or multiple-order-up granularities, check the\n             feasibility of creating plots for any two cyclic granularities\n             and recommend probability distributions plots for exploring\n             periodicity in the data.",
    "version": "0.1.3",
    "maintainer": "Sayani Gupta <gupta.sayani@gmail.com>",
    "url": "https://github.com/Sayani07/gravitas/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14670,
    "package_name": "gregRy",
    "title": "GREGORY Estimation",
    "description": "Functions which make using the Generalized Regression Estimator(GREG)\n    J.N.K. Rao, Isabel Molina, (2015) <doi:10.3390/f11020244> \n    and the Generalized Regression Estimator Operating on Resolutions of Y (GREGORY) easier. \n    The functions are designed to work well within a forestry context, and estimate multiple \n    estimation units at once. Compared to other survey estimation packages, this function has greater flexibility when \n    describing the linear model.",
    "version": "0.1.0",
    "maintainer": "Olek Wojcik <olkowaty@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14671,
    "package_name": "gremlin",
    "title": "Mixed-Effects REML Incorporating Generalized Inverses",
    "description": "Fit linear mixed-effects models using restricted (or residual)\n    maximum likelihood (REML) and with generalized inverse matrices to specify\n    covariance structures for random effects. In particular, the package is\n    suited to fit quantitative genetic mixed models, often referred to as\n    'animal models'. Implements the average information algorithm as the main\n    tool to maximize the restricted log-likelihood, but with other algorithms\n    available.",
    "version": "1.1.0",
    "maintainer": "Matthew Wolak <matthewwolak@gmail.com>",
    "url": "https://github.com/matthewwolak/gremlin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14674,
    "package_name": "gretlR",
    "title": "A Seamless Integration of 'Gretl' and 'R'",
    "description": "It allows running 'gretl' (<http://gretl.sourceforge.net/index.html>) program from R, R Markdown and Quarto. 'gretl' ('Gnu' Regression, 'Econometrics', and Time-series Library) is a statistical software for Econometric analysis.  This package does not only integrate 'gretl' and 'R' but also serves  as a 'gretl' Knit-Engine for 'knitr' package. Write all your 'gretl' commands in 'R', R Markdown chunk.",
    "version": "0.1.4",
    "maintainer": "Sagiru Mati <smati@smati.com.ng>",
    "url": "https://CRAN.R-project.org/package=gretlR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14677,
    "package_name": "grf",
    "title": "Generalized Random Forests",
    "description": "Forest-based statistical estimation and inference.\n  GRF provides non-parametric methods for heterogeneous treatment effects estimation\n  (optionally using right-censored outcomes, multiple treatment arms or outcomes, or instrumental variables),\n  as well as least-squares regression, quantile regression, and survival regression,\n  all with support for missing covariates.",
    "version": "2.5.0",
    "maintainer": "Erik Sverdrup <erik.sverdrup@monash.edu>",
    "url": "https://github.com/grf-labs/grf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14694,
    "package_name": "grizbayr",
    "title": "Bayesian Inference for A|B and Bandit Marketing Tests",
    "description": "Uses simple Bayesian conjugate prior update rules to calculate \n    the win probability of each option, value remaining in the test, and \n    percent lift over the baseline for various marketing objectives.\n    References: \n    Fink, Daniel (1997) \"A Compendium of Conjugate Priors\" <https://www.johndcook.com/CompendiumOfConjugatePriors.pdf>.\n    Stucchio, Chris (2015) \"Bayesian A/B Testing at VWO\" <https://vwo.com/downloads/VWO_SmartStats_technical_whitepaper.pdf>.",
    "version": "1.3.5",
    "maintainer": "Ryan Angi <angi.ryan@gmail.com>",
    "url": "https://github.com/rangi513/grizbayr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14699,
    "package_name": "groc",
    "title": "Generalized Regression on Orthogonal Components",
    "description": "Robust multiple or multivariate linear regression, nonparametric regression on orthogonal components, classical or robust partial least squares models as described in Bilodeau, Lafaye De Micheaux and Mahdi (2015) <doi:10.18637/jss.v065.i01>.",
    "version": "1.0.10",
    "maintainer": "Pierre Lafaye De Micheaux <lafaye@unsw.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14705,
    "package_name": "groupWQS",
    "title": "Grouped Weighted Quantile Sum Regression",
    "description": "Fits weighted quantile sum (WQS) regressions for one or more chemical groups with continuous or binary outcomes. Wheeler D, Czarnota J.(2016) <doi:10.1289/isee.2016.4698>.",
    "version": "0.0.3",
    "maintainer": "Matthew Carli <carlimm@mymail.vcu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14707,
    "package_name": "groupdata2",
    "title": "Creating Groups from Data",
    "description": "Methods for dividing data into groups. \n    Create balanced partitions and cross-validation folds. \n    Perform time series windowing and general grouping and splitting of data. \n    Balance existing groups with up- and downsampling or collapse them to fewer groups.",
    "version": "2.0.5",
    "maintainer": "Ludvig Renbo Olsen <r-pkgs@ludvigolsen.dk>",
    "url": "https://github.com/ludvigolsen/groupdata2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14710,
    "package_name": "groupedSurv",
    "title": "Efficient Estimation of Grouped Survival Models Using the Exact\nLikelihood Function",
    "description": "These 'Rcpp'-based functions compute the efficient score statistics for grouped time-to-event data (Prentice and Gloeckler, 1978), with the optional inclusion of baseline covariates. Functions for estimating the parameter of interest and nuisance parameters, including baseline hazards, using maximum likelihood are also provided. A parallel set of functions allow for the incorporation of family structure of related individuals (e.g., trios). Note that the current implementation of the frailty model (Ripatti and Palmgren, 2000) is sensitive to departures from model assumptions, and should be considered experimental. For these data, the exact proportional-hazards-model-based likelihood is computed by evaluating multiple variable integration. The integration is accomplished using the 'Cuba' library (Hahn, 2005), and the source files are included in this package. The maximization process is carried out using Brent's algorithm, with the C++ code file from John Burkardt and John Denker (Brent, 2002).",
    "version": "1.0.5.1",
    "maintainer": "Alexander Sibley <dcibioinformatics@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14717,
    "package_name": "growfunctions",
    "title": "Bayesian Non-Parametric Dependent Models for Time-Indexed\nFunctional Data",
    "description": "Estimates a collection of time-indexed functions under\n    either of Gaussian process (GP) or intrinsic Gaussian Markov\n    random field (iGMRF) prior formulations where a Dirichlet process\n    mixture allows sub-groupings of the functions to share the same\n    covariance or precision parameters.  The GP and iGMRF formulations\n    both support any number of additive covariance or precision terms,\n    respectively, expressing either or both of multiple trend and\n    seasonality.",
    "version": "0.17",
    "maintainer": "Terrance Savitsky <tds151@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14723,
    "package_name": "growthrate",
    "title": "Bayesian reconstruction of growth velocity",
    "description": "A nonparametric empirical Bayes method for recovering\n        gradients (or growth velocities) from observations of smooth\n        functions (e.g., growth curves) at isolated time points.",
    "version": "1.3",
    "maintainer": "Ian W. McKeague <im2131@columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14726,
    "package_name": "grpSLOPE",
    "title": "Group Sorted L1 Penalized Estimation",
    "description": "Group SLOPE (Group Sorted L1 Penalized Estimation) is\n    a penalized linear regression method that is used for adaptive\n    selection of groups of significant predictors in a high-dimensional\n    linear model. The Group SLOPE method can control the (group) false\n    discovery rate at a user-specified level (i.e., control the expected\n    proportion of irrelevant among all selected groups of predictors).\n    For additional information about the implemented methods please see\n    Brzyski, Gossmann, Su, Bogdan (2018) <doi:10.1080/01621459.2017.1411269>.",
    "version": "0.3.4",
    "maintainer": "Alexej Gossmann <alexej.go@googlemail.com>",
    "url": "https://github.com/agisga/grpSLOPE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14734,
    "package_name": "grwat",
    "title": "River Hydrograph Separation and Analysis",
    "description": "River hydrograph separation and daily runoff time series analysis. Provides\n  various filters to separate baseflow and quickflow. Implements advanced separation \n  technique by Rets et al. (2022) <doi:10.1134/S0097807822010146> which involves \n  meteorological data to reveal genetic components of the runoff: ground, rain, thaw \n  and spring (seasonal thaw). High-performance C++17 computation, annually aggregated \n  variables, statistical testing and numerous plotting functions for high-quality \n  visualization.",
    "version": "0.1",
    "maintainer": "Timofey Samsonov <tsamsonov@geogr.msu.ru>",
    "url": "https://github.com/tsamsonov/grwat,\nhttps://tsamsonov.github.io/grwat/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14736,
    "package_name": "gsDesign2",
    "title": "Group Sequential Design with Non-Constant Effect",
    "description": "The goal of 'gsDesign2' is to enable fixed or group sequential\n    design under non-proportional hazards. To enable highly flexible enrollment,\n    time-to-event and time-to-dropout assumptions, 'gsDesign2' offers piecewise\n    constant enrollment, failure rates, and dropout rates for a stratified\n    population. This package includes three methods for designs:\n    average hazard ratio, weighted logrank tests in Yung and Liu (2019)\n    <doi:10.1111/biom.13196>, and MaxCombo tests.\n    Substantial flexibility on top of what is in the 'gsDesign' package\n    is intended for selecting boundaries.",
    "version": "1.1.7",
    "maintainer": "Yujie Zhao <yujie.zhao@merck.com>",
    "url": "https://merck.github.io/gsDesign2/,\nhttps://github.com/Merck/gsDesign2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14741,
    "package_name": "gsarima",
    "title": "Two Functions for Generalized SARIMA Time Series Simulation",
    "description": "Write SARIMA models in (finite) AR representation and simulate \n\tgeneralized multiplicative seasonal autoregressive moving average (time) series \n\twith Normal / Gaussian, Poisson or negative binomial distribution. \n\tThe methodology of this method is described in Briet OJT, Amerasinghe PH, and \n\tVounatsou P (2013) <doi:10.1371/journal.pone.0065761>.",
    "version": "0.1-5",
    "maintainer": "Olivier Briet <o.briet@gmail.com>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14742,
    "package_name": "gsbDesign",
    "title": "Group Sequential Bayes Design",
    "description": "Group Sequential Operating Characteristics for Clinical,\n        Bayesian two-arm Trials with known Sigma and Normal Endpoints,\n\tas described in Gerber and Gsponer (2016) <doi: 10.18637/jss.v069.i11>.",
    "version": "1.0-3",
    "maintainer": "Bjoern Bornkamp <bbnkmp@mail.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14751,
    "package_name": "gsignal",
    "title": "Signal Processing",
    "description": "R implementation of the 'Octave' package 'signal', containing\n    a variety of signal processing tools, such as signal generation and\n    measurement, correlation and convolution, filtering, filter design,\n    filter analysis and conversion, power spectrum analysis, system\n    identification, decimation and sample rate change, and windowing.",
    "version": "0.3-7",
    "maintainer": "Geert van Boxtel <G.J.M.vanBoxtel@gmail.com>",
    "url": "https://github.com/gjmvanboxtel/gsignal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14757,
    "package_name": "gspcr",
    "title": "Generalized Supervised Principal Component Regression",
    "description": "Generalization of supervised principal component regression (SPCR; \n    Bair et al., 2006, <doi:10.1198/016214505000000628>) to support continuous, \n    binary, and discrete variables as outcomes and predictors \n    (inspired by the 'superpc' R package <https://cran.r-project.org/package=superpc>).",
    "version": "0.9.5",
    "maintainer": "Edoardo Costantini <costantini.edoardo@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14760,
    "package_name": "gss",
    "title": "General Smoothing Splines",
    "description": "A comprehensive package for structural multivariate\n        function estimation using smoothing splines.",
    "version": "2.2-10",
    "maintainer": "Chong Gu <chong@purdue.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14761,
    "package_name": "gstar",
    "title": "Generalized Space-Time Autoregressive Model",
    "description": "Multivariate time series analysis based on Generalized Space-Time Autoregressive Model by Ruchjana et al.(2012) <doi:10.1063/1.4724118>.",
    "version": "0.1.0",
    "maintainer": "Ahmad Zaenal <ahmadzaenal125@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14766,
    "package_name": "gsynth",
    "title": "Generalized Synthetic Control Method",
    "description": "Conducts causal inference with interactive fixed-effect models. It imputes counterfactuals for each treated unit using control group information based on a linear interactive fixed effects model that incorporates unit-specific intercepts interacted with time-varying coefficients. This method generalizes the synthetic control method to the case of multiple treated units and variable treatment periods, and improves efficiency and interpretability. ",
    "version": "1.3.1",
    "maintainer": "Yiqing Xu <yiqingxu@stanford.edu>",
    "url": "https://yiqingxu.org/packages/gsynth/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14772,
    "package_name": "gte",
    "title": "Generalized Turnbull's Estimator",
    "description": "Generalized Turnbull's estimator proposed by Dehghan and Duchesne\n    (2011).",
    "version": "1.2-4",
    "maintainer": "Thierry Duchesne <thierry.duchesne@mat.ulaval.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14788,
    "package_name": "gtsummary",
    "title": "Presentation-Ready Data Summary and Analytic Result Tables",
    "description": "Creates presentation-ready tables summarizing data sets,",
    "version": "2.5.0.9001",
    "maintainer": "",
    "url": "https://github.com/ddsjoberg/gtsummary",
    "exports": [],
    "topics": ["easy-to-use", "gt", "html5", "r", "r-package", "regression-models", "reproducibility", "reproducible-research", "rstats", "statistics", "summary-statistics", "summary-tables", "table1", "tableone"],
    "score": "NA",
    "stars": 1160
  },
  {
    "id": 14816,
    "package_name": "gwbr",
    "title": "Local and Global Beta Regression",
    "description": "Fit a regression model for when the response variable is presented as a ratio or proportion. This adjustment can occur globally, with the same estimate for the entire study space, or locally, where a beta regression model is fitted for each region, considering only influential locations for that area. Da Silva, A. R. and Lima, A. O. (2017) <doi:10.1016/j.spasta.2017.07.011>.",
    "version": "1.0.5",
    "maintainer": "Roberto Marques <robertomarques_23@yahoo.com.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14824,
    "package_name": "h0",
    "title": "A Robust Bayesian Meta-Analysis for Estimating the Hubble\nConstant via Time Delay Cosmography",
    "description": "We provide a toolbox to conduct a Bayesian meta-analysis for estimating the current expansion rate of the Universe, called the Hubble constant H0, via time delay cosmography. The input data are Fermat potential difference and time delay estimates. For a robust inference, we assume a Student's t error for these inputs. Given these inputs, the meta-analysis produces posterior samples of the model parameters including the Hubble constant via Metropolis-Hastings within Gibbs. The package provides an option to implement repelling-attracting Metropolis-Hastings within Gibbs in a case where the parameter space has multiple modes.",
    "version": "1.0.1",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14848,
    "package_name": "hal9001",
    "title": "The Scalable Highly Adaptive Lasso",
    "description": "A scalable implementation of the highly adaptive lasso algorithm,\n  including routines for constructing sparse matrices of basis functions of the\n  observed data, as well as a custom implementation of Lasso regression tailored\n  to enhance efficiency when the matrix of predictors is composed exclusively of\n  indicator functions. For ease of use and increased flexibility, the Lasso\n  fitting routines invoke code from the 'glmnet' package by default. The highly\n  adaptive lasso was first formulated and described by MJ van der Laan (2017)\n  <doi:10.1515/ijb-2015-0097>, with practical demonstrations of its performance\n  given by Benkeser and van der Laan (2016) <doi:10.1109/DSAA.2016.93>. This\n  implementation of the highly adaptive lasso algorithm was described by Hejazi,\n  Coyle, and van der Laan (2020) <doi:10.21105/joss.02526>.",
    "version": "0.4.6",
    "maintainer": "Jeremy Coyle <jeremyrcoyle@gmail.com>",
    "url": "https://github.com/tlverse/hal9001",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14866,
    "package_name": "haplo.stats",
    "title": "Statistical Analysis of Haplotypes with Traits and Covariates\nwhen Linkage Phase is Ambiguous",
    "description": "Routines for the analysis of indirectly measured haplotypes. The statistical methods assume that all subjects are unrelated and that haplotypes are ambiguous (due to unknown linkage phase of the genetic markers). The main functions are: haplo.em(), haplo.glm(), haplo.score(), and haplo.power(); all of which have detailed examples in the vignette.",
    "version": "1.9.7",
    "maintainer": "Jason P. Sinnwell <sinnwell.jason@mayo.edu>",
    "url": "https://analytictools.mayo.edu/research/haplo-stats/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14871,
    "package_name": "harbinger",
    "title": "A Unified Time Series Event Detection Framework",
    "description": "By analyzing time series, it is possible to observe significant changes in the behavior of observations that frequently characterize events. Events present themselves as anomalies, change points, or motifs. In the literature, there are several methods for detecting events. However, searching for a suitable time series method is a complex task, especially considering that the nature of events is often unknown. This work presents Harbinger, a framework for integrating and analyzing event detection methods. Harbinger contains several state-of-the-art methods described in Salles et al. (2020) <doi:10.5753/sbbd.2020.13626>.",
    "version": "1.2.747",
    "maintainer": "Eduardo Ogasawara <eogasawara@ieee.org>",
    "url": "https://cefet-rj-dal.github.io/harbinger/,\nhttps://github.com/cefet-rj-dal/harbinger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14894,
    "package_name": "hbbr",
    "title": "Hierarchical Bayesian Benefit-Risk Assessment Using Discrete\nChoice Experiment",
    "description": "Implements assessment of benefit-risk balance using Bayesian Discrete Choice Experiment. For more details see the article by Mukhopadhyay et al. (2019) <DOI:10.1080/19466315.2018.1527248>. ",
    "version": "1.1.2",
    "maintainer": "Saurabh Mukhopadhyay <stat.mukherjee@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14896,
    "package_name": "hbmem",
    "title": "Hierarchical Bayesian Analysis of Recognition Memory",
    "description": "Contains functions for fitting hierarchical versions of\n        EVSD, UVSD, DPSD, DPSD with d' restricted to be positive, and\n        our gamma signal detection model to recognition memory\n        confidence-ratings data.",
    "version": "0.3-4",
    "maintainer": "Mike Pratte <prattems@gmail.com>",
    "url": "https://pcn.psychology.msstate.edu/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14897,
    "package_name": "hbsae",
    "title": "Hierarchical Bayesian Small Area Estimation",
    "description": "Functions to compute small area estimates based on a basic area or\n    unit-level model. The model is fit using restricted maximum likelihood, or\n    in a hierarchical Bayesian way. In the latter case numerical integration is\n    used to average over the posterior density for the between-area variance.\n    The output includes the model fit, small area estimates and corresponding\n    mean squared errors, as well as some model selection measures. Additional functions\n    provide means to compute aggregate estimates and mean squared errors, to minimally\n    adjust the small area estimates to benchmarks at a higher aggregation\n    level, and to graphically compare different sets of small area estimates.",
    "version": "1.2",
    "maintainer": "Harm Jan Boonstra <hjboonstra@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14898,
    "package_name": "hbsaems",
    "title": "Hierarchical Bayes Small Area Estimation Model using 'Stan'",
    "description": "Implementing Hierarchical Bayesian Small Area Estimation \n    models using the 'brms' package as the computational backend. The \n    modeling framework follows the methodological foundations described in area-level \n    models. This package is designed to facilitate a principled Bayesian workflow, \n    enabling users to conduct prior predictive checks, model fitting, posterior \n    predictive checks, model comparison, and sensitivity analysis in a coherent and \n    reproducible manner. It supports flexible model specifications via 'brms' and \n    promotes transparency in model development, aligned with the recommendations of \n    modern Bayesian data analysis practices, implementing methods described in \n    Rao and Molina (2015) <doi:10.1002/9781118735855>.",
    "version": "0.1.1",
    "maintainer": "Achmad Syahrul Choir <madsyair@stis.ac.id>",
    "url": "https://github.com/madsyair/hbsaems",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14901,
    "package_name": "hcci",
    "title": "Interval Estimation of Linear Models with Heteroskedasticity",
    "description": "Calculates the interval estimates for the parameters of linear models with heteroscedastic regression using bootstrap - (Wild Bootstrap) and double bootstrap-t (Wild Bootstrap). It is also possible to calculate confidence intervals using the percentile bootstrap and percentile bootstrap double. The package can calculate consistent estimates of the covariance matrix of the parameters of linear regression models with heteroscedasticity of unknown form. The package also provides a function to consistently calculate the covariance matrix of the parameters of linear models with heteroscedasticity of unknown form. The bootstrap methods exported by the package are based on the master's thesis of the first author, available at <https://raw.githubusercontent.com/prdm0/hcci/master/references/dissertacao_mestrado.pdf>. The hcci package in previous versions was cited in the book VINOD, Hrishikesh D. Hands-on Intermediate Econometrics Using R: Templates for Learning Quantitative Methods and R Software. 2022, p. 441, ISBN 978-981-125-617-2 (hardcover). The simple bootstrap schemes are based on the works of Cribari-Neto F and Lima M. G. (2009) <doi:10.1080/00949650801935327>, while the double bootstrap schemes for the parameters that index the linear models with heteroscedasticity of unknown form are based on the works of Beran (1987) <doi:10.2307/2336685>. The use of bootstrap for the calculation of interval estimates in regression models with heteroscedasticity of unknown form from a weighting of the residuals was proposed by Wu (1986) <doi:10.1214/aos/1176350142>. This bootstrap scheme is known as weighted or wild bootstrap.",
    "version": "1.2.0",
    "maintainer": "Pedro Rafael Diniz Marinho <pedro.rafael.marinho@gmail.com>",
    "url": "https://github.com/prdm0/hcci",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14914,
    "package_name": "hdbayes",
    "title": "Bayesian Analysis of Generalized Linear Models with Historical\nData",
    "description": "User-friendly functions for leveraging (multiple) historical data set(s) in Bayesian analysis of generalized \n    linear models (GLMs) and survival models, along with support for Bayesian model averaging (BMA). The package provides \n    functions for sampling from posterior distributions under various informative priors, including the prior induced by the \n    Bayesian hierarchical model, power prior by Ibrahim and Chen (2000) <doi:10.1214/ss/1009212673>, normalized power prior by \n    Duan et al. (2006) <doi:10.1002/env.752>, normalized asymptotic power prior by Ibrahim et al. (2015) <doi:10.1002/sim.6728>, \n    commensurate prior by Hobbs et al. (2011) <doi:10.1111/j.1541-0420.2011.01564.x>, robust meta-analytic-predictive \n    prior by Schmidli et al. (2014) <doi:10.1111/biom.12242>, latent exchangeability prior by Alt et al. (2024) \n    <doi:10.1093/biomtc/ujae083>, and a normal (or half-normal) prior. The package also includes functions for computing\n    model averaging weights, such as BMA, pseudo-BMA, pseudo-BMA with the Bayesian bootstrap, and stacking (Yao et al.,\n    2018 <doi:10.1214/17-BA1091>), as well as for generating posterior samples from the ensemble distributions to reflect \n    model uncertainty. In addition to GLMs, the package supports survival models including: (1) accelerated failure time \n    (AFT) models, (2) piecewise exponential (PWE) models, i.e., proportional hazards models with piecewise constant\n    baseline hazards, and (3) mixture cure rate models that assume a common probability of cure across subjects, paired \n    with a PWE model for the non-cured population. Functions for computing marginal log-likelihoods under each implemented \n    prior are also included. The package compiles all the 'CmdStan' models once during installation using the 'instantiate' package.",
    "version": "0.2.0",
    "maintainer": "Ethan M. Alt <ethanalt@live.unc.edu>",
    "url": "https://github.com/ethan-alt/hdbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14915,
    "package_name": "hdbcp",
    "title": "Bayesian Change Point Detection for High-Dimensional Data",
    "description": "Functions implementing change point detection methods using the maximum pairwise Bayes factor approach.\n    Additionally, the package includes tools for generating simulated datasets for comparing and evaluating change point detection techniques.",
    "version": "1.0.0",
    "maintainer": "JaeHoon Kim <jhkimstat@gmail.com>",
    "url": "https://github.com/JaeHoonKim98/hdbcp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14916,
    "package_name": "hdbinseg",
    "title": "Change-Point Analysis of High-Dimensional Time Series via Binary\nSegmentation",
    "description": "Binary segmentation methods for detecting and estimating multiple change-points in the mean or second-order structure of high-dimensional time series as described in Cho and Fryzlewicz (2014) <doi:10.1111/rssb.12079> and Cho (2016) <doi:10.1214/16-EJS1155>.",
    "version": "1.0.3",
    "maintainer": "Haeran Cho <haeran.cho@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14917,
    "package_name": "hdbma",
    "title": "Bayesian Mediation Analysis with High-Dimensional Data",
    "description": "Mediation analysis is used to identify and quantify intermediate effects from factors that intervene the observed relationship between an exposure/predicting variable and an outcome. We use a Bayesian adaptive lasso method to take care of the hierarchical structures and high dimensional exposures or mediators.",
    "version": "1.0",
    "maintainer": "Qingzhao Yu <qyu@lsuhsc.edu>",
    "url": "https://www.r-project.org,\nhttps://publichealth.lsuhsc.edu/Faculty_pages/qyu/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14927,
    "package_name": "hdftsa",
    "title": "High-Dimensional Functional Time Series Analysis",
    "description": "Offers methods for visualizing, modelling, and forecasting high-dimensional functional time series, also known as functional panel data. Documentation about 'hdftsa' is provided via the paper by Cristian F. Jimenez-Varon, Ying Sun and Han Lin Shang (2024, <doi:10.1080/10618600.2024.2319166>).",
    "version": "1.0",
    "maintainer": "Han Lin Shang <hanlin.shang@mq.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14929,
    "package_name": "hdiVAR",
    "title": "Statistical Inference for Noisy Vector Autoregression",
    "description": "The model is high-dimensional vector autoregression with measurement error, also known as linear gaussian state-space model. Provable sparse expectation-maximization algorithm is provided for the estimation of transition matrix and noise variances. Global and simultaneous testings are implemented for transition matrix with false discovery rate control. For more information, see the accompanying paper: Lyu, X., Kang, J., & Li, L. (2023). \"Statistical inference for high-dimensional vector autoregression with measurement error\", Statistica Sinica.",
    "version": "1.0.2",
    "maintainer": "Xiang Lyu <xianglyu.public@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14930,
    "package_name": "hdm",
    "title": "High-Dimensional Metrics",
    "description": "Implementation of selected high-dimensional statistical and\n    econometric methods for estimation and inference. Efficient estimators and\n    uniformly valid confidence intervals for various low-dimensional causal/\n    structural parameters are provided which appear in high-dimensional\n    approximately sparse models. Including functions for fitting heteroscedastic\n    robust Lasso regressions with non-Gaussian errors and for instrumental variable\n    (IV) and treatment effect estimation in a high-dimensional setting. Moreover,\n    the methods enable valid post-selection inference and rely on a theoretically\n    grounded, data-driven choice of the penalty.\n    Chernozhukov, Hansen, Spindler (2016) <arXiv:1603.01700>.",
    "version": "0.3.2",
    "maintainer": "Martin Spindler <martin.spindler@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14931,
    "package_name": "hdme",
    "title": "High-Dimensional Regression with Measurement Error",
    "description": "Penalized regression for generalized linear models for\n  measurement error problems (aka. errors-in-variables). The package\n  contains a version of the lasso (L1-penalization) which corrects\n  for measurement error (Sorensen et al. (2015) <doi:10.5705/ss.2013.180>). \n  It also contains an implementation of the Generalized Matrix Uncertainty \n  Selector, which is a version the (Generalized) Dantzig Selector for the \n  case of measurement error (Sorensen et al. (2018) <doi:10.1080/10618600.2018.1425626>).",
    "version": "0.6.0",
    "maintainer": "Oystein Sorensen <oystein.sorensen.1985@gmail.com>",
    "url": "https://github.com/osorensen/hdme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14933,
    "package_name": "hdpGLM",
    "title": "Hierarchical Dirichlet Process Generalized Linear Models",
    "description": "Implementation of MCMC algorithms to estimate the Hierarchical Dirichlet Process Generalized Linear Model (hdpGLM) presented in the paper Ferrari (2020) Modeling Context-Dependent Latent Heterogeneity, Political Analysis <DOI:10.1017/pan.2019.13> and <doi:10.18637/jss.v107.i10>.",
    "version": "1.0.5",
    "maintainer": "Diogo Ferrari <diogoferrari@gmail.com>",
    "url": "https://github.com/DiogoFerrari/hdpGLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14935,
    "package_name": "hdqr",
    "title": "Fast Algorithm for Penalized Quantile Regression",
    "description": "Implements an efficient algorithm for fitting the entire regularization path of quantile regression models with elastic-net penalties using a generalized coordinate descent scheme. The framework also supports SCAD and MCP penalties. It is designed for high-dimensional datasets and emphasizes numerical accuracy and computational efficiency. This package implements the algorithms proposed in Tang, Q., Zhang, Y., & Wang, B. (2022) <https://openreview.net/pdf?id=RvwMTDYTOb>.",
    "version": "1.0.2",
    "maintainer": "Qian Tang <qian-tang@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14936,
    "package_name": "hdrcde",
    "title": "Highest Density Regions and Conditional Density Estimation",
    "description": "Computation of highest density regions in one and two dimensions, kernel estimation of univariate density functions conditional on one covariate,and multimodal regression.",
    "version": "3.4",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "url": "https://pkg.robjhyndman.com/hdrcde/,\nhttps://github.com/robjhyndman/hdrcde",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14937,
    "package_name": "hds",
    "title": "Hazard Discrimination Summary",
    "description": "Functions for calculating the hazard discrimination summary and its\n    standard errors, as described in Liang and Heagerty (2016) <doi:10.1111/biom.12628>.",
    "version": "0.8.1",
    "maintainer": "C. Jason Liang <liangcj@gmail.com>",
    "url": "https://github.com/liangcj/hds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14940,
    "package_name": "hdthreshold",
    "title": "Inference on Many Jumps in Nonparametric Panel Regression Models",
    "description": "Provides uniform testing procedures for existence and heterogeneity of threshold \n    effects in high-dimensional nonparametric panel regression models. The package accompanies\n    the paper Chen, Keilbar, Su and Wang (2023) \"Inference on many jumps in nonparametric panel\n    regression models\". arXiv preprint <doi:10.48550/arXiv.2312.01162>.",
    "version": "1.0.0",
    "maintainer": "Georg Keilbar <georg.keilbar@hu-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14989,
    "package_name": "heritability",
    "title": "Marker-Based Estimation of Heritability Using Individual Plant\nor Plot Data",
    "description": "Implements marker-based estimation of heritability when observations on genetically identical replicates are available. These can be either observations on individual plants or plot-level data in a field trial. Heritability can then be estimated using a mixed model for the individual plant or plot data. For comparison, also mixed-model based estimation using genotypic means and estimation of repeatability with ANOVA are implemented. For illustration the package contains several datasets for the model species Arabidopsis thaliana.",
    "version": "1.4",
    "maintainer": "Willem Kruijer <willem.kruijer@wur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14990,
    "package_name": "heritable",
    "title": "Heritability Estimation from Mixed Models",
    "description": "Reporting heritability estimates is an important to quantitative genetics \n  studies and breeding experiments. Here we provide functions to calculate various broad-sense\n  heritabilities from 'asreml' and 'lme4' model objects. All methods we have implemented \n  in this package have extensively discussed in the article by Schmidt et al. (2019) <doi:10.1534/genetics.119.302134>.",
    "version": "0.1.0",
    "maintainer": "Fonti Kar <fonti.kar@anu.edu.au>",
    "url": "https://anu-aagi.github.io/heritable/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14995,
    "package_name": "hesim",
    "title": "Health Economic Simulation Modeling and Decision Analysis",
    "description": "A modular and computationally efficient R package for  \n  parameterizing, simulating, and analyzing health economic simulation \n  models. The package supports cohort discrete time state transition models \n  (Briggs et al. 1998) <doi:10.2165/00019053-199813040-00003>,\n  N-state partitioned survival models (Glasziou et al. 1990)\n  <doi:10.1002/sim.4780091106>, and individual-level continuous \n  time state transition models (Siebert et al. 2012) <doi:10.1016/j.jval.2012.06.014>,\n  encompassing both Markov (time-homogeneous and time-inhomogeneous) and \n  semi-Markov processes. Decision uncertainty from a cost-effectiveness analysis is \n  quantified with standard graphical and tabular summaries of a probabilistic \n  sensitivity analysis (Claxton et al. 2005, Barton et al. 2008) <doi:10.1002/hec.985>, \n  <doi:10.1111/j.1524-4733.2008.00358.x>. Use of C++ and data.table\n  make individual-patient simulation, probabilistic sensitivity analysis, \n  and incorporation of patient heterogeneity fast.",
    "version": "0.5.7",
    "maintainer": "Devin Incerti <devin.incerti@gmail.com>",
    "url": "https://hesim-dev.github.io/hesim/,\nhttps://github.com/hesim-dev/hesim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 14996,
    "package_name": "hetGP",
    "title": "Heteroskedastic Gaussian Process Modeling and Design under\nReplication",
    "description": "Performs Gaussian process regression with heteroskedastic noise following the model by Binois, M., Gramacy, R., Ludkovski, M. (2016) <doi:10.48550/arXiv.1611.05902>, with implementation details in Binois, M. & Gramacy, R. B. (2021) <doi:10.18637/jss.v098.i13>. The input dependent noise is modeled as another Gaussian process. Replicated observations are encouraged as they yield computational savings. Sequential design procedures based on the integrated mean square prediction error and lookahead heuristics are provided, and notably fast update functions when adding new observations.",
    "version": "1.1.8",
    "maintainer": "Mickael Binois <mickael.binois@inria.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15003,
    "package_name": "hetsurrSurv",
    "title": "Assessing Heterogeneity in Surrogacy Using Censored Data",
    "description": "Provides functions to assess and test for heterogeneity in the utility of a surrogate marker with respect to a baseline covariate using censored (survival data), and to test for heterogeneity across multiple time points.  More details are available in Parast et al (2024) <doi:10.1002/sim.10122>. ",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15004,
    "package_name": "hett",
    "title": "Heteroscedastic t-Regression",
    "description": "Functions for the fitting and summarizing of heteroscedastic t-regression.",
    "version": "0.3-3",
    "maintainer": "Julian Taylor <julian.taylor@adelaide.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15006,
    "package_name": "hettreatreg",
    "title": "Heterogeneous Treatment Effects in Regression Analysis",
    "description": "Computes diagnostics for linear regression when treatment effects are heterogeneous.\n    The output of 'hettreatreg' represents ordinary least squares (OLS) \n    estimates of the effect of a binary treatment as a weighted average of the average treatment effect \n    on the treated (ATT) and the average treatment effect on the untreated (ATU). \n    The program estimates the OLS weights on these parameters, computes the associated model diagnostics, \n    and reports the implicit OLS estimate of the average treatment effect (ATE). \n    See Sloczynski (2019), <http://people.brandeis.edu/~tslocz/Sloczynski_paper_regression.pdf>.",
    "version": "0.1.0",
    "maintainer": "Mark McAvoy <mcavoy@brandeis.edu>",
    "url": "https://github.com/tslocz/hettreatreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15007,
    "package_name": "hettx",
    "title": "Fisherian and Neymanian Methods for Detecting and Measuring\nTreatment Effect Variation",
    "description": "Implements methods developed by Ding, Feller, and Miratrix (2016) <doi:10.1111/rssb.12124> <arXiv:1412.5000>,\n    and Ding, Feller, and Miratrix (2018) <doi:10.1080/01621459.2017.1407322> <arXiv:1605.06566>\n    for testing whether there is unexplained variation in treatment effects across observations, and for characterizing\n    the extent of the explained and unexplained variation in treatment effects. The package includes wrapper functions\n    implementing the proposed methods, as well as helper functions for analyzing and visualizing the results of the test.",
    "version": "0.1.3",
    "maintainer": "Ben Fifield <benfifield@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15019,
    "package_name": "hfr",
    "title": "Estimate Hierarchical Feature Regression Models",
    "description": "Provides functions for the estimation, plotting, predicting and cross-validation of hierarchical feature regression models as described in Pfitzinger (2024). Cluster Regularization via a Hierarchical Feature Regression. Econometrics and Statistics (in press). <doi:10.1016/j.ecosta.2024.01.003>.",
    "version": "0.7.1",
    "maintainer": "Johann Pfitzinger <johann.pfitzinger@gmail.com>",
    "url": "https://hfr.residualmetrics.com,\nhttps://github.com/jpfitzinger/hfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15022,
    "package_name": "hglm.data",
    "title": "Data for the 'hglm' Package",
    "description": "This data-only package was created for distributing data used in the examples of the 'hglm' package.",
    "version": "1.0-1",
    "maintainer": "Xia Shen <xia.shen@ki.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15031,
    "package_name": "hht",
    "title": "The Hilbert-Huang Transform: Tools and Methods",
    "description": "Builds on the EMD package to provide additional tools for empirical mode decomposition (EMD) and Hilbert spectral analysis. It also implements the ensemble empirical decomposition (EEMD) and the complete ensemble empirical mode decomposition (CEEMD) methods to avoid mode mixing and intermittency problems found in EMD analysis.  The package comes with several plotting methods that can be used to view intrinsic mode functions, the HHT spectrum, and the Fourier spectrum. ",
    "version": "2.1.6",
    "maintainer": "Daniel C. Bowman <danny.c.bowman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15054,
    "package_name": "highfrequency",
    "title": "Tools for Highfrequency Data Analysis",
    "description": "Provide functionality to manage, clean and match highfrequency\n    trades and quotes data, calculate various liquidity measures, estimate and\n    forecast volatility, detect price jumps and investigate microstructure noise and intraday\n    periodicity. A detailed vignette can be found in the open-access paper \n    \"Analyzing Intraday Financial Data in R: The highfrequency Package\" \n    by Boudt, Kleen, and Sjoerup (2022, <doi:10.18637/jss.v104.i08>). ",
    "version": "1.0.3",
    "maintainer": "Kris Boudt <kris.boudt@ugent.be>",
    "url": "https://github.com/jonathancornelissen/highfrequency",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15068,
    "package_name": "himach",
    "title": "Find Routes for Supersonic Aircraft",
    "description": "For supersonic aircraft, flying subsonic over land,\n    find the best route between airports. Allow for coastal buffer and\n    potentially closed regions. Use a minimal model of aircraft\n    performance: the focus is on time saved versus subsonic flight, rather\n    than on vertical flight profile. For modelling and forecasting, not for planning your\n    flight!",
    "version": "1.0.0",
    "maintainer": "David Marsh <david6marsh@gmail.com>",
    "url": "https://github.com/david6marsh/himach,\nhttps://david6marsh.github.io/himach/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15083,
    "package_name": "historicalborrow",
    "title": "Non-Longitudinal Bayesian Historical Borrowing Models",
    "description": "Historical borrowing in clinical trials can improve\n  precision and operating characteristics. This package supports\n  a hierarchical model and a mixture model to borrow historical\n  control data from other studies to better characterize the\n  control response of the current study. It also quantifies\n  the amount of borrowing through benchmark models (independent\n  and pooled). Some of the methods are discussed by\n  Viele et al. (2013) <doi:10.1002/pst.1589>.",
    "version": "1.1.0",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "url": "https://wlandau.github.io/historicalborrow/,\nhttps://github.com/wlandau/historicalborrow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15084,
    "package_name": "historicalborrowlong",
    "title": "Longitudinal Bayesian Historical Borrowing Models",
    "description": "Historical borrowing in clinical trials can improve\n  precision and operating characteristics. This package supports\n  a longitudinal hierarchical model to borrow historical\n  control data from other studies to better characterize the\n  control response of the current study. It also quantifies\n  the amount of borrowing through longitudinal benchmark models (independent\n  and pooled). The hierarchical model approach to historical borrowing\n  is discussed by Viele et al. (2013) <doi:10.1002/pst.1589>.",
    "version": "0.1.0",
    "maintainer": "William Michael Landau <will.landau.oss@gmail.com>",
    "url": "https://wlandau.github.io/historicalborrowlong/,\nhttps://github.com/wlandau/historicalborrowlong",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15095,
    "package_name": "hlt",
    "title": "Higher-Order Item Response Theory",
    "description": "Higher-order latent trait theory (item response theory). We\n    implement the generalized partial credit model with a second-order latent\n    trait structure. Latent regression can be done on the second-order latent\n    trait. For a pre-print of the methods,\n    see, \"Latent Regression in Higher-Order Item Response Theory with the R\n    Package hlt\" <https://mkleinsa.github.io/doc/hlt_proof_draft_brmic.pdf>.",
    "version": "1.3.1",
    "maintainer": "Michael Kleinsasser <mjkleinsa@gmail.com>",
    "url": "https://github.com/mkleinsa/hlt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15097,
    "package_name": "hmclearn",
    "title": "Fit Statistical Models Using Hamiltonian Monte Carlo",
    "description": "Provide users with a framework to learn the intricacies of the Hamiltonian Monte Carlo algorithm with hands-on experience by tuning and fitting their own models.  All of the code is written in R.  Theoretical references are listed below:.\n    Neal, Radford (2011) \"Handbook of Markov Chain Monte Carlo\" ISBN: 978-1420079418, \n    Betancourt, Michael (2017) \"A Conceptual Introduction to Hamiltonian Monte Carlo\" <arXiv:1701.02434>, \n    Thomas, S., Tu, W. (2020) \"Learning Hamiltonian Monte Carlo in R\" <arXiv:2006.16194>,\n    Gelman, A., Carlin, J. B., Stern, H. S., Dunson, D. B., Vehtari, A., & Rubin, D. B. (2013) \"Bayesian Data Analysis\" ISBN: 978-1439840955, \n    Agresti, Alan (2015) \"Foundations of Linear and Generalized Linear Models ISBN: 978-1118730034, \n    Pinheiro, J., Bates, D. (2006) \"Mixed-effects Models in S and S-Plus\" ISBN: 978-1441903174.",
    "version": "0.0.5",
    "maintainer": "Samuel Thomas <samthoma@iu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15099,
    "package_name": "hmde",
    "title": "Hierarchical Methods for Differential Equations",
    "description": "Wrapper for 'Stan' that offers a number of in-built models to implement a hierarchical Bayesian longitudinal model for repeat observation data. Model choice selects the differential equation that is fit to the observations. Single and multi-individual models are available. O'Brien et al. (2024) <doi:10.1111/2041-210X.14463>.",
    "version": "1.3.1",
    "maintainer": "Tess O'Brien <tess_obrien@fastmail.com>",
    "url": "https://traitecoevo.github.io/hmde/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15101,
    "package_name": "hmix",
    "title": "Hidden Markov Model for Predicting Time Sequences with Mixture\nSampling",
    "description": "An algorithm for time series analysis that leverages hidden Markov models, cluster analysis, and mixture distributions to segment data, detect patterns and predict future sequences.",
    "version": "1.0.2",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/hmix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15109,
    "package_name": "hmstimer",
    "title": "'hms' Based Timer",
    "description": "Tracks elapsed clock time using a `hms::hms()` scalar. \n It was was originally developed to time Bayesian model runs. \n It should not be used to estimate how long extremely fast code takes to execute \n as the package code adds a small time cost.",
    "version": "0.3.0",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://github.com/poissonconsulting/hmstimer,\nhttps://poissonconsulting.github.io/hmstimer/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15117,
    "package_name": "holiglm",
    "title": "Holistic Generalized Linear Models",
    "description": "Holistic generalized linear models (HGLMs) extend generalized linear models (GLMs) by enabling the possibility to add further constraints to the model. The 'holiglm' package simplifies estimating HGLMs using convex optimization. Additional information about the package can be found in the reference manual, the 'README' and the accompanying paper <doi:10.18637/jss.v108.i07>.",
    "version": "1.0.1",
    "maintainer": "Benjamin Schwendinger <benjaminschwe@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15122,
    "package_name": "homals",
    "title": "Gifi Methods for Optimal Scaling",
    "description": "Performs a homogeneity analysis (multiple correspondence analysis) and various extensions. Rank restrictions on the category quantifications can be imposed (nonlinear PCA). The categories are transformed by means of optimal scaling with options for nominal, ordinal, and numerical scale levels (for rank-1 restrictions). Variables can be grouped into sets, in order to emulate regression analysis and canonical correlation analysis. ",
    "version": "1.0-11",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15131,
    "package_name": "hopit",
    "title": "Hierarchical Ordered Probit Models with Application to Reporting\nHeterogeneity",
    "description": "Self-reported health, happiness, attitudes, and other statuses or perceptions are often the subject of biases that may come from different sources. For example, the evaluation of an individual’s own health may depend on previous medical diagnoses, functional status, and symptoms and signs of illness; as on well as life-style behaviors, including contextual social, gender, age-specific, linguistic and other cultural factors (Jylha 2009 <doi:10.1016/j.socscimed.2009.05.013>; Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The hopit package offers versatile functions for analyzing different self-reported ordinal variables, and for helping to estimate their biases. Specifically, the package provides the function to fit a generalized ordered probit model that regresses original self-reported status measures on two sets of independent variables (King et al. 2004 <doi:10.1017/S0003055403000881>; Jurges 2007  <doi:10.1002/hec.1134>; Oksuzyan et al. 2019  <doi:10.1016/j.socscimed.2019.03.002>). The first set of variables (e.g., health variables) included in the regression are individual statuses and characteristics that are directly related to the self-reported variable. In the case of self-reported health, these could be chronic conditions, mobility level, difficulties with daily activities, performance on grip strength tests, anthropometric measures, and lifestyle behaviors. The second set of independent variables (threshold variables) is used to model cut-points between adjacent self-reported response categories as functions of individual characteristics, such as gender, age group, education, and country (Oksuzyan et al. 2019 <doi:10.1016/j.socscimed.2019.03.002>). The model helps to adjust for specific socio-demographic and cultural differences in how the continuous latent health is projected onto the ordinal self-rated measure. The fitted model can be used to calculate an individual predicted latent status variable, a latent index, and standardized latent coefficients; and makes it possible to reclassify a categorical status measure that has been adjusted for inter-individual differences in reporting behavior.",
    "version": "0.11.6",
    "maintainer": "Maciej J. Danko <Maciej.Danko@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15150,
    "package_name": "hpfilter",
    "title": "The One- And Two-Sided Hodrick-Prescott Filter",
    "description": "Provides two functions that implement the one-sided and\n    two-sided versions of the Hodrick-Prescott filter. The one-sided\n    version is a Kalman filter-based implementation, whereas the two-\n    sided version uses sparse matrices for improved efficiency.\n    References:\n    Hodrick, R. J., and Prescott, E. C. (1997) <doi:10.2307/2953682>\n    Mcelroy, T. (2008) <doi:10.1111/j.1368-423X.2008.00230.x>\n    Meyer-Gohde, A. (2010) <https://ideas.repec.org/c/dge/qmrbcd/181.html>\n    For more references, see the vignette.",
    "version": "1.0.2",
    "maintainer": "Alexandru Monahov <alexandru.monahov@proton.me>",
    "url": "https://www.alexandrumonahov.eu.org/projects",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15153,
    "package_name": "hqreg",
    "title": "Regularization Paths for Lasso or Elastic-Net Penalized Huber\nLoss Regression and Quantile Regression",
    "description": "Offers efficient algorithms for fitting regularization paths for lasso or elastic-net penalized regression models with Huber loss, quantile loss or squared loss. Reference: Congrui Yi and Jian Huang (2017) <doi:10.1080/10618600.2016.1256816>.",
    "version": "1.4-1",
    "maintainer": "Congrui Yi <eric.ycr@gmail.com>",
    "url": "https://github.com/CY-dev/hqreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15156,
    "package_name": "hrf",
    "title": "Hemodynamic Response Function",
    "description": "Computes the hemodynamic response function (HRF) for task \n    functional magnetic resonance imaging (fMRI) data. Also includes functions\n    for constructing a design matrix from task fMRI event timings, and for\n    comparing multiple design matrices in a general linear model (GLM). A\n    wrapper function is provided for GLM analysis of CIFTI-format data. Lastly,\n    there are supporting functions which provide visual summaries of the\n    HRFs and design matrices.",
    "version": "0.1.3",
    "maintainer": "Amanda Mejia <mandy.mejia@gmail.com>",
    "url": "https://github.com/mandymejia/hrf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15158,
    "package_name": "hrt",
    "title": "Heteroskedasticity Robust Testing",
    "description": "Functions for testing affine hypotheses on the regression coefficient vector in regression models with heteroskedastic errors: (i) a function for computing various test statistics (in particular using HC0-HC4 covariance estimators based on unrestricted or restricted residuals); (ii) a function for numerically approximating the size of a test based on such test statistics and a user-supplied critical value; and, most importantly, (iii) a function for determining size-controlling critical values for such test statistics and a user-supplied significance level (also incorporating a check of conditions under which such a size-controlling critical value exists). The three functions are based on results in Poetscher and Preinerstorfer (2021) \"Valid Heteroskedasticity Robust Testing\" <doi:10.48550/arXiv.2104.12597>, which will appear as <doi:10.1017/S0266466623000269>.",
    "version": "1.0.2",
    "maintainer": "David Preinerstorfer <david.preinerstorfer@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15179,
    "package_name": "hts",
    "title": "Hierarchical and Grouped Time Series",
    "description": "Provides methods for analysing and forecasting hierarchical and \n    grouped time series. The available forecast methods include bottom-up,\n    top-down, optimal combination reconciliation (Hyndman et al. 2011) \n    <doi:10.1016/j.csda.2011.03.006>, and trace minimization reconciliation\n    (Wickramasuriya et al. 2018) <doi:10.1080/01621459.2018.1448825>.",
    "version": "6.0.3",
    "maintainer": "Earo Wang <earo.wang@gmail.com>",
    "url": "https://pkg.earo.me/hts/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15180,
    "package_name": "htsDegenerateR",
    "title": "Degenerate Hierarchical Time Series Reconciliation",
    "description": "Takes the MinT implementation of the 'hts'<https://cran.r-project.org/package=hts> package and adapts it to allow degenerate hierarchical structures. Instead of the \"nodes\" argument, this function takes an S matrix which is more versatile in the structures it allows. For a demo, see Steinmeister and Pauly (2024)<doi:10.15488/17729>. The MinT algorithm is based on Wickramasuriya et al. (2019)<doi:10.1080/01621459.2018.1448825>.",
    "version": "0.1.0",
    "maintainer": "Louis Steinmeister <louis.steinmeister@udo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15199,
    "package_name": "huge",
    "title": "High-Dimensional Undirected Graph Estimation",
    "description": "Provides a general framework for\n        high-dimensional undirected graph estimation. It integrates\n        data preprocessing, neighborhood screening, graph estimation,\n        and model selection techniques into a pipeline. In\n        preprocessing stage, the nonparanormal(npn) transformation is\n        applied to help relax the normality assumption. In the graph\n        estimation stage, the graph structure is estimated by\n        Meinshausen-Buhlmann graph estimation or the graphical lasso,\n        and both methods can be further accelerated by the lossy\n        screening rule preselecting the neighborhood of each variable\n        by correlation thresholding. We target on high-dimensional data\n        analysis usually d >> n, and the computation is\n        memory-optimized using the sparse matrix output. We also\n        provide a computationally efficient approach, correlation\n        thresholding graph estimation. Three\n        regularization/thresholding parameter selection methods are\n        included in this package: (1)stability approach for\n        regularization selection (2) rotation information criterion (3)\n        extended Bayesian information criterion which is only available\n        for the graphical lasso.",
    "version": "1.3.5",
    "maintainer": "Haoming Jiang <jianghm.ustc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15208,
    "package_name": "hurdlr",
    "title": "Zero-Inflated and Hurdle Modelling Using Bayesian Inference",
    "description": "When considering count data, it is often the case that many more zero counts than would be expected of some given distribution are observed. It is well established that data such as this can be reliably modelled using zero-inflated or hurdle distributions, both of which may be applied using the functions in this package. Bayesian analysis methods are used to best model problematic count data that cannot be fit to any typical distribution. The package functions are flexible and versatile, and can be applied to varying count distributions, parameter estimation with or without explanatory variable information, and are able to allow for multiple hurdles as it is also not uncommon that count data have an abundance of large-number observations which would be considered outliers of the typical distribution. In lieu of throwing out data or misspecifying the typical distribution, these extreme observations can be applied to a second, extreme distribution. With the given functions of this package, such a two-hurdle model may be easily specified in order to best manage data that is both zero-inflated and over-dispersed.",
    "version": "0.1",
    "maintainer": "Earvin Balderama <ebalderama@luc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15209,
    "package_name": "hurricaneexposure",
    "title": "Explore and Map County-Level Hurricane Exposure in the United\nStates",
    "description": "Allows users to create time series of tropical storm\n    exposure histories for chosen counties for a number of hazard metrics\n    (wind, rain, distance from the storm, etc.). This package interacts\n    with data available through the 'hurricaneexposuredata' package, which\n    is available in a 'drat' repository. To access this data package, see the \n    instructions at <https://github.com/geanders/hurricaneexposure>. \n    The size of the 'hurricaneexposuredata' package is\n    approximately 20 MB. This work was supported in part by grants from the National\n    Institute of Environmental Health Sciences (R00ES022631), the National Science\n    Foundation (1331399), and a NASA Applied Sciences Program/Public Health Program\n    Grant (NNX09AV81G).",
    "version": "0.1.1",
    "maintainer": "Brooke Anderson <brooke.anderson@colostate.edu>",
    "url": "https://github.com/geanders/hurricaneexposure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15212,
    "package_name": "huxtable",
    "title": "Easily Create and Style Tables for LaTeX, HTML and Other Formats",
    "description": "Creates styled tables for data presentation. Export to HTML, LaTeX,\n  RTF, 'Word', 'Excel', 'PowerPoint', 'typst', SVG and PNG. Simple, modern \n  interface to manipulate borders, size, position, captions, colours, \n  text styles and number formatting. Table cells can span multiple rows and/or columns.\n  Includes  a 'huxreg' function to create regression tables, and 'quick_*' \n  one-liners to print tables to a new document.",
    "version": "5.8.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "url": "https://hughjonesd.github.io/huxtable/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15219,
    "package_name": "hwwntest",
    "title": "Tests of White Noise using Wavelets",
    "description": "Provides methods to test whether time series is consistent\n\twith white noise. Two new tests based on Haar wavelets and general\n\twavelets described by Nason and Savchev (2014)\n\t<doi:10.1002/sta4.69> are provided and, for comparison purposes\n\tthis package also implements the\n\tB test of Bartlett (1967) <doi:10.2307/2333850>. Functionality\n\tis provided to compute an approximation to the theoretical\n\tpower of the general wavelet test in the case of general\n\tARMA alternatives.",
    "version": "1.3.2",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15220,
    "package_name": "hySAINT",
    "title": "Hybrid Genetic and Simulated Annealing Algorithm for High\nDimensional Linear Models with Interaction Effects",
    "description": "We provide a stage-wise selection method using genetic algorithms, designed to efficiently identify main and two-way interactions within high-dimensional linear regression models. Additionally, it implements simulated annealing algorithm during the mutation process. The relevant paper can be found at: Ye, C.,and Yang,Y. (2019) <doi:10.1109/TIT.2019.2913417>.",
    "version": "1.2.1",
    "maintainer": "Leiyue Li <lli289.git@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15232,
    "package_name": "hydroEvents",
    "title": "Extract Event Statistics in Hydrologic Time Series",
    "description": "Events from individual hydrologic time series are extracted, and events\n are matched across multiple time series. The package has been applied in studies\n such as Wasko and Guo (2022) <doi:10.1002/hyp.14563> and Mohammadpour Khoie,\n Guo and Wasko (2025) <doi:10.1016/j.envsoft.2025.106521>.",
    "version": "0.13.0",
    "maintainer": "Conrad Wasko <conrad.wasko@gmail.com>",
    "url": "https://github.com/conradwasko/hydroEvents",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15233,
    "package_name": "hydroGOF",
    "title": "Goodness-of-Fit Functions for Comparison of Simulated and\nObserved Hydrological Time Series",
    "description": "S3 functions implementing both statistical and graphical goodness-of-fit measures between observed and simulated values, mainly oriented to be used during the calibration, validation, and application of hydrological models. Missing values in observed and/or simulated values can be removed before computations. Comments / questions / collaboration of any kind are very welcomed.",
    "version": "0.6-0.1",
    "maintainer": "Mauricio Zambrano-Bigiarini <mzb.devel@gmail.com>",
    "url": "https://github.com/hzambran/hydroGOF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15241,
    "package_name": "hydroroute",
    "title": "Trace Longitudinal Hydropeaking Waves",
    "description": "Implements an empirical approach referred to as PeakTrace which uses multiple hydrographs to detect and follow hydropower plant-specific hydropeaking waves at the sub-catchment scale and to describe how hydropeaking flow parameters change along the longitudinal flow path. The method is based on the identification of associated events and uses (linear) regression models to describe translation and retention processes between neighboring hydrographs. Several regression model results are combined to arrive at a power plant-specific model. The approach is proposed and validated in Greimel et al. (2022) <doi:10.1002/rra.3978>. The identification of associated events is based on the event detection implemented in 'hydropeak'.",
    "version": "0.1.2",
    "maintainer": "Bettina Grün <Bettina.Gruen@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15243,
    "package_name": "hydrostats",
    "title": "Hydrologic Indices for Daily Time Series Data",
    "description": "Calculates a suite of hydrologic indices for daily time series data that are widely used in hydrology and stream ecology.",
    "version": "0.2.9",
    "maintainer": "Nick Bond <n.bond@latrobe.edu.au>",
    "url": "https://github.com/nickbond/hydrostats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15264,
    "package_name": "hypr",
    "title": "Hypothesis Matrix Translation",
    "description": "Translation between experimental null hypotheses, hypothesis matrices, and contrast matrices as used in linear regression models. The package is based on the method described in Schad et al. (2019) <doi:10.1016/j.jml.2019.104038> and Rabe et al. (2020) <doi:10.21105/joss.02134>.",
    "version": "0.2.8",
    "maintainer": "Maximilian M. Rabe <maximilian.rabe@uni-potsdam.de>",
    "url": "https://maxrabe.com/hypr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15267,
    "package_name": "hystar",
    "title": "Fit the Hysteretic Threshold Autoregressive Model",
    "description": "Estimate parameters of the hysteretic threshold autoregressive\n    (HysTAR) model, using conditional least squares.\n    In addition, you can generate time series data from the HysTAR model.\n    For details, see Li, Guan, Li and Yu (2015) <doi:10.1093/biomet/asv017>.",
    "version": "1.0.0",
    "maintainer": "Daan de Jong <daandejong94@gmail.com>",
    "url": "https://github.com/daandejongen/hystar/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15272,
    "package_name": "iAR",
    "title": "Irregularly Observed Autoregressive Models",
    "description": "Data sets, functions and scripts with examples to implement autoregressive models for irregularly observed time series. The models available in this package are the irregular autoregressive model (Eyheramendy et al.(2018) <doi:10.1093/mnras/sty2487>), the complex irregular autoregressive model (Elorrieta et al.(2019) <doi:10.1051/0004-6361/201935560>) and the bivariate irregular autoregressive model (Elorrieta et al.(2021) <doi:10.1093/mnras/stab1216>).",
    "version": "1.3.2",
    "maintainer": "Elorrieta Felipe <felipe.elorrieta@usach.cl>",
    "url": "https://github.com/felipeelorrieta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15275,
    "package_name": "iBART",
    "title": "Iterative Bayesian Additive Regression Trees Descriptor\nSelection Method",
    "description": "A statistical method based on Bayesian Additive Regression Trees with Global \n    Standard Error Permutation Test (BART-G.SE) for descriptor selection \n    and symbolic regression. It finds the symbolic formula of the regression function \n    y=f(x) as described in Ye, Senftle, and Li (2023) <arXiv:2110.10195>.",
    "version": "1.0.0",
    "maintainer": "Shengbin Ye <sy53@rice.edu>",
    "url": "https://github.com/mattsheng/iBART",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15279,
    "package_name": "iBST",
    "title": "Improper Bagging Survival Tree",
    "description": "Fit a full or subsampling bagging survival tree on a mixture of population (susceptible and nonsusceptible)\n             using either a pseudo R2 criterion or an adjusted Logrank criterion. The predictor is \n             evaluated using the Out Of Bag Integrated Brier Score (IBS) and several scores of importance\n             are computed for variable selection. The thresholds values for variable selection are \n             computed using a nonparametric permutation test. \n             See 'Cyprien Mbogning' and 'Philippe Broet' (2016)<doi:10.1186/s12859-016-1090-x> for \n             an overview about the methods implemented in this package.",
    "version": "1.2",
    "maintainer": "Cyprien Mbogning <cyprien.mbogning@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15313,
    "package_name": "iNZightRegression",
    "title": "Tools for Exploring Regression Models with 'iNZight'",
    "description": "Provides a suite of functions to use with regression models, including summaries, residual plots, and factor comparisons. Used as part of the Model Fitting module of 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions.",
    "version": "1.3.5",
    "maintainer": "Tom Elliott <tom.elliott@auckland.ac.nz>",
    "url": "https://inzight.nz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15314,
    "package_name": "iNZightTS",
    "title": "Time Series for 'iNZight'",
    "description": "Provides a collection of functions for working with time series data, including functions for drawing, decomposing, and forecasting. Includes capabilities to compare multiple series and fit both additive and multiplicative models. Used by 'iNZight', a graphical user interface providing easy exploration and visualisation of data for students of statistics, available in both desktop and online versions. Holt (1957) <doi:10.1016/j.ijforecast.2003.09.015>, Winters (1960) <doi:10.1287/mnsc.6.3.324>, Cleveland, Cleveland, & Terpenning (1990) \"STL: A Seasonal-Trend Decomposition Procedure Based on Loess\".",
    "version": "2.0.2",
    "maintainer": "Tom Elliott <tom.elliott@auckland.ac.nz>",
    "url": "https://inzight.nz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15320,
    "package_name": "iRegression",
    "title": "Regression Methods for Interval-Valued Variables",
    "description": "Contains some important regression methods for interval-valued variables. For each method, it is available the fitted values, residuals and some goodness-of-fit measures.",
    "version": "1.2.1",
    "maintainer": "Eufrasio de A. Lima Neto <eufrasio@de.ufpb.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15341,
    "package_name": "iWeigReg",
    "title": "Improved Methods for Causal Inference and Missing Data Problems",
    "description": "Improved methods based on inverse probability weighting\n        and outcome regression for causal inference and missing data\n        problems.",
    "version": "1.1",
    "maintainer": "Zhiqiang Tan <ztan@stat.rutgers.edu>",
    "url": "http://www.stat.rutgers.edu/~ztan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15354,
    "package_name": "ibdfindr",
    "title": "HMM Toolkit for Inferring IBD Segments from SNP Genotypes",
    "description": "Implements continuous-time hidden Markov models (HMMs) to\n    infer identity-by-descent (IBD) segments shared by two individuals\n    from their single-nucleotide polymorphism (SNP) genotypes. Provides\n    posterior probabilities at each marker (forward-backward algorithm),\n    prediction of IBD segments (Viterbi algorithm), and functions for\n    visualising results. Supports both autosomal data and X-chromosomal\n    data.",
    "version": "0.3.1",
    "maintainer": "Magnus Dehli Vigeland <m.d.vigeland@medisin.uio.no>",
    "url": "https://github.com/magnusdv/ibdfindr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15368,
    "package_name": "icRSF",
    "title": "A Modified Random Survival Forest Algorithm",
    "description": "Implements a modification to the Random Survival Forests algorithm for obtaining variable importance in high dimensional datasets. The proposed algorithm is appropriate for settings in which a silent event is observed through sequentially administered, error-prone self-reports or laboratory based diagnostic tests.  The modified algorithm incorporates a formal likelihood framework that accommodates sequentially administered, error-prone self-reports or laboratory based diagnostic tests. The original Random Survival Forests algorithm is modified by the introduction of a new splitting criterion based on a likelihood ratio test statistic.",
    "version": "1.2",
    "maintainer": "Hui Xu <huix@schoolph.umass.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15374,
    "package_name": "iccCounts",
    "title": "Intraclass Correlation Coefficient for Count Data",
    "description": "Estimates the intraclass correlation coefficient (ICC) for count data to assess repeatability (intra-methods concordance) and concordance (between-method concordance). In the concordance setting, the ICC is equivalent to the concordance correlation coefficient estimated by variance components. The ICC is estimated using the estimates from generalized linear mixed models. The within-subjects distributions considered are: Poisson; Negative Binomial with additive and proportional extradispersion; Zero-Inflated Poisson; and Zero-Inflated Negative Binomial with additive and proportional extradispersion. The statistical methodology used to estimate the ICC with count data can be found in Carrasco (2010) <doi:10.1111/j.1541-0420.2009.01335.x>.",
    "version": "1.1.2",
    "maintainer": "Josep L. Carrasco <jlcarrasco@ub.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15384,
    "package_name": "icenReg",
    "title": "Regression Models for Interval Censored Data",
    "description": "Regression models for interval censored data. Currently supports\n    Cox-PH, proportional odds, and accelerated failure time models. Allows for\n    semi and fully parametric models (parametric only for accelerated failure\n    time models) and Bayesian parametric models. Includes functions for easy visual\n    diagnostics of model fits and imputation of censored data.",
    "version": "2.0.16",
    "maintainer": "Clifford Anderson-Bergman <pistacliffcho@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15385,
    "package_name": "icensBKL",
    "title": "Accompanion to the Book on Interval Censoring by Bogaerts,\nKomarek, and Lesaffre",
    "description": "Contains datasets and several smaller functions suitable for analysis of interval-censored data. The package complements the book Bogaerts, Komárek and Lesaffre (2017, ISBN: 978-1-4200-7747-6) \"Survival Analysis with Interval-Censored Data: A Practical Approach\" <https://www.routledge.com/Survival-Analysis-with-Interval-Censored-Data-A-Practical-Approach-with/Bogaerts-Komarek-Lesaffre/p/book/9781420077476>. Full R code related to the examples presented in the book can be found at <https://ibiostat.be/online-resources/icbook/supplemental>. Packages mentioned in the \"Suggests\" section are used in those examples.",
    "version": "1.5",
    "maintainer": "Arnošt Komárek <arnost.komarek@mff.cuni.cz>",
    "url": "https://ibiostat.be/online-resources/icbook/supplemental/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15386,
    "package_name": "icensmis",
    "title": "Study Design and Data Analysis in the Presence of Error-Prone\nDiagnostic Tests and Self-Reported Outcomes",
    "description": "We consider studies in which information from error-prone\n    diagnostic tests or self-reports are gathered sequentially to determine the\n    occurrence of a silent event. Using a likelihood-based approach\n    incorporating the proportional hazards assumption, we provide functions to\n    estimate the survival distribution and covariate effects. We also provide \n    functions for power and sample size calculations for this setting.\n    Please refer to Xiangdong Gu, Yunsheng Ma, and Raji Balasubramanian (2015) \n    <doi: 10.1214/15-AOAS810>, Xiangdong Gu and Raji Balasubramanian (2016) \n    <doi: 10.1002/sim.6962>, Xiangdong Gu, Mahlet G Tadesse, Andrea S Foulkes,\n    Yunsheng Ma, and Raji Balasubramanian (2020) <doi: 10.1186/s12911-020-01223-w>.",
    "version": "1.5.0",
    "maintainer": "Xiangdong Gu <ustcgxd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15403,
    "package_name": "icpack",
    "title": "Survival Analysis of Interval-Censored Data",
    "description": "Survival analysis of interval-censored data with proportional hazards, and an explicit smooth estimate of the baseline log-hazard with P-splines. ",
    "version": "0.1.0",
    "maintainer": "Hein Putter <h.putter@lumc.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15412,
    "package_name": "idealstan",
    "title": "Robust Measurement with 'Stan'",
    "description": "Offers item-response theory (IRT) ideal-point measurement modeling for diverse distributions, missing data, and over-time variation. Full and approximate Bayesian sampling with 'Stan' (<https://mc-stan.org/>).",
    "version": "0.99.2",
    "maintainer": "",
    "url": "https://github.com/saudiwin/idealstan",
    "exports": [],
    "topics": ["bayesian", "irt", "rstats", "stan"],
    "score": "NA",
    "stars": 56
  },
  {
    "id": 15413,
    "package_name": "ideamdb",
    "title": "Easy Manipulation of IDEAM's Climatological Data",
    "description": "Time series plain text conversion and data visualization. It allows\n    to transform IDEAM (Instituto de Hidrologia, Meteorologia y Estudios Ambientales)\n    daily series from plain text to CSV files or data frames in R. Additionally,\n    it is possible to obtain exploratory graphs from times series. IDEAM’s data\n    is freely delivered under formal request through the official web page\n    <http://www.ideam.gov.co/solicitud-de-informacion>.",
    "version": "0.0.9",
    "maintainer": "Luz Maria Morales <lummoralesgo@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15434,
    "package_name": "ieTest",
    "title": "Indirect Effects Testing Methods in Mediation Analysis",
    "description": "Used in testing if the indirect effect from linear regression mediation analysis is equal to 0. Includes established methods such as the Sobel Test, Joint Significant test (maxP), and tests based off the distribution of the Product or Normal Random Variables. Additionally, this package adds more powerful tests based on Intersection-Union theory. These tests are the S-Test, the ps-test, and the ascending squares test. These new methods are uniformly more powerful than maxP, which is more powerful than Sobel and less anti-conservative than the Product of Normal Random Variables. These methods are explored by Kidd and Lin, (2024) <doi:10.1007/s12561-023-09386-6> and Kidd et al., (2025) <doi:10.1007/s10260-024-00777-7>. ",
    "version": "2.0",
    "maintainer": "John Kidd <jkidd@uvu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15444,
    "package_name": "ifo",
    "title": "Client for the Ifo Institute Time Series",
    "description": "Download ifo business survey data and more time series from\n    ifo institute <https://www.ifo.de/en/ifo-time-series>.",
    "version": "0.2.2",
    "maintainer": "Maximilian Mücke <muecke.maximilian@gmail.com>",
    "url": "https://m-muecke.github.io/ifo/, https://github.com/m-muecke/ifo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15445,
    "package_name": "ifpd",
    "title": "Indonesia Food Prices Data",
    "description": "Imputation of missing values using the last observation carried forward technique on Indonesia food prices data that is time series data. Also, this technique applies imputation to data whose dates do not appear directly. So that the series assumptions in the time series data are met.",
    "version": "0.1.0",
    "maintainer": "Fadhlul Mubarak <mubarakfadhlul@gmail.com>",
    "url": "https://github.com/mubarakfadhlul/ifpd",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15450,
    "package_name": "igcop",
    "title": "Computational Tools for the IG and IGL Copula Families",
    "description": "Compute distributional quantities for an\n    Integrated Gamma (IG) or Integrated Gamma Limit (IGL) copula, such\n    as a cdf and density. Compute corresponding conditional quantities\n    such as the cdf and quantiles. Generate\n    data from an IG or IGL copula. See the vignette for formulas,\n    or for a derivation, see Coia, V (2017) \"Forecasting of Nonlinear \n    Extreme Quantiles Using Copula Models.\" PhD Dissertation, \n    The University of British Columbia.",
    "version": "1.0.2",
    "maintainer": "Vincenzo Coia <vincenzo.coia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15452,
    "package_name": "iglm",
    "title": "Regression under Network Interference",
    "description": "An implementation of generalized linear models (GLMs) for studying relationships among attributes in connected populations, where responses of connected units can be dependent, as introduced by Fritz et al. (2025) <doi:10.1080/01621459.2025.2565851>. 'igml' extends GLMs for independent responses to dependent responses and can be used for studying spillover in connected populations and other network-mediated phenomena.",
    "version": "1.1",
    "maintainer": "Cornelius Fritz <corneliusfritz2010@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15472,
    "package_name": "imabc",
    "title": "Incremental Mixture Approximate Bayesian Computation (IMABC)",
    "description": "Provides functionality to perform a likelihood-free method for estimating the parameters of complex models\n    that results in a simulated sample from the posterior distribution of model parameters given targets. The method begins\n    with a accept/reject approximate bayes computation (ABC) step applied to a sample of points from the prior distribution\n    of model parameters. Accepted points result in model predictions that are within the initially specified tolerance\n    intervals around the target points. The sample is iteratively updated by drawing additional points from a mixture of\n    multivariate normal distributions, accepting points within tolerance intervals. As the algorithm proceeds, the\n    acceptance intervals are narrowed. The algorithm returns a set of points and sampling weights that account for the\n    adaptive sampling scheme. For more details see Rutter, Ozik, DeYoreo, and Collier (2018) <arXiv:1804.02090>.",
    "version": "1.0.0",
    "maintainer": "\"Christopher, E. Maerzluft\" <cmaerzlu@rand.org>",
    "url": "https://github.com/carolyner/imabc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15535,
    "package_name": "imputeR",
    "title": "A General Multivariate Imputation Framework",
    "description": "Multivariate Expectation-Maximization (EM) based imputation framework that offers several different algorithms. These include regularisation methods like Lasso and Ridge regression, tree-based models and dimensionality reduction methods like PCA and PLS.",
    "version": "2.2",
    "maintainer": "Steffen Moritz <steffen.moritz10@gmail.com>",
    "url": "http://github.com/SteffenMoritz/imputeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15536,
    "package_name": "imputeREE",
    "title": "Impute Missing Rare Earth Element Data in Zircon",
    "description": "Set of functions to impute missing rare earth data, calculate La \n    and Pr concentrations and Ce anomalies in zircons based on the\n    Chondrite-Onuma and Chondrite-Lattice of Carrasco-Godoy and \n    Campbell (2023) <doi:10.1007/s00410-023-02025-9> and the Logarithmic regression from  \n    Zhong et al. (2019) <doi:10.1007/s00710-019-00682-y>.",
    "version": "0.0.5",
    "maintainer": "Carlos Carrasco Godoy <carlos.carrasco@anu.edu.au>",
    "url": "https://github.com/cicarrascog/imputeREE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15538,
    "package_name": "imputeTestbench",
    "title": "Test Bench for the Comparison of Imputation Methods",
    "description": "Provides a test bench for the comparison of missing data imputation \n    methods in uni-variate time series. Imputation methods are compared using \n    different error metrics. Proposed imputation methods and alternative error \n    metrics can be used.",
    "version": "3.0.3",
    "maintainer": "Marcus W. Beck <mbafs2012@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15548,
    "package_name": "inca",
    "title": "Integer Calibration",
    "description": "Specific functions are provided for rounding real weights to integers and performing an integer programming algorithm for calibration problems. These functions are useful for census-weights adjustments, survey calibration, or for performing linear regression with integer parameters <https://www.nass.usda.gov/Education_and_Outreach/Reports,_Presentations_and_Conferences/reports/New_Integer_Calibration_%20Procedure_2016.pdf>. This research was supported in part by the U.S. Department of Agriculture, National Agriculture Statistics Service. The findings and conclusions in this publication are those of the authors and should not be construed to represent any official USDA, or US Government determination or policy.",
    "version": "0.1.0",
    "maintainer": "Luca Sartore <drwolf85@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15556,
    "package_name": "incubate",
    "title": "Parametric Time-to-Event Analysis with Variable Incubation\nPhases",
    "description": "Fit parametric models for time-to-event data that show an initial\n    'incubation period', i.e., a variable delay phase where the hazard is zero. The\n    delayed Weibull distribution serves as foundational data model. The\n    specific method of 'MPSE' (maximum product of spacings estimation) and MLE-based methods are used for parameter\n    estimation. Bootstrap confidence intervals for parameters and significance\n    tests in a two group setting are provided.",
    "version": "1.3.0",
    "maintainer": "Matthias Kuhn <matthias.kuhn@tu-dresden.de>",
    "url": "https://gitlab.com/imb-dev/incubate/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15562,
    "package_name": "india",
    "title": "Influence Diagnostics in Statistical Models",
    "description": "Set of routines for influence diagnostics by using case-deletion in ordinary least \n    squares, nonlinear regression [Ross (1987). <doi:10.2307/3315198>], ridge estimation [Walker and Birch (1988). <doi:10.1080/00401706.1988.10488370>] \n    and least absolute deviations (LAD) regression [Sun and Wei (2004). <doi:10.1016/j.spl.2003.08.018>].",
    "version": "0.1-1",
    "maintainer": "Felipe Osorio <faosorios.stat@gmail.com>",
    "url": "https://github.com/faosorios/india",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15565,
    "package_name": "indirect",
    "title": "Elicitation of Independent Conditional Means Priors for\nGeneralised Linear Models",
    "description": "Functions are provided to facilitate prior elicitation for Bayesian generalised linear models using independent conditional means priors. The package supports the elicitation of multivariate normal priors for generalised linear models. The approach can be applied to indirect elicitation for a generalised linear model that is linear in the parameters. The package is designed such that the facilitator executes functions within the R console during the elicitation session to provide graphical and numerical feedback at each design point. Various methodologies for eliciting fractiles (equivalently, percentiles or quantiles) are supported, including versions of the approach of Hosack et al. (2017) <doi:10.1016/j.ress.2017.06.011>. For example, experts may be asked to provide central credible intervals that correspond to a certain probability. Or experts may be allowed to vary the probability allocated to the central credible interval for each design point. Additionally, a median may or may not be elicited. ",
    "version": "0.2.1",
    "maintainer": "Geoff Hosack <geoff.hosack@csiro.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15580,
    "package_name": "inferference",
    "title": "Methods for Causal Inference with Interference",
    "description": "Provides methods for estimating causal effects in the presence of interference described in  B. Saul and M. Hugdens (2017) <doi:10.18637/jss.v082.i02>. Currently it implements the inverse-probability weighted (IPW) estimators proposed by E.J. Tchetgen Tchetgen and T.J. Vanderweele (2012) <doi:10.1177/0962280210386779>.",
    "version": "1.0.3",
    "maintainer": "Bradley Saul <bradleysaul@fastmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15633,
    "package_name": "intRegGOF",
    "title": "Integrated Regression Goodness of Fit",
    "description": "Performs Goodness of Fit for regression models \n  using Integrated Regression method. Works for several \n  different fitting techniques.",
    "version": "0.85-5",
    "maintainer": "Jorge Luis Ojeda Cabrera <jojeda@unizar.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15640,
    "package_name": "intccr",
    "title": "Semiparametric Competing Risks Regression under Interval\nCensoring",
    "description": "Semiparametric regression models on the cumulative incidence function for interval-censored competing risks data as described in Bakoyannis, Yu, & Yiannoutsos (2017) /doi{10.1002/sim.7350} and the models with missing event types as described in Park, Bakoyannis, Zhang, & Yiannoutsos (2021) \\doi{10.1093/biostatistics/kxaa052}. The proportional subdistribution hazards model (Fine-Gray model), the proportional odds model, and other models that belong to the class of semiparametric generalized odds rate transformation models.",
    "version": "3.0.4",
    "maintainer": "Jun Park <jun.park@alumni.iu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15652,
    "package_name": "interactions",
    "title": "Comprehensive, User-Friendly Toolkit for Probing Interactions",
    "description": "A suite of functions for conducting and interpreting analysis",
    "version": "1.2.0",
    "maintainer": "",
    "url": "https://github.com/jacob-long/interactions",
    "exports": [],
    "topics": ["interactions", "moderation", "r", "r-package", "rstats", "social-sciences", "statistics"],
    "score": "NA",
    "stars": 133
  },
  {
    "id": 15659,
    "package_name": "interflex",
    "title": "Multiplicative Interaction Models Diagnostics and Visualization",
    "description": "Performs diagnostic tests of multiplicative interaction models and plots non-linear marginal effects of a treatment on an outcome across different values of a moderator.",
    "version": "1.2.8",
    "maintainer": "Yiqing Xu <yiqingxu@stanford.edu>",
    "url": "https://yiqingxu.org/packages/interflex/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15672,
    "package_name": "interpretR",
    "title": "Binary Classifier and Regression Model Interpretation Functions",
    "description": "Compute permutation- based performance measures and create partial\n    dependence plots for (cross-validated) 'randomForest' and 'ada' models.",
    "version": "0.2.5",
    "maintainer": "Michel Ballings <michel.ballings@GMail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15676,
    "package_name": "interval",
    "title": "Weighted Logrank Tests and NPMLE for Interval Censored Data",
    "description": "Functions to fit nonparametric survival curves, plot them, and perform logrank or Wilcoxon type tests [see Fay and Shaw <doi:10.18637/jss.v036.i02>].",
    "version": "1.1-1.0",
    "maintainer": "Michael P. Fay <mfay@niaid.nih.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15678,
    "package_name": "intervalpsych",
    "title": "Analyzing Interval Data in Psychometrics",
    "description": "Implements the Interval Consensus Model (ICM) for analyzing continuous bounded interval-valued responses in psychometrics using 'Stan' for 'Bayesian' estimation. Provides functions for transforming interval data to simplex representations, fitting item response theory (IRT) models with isometric log-ratio (ILR) and sum log-ratio (SLR) link functions, and visualizing results. The package enables aggregation and analysis of interval-valued response data commonly found in psychological measurement and related disciplines. Based on Kloft et al. (2024) <doi:10.31234/osf.io/dzvw2>.",
    "version": "0.1.0",
    "maintainer": "Matthias Kloft <kloft.dev+intervalpsych@gmail.com>",
    "url": "https://matthiaskloft.github.io/intervalpsych/,\nhttps://github.com/matthiaskloft/intervalpsych",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15685,
    "package_name": "intradayModel",
    "title": "Modeling and Forecasting Financial Intraday Signals",
    "description": "Models, analyzes, and forecasts financial intraday signals. This package\n    currently supports a univariate state-space model for intraday trading volume provided\n    by Chen (2016) <doi:10.2139/ssrn.3101695>.",
    "version": "0.0.1",
    "maintainer": "Daniel P. Palomar <daniel.p.palomar@gmail.com>",
    "url": "https://github.com/convexfi/intradayModel,\nhttps://www.danielppalomar.com,\nhttps://dx.doi.org/10.2139/ssrn.3101695",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15687,
    "package_name": "intrinsicFRP",
    "title": "An R Package for Factor Model Asset Pricing",
    "description": "Functions for evaluating and testing asset pricing models, including\n    estimation and testing of factor risk premia, selection of \"strong\" risk \n    factors (factors having nonzero population correlation with test asset\n    returns), heteroskedasticity and autocorrelation robust covariance matrix\n    estimation and testing for model misspecification and identification. \n    The functions for estimating and testing factor risk \n    premia implement the Fama-MachBeth (1973) <doi:10.1086/260061> two-pass \n    approach, the misspecification-robust approaches of Kan-Robotti-Shanken (2013) \n    <doi:10.1111/jofi.12035>, and the approaches based on tradable factor risk\n    premia of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683>. The \n    functions for selecting the \"strong\" risk factors are based on the Oracle\n    estimator of Quaini-Trojani-Yuan (2023) <doi:10.2139/ssrn.4574683> and the \n    factor screening procedure of Gospodinov-Kan-Robotti (2014) <doi:10.2139/ssrn.2579821>. \n    The functions for evaluating model misspecification implement the HJ\n    model misspecification distance of Kan-Robotti (2008) <doi:10.1016/j.jempfin.2008.03.003>,\n    which is a modification of the prominent Hansen-Jagannathan (1997)\n    <doi:10.1111/j.1540-6261.1997.tb04813.x> distance.\n    The functions for testing model identification \n    specialize the Kleibergen-Paap (2006) <doi:10.1016/j.jeconom.2005.02.011> \n    and the Chen-Fang (2019) <doi:10.1111/j.1540-6261.1997.tb04813.x> rank test \n    to the regression coefficient matrix of test asset returns on risk factors.\n    Finally, the function for heteroskedasticity and autocorrelation robust \n    covariance estimation implements the Newey-West (1994) <doi:10.2307/2297912>\n    covariance estimator.",
    "version": "2.1.0",
    "maintainer": "Alberto Quaini <alberto91quaini@gmail.com>",
    "url": "https://github.com/a91quaini/intrinsicFRP",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15693,
    "package_name": "invGauss",
    "title": "Threshold Regression that Fits the (Randomized Drift) Inverse\nGaussian Distribution to Survival Data",
    "description": "Fits the (randomized drift) inverse Gaussian distribution to survival data. The model is described in Aalen OO, Borgan O, Gjessing HK. Survival and Event History Analysis. A Process Point of View. Springer, 2008. It is based on describing time to event as the barrier hitting time of a Wiener process, where drift towards the barrier has been randomized with a Gaussian distribution. The model allows covariates to influence starting values of the Wiener process and/or average drift towards a barrier, with a user-defined choice of link functions. ",
    "version": "1.2",
    "maintainer": "Hakon K. Gjessing <hakon.gjessing@uib.no>",
    "url": "http://www.uib.no/smis/gjessing/projects/invgauss/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15695,
    "package_name": "invctr",
    "title": "Infix Functions For Vector Operations",
    "description": "Vector operations between grapes: An infix-only package! The 'invctr' functions perform common and less common operations on vectors, data frames matrices and list objects:\n    - Extracting a value (range), or, finding the indices of a value (range).\n    - Trimming, or padding a vector with a value of your choice.\n    - Simple polynomial regression.\n    - Set and membership operations.\n    - General check & replace function for NAs, Inf and other values.",
    "version": "0.2.0",
    "maintainer": "Fred Hasselman <fred.hasselman@ru.nl>",
    "url": "https://github.com/FredHasselman/invctr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15703,
    "package_name": "invitroTKstats",
    "title": "In Vitro Toxicokinetic Data Processing and Analysis Pipeline",
    "description": "A set of tools for processing and analyzing in vitro toxicokinetic\n             measurements in a standardized and reproducible pipeline. The package\n             was developed to perform frequentist and Bayesian estimation on a\n             variety of in vitro toxicokinetic measurements including -- but not\n             limited to -- chemical fraction unbound in the presence of plasma\n             (f_up), intrinsic hepatic clearance (Clint,\n             uL/min/million hepatocytes), and membrane permeability for\n             oral absorption (Caco2). The methods provided\n             by the package were described in Wambaugh et al. (2019) <doi:10.1093/toxsci/kfz205>.",
    "version": "0.0.13",
    "maintainer": "Sarah E. Davidson-Fritz <davidsonfritz.sarah@epa.gov>",
    "url": "https://github.com/USEPA/invitroTKstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15710,
    "package_name": "iosmooth",
    "title": "Functions for Smoothing with Infinite Order Flat-Top Kernels",
    "description": "Density, spectral density, and regression estimation using infinite\n    order flat-top kernels.",
    "version": "0.94",
    "maintainer": "Timothy L. McMurry <tmcmurry@virginia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15730,
    "package_name": "iperform",
    "title": "Time Series Performance",
    "description": "A tool to calculate the performance of a time series in a specific date or period. It is more intended for data analysis in the fields of finance, banking, telecommunications or operational marketing.",
    "version": "0.0.3",
    "maintainer": "Patrick Ilunga <patrick.ilunga@unikin.ac.cd>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15738,
    "package_name": "ipmr",
    "title": "Integral Projection Models",
    "description": "Flexibly implements Integral Projection Models using a\n  mathematical(ish) syntax. This package will not help with the vital rate\n  modeling process, but will help convert those regression models into an\n  IPM. 'ipmr' handles density dependence and environmental stochasticity, with a\n  couple of options for implementing the latter. In addition, provides functions\n  to avoid unintentional eviction of individuals from models. Additionally, \n  provides model diagnostic tools, plotting functionality, \n  stochastic/deterministic simulations, and analysis tools.\n  Integral projection models are described in depth by Easterling et al. (2000) \n  <doi:10.1890/0012-9658(2000)081[0694:SSSAAN]2.0.CO;2>, Merow et al. (2013) \n  <doi:10.1111/2041-210X.12146>, Rees et al. (2014) <doi:10.1111/1365-2656.12178>,\n  and Metcalf et al. (2015) <doi:10.1111/2041-210X.12405>. \n  Williams et al. (2012) <doi:10.1890/11-2147.1> discuss the problem of \n  unintentional eviction.",
    "version": "0.0.7",
    "maintainer": "Sam Levin <levisc8@gmail.com>",
    "url": "https://padrinoDB.github.io/ipmr/,\nhttps://github.com/padrinoDB/ipmr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15739,
    "package_name": "ipolygrowth",
    "title": "Individual Growth Curve Parameter Calculation using Polynomial\nFunctions",
    "description": "Calculation of key bacterial growth curve parameters using fourth degree polynomial functions. \n    Six growth curve parameters are provided including peak growth rate, doubling time, lag time, maximum growth, and etc.\n    'ipolygrowth' takes time series data from individual biological samples (with technical replicates) or multiple samples.",
    "version": "1.0.0",
    "maintainer": "Jifan Wang <wjifan@hotmail.com>",
    "url": "https://github.com/kivanvan/ipolygrowth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15741,
    "package_name": "ipred",
    "title": "Improved Predictors",
    "description": "Improved predictive models by indirect classification and\n  bagging for classification, regression and survival problems \n  as well as resampling based estimators of prediction error. ",
    "version": "0.9-15",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15784,
    "package_name": "isni",
    "title": "Index of Local Sensitivity to Nonignorability",
    "description": "The current version provides functions to compute, print and summarize the Index of Sensitivity to Nonignorability (ISNI) in the generalized linear model for independent data, and in the marginal multivariate Gaussian model and the mixed-effects models for continuous and binary longitudinal/clustered data. It allows for arbitrary patterns of missingness in the regression outcomes  caused by dropout and/or intermittent missingness. One can compute the sensitivity index without estimating any nonignorable models or positing specific magnitude of nonignorability. Thus ISNI provides a simple quantitative assessment of how robust the standard estimates assuming missing at random  is with respect to the assumption of ignorability. For a tutorial, download at <https://huixie.people.uic.edu/Research/ISNI_R_tutorial.pdf>.\tFor more details, see Troxel Ma and Heitjan (2004) and Xie and Heitjan (2004) <doi:10.1191/1740774504cn005oa> and Ma Troxel and Heitjan (2005) <doi:10.1002/sim.2107> and  Xie (2008) <doi:10.1002/sim.3117> and  Xie (2012) <doi:10.1016/j.csda.2010.11.021> and Xie and Qian (2012) <doi:10.1002/jae.1157>.",
    "version": "1.3",
    "maintainer": "Hui Xie <huixie@uic.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15787,
    "package_name": "isoSurv",
    "title": "Isotonic Regression on Survival Analysis",
    "description": "Nonparametric estimation on survival analysis under order-restrictions.",
    "version": "0.3.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15798,
    "package_name": "isodistrreg",
    "title": "Isotonic Distributional Regression (IDR)",
    "description": "Distributional regression under stochastic order restrictions for\n    numeric and binary response variables and partially ordered covariates. See\n    Henzi, Ziegel, Gneiting (2020) <arXiv:1909.03725>.",
    "version": "0.1.0",
    "maintainer": "Alexander Henzi <henzi.alexander@gmail.com>",
    "url": "https://github.com/AlexanderHenzi/isodistrreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15808,
    "package_name": "isotonic.pen",
    "title": "Penalized Isotonic Regression in one and two dimensions",
    "description": "Given a response y and a one- or two-dimensional predictor, the isotonic regression estimator is calculated with the usual orderings.  ",
    "version": "1.0",
    "maintainer": "Mary Meyer <meyer@stat.colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15818,
    "package_name": "itdr",
    "title": "Integral Transformation Methods for SDR in Regression",
    "description": "The itdr() routine allows  for the estimation of sufficient dimension reduction subspaces in univariate regression such as the central mean subspace or central subspace in regression. This is achieved using Fourier transformation methods proposed by Zhu and Zeng (2006) <doi:10.1198/016214506000000140>, convolution transformation methods proposed by Zeng and Zhu (2010) <doi:10.1016/j.jmva.2009.08.004>, and iterative Hessian transformation methods proposed by Cook and Li (2002) <doi:10.1214/aos/1021379861>. Additionally, mitdr() function provides optimal estimators for sufficient dimension reduction subspaces in multivariate regression by optimizing a discrepancy function using a Fourier transform approach proposed by Weng and Yin (2022) <doi:10.5705/ss.202020.0312>, and selects the sufficient variables using Fourier transform sparse inverse regression estimators proposed by Weng (2022) <doi:10.1016/j.csda.2021.107380>.",
    "version": "2.0.1",
    "maintainer": "Tharindu P. De Alwis <talwis@wpi.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15820,
    "package_name": "iterLap",
    "title": "Approximate Probability Densities by Iterated Laplace\nApproximations",
    "description": "The iterLap (iterated Laplace approximation) algorithm approximates a\n             general (possibly non-normalized) probability density on R^p, by repeated\n             Laplace approximations to the difference between current approximation \n             and true density (on log scale). The final approximation is a mixture of\n             multivariate normal distributions and might be used for example as a\n             proposal distribution for importance sampling (eg in Bayesian applications). \n             The algorithm can be seen as a computational generalization of the Laplace \n             approximation suitable for skew or multimodal densities.",
    "version": "1.1-4",
    "maintainer": "Bjoern Bornkamp <bbnkmp@mail.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15834,
    "package_name": "its.analysis",
    "title": "Running Interrupted Time Series Analysis",
    "description": "Two functions for running and then post-estimating an Interrupted Time Series Analysis model. This is a solution for running time series analyses on temporally short data. See English (2019) 'The its.analysis R package - Modelling short time series data' <https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3398189> for an overview of the method.",
    "version": "1.6.0",
    "maintainer": "Patrick English <p.english@exeter.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15835,
    "package_name": "itsadug",
    "title": "Interpreting Time Series and Autocorrelated Data Using GAMMs",
    "description": "GAMM (Generalized Additive Mixed Modeling; Lin & Zhang, 1999)\n    as implemented in the R package 'mgcv' (Wood, S.N., 2006; 2011) is a nonlinear\n    regression analysis which is particularly useful for time course data such as\n    EEG, pupil dilation, gaze data (eye tracking), and articulography recordings,\n    but also for behavioral data such as reaction times and response data. As time\n    course measures are sensitive to autocorrelation problems, GAMMs implements\n    methods to reduce the autocorrelation problems. This package includes functions\n    for the evaluation of GAMM models (e.g., model comparisons, determining regions\n    of significance, inspection of autocorrelational structure in residuals)\n    and interpreting of GAMMs (e.g., visualization of complex interactions, and\n    contrasts).",
    "version": "2.5",
    "maintainer": "Jacolien van Rij <j.c.van.rij@rug.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15838,
    "package_name": "itsmr",
    "title": "Time Series Analysis Using the Innovations Algorithm",
    "description": "Provides functions for modeling and forecasting time series data. Forecasting is based on the innovations algorithm. A description of the innovations algorithm can be found in the textbook \"Introduction to Time Series and Forecasting\" by Peter J. Brockwell and Richard A. Davis.",
    "version": "1.11",
    "maintainer": "George Weigt <9634295@gmail.com>",
    "url": "https://georgeweigt.github.io/itsmr-refman.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15843,
    "package_name": "ivdesc",
    "title": "Profiling Compliers and Non-Compliers for Instrumental Variable\nAnalysis",
    "description": "Estimating the mean and variance of a covariate for the complier, never-taker and always-taker subpopulation in the context of instrumental variable estimation. This package implements the method described in Marbach and Hangartner (2020) <doi:10.1017/pan.2019.48> and Hangartner, Marbach, Henckel, Maathuis, Kelz and Keele (2021) <doi:10.48550/arXiv.2103.06328>.",
    "version": "1.1.2",
    "maintainer": "Moritz Marbach <m.marbach@ucl.ac.uk>",
    "url": "https://github.com/sumtxt/ivdesc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15845,
    "package_name": "ivdoctr",
    "title": "Ensures Mutually Consistent Beliefs When Using IVs",
    "description": "Uses data and researcher's beliefs on measurement error and\n    instrumental variable (IV) endogeneity to generate the space of consistent \n    beliefs across measurement error, instrument endogeneity, and instrumental \n    relevance for IV regressions. \n    Package based on DiTraglia and Garcia-Jimeno (2020) <doi:10.1080/07350015.2020.1753528>.",
    "version": "1.0.1",
    "maintainer": "Mallick Hossain <emallickhossain@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15848,
    "package_name": "ivmodel",
    "title": "Statistical Inference and Sensitivity Analysis for Instrumental\nVariables Model",
    "description": "Carries out instrumental variable\n    estimation of causal effects, including power analysis, sensitivity analysis,\n    and diagnostics. See Kang, Jiang, Zhao, and Small (2020) <http://pages.cs.wisc.edu/~hyunseung/> for details.",
    "version": "1.9.1",
    "maintainer": "Hyunseung Kang <hyunseung@stat.wisc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15849,
    "package_name": "ivmte",
    "title": "Instrumental Variables: Extrapolation by Marginal Treatment\nEffects",
    "description": "The marginal treatment effect was introduced by Heckman and\n    Vytlacil (2005) <doi:10.1111/j.1468-0262.2005.00594.x> to provide a\n    choice-theoretic interpretation to instrumental variables models that\n    maintain the monotonicity condition of Imbens and Angrist (1994)\n    <doi:10.2307/2951620>. This interpretation can be used to extrapolate from\n    the compliers to estimate treatment effects for other subpopulations. This\n    package provides a flexible set of methods for conducting this\n    extrapolation. It allows for parametric or nonparametric sieve estimation,\n    and allows the user to maintain shape restrictions such as monotonicity. The\n    package operates in the general framework developed by Mogstad, Santos and\n    Torgovitsky (2018) <doi:10.3982/ECTA15463>, and accommodates either point\n    identification or partial identification (bounds). In the partially\n    identified case, bounds are computed using either linear programming\n    or quadratically constrained quadratic programming. Support for\n    four solvers is provided. Gurobi and the Gurobi R API\n    can be obtained from <http://www.gurobi.com/index>. CPLEX can be obtained\n    from <https://www.ibm.com/analytics/cplex-optimizer>. CPLEX R APIs 'Rcplex'\n    and 'cplexAPI' are available from CRAN. MOSEK and the MOSEK R API can be\n    obtained from <https://www.mosek.com/>. The lp_solve library is freely\n    available from <http://lpsolve.sourceforge.net/5.5/>, and is included when\n    installing its API 'lpSolveAPI', which is available from CRAN.",
    "version": "1.4.0",
    "maintainer": "Joshua Shea <jkcshea@uchicago.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15852,
    "package_name": "ivreg",
    "title": "Instrumental-Variables Regression by '2SLS', '2SM', or '2SMM',\nwith Diagnostics",
    "description": "Instrumental variable estimation for linear models by two-stage least-squares (2SLS) regression or by robust-regression via M-estimation (2SM) or MM-estimation (2SMM). The main ivreg() model-fitting function is designed to provide a workflow as similar as possible to standard lm() regression. A wide range of methods is provided for fitted ivreg model objects, including extensive functionality for computing and graphing regression diagnostics in addition to other standard model tools.",
    "version": "0.6-5",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "https://zeileis.github.io/ivreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15855,
    "package_name": "ivx",
    "title": "Robust Econometric Inference",
    "description": "Drawing statistical inference on the coefficients\n    of a short- or long-horizon predictive regression with persistent\n    regressors by using the IVX method of Magdalinos and Phillips (2009)\n    <doi:10.1017/S0266466608090154> and Kostakis, Magdalinos and\n    Stamatogiannis (2015) <doi:10.1093/rfs/hhu139>.",
    "version": "1.1.1",
    "maintainer": "Kostas Vasilopoulos <k.vasilopoulo@gmail.com>",
    "url": "https://github.com/kvasilopoulos/ivx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15858,
    "package_name": "iwmm",
    "title": "Importance weighted moment matching",
    "description": "iwmm provides functions for adaptive importance sampling.",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/topipa/iwmm",
    "exports": [],
    "topics": ["bayesian", "bayesian-data-analysis", "bayesian-methods", "r", "r-package", "stan"],
    "score": "NA",
    "stars": 8
  },
  {
    "id": 15861,
    "package_name": "jSDM",
    "title": "Joint Species Distribution Models",
    "description": "Fits joint species distribution models ('jSDM') in a\n    hierarchical Bayesian framework (Warton and al. 2015\n    <doi:10.1016/j.tree.2015.09.007>). The Gibbs sampler is written in\n    'C++'. It uses 'Rcpp', 'Armadillo' and 'GSL' to maximize\n    computation efficiency.",
    "version": "0.2.7",
    "maintainer": "Ghislain Vieilledent <ghislain.vieilledent@cirad.fr>",
    "url": "https://ecology.ghislainv.fr/jSDM/,\nhttps://github.com/ghislainv/jSDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15864,
    "package_name": "jab.adverse.reactions",
    "title": "Possible Adverse Events/Reactions from the\nVaccinations/Experimental Gene Therapies",
    "description": "Provides data about the possible adverse events/reactions\n    resulting from being injected with a vaccine/experimental gene therapy.\n    Currently, this data set only includes information from six reference\n    sources. Refer to the CITATION.cff file for the complete citations of\n    the reference sources. For information about vaccination$/immunization$\n    hazards, visit <https://www.questionuniverse.com/rethink.html#vaccine>,\n    <https://www.ecoccs.com/healing.html#vaccines>,\n    <https://www.questionuniverse.com/rethink_current_crisis.html#cov_vaccin>,\n    and <https://www.questionuniverse.com/vaccination.html>.",
    "version": "1.0.3",
    "maintainer": "Irucka Embry <iembry@ecoccs.com>",
    "url": "https://gitlab.com/iembry/jab.adverse.reactions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15875,
    "package_name": "jage",
    "title": "Estimation of Developmental Age",
    "description": "Bayesian methods for estimating developmental age from ordinal dental data. For an explanation of the model used, see Konigsberg (2015) <doi:10.3109/03014460.2015.1045430>. For details on the conditional correlation correction, see Sgheiza (2022) <doi:10.1016/j.forsciint.2021.111135>. Dental scoring is based on Moorrees, Fanning, and Hunt (1963) <doi:10.1177/00220345630420062701>.",
    "version": "0.1.0",
    "maintainer": "Valerie Sgheiza <v.skate.za@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15876,
    "package_name": "jaggR",
    "title": "Supporting Files and Functions for the Book Bayesian Modelling\nwith 'JAGS'",
    "description": "All the data and functions used to produce the book. We do not expect\n    most people to use the package for any other reason than to get simple access to the\n    'JAGS' model files, the data, and perhaps run some of the simple examples.\n    The authors of the book are David Lucy (now sadly deceased) and James Curran. It is \n    anticipated that a manuscript will be provided to Taylor and Francis around February 2020, with\n    bibliographic details to follow at that point. Until such time, further information\n    can be obtained by emailing James Curran.",
    "version": "0.1.1",
    "maintainer": "James Curran <j.curran@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15877,
    "package_name": "jagsUI",
    "title": "A Wrapper Around 'rjags' to Streamline 'JAGS' Analyses",
    "description": "A set of wrappers around 'rjags' functions to run Bayesian analyses in 'JAGS' (specifically, via 'libjags').  A single function call can control adaptive, burn-in, and sampling MCMC phases, with MCMC chains run in sequence or in parallel. Posterior distributions are automatically summarized (with the ability to exclude some monitored nodes if desired) and functions are available to generate figures based on the posteriors (e.g., predictive check plots, traceplots). Function inputs, argument syntax, and output format are nearly identical to the 'R2WinBUGS'/'R2OpenBUGS' packages to allow easy switching between MCMC samplers.",
    "version": "1.6.3",
    "maintainer": "Ken Kellner <contact@kenkellner.com>",
    "url": "https://kenkellner.com/jagsUI/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15878,
    "package_name": "jagshelper",
    "title": "Extracting and Visualizing Output from 'jagsUI'",
    "description": "Tools are provided to streamline Bayesian analyses in 'JAGS' using \n    the 'jagsUI' package.  Included are functions for extracting output in \n    simpler format, functions for streamlining assessment of convergence, and \n    functions for producing summary plots of output.  Also included is a \n    function that provides a simple template for running 'JAGS' from 'R'.\n    Referenced materials can be found at <DOI:10.1214/ss/1177011136>.",
    "version": "0.4.1",
    "maintainer": "Matt Tyers <matttyersstat@gmail.com>",
    "url": "https://github.com/mbtyers/jagshelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15880,
    "package_name": "jalcal",
    "title": "Convert Between Jalaali (Persian or Solar Hijri) and Gregorian\nCalendar Dates",
    "description": "The Jalaali calendar, also known as the Persian or Solar Hijri \n    calendar, is the official calendar of Iran and Afghanistan. It starts on \n    Nowruz, the spring equinox, and follows an astronomical system for \n    determining leap years. Each year consists of 365 or 366 days, divided into \n    12 months. This package provides functions for converting dates between the \n    Jalaali and Gregorian calendars. The conversion calculations are based on \n    the work of Kazimierz M. Borkowski (1996) (<doi:10.1007/BF00055188>), who \n    used an analytical model of Earth's motion to compute equinoxes from AD 550 \n    to 3800 and determine leap years based on Tehran time.",
    "version": "0.3.0",
    "maintainer": "Abdollah Jalilian <stat4aj@gmail.com>",
    "url": "https://github.com/jalilian/jalcal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15910,
    "package_name": "jlmerclusterperm",
    "title": "Cluster-Based Permutation Analysis for Densely Sampled Time Data",
    "description": "An implementation of fast cluster-based permutation analysis\n    (CPA) for densely-sampled time data developed in Maris & Oostenveld,\n    2007 <doi:10.1016/j.jneumeth.2007.03.024>. Supports (generalized,\n    mixed-effects) regression models for the calculation of timewise\n    statistics. Provides both a wholesale and a piecemeal interface to the\n    CPA procedure with an emphasis on interpretability and diagnostics.\n    Integrates 'Julia' libraries 'MixedModels.jl' and 'GLM.jl' for\n    performance improvements, with additional functionalities for\n    interfacing with 'Julia' from 'R' powered by the 'JuliaConnectoR'\n    package.",
    "version": "1.1.4",
    "maintainer": "June Choe <jchoe001@gmail.com>",
    "url": "https://github.com/yjunechoe/jlmerclusterperm,\nhttps://yjunechoe.github.io/jlmerclusterperm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15919,
    "package_name": "jmv",
    "title": "The 'jamovi' Analyses",
    "description": "A suite of common statistical methods such as descriptives,\n    t-tests, ANOVAs, regression, correlation matrices, proportion tests,\n    contingency tables, and factor analysis. This package is also useable from\n    the 'jamovi' statistical spreadsheet (see <https://www.jamovi.org> for more\n    information).",
    "version": "2.7.7",
    "maintainer": "Jonathon Love <jon@thon.cc>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15928,
    "package_name": "joineRML",
    "title": "Joint Modelling of Multivariate Longitudinal Data and Time-to-Event",
    "description": "Fits the joint model proposed by Henderson and colleagues (2000)",
    "version": "0.4.7",
    "maintainer": "",
    "url": "https://github.com/graemeleehickey/joineRML",
    "exports": [],
    "topics": ["armadillo", "biostatistics", "clinical-trials", "cox", "dynamic", "joint-models", "longitudinal-data", "multivariate-analysis", "multivariate-data", "multivariate-longitudinal-data", "prediction", "r-package", "rcpp", "regression-models", "statistics", "survival"],
    "score": "NA",
    "stars": 33
  },
  {
    "id": 15931,
    "package_name": "joint.Cox",
    "title": "Joint Frailty-Copula Models for Tumour Progression and Death in\nMeta-Analysis",
    "description": "Fit survival data and perform dynamic prediction under joint frailty-copula models for tumour progression and death.\n Likelihood-based methods are employed for estimating model parameters, where the baseline hazard functions are modeled by the cubic M-spline or the Weibull model.\n The methods are applicable for meta-analytic data containing individual-patient information from several studies.\n Survival outcomes need information on both terminal event time (e.g., time-to-death) and non-terminal event time (e.g., time-to-tumour progression).\n Methodologies were published in\n Emura et al. (2017) <doi:10.1177/0962280215604510>, Emura et al. (2018) <doi:10.1177/0962280216688032>,\n Emura et al. (2020) <doi:10.1177/0962280219892295>, Shinohara et al. (2020) <doi:10.1080/03610918.2020.1855449>,\n Wu et al. (2020) <doi:10.1007/s00180-020-00977-1>, and Emura et al. (2021) <doi:10.1177/09622802211046390>.\n See also the book of Emura et al. (2019) <doi:10.1007/978-981-13-3516-7>.\n Survival data from ovarian cancer patients are also available.",
    "version": "3.16",
    "maintainer": "Takeshi Emura <takeshiemura@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15941,
    "package_name": "jointseg",
    "title": "Joint Segmentation of Multivariate (Copy Number) Signals",
    "description": "Methods for fast segmentation of multivariate\n    signals into piecewise constant profiles and for generating realistic\n    copy-number profiles. A typical application is the joint segmentation of total\n    DNA copy numbers and allelic ratios obtained from Single Nucleotide Polymorphism\n    (SNP) microarrays in cancer studies. The methods are described in Pierre-Jean, \n    Rigaill and Neuvial (2015) <doi:10.1093/bib/bbu026>.",
    "version": "1.0.3",
    "maintainer": "Morgane Pierre-Jean <mpierrejean.pro@gmail.com>",
    "url": "https://github.com/mpierrejean/jointseg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15943,
    "package_name": "jollofR",
    "title": "Small Area Population Estimation by Demographics",
    "description": "Automatic disaggregation of small-area population estimates by demographic groups (e.g., age, sex, race, marital status, educational level, etc) along with the estimates of uncertainty, using advanced Bayesian statistical modelling approaches based on integrated nested Laplace approximation (INLA) Rue et al. (2009) <doi:10.1111/j.1467-9868.2008.00700.x> and stochastic partial differential equation (SPDE) methods Lindgren et al. (2011) <doi:10.1111/j.1467-9868.2011.00777.x>. The package implements hierarchical Bayesian modeling frameworks for small area estimation as described in Leasure et al. (2020) <doi:10.1073/pnas.1913050117> and Nnanatu et al. (2025) <doi:10.1038/s41467-025-59862-4>.",
    "version": "0.6.5",
    "maintainer": "Chibuzor Christopher Nnanatu <cc.nnanatu@soton.ac.uk>",
    "url": "https://github.com/wpgp/jollofR/, https://wpgp.github.io/jollofR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15984,
    "package_name": "jtdm",
    "title": "Joint Modelling of Functional Traits",
    "description": "Fitting and analyzing a Joint Trait Distribution Model. The Joint Trait Distribution Model is implemented in the Bayesian framework using conjugate priors and posteriors, thus guaranteeing fast inference. In particular the package computes joint probabilities and multivariate confidence intervals, and enables the investigation of how they depend on the environment through partial response curves. The method implemented by the package is described in Poggiato et al. (2023) <doi:10.1111/geb.13706>.",
    "version": "0.1-3",
    "maintainer": "Giovanni Poggiato <giov.poggiato@gmail.com>",
    "url": "https://github.com/giopogg/jtdm, https://giopogg.github.io/jtdm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15986,
    "package_name": "jtools",
    "title": "Analysis and Presentation of Social Scientific Data",
    "description": "This is a collection of tools for more efficiently understanding \n  and sharing the results of (primarily) regression analyses. There are also a\n  number of miscellaneous functions for statistical and programming purposes. \n  Support for models produced by the survey and lme4 packages are points of \n  emphasis.",
    "version": "2.3.0",
    "maintainer": "Jacob A. Long <jacob.long@sc.edu>",
    "url": "https://jtools.jacob-long.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15987,
    "package_name": "jubilee",
    "title": "Forecasting Long-Term Growth of the U.S. Stock Market and\nBusiness Cycles",
    "description": "A long-term forecast model called \"Jubilee-Tectonic model\" is implemented to forecast future returns of the U.S. stock market, Treasury yield, and gold price. The five-factor model forecasts the 10-year and 20-year future equity returns with high R-squared above 80 percent. It is based on linear growth and mean reversion characteristics in the U.S. stock market. This model also enhances the CAPE model by introducing the hypothesis that there are fault lines in the historical CAPE, which can be calibrated and corrected through statistical learning. In addition, it contains a module for business cycles, optimal interest rate, and recession forecasts.",
    "version": "0.3.3",
    "maintainer": "Stephen H-T. Lihn <stevelihn@gmail.com>",
    "url": "https://ssrn.com/abstract=3156574\nhttps://ssrn.com/abstract=3422278\nhttps://ssrn.com/abstract=3435667",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 15991,
    "package_name": "jumps",
    "title": "Hodrick-Prescott Filter with Jumps",
    "description": "A set of functions to compute the Hodrick-Prescott (HP)\n    filter with automatically selected jumps. The original\n    HP filter extracts a smooth trend from a time series, and our version\n    allows for a small number of automatically identified jumps.\n    See Maranzano and Pelagatti (2024) <doi:10.2139/ssrn.4896170> for details.",
    "version": "1.0",
    "maintainer": "Matteo Pelagatti <matteo.pelagatti@unimib.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16002,
    "package_name": "kader",
    "title": "Kernel Adaptive Density Estimation and Regression",
    "description": "Implementation of various kernel adaptive methods in nonparametric curve\n    estimation like density estimation as introduced in Stute and Srihera (2011)\n    <doi:10.1016/j.spl.2011.01.013> and Eichner and Stute (2013)\n    <doi:10.1016/j.jspi.2012.03.011> for pointwise estimation, and like regression\n    as described in Eichner and Stute (2012) <doi:10.1080/10485252.2012.760737>.",
    "version": "0.0.8",
    "maintainer": "Gerrit Eichner <gerrit.eichner@math.uni-giessen.de>",
    "url": "http://github.com/GerritEichner/kader",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16006,
    "package_name": "kalmanfilter",
    "title": "Kalman Filter",
    "description": "'Rcpp' implementation of the multivariate Kalman filter for state space models that can handle missing values and exogenous data in the observation and state equations. There is also a function to handle time varying parameters.\n  Kim, Chang-Jin and Charles R. Nelson (1999) \"State-Space Models with Regime Switching: Classical and Gibbs-Sampling Approaches with Applications\" <doi:10.7551/mitpress/6444.001.0001><http://econ.korea.ac.kr/~cjkim/>. ",
    "version": "2.2.0",
    "maintainer": "Alex Hubbard <hubbard.alex@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16018,
    "package_name": "kardl",
    "title": "Make Symmetric and Asymmetric ARDL Estimations",
    "description": "Implements estimation procedures for Autoregressive Distributed Lag (ARDL) \n    and Nonlinear ARDL (NARDL) models, which allow researchers to investigate both \n    short- and long-run relationships in time series data under mixed orders of integration. \n    The package supports simultaneous modeling of symmetric and asymmetric regressors, \n    flexible treatment of short-run and long-run asymmetries, and automated equation handling. \n    It includes several cointegration testing approaches such as the Pesaran-Shin-Smith F \n    and t bounds tests, the Banerjee error correction test, and the restricted ECM test, \n    together with diagnostic tools including Wald tests for asymmetry, ARCH tests, and \n    stability procedures (CUSUM and CUSUMQ). Methodological foundations are provided in \n    Pesaran, Shin, and Smith (2001) <doi:10.1016/S0304-4076(01)00049-5> and Shin, Yu, and \n    Greenwood-Nimmo (2014, ISBN:9780123855079).",
    "version": "0.1.1",
    "maintainer": "Huseyin Karamelikli <hakperest@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16029,
    "package_name": "kcmeans",
    "title": "Conditional Expectation Function Estimation with\nK-Conditional-Means",
    "description": "Implementation of the KCMeans regression estimator studied by \n    Wiemann (2023) <arXiv:2311.17021> for expectation function estimation conditional on \n    categorical variables. Computation leverages the unconditional KMeans \n    implementation in one dimension using dynamic programming algorithm of\n    Wang and Song (2011) <doi:10.32614/RJ-2011-015>, allowing for global solutions in time polynomial in \n    the number of observed categories.",
    "version": "0.1.0",
    "maintainer": "Thomas Wiemann <wiemann@uchicago.edu>",
    "url": "https://github.com/thomaswiemann/kcmeans",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16030,
    "package_name": "kcpRS",
    "title": "Kernel Change Point Detection on the Running Statistics",
    "description": "The running statistics of interest is first extracted using a time window which is slid across the time series, and in each window, the running statistics value is computed. KCP (Kernel Change Point) detection proposed by Arlot et al. (2012) <arXiv:1202.3878> is then implemented to flag the change points on the running statistics (Cabrieto et al., 2018, <doi:10.1016/j.ins.2018.03.010>). Change points are located by minimizing a variance criterion based on the pairwise similarities between running statistics which are computed via the Gaussian kernel. KCP can locate change points for a given k number of change points. To determine the optimal k, the KCP permutation test is first carried out by comparing the variance of the running statistics extracted from the original data to that of permuted data. If this test is significant, then there is sufficient evidence for at least one change point in the data. Model selection is then used to determine the optimal k>0.",
    "version": "1.1.1",
    "maintainer": "Kristof Meers <kristof.meers+cran@kuleuven.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16055,
    "package_name": "kergp",
    "title": "Gaussian Process Laboratory",
    "description": "Gaussian process regression with an emphasis on kernels.\n    Quantitative and qualitative inputs are accepted. Some pre-defined\n    kernels are available, such as radial or tensor-sum for\n    quantitative inputs, and compound symmetry, low rank, group kernel\n    for qualitative inputs. The user can define new kernels and\n    composite kernels through a formula mechanism. Useful methods\n    include parameter estimation by maximum likelihood, simulation,\n    prediction and leave-one-out validation.",
    "version": "0.5.8",
    "maintainer": "Olivier Roustant <roustant@insa-toulouse.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16061,
    "package_name": "kernhaz",
    "title": "Kernel Estimation of Hazard Function in Survival Analysis",
    "description": "Producing kernel estimates of the unconditional and conditional hazard\n        function for right-censored data including methods of bandwidth selection.",
    "version": "0.1.0",
    "maintainer": "Iveta Selingerova <selingerova@math.muni.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16068,
    "package_name": "kesernetwork",
    "title": "Visualization of the KESER Network",
    "description": "A shiny app to visualize the knowledge networks for the code concepts. Using co-occurrence matrices of EHR codes from Veterans Affairs (VA) and Massachusetts General Brigham (MGB), the knowledge extraction via sparse embedding regression (KESER) algorithm was used to construct knowledge networks for the code concepts. Background and details about the method can be found at Chuan et al. (2021) <doi:10.1038/s41746-021-00519-z>.",
    "version": "0.1.0",
    "maintainer": "Su-Chun Cheng <scheng@parse-health.org>",
    "url": "https://github.com/celehs/kesernetwork",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16099,
    "package_name": "kinematics",
    "title": "Studying Sampled Trajectories",
    "description": "Allows analyzing time series representing two-dimensional movements.\n    It accepts a data frame with a time (t), horizontal (x) and vertical (y) coordinate as columns,\n    and returns several dynamical properties such as speed, acceleration or curvature.",
    "version": "1.0.0",
    "maintainer": "Pablo Rodriguez-Sanchez <pablo.rodriguez.sanchez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16111,
    "package_name": "kiwisR",
    "title": "A Wrapper for Querying KISTERS 'WISKI' Databases via the 'KiWIS'\nAPI",
    "description": "A wrapper for querying 'WISKI' databases via the 'KiWIS' 'REST' API. 'WISKI' is an 'SQL' relational database \n  used for the collection and storage of water data developed by KISTERS and 'KiWIS' is a 'REST' service that provides\n  access to 'WISKI' databases via HTTP requests (<https://www.kisters.eu/water-weather-and-environment/>). \n  Contains a list of default databases (called 'hubs') and also allows users to provide their own 'KiWIS' URL. \n  Supports the entire query process- from metadata to specific time series values. All data is returned as tidy tibbles.",
    "version": "0.2.4",
    "maintainer": "Ryan Whaley <rdgwhaley@gmail.com>",
    "url": "https://github.com/rywhale/kiwisR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16123,
    "package_name": "km.ci",
    "title": "Confidence Intervals for the Kaplan-Meier Estimator",
    "description": "Computes various confidence intervals for the Kaplan-Meier\n        estimator, namely: Peto's CI, Rothman CI, CI's based on\n        Greenwood's variance, Thomas and Grunkemeier CI and the\n        simultaneous confidence bands by Nair and Hall and Wellner.",
    "version": "0.5-6",
    "maintainer": "Tobias Verbeke <tobias.verbeke@openanalytics.eu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16125,
    "package_name": "kmc",
    "title": "Kaplan-Meier Estimator with Constraints for Right Censored Data\n-- a Recursive Computational Algorithm",
    "description": "Given constraints for right censored data, we use a recursive computational algorithm to calculate the the \"constrained\" Kaplan-Meier estimator. The constraint is assumed given in linear estimating equations or mean functions. We also illustrate how this leads to the empirical likelihood ratio test with right censored data and accelerated failure time model with given coefficients. EM algorithm from emplik package is used to get the initial value. The properties and performance of the EM algorithm is discussed in Mai Zhou and Yifan Yang (2015)<doi: 10.1007/s00180-015-0567-9> and Mai Zhou and Yifan Yang (2017) <doi: 10.1002/wics.1400>. More applications could be found in Mai Zhou (2015) <doi: 10.1201/b18598>.",
    "version": "0.4-2",
    "maintainer": "Yifan Yang <yfyang.86@hotmail.com>",
    "url": "https://github.com/yfyang86/kmc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16130,
    "package_name": "kmi",
    "title": "Kaplan-Meier Multiple Imputation for the Analysis of Cumulative\nIncidence Functions in the Competing Risks Setting",
    "description": "Performs a Kaplan-Meier multiple imputation to recover the missing potential censoring information from competing risks events, so that standard right-censored methods could be applied to the imputed data sets to perform analyses of the cumulative incidence functions (Allignol and Beyersmann, 2010 <doi:10.1093/biostatistics/kxq018>).",
    "version": "0.5.5",
    "maintainer": "Arthur Allignol <arthur.allignol@gmail.com>",
    "url": "https://github.com/aallignol/kmi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16143,
    "package_name": "knnp",
    "title": "Time Series Prediction using K-Nearest Neighbors Algorithm\n(Parallel)",
    "description": "Two main functionalities are provided. One of them is predicting values with \n    k-nearest neighbors algorithm and the other is optimizing the parameters k and d of the algorithm.\n    These are carried out in parallel using multiple threads.",
    "version": "2.0.0",
    "maintainer": "Daniel Bastarrica Lacalle <danibast@ucm.es>",
    "url": "https://github.com/Grasia/knnp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16145,
    "package_name": "knobi",
    "title": "Known-Biomass Production Model (KBPM)",
    "description": "Application of a Known Biomass Production Model (KBPM): (1) the fitting of KBPM to each stock; (2) the estimation of the effects of environmental variability; (3) the retrospective analysis to identify regime shifts; (4) the estimation of forecasts. For more details see Schaefer (1954) <https://www.iattc.org/GetAttachment/62d510ee-13d0-40f2-847b-0fde415476b8/Vol-1-No-2-1954-SCHAEFER,-MILNER-B-_Some-aspects-of-the-dynamics-of-populations-important-to-the-management-of-the-commercial-marine-fisheries.pdf>, Pella and Tomlinson (1969) <https://www.iattc.org/GetAttachment/9865079c-6ee7-40e2-9e30-c4523ff81ddf/Vol-13-No-3-1969-PELLA,-JEROME-J-,-and-PATRICK-K-TOMLINSON_A-generalized-stock-production-model.pdf> and MacCall (2002) <doi:10.1577/1548-8675(2002)022%3C0272:UOKBPM%3E2.0.CO;2>.",
    "version": "0.1.0",
    "maintainer": "Anxo Paz <anxo.paz@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16152,
    "package_name": "kofdata",
    "title": "Get Data from the 'KOF Datenservice' API",
    "description": "Read Swiss time series data from the 'KOF Data' API, <https://datenservice.kof.ethz.ch>. The API provides macro economic time series data mostly about Switzerland. The package itself is a set of wrappers around the 'KOF Datenservice' API. The 'kofdata' package is able to consume public information as well as data that requires an API token. ",
    "version": "0.2.1",
    "maintainer": "Matthias Bannert <bannert@kof.ethz.ch>",
    "url": "https://github.com/KOF-ch/kofdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16160,
    "package_name": "kosel",
    "title": "Variable Selection by Revisited Knockoffs Procedures",
    "description": "Performs variable selection for many types of L1-regularised regressions using the revisited knockoffs procedure. This procedure uses a matrix of knockoffs of the covariates independent from the response variable Y. The idea is to determine if a covariate belongs to the model depending on whether it enters the model before or after its knockoff. The procedure suits for a wide range of regressions with various types of response variables. Regression models available are exported from the R packages 'glmnet' and 'ordinalNet'. Based on the paper linked to via the URL below: Gegout A., Gueudin A., Karmann C. (2019) <arXiv:1907.03153>.",
    "version": "0.0.1",
    "maintainer": "Clemence Karmann <clemence.karmann@gmail.com>",
    "url": "https://arxiv.org/pdf/1907.03153.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16179,
    "package_name": "kssa",
    "title": "Known Sub-Sequence Algorithm",
    "description": "Implements the Known Sub-Sequence Algorithm <doi:10.1016/j.aaf.2021.12.013>, which helps to automatically identify and validate the best method for missing data imputation in a time series. Supports the comparison of multiple state-of-the-art algorithms.",
    "version": "0.0.5",
    "maintainer": "Iván Felipe Benavides <pipeben@gmail.com>",
    "url": "https://github.com/steffenmoritz/kssa,\nhttps://steffenmoritz.github.io/kssa/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16191,
    "package_name": "kyotil",
    "title": "Utility Functions for Statistical Analysis Report Generation and\nMonte Carlo Studies",
    "description": "Helper functions for creating formatted summary of regression models, writing publication-ready tables to latex files, and running Monte Carlo experiments.",
    "version": "2024.11-01",
    "maintainer": "Youyi Fong <youyifong@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16192,
    "package_name": "kza",
    "title": "Kolmogorov-Zurbenko Adaptive Filters",
    "description": "Time Series Analysis including break detection, spectral analysis, KZ Fourier Transforms.",
    "version": "4.1.0.1",
    "maintainer": "Brian Close <brian.close@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16195,
    "package_name": "l1ball",
    "title": "L1-Ball Prior for Sparse Regression",
    "description": "Provides function for the l1-ball prior on high-dimensional regression. The main function, l1ball(), yields posterior samples for linear regression, as introduced by Xu and Duan (2020) <arXiv:2006.01340>.",
    "version": "0.1.0",
    "maintainer": "Maoran Xu <maoranxu@ufl.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16199,
    "package_name": "l2boost",
    "title": "Exploring Friedman's Boosting Algorithm for Regularized Linear\nRegression",
    "description": "Efficient implementation of Friedman's boosting algorithm with\n    l2-loss function and coordinate direction (design matrix columns) basis\n    functions.",
    "version": "1.0.3",
    "maintainer": "John Ehrlinger <john.ehrlinger@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16205,
    "package_name": "label.switching",
    "title": "Relabelling MCMC Outputs of Mixture Models",
    "description": "The Bayesian estimation of mixture models (and more general hidden Markov models) suffers from the label switching phenomenon, making the MCMC output non-identifiable. This package can be used in order to deal with this problem using various relabelling algorithms.",
    "version": "1.8",
    "maintainer": "Panagiotis Papastamoulis <papapast@yahoo.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16230,
    "package_name": "lambdaTS",
    "title": "Variational Seq2Seq Model with Lambda Transformer for Time\nSeries Analysis",
    "description": "Time series analysis based on lambda transformer and variational seq2seq, built on 'Torch'.",
    "version": "1.1",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16236,
    "package_name": "landest",
    "title": "Landmark Estimation of Survival and Treatment Effect",
    "description": "Provides functions to estimate survival and a treatment effect using a landmark estimation approach.",
    "version": "1.2",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16239,
    "package_name": "landmix",
    "title": "Landmark Prediction for Mixture Data",
    "description": "Non-parametric prediction of survival outcomes for mixture data that incorporates covariates and a landmark time. Details are described in Garcia (2021) <doi:10.1093/biostatistics/kxz052>.",
    "version": "1.0",
    "maintainer": "Layla Parast <parast@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16240,
    "package_name": "landmulti",
    "title": "Landmark Prediction with Multiple Short-Term Events",
    "description": "Contains functions for a flexible varying-coefficient landmark model by incorporating multiple short-term events into the prediction of long-term survival probability. For more information about landmark prediction please see Li, W., Ning, J., Zhang, J., Li, Z., Savitz, S.I., Tahanan, A., Rahbar.M.H., (2023+). \"Enhancing Long-term Survival Prediction with Multiple Short-term Events: Landmarking with A Flexible Varying Coefficient Model\".",
    "version": "0.5.0",
    "maintainer": "Wen Li <wen.li@uth.tmc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16258,
    "package_name": "lassopv",
    "title": "Nonparametric P-Value Estimation for Predictors in Lasso",
    "description": "Estimate the p-values for predictors x against target variable y in lasso regression, using the regularization strength when each predictor enters the active set of regularization path for the first time as the statistic. This is based on the assumption that predictors (of the same variance) that (first) become active earlier tend to be more significant. Three null distributions are supported: normal and spherical, which are computed separately for each predictor and analytically under approximation, which aims at efficiency and accuracy for small p-values.",
    "version": "0.2.0",
    "maintainer": "Lingfei Wang <Lingfei.Wang.github@outlook.com>",
    "url": "https://github.com/lingfeiwang/lassopv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16275,
    "package_name": "latticeDensity",
    "title": "Density Estimation and Nonparametric Regression on Irregular\nRegions",
    "description": "Functions that compute the\n  lattice-based density and regression estimators for two-dimensional regions\n  with irregular boundaries and holes.  The density estimation technique is \n  described in Barry and McIntyre (2011) \n  <doi:10.1016/j.ecolmodel.2011.02.016>,\n  while the non-parametric regression technique is described in McIntyre and\n  Barry (2018)  <doi:10.1080/10618600.2017.1375935>.",
    "version": "1.2.7",
    "maintainer": "Ronald Barry <rpbarry@alaska.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16277,
    "package_name": "lava",
    "title": "Latent Variable Models",
    "description": "A general implementation of Structural Equation Models",
    "version": "1.8.3",
    "maintainer": "Klaus K. Holst <klaus@holst.it>",
    "url": "https://github.com/kkholst/lava",
    "exports": [],
    "topics": ["latent-variable-models", "r", "simulation", "statistics", "structural-equation-models"],
    "score": "NA",
    "stars": 33
  },
  {
    "id": 16286,
    "package_name": "lavacreg",
    "title": "Latent Variable Count Regression Models",
    "description": "Estimation of a multi-group count regression models (i.e., Poisson, \n    negative binomial) with latent covariates. This packages provides two extensions\n    compared to ordinary count regression models based on a generalized linear model:\n    First, measurement models for the predictors can be specified allowing to account \n    for measurement error. Second, the count regression can be simultaneously estimated \n    in multiple groups with stochastic group weights. The marginal maximum likelihood \n    estimation is described in Kiefer & Mayer (2020) <doi:10.1080/00273171.2020.1751027>.",
    "version": "0.2-2",
    "maintainer": "Christoph Kiefer <christoph.kiefer@uni-bielefeld.de>",
    "url": "https://github.com/chkiefer/lavacreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16294,
    "package_name": "lazybar",
    "title": "Progress Bar with Remaining Time Forecast Method",
    "description": "A simple progress bar showing estimated remaining time. \n    Multiple forecast methods and user defined forecast method for \n    the remaining time are supported.",
    "version": "0.1.0",
    "maintainer": "Yangzhuoran Yang <Fin.Yang@monash.edu>",
    "url": "https://pkg.yangzhuoranyang.com/lazybar/,\nhttps://github.com/FinYang/lazybar/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16298,
    "package_name": "lazytrade",
    "title": "Learn Computer and Data Science using Algorithmic Trading",
    "description": "Provide sets of functions and methods to learn and practice data science using idea of algorithmic trading.\n    Main goal is to process information within \"Decision Support System\" to come up with analysis or predictions.\n    There are several utilities such as dynamic and adaptive risk management using reinforcement learning\n    and even functions to generate predictions of price changes using pattern recognition deep regression learning.\n    Summary of Methods used: Awesome H2O tutorials: <https://github.com/h2oai/awesome-h2o>, \n    Market Type research of Van Tharp Institute: <https://vantharp.com/>,\n    Reinforcement Learning R package: <https://CRAN.R-project.org/package=ReinforcementLearning>.",
    "version": "0.5.4",
    "maintainer": "Vladimir Zhbanko <vladimir.zhbanko@gmail.com>",
    "url": "https://vladdsm.github.io/myblog_attempt/topics/lazy%20trading/,\nhttps://github.com/vzhomeexperiments/lazytrade",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16303,
    "package_name": "lbm",
    "title": "Log Binomial Regression Model in Exact Method",
    "description": "Fit the log binomial regression model (LBM) by Exact method. Limited parameter space of \n    LBM causes trouble to find admissible estimates and fail to converge when MLE is close to or on \n    the boundary of space. Exact method utilizes the property of boundary vectors to re-parametrize \n    the model without losing any information, and fits the model on the standard fitting algorithm \n    with no convergence issues. ",
    "version": "0.9.0.2",
    "maintainer": "Chao Zhu <zhuchao9966@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16306,
    "package_name": "lbreg",
    "title": "Log-Binomial Regression with Constrained Optimization",
    "description": "Maximum likelihood estimation of log-binomial regression with special functionality when the MLE is on the boundary of the parameter space.",
    "version": "1.3",
    "maintainer": "Bernardo Andrade <bbandrade@unb.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16310,
    "package_name": "lchemix",
    "title": "A Bayesian Multi-Dimensional Couple-Based Latent Risk Model",
    "description": "A joint latent class model where a hierarchical structure exists, with an interaction between female and male partners of a couple. A Bayesian perspective to inference and Markov chain Monte Carlo algorithms to obtain posterior estimates of model parameters. The reference paper is: Beom Seuk Hwang, Zhen Chen, Germaine M.Buck Louis, Paul S. Albert, (2018) \"A Bayesian multi-dimensional couple-based latent risk model with an application to infertility\". Biometrics, 75, 315-325. <doi:10.1111/biom.12972>.",
    "version": "0.1.0",
    "maintainer": "Weimin Zhang <zhangwm@hotmail.com>",
    "url": "http://github.com/wzhang17/lchemix.git,\nhttps://doi.org/10.1111/biom.12972",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16311,
    "package_name": "lcmm",
    "title": "Extended Mixed Models Using Latent Classes and Latent Processes",
    "description": "Estimation of various extensions of the mixed models including latent class mixed models, joint latent class mixed models, mixed models for curvilinear outcomes, mixed models for multivariate longitudinal outcomes using a maximum likelihood estimation method (Proust-Lima, Philipps, Liquet (2017) <doi:10.18637/jss.v078.i02>).",
    "version": "2.2.2",
    "maintainer": "Cecile Proust-Lima <cecile.proust-lima@u-bordeaux.fr>",
    "url": "https://cecileproust-lima.github.io/lcmm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16315,
    "package_name": "lcra",
    "title": "Bayesian Joint Latent Class and Regression Models",
    "description": "For fitting Bayesian joint latent class and regression models using\n    Gibbs sampling. See the documentation for the model.\n    The technical details of the model implemented here are described in Elliott,\n    Michael R., Zhao, Zhangchen, Mukherjee, Bhramar, Kanaya, Alka, Needham,\n    Belinda L., \"Methods to account for uncertainty in latent class assignments when\n    using latent classes as predictors in regression models, with application to\n    acculturation strategy measures\" (2020) In press at Epidemiology\n    <doi:10.1097/EDE.0000000000001139>.",
    "version": "1.1.5",
    "maintainer": "Michael Kleinsasser <biostat-cran-manager@umich.edu>",
    "url": "https://github.com/umich-biostatistics/lcra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16319,
    "package_name": "lda",
    "title": "Collapsed Gibbs Sampling Methods for Topic Models",
    "description": "Implements latent Dirichlet allocation (LDA)\n\t     and related models.  This includes (but is not limited\n\t     to) sLDA, corrLDA, and the mixed-membership stochastic\n\t     blockmodel.  Inference for all of these models is\n\t     implemented via a fast collapsed Gibbs sampler written\n\t     in C.  Utility functions for reading/writing data\n\t     typically used in topic models, as well as tools for\n\t     examining posterior distributions are also included.",
    "version": "1.5.2",
    "maintainer": "Santiago Olivella <olivella@unc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16325,
    "package_name": "lddmm",
    "title": "Longitudinal Drift-Diffusion Mixed Models (LDDMM)",
    "description": "Implementation of the drift-diffusion mixed model for category learning as described in Paulon et al. (2021) <doi:10.1080/01621459.2020.1801448>.",
    "version": "0.4.2",
    "maintainer": "Giorgio Paulon <giorgio.paulon@utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16354,
    "package_name": "leaps",
    "title": "Regression Subset Selection",
    "description": "Regression subset selection, including exhaustive search.",
    "version": "3.2",
    "maintainer": "Thomas Lumley <t.lumley@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16355,
    "package_name": "learnB4SS",
    "title": "Learning materials for the learnB4SS workshop",
    "description": "It provides the learning materials from the workshop 'Learn Bayesian",
    "version": "1.0.7.9000",
    "maintainer": "",
    "url": "https://github.com/learnB4SS/learnB4SS",
    "exports": [],
    "topics": ["bayes", "bayesian", "bayesian-inference", "bayesian-statistics", "brms", "learnr", "r-package", "rstats", "tutorial", "workshop"],
    "score": "NA",
    "stars": 4
  },
  {
    "id": 16369,
    "package_name": "legion",
    "title": "Forecasting Using Multivariate Models",
    "description": "Functions implementing multivariate state space models for purposes of time series analysis and forecasting.\n             The focus of the package is on multivariate models, such as Vector Exponential Smoothing,\n             Vector ETS (Error-Trend-Seasonal model) etc. It currently includes Vector Exponential\n             Smoothing (VES, de Silva et al., 2010, <doi:10.1177/1471082X0901000401>), Vector ETS (Svetunkov et al., 2023,\n             <doi:10.1016/j.ejor.2022.04.040>) and simulation function for VES.",
    "version": "0.2.1",
    "maintainer": "Ivan Svetunkov <ivan@svetunkov.com>",
    "url": "https://github.com/config-i1/legion",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16374,
    "package_name": "lehuynh",
    "title": "Le-Huynh Truc-Ly's R Code and Templates",
    "description": "Miscellaneous R functions (for graphics, data import, \n    data transformation, and general utilities) and templates (for exploratory \n    analysis, Bayesian modeling, and crafting scientific manuscripts). ",
    "version": "0.1.1",
    "maintainer": "Truc-Ly Le-Huynh <trucly.lehuynh@gmail.com>",
    "url": "https://github.com/le-huynh/lehuynh,\nhttps://le-huynh.github.io/lehuynh/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16391,
    "package_name": "lessR",
    "title": "Less Code with More Comprehensive Results",
    "description": "Each function replaces multiple standard R functions. For example,\n    two function calls, Read() and CountAll(), generate summary statistics for\n    all variables in the data frame, plus histograms and bar charts. Other\n    functions provide data aggregation via pivot tables; comprehensive\n    regression, ANOVA, and t-test; visualizations including integrated\n    Violin/Box/Scatter plot for a numerical variable, bar chart, histogram,\n    box plot, density curves, calibrated power curve; reading multiple data\n    formats with the same call; variable labels; time series with aggregation\n    and forecasting; color themes; and Trellis (facet) graphics. Also includes\n    a confirmatory factor analysis of multiple-indicator measurement models,\n    pedagogical routines for data simulation (e.g., Central Limit Theorem),\n    generation and rendering of regression instructions for interpretative output,\n    and both interactive construction of visualizations and interactive\n    visualizations with plotly.",
    "version": "4.5",
    "maintainer": "David W. Gerbing <gerbing@pdx.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16394,
    "package_name": "lestat",
    "title": "A Package for Learning Statistics",
    "description": "Some simple objects and functions to do \n             statistics using linear models and a Bayesian framework. ",
    "version": "1.9",
    "maintainer": "Petter Mostad <mostad@chalmers.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16408,
    "package_name": "lfl",
    "title": "Linguistic Fuzzy Logic",
    "description": "Various algorithms related to linguistic fuzzy logic: mining for linguistic fuzzy association\n    rules, composition of fuzzy relations, performing perception-based logical deduction (PbLD), \n    and forecasting time-series using fuzzy rule-based ensemble (FRBE). The package also contains basic\n    fuzzy-related algebraic functions capable of handling missing values in different styles (Bochvar,\n    Sobocinski, Kleene etc.), computation of Sugeno integrals and fuzzy transform.",
    "version": "2.3.1",
    "maintainer": "Michal Burda <michal.burda@osu.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16412,
    "package_name": "lg",
    "title": "Locally Gaussian Distributions: Estimation and Methods",
    "description": "An implementation of locally Gaussian distributions. It provides methods for\n    implementing locally Gaussian multivariate density estimation, conditional density \n    estimation, various independence tests for iid and time series data, a test for conditional \n    independence and a test for financial contagion.",
    "version": "0.4.1",
    "maintainer": "Håkon Otneim <hakon.otneim@nhh.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16416,
    "package_name": "lgpr",
    "title": "Longitudinal Gaussian Process Regression",
    "description": "Interpretable nonparametric modeling of longitudinal data\n    using additive Gaussian process regression. Contains functionality\n    for inferring covariate effects and assessing covariate relevances.\n    Models are specified using a convenient formula syntax, and can include\n    shared, group-specific, non-stationary, heterogeneous and temporally\n    uncertain effects. Bayesian inference for model parameters is performed\n    using 'Stan'. The modeling approach and methods are described in detail in\n    Timonen et al. (2021) <doi:10.1093/bioinformatics/btab021>.",
    "version": "1.2.5",
    "maintainer": "Juho Timonen <juho.timonen@iki.fi>",
    "url": "https://github.com/jtimonen/lgpr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16420,
    "package_name": "lgspline",
    "title": "Lagrangian Multiplier Smoothing Splines for Smooth Function\nEstimation",
    "description": "Implements Lagrangian multiplier smoothing splines for flexible\n    nonparametric regression and function estimation. Provides tools for fitting,\n    prediction, and inference using a constrained optimization approach to \n    enforce smoothness.\n    Supports generalized linear models, Weibull accelerated failure time (AFT) models, \n    quadratic programming problems, and customizable arbitrary correlation structures. \n    Options for fitting in parallel are provided. The method builds upon the framework described \n    by Ezhov et al. (2018) <doi:10.1515/jag-2017-0029> using Lagrangian multipliers \n    to fit cubic splines. For more information on correlation structure estimation, \n    see Searle et al. (2009) <ISBN:978-0470009598>. For quadratic programming and \n    constrained optimization in general, see Nocedal & Wright (2006) <doi:10.1007/978-0-387-40065-5>. \n    For a comprehensive background on smoothing splines, see Wahba (1990) \n    <doi:10.1137/1.9781611970128> and Wood (2006) <ISBN:978-1584884743> \n    \"Generalized Additive Models: An Introduction with R\".",
    "version": "0.3.0",
    "maintainer": "Matthew Davis <matthewlouisdavis@gmail.com>",
    "url": "https://github.com/matthewlouisdavisBioStat/lgspline",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16443,
    "package_name": "liftLRD",
    "title": "Wavelet Lifting Estimators of the Hurst Exponent for Regularly\nand Irregularly Sampled Time Series",
    "description": "Implementations of Hurst exponent estimators based on the relationship between wavelet lifting scales and wavelet energy of Knight et al (2017) <doi:10.1007/s11222-016-9698-2>. ",
    "version": "1.0-9",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16467,
    "package_name": "limorhyde",
    "title": "Differential Analysis of Rhythmic Transcriptome Data",
    "description": "A flexible approach, inspired by cosinor regression, for\n  differential analysis of rhythmic transcriptome data. See Singer and Hughey\n  (2018) <doi:10.1177/0748730418813785>.",
    "version": "1.0.1",
    "maintainer": "Jake Hughey <jakejhughey@gmail.com>",
    "url": "https://limorhyde.hugheylab.org,\nhttps://github.com/hugheylab/limorhyde",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16475,
    "package_name": "lindia",
    "title": "Automated Linear Regression Diagnostic",
    "description": "Provides a set of streamlined functions that allow\n    easy generation of linear regression diagnostic plots necessarily \n    for checking linear model assumptions.\n    This package is meant for easy scheming of linear \n    regression diagnostics, while preserving merits of \n    \"The Grammar of Graphics\" as implemented in 'ggplot2'.\n    See the 'ggplot2' website for more information regarding the\n    specific capability of graphics.",
    "version": "0.10",
    "maintainer": "Yeuk Yu Lee <lyeukyu@gmail.com>",
    "url": "https://github.com/yeukyul/lindia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16478,
    "package_name": "lineartestr",
    "title": "Linear Specification Testing",
    "description": "Tests whether the linear hypothesis of a model\n    is correct specified using Dominguez-Lobato test. Also Ramsey's RESET (Regression Equation \n    Specification Error Test) test is implemented and Wald tests can be carried out. \n    Although RESET test is widely used to \n    test the linear hypothesis of a model, Dominguez and Lobato (2019) proposed a novel \n    approach that generalizes well known specification tests such as Ramsey's. This test \n    relies on wild-bootstrap; this package implements this approach to be \n    usable with any function that fits linear models and is compatible with \n    the update() function such as 'stats'::lm(), 'lfe'::felm() and 'forecast'::Arima(), \n    for ARMA (autoregressive–moving-average) models. \n    Also the package can handle custom statistics such as Cramer von Mises and Kolmogorov \n    Smirnov, described by the authors, and custom distributions such as Mammen (discrete \n    and continuous) and Rademacher.\n    Manuel A. Dominguez & Ignacio N. Lobato (2019) <doi:10.1080/07474938.2019.1687116>.",
    "version": "1.0.0",
    "maintainer": "Federico Garza <fede.garza.ramirez@gmail.com>",
    "url": "https://github.com/FedericoGarza/lineartestr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16485,
    "package_name": "linevis",
    "title": "Interactive Time Series Visualizations",
    "description": "Create interactive time series visualizations.\n    'linevis' includes an extensive API to manipulate time series after creation,\n    and supports getting data out of the visualization. Based on the\n    'timevis' package and the 'vis.js' Timeline 'JavaScript' library <https://visjs.github.io/vis-timeline/docs/graph2d/>.",
    "version": "1.0.0",
    "maintainer": "Thomas Charlon <charlon@protonmail.com>",
    "url": "https://gitlab.com/thomaschln/linevis",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16508,
    "package_name": "lipidomeR",
    "title": "Integrative Visualizations of the Lipidome",
    "description": "Create lipidome-wide heatmaps of statistics with the 'lipidomeR'.\n    The 'lipidomeR' provides a streamlined pipeline for the systematic \n    interpretation of the lipidome through publication-ready visualizations \n    of regression models fitted on lipidomics data. \n    With 'lipidomeR', associations between covariates and the lipidome \n    can be interpreted systematically and intuitively through heatmaps, where \n    lipids are categorized by the lipid class and are presented on \n    two-dimensional maps organized by the lipid size and level of saturation. \n    This way, the 'lipidomeR' helps you gain an immediate understanding of \n    the multivariate patterns in the lipidome already at first glance.\n    You can create lipidome-wide heatmaps of statistical associations, changes, \n    differences, variation, or other lipid-specific values.  \n    The heatmaps are provided with publication-ready quality and the results \n    behind the visualizations are based on rigorous statistical models.",
    "version": "0.1.2",
    "maintainer": "Tommi Suvitaival <TSUV0001@RegionH.DK>",
    "url": "https://tommi-s.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16511,
    "package_name": "lira",
    "title": "LInear Regression in Astronomy",
    "description": "Performs Bayesian linear regression and forecasting in astronomy. The method accounts for heteroscedastic errors in both the independent and the dependent variables, intrinsic scatters (in both variables) and scatter correlation, time evolution of slopes, normalization, scatters, Malmquist and Eddington bias, upper limits and break of linearity. The posterior distribution of the regression parameters is sampled with a Gibbs method exploiting the JAGS library.",
    "version": "2.0.1",
    "maintainer": "Mauro Sereno <mauro.sereno@unibo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16533,
    "package_name": "liureg",
    "title": "Liu Regression with Liu Biasing Parameters and Statistics",
    "description": "Linear Liu regression coefficient's estimation and testing with different Liu related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Akdeniz and Kaciranlar (1995) <doi:10.1080/03610929508831585>\n  ii.  Druilhet and Mom (2008) <doi:10.1016/j.jmva.2006.06.011>\n  iii. Imdadullah, Aslam, and Saima (2017)\n  iv.  Liu (1993) <doi:10.1080/03610929308831027>\n  v.   Liu (2001) <doi:10.1016/j.jspi.2010.05.030>.",
    "version": "1.1.2",
    "maintainer": "Imdad Ullah Muhammad <mimdadasad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16541,
    "package_name": "lm.beta",
    "title": "Add Standardized Regression Coefficients to Linear-Model-Objects",
    "description": "Adds standardized regression coefficients to objects created by 'lm'. Also extends the S3 methods 'print', 'summary' and 'coef' with additional boolean argument 'standardized' and provides 'xtable'-support.",
    "version": "1.7-3",
    "maintainer": "Stefan Behrendt <r@behrendt-stefan.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16542,
    "package_name": "lm.br",
    "title": "Linear Model with Breakpoint",
    "description": "Exact significance tests for a changepoint in linear or multiple linear regression.  \n  Confidence regions with exact coverage probabilities for the changepoint.  Based on \n  Knowles, Siegmund and Zhang (1991) <doi:10.1093/biomet/78.1.15>.",
    "version": "2.9.8",
    "maintainer": "Marc Adams <lm.br.pkg@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16547,
    "package_name": "lmSubsets",
    "title": "Exact Variable-Subset Selection in Linear Regression",
    "description": "Exact and approximation algorithms for variable-subset\n  selection in ordinary linear regression models.  Either compute all\n  submodels with the lowest residual sum of squares, or determine the\n  single-best submodel according to a pre-determined statistical\n  criterion.  Hofmann et al. (2020) <doi:10.18637/jss.v093.i03>.",
    "version": "0.5-2",
    "maintainer": "Marc Hofmann <marc.hofmann@gmail.com>",
    "url": "https://github.com/marc-hofmann/lmSubsets.R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16549,
    "package_name": "lmboot",
    "title": "Bootstrap in Linear Models",
    "description": "Various efficient and robust bootstrap methods are implemented for \n    linear models with least squares estimation.  Functions within this package \n    allow users to create bootstrap sampling distributions for model parameters, \n    test hypotheses about parameters, and visualize the bootstrap sampling or null \n    distributions.  Methods implemented for linear models include the wild bootstrap by \n    Wu (1986) <doi:10.1214/aos/1176350142>, the residual and paired bootstraps by\n    Efron (1979, ISBN:978-1-4612-4380-9), the delete-1 jackknife by \n    Quenouille (1956) <doi:10.2307/2332914>, and the Bayesian bootstrap by \n    Rubin (1981) <doi:10.1214/aos/1176345338>.",
    "version": "0.0.1",
    "maintainer": "Megan Heyman <heyman@rose-hulman.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16553,
    "package_name": "lme4GS",
    "title": "'lme4' for Genomic Selection",
    "description": "Flexible functions that use 'lme4' as computational engine for \n             fitting models used in Genomic Selection (GS). GS is a technology used for genetic \n             improvement, and it has many advantages over phenotype-based selection. There \n             are several statistical models that adequately approach the statistical \n             challenges in GS, such as in linear mixed models (LMMs). The 'lme4' is the \n             standard package for fitting linear and generalized LMMs in the R-package, \n             but its use for genetic analysis is limited because it does not allow the \n             correlation between individuals or groups of individuals to be defined. The \n             'lme4GS' package is focused on fitting LMMs with covariance structures defined \n             by the user, bandwidth selection, and genomic prediction. The new package is \n             focused on genomic prediction of the models used in GS and can fit LMMs using\n             different variance-covariance matrices. Several examples of GS models are \n             presented using this package as well as the analysis using real data. For more\n             details see Caamal-Pat et.al. (2021) <doi:10.3389/fgene.2021.680569>.",
    "version": "0.1",
    "maintainer": "Paulino Perez Rodriguez <perpdgo@colpos.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16562,
    "package_name": "lmhelprs",
    "title": "Helper Functions for Linear Model Analysis",
    "description": "A collection of helper functions for multiple\n  regression models fitted by lm(). Most of them are simple\n  functions for simple tasks which can be done with coding,\n  but may not be easy for occasional users of R. Most of\n  the tasks addressed are those sometimes needed when\n  using the 'manymome' package (Cheung and Cheung, 2023,\n  <doi:10.3758/s13428-023-02224-z>) and 'stdmod' package\n  (Cheung, Cheung, Lau, Hui, and Vong, 2022,\n  <doi:10.1037/hea0001188>). However, they can also be used\n  in other scenarios.",
    "version": "0.4.3",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "url": "https://sfcheung.github.io/lmhelprs/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16563,
    "package_name": "lmls",
    "title": "Gaussian Location-Scale Regression",
    "description": "The Gaussian location-scale regression model is a multi-predictor\n    model with explanatory variables for the mean (= location) and the standard\n    deviation (= scale) of a response variable. This package implements maximum\n    likelihood and Markov chain Monte Carlo (MCMC) inference (using algorithms\n    from Girolami and Calderhead (2011) <doi:10.1111/j.1467-9868.2010.00765.x>\n    and Nesterov (2009) <doi:10.1007/s10107-007-0149-x>), a parametric\n    bootstrap algorithm, and diagnostic plots for the model class.",
    "version": "0.1.1",
    "maintainer": "Hannes Riebl <hriebl@posteo.de>",
    "url": "https://hriebl.github.io/lmls/, https://github.com/hriebl/lmls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16564,
    "package_name": "lmm",
    "title": "Linear Mixed Models",
    "description": "It implements Expectation/Conditional Maximization Either (ECME)\n             and rapidly converging algorithms as well as\n             Bayesian inference for linear mixed models, \n             which is described in Schafer, J.L. (1998)\n             \"Some improved procedures for linear mixed models\".\n             Dept. of Statistics, The Pennsylvania State University.",
    "version": "1.4",
    "maintainer": "Jing hua Zhao <jinghuazhao@hotmail.com>",
    "url": "https://github.com/jinghuazhao/R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16566,
    "package_name": "lmmpar",
    "title": "Parallel Linear Mixed Model",
    "description": "Embarrassingly Parallel Linear Mixed Model calculations spread across local cores which repeat until convergence.",
    "version": "0.1.0",
    "maintainer": "Fulya Gokalp Yavuz <fulyagokalp@gmail.com>",
    "url": "https://github.com/fulyagokalp/lmmpar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16567,
    "package_name": "lmodel2",
    "title": "Model II Regression",
    "description": "Computes model II simple linear regression using ordinary\n least squares (OLS), major axis (MA), standard major axis (SMA), and\n ranged major axis (RMA).",
    "version": "1.7-4",
    "maintainer": "Jari Oksanen <jhoksane@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16572,
    "package_name": "lmreg",
    "title": "Data and Functions Used in Linear Models and Regression with R:\nAn Integrated Approach",
    "description": "Data files and a few functions used in the book \n             'Linear Models and Regression with R: An Integrated Approach' \n             by Debasis Sengupta and Sreenivas Rao Jammalamadaka (2019).",
    "version": "1.3",
    "maintainer": "Kaushik Jana <kaushikjana11@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16573,
    "package_name": "lmridge",
    "title": "Linear Ridge Regression with Ridge Penalty and Ridge Statistics",
    "description": "Linear ridge regression coefficient's estimation and testing with different ridge related measures such as MSE, R-squared etc.\n  REFERENCES\n  i.   Hoerl and Kennard (1970) <doi:10.1080/00401706.1970.10488634>,\n  ii.  Halawa and El-Bassiouni (2000) <doi:10.1080/00949650008812006>,\n  iii. Imdadullah, Aslam, and Saima (2017),\n  iv.  Marquardt (1970) <doi:10.2307/1267205>.",
    "version": "1.2.2",
    "maintainer": "Imdad Ullah Muhammad <mimdadasad@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16574,
    "package_name": "lmtest",
    "title": "Testing Linear Regression Models",
    "description": "A collection of tests, data sets, and examples\n for diagnostic checking in linear regression models. Furthermore,\n some generic tools for inference in parametric models are provided.",
    "version": "0.9-40",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16575,
    "package_name": "lmtestrob",
    "title": "Outlier Robust Specification Testing",
    "description": "Robust test(s) for model diagnostics in regression. The current version contains a robust test for functional specification (linearity). The test is based on the robust bounded-influence test by Heritier and Ronchetti (1994) <doi:10.1080/01621459.1994.10476822>.",
    "version": "0.1",
    "maintainer": "Mikhail Zhelonkin <Mikhail.Zhelonkin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16578,
    "package_name": "lmw",
    "title": "Linear Model Weights",
    "description": "Computes the implied weights of linear regression models for estimating\n     average causal effects and provides diagnostics based on these weights. These\n     diagnostics rely on the analyses in Chattopadhyay and Zubizarreta (2023)\n     <doi:10.1093/biomet/asac058> where\n     several regression estimators are represented as weighting estimators, in connection\n     to inverse probability weighting. 'lmw' provides tools to diagnose\n     representativeness, balance, extrapolation, and influence for these models,\n     clarifying the target population of inference. Tools are also available to\n     simplify estimating treatment effects for specific target populations of interest.",
    "version": "0.0.2",
    "maintainer": "Noah Greifer <ngreifer@iq.harvard.edu>",
    "url": "https://github.com/ngreifer/lmw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16581,
    "package_name": "lnmixsurv",
    "title": "Bayesian Mixture Log-Normal Survival Model",
    "description": "Bayesian Survival models via the mixture of Log-Normal distribution extends the well-known survival models and accommodates different behaviour over time and considers higher censored survival times. The proposal combines mixture distributions Fruhwirth-Schnatter(2006) <doi:10.1007/s11336-009-9121-4>, and data augmentation techniques Tanner and Wong (1987) <doi:10.1080/01621459.1987.10478458>.",
    "version": "3.1.6",
    "maintainer": "Victor Hugo Soares Ney <victor.s.ney@gmail.com>",
    "url": "https://vivianalobo.github.io/lnmixsurv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16585,
    "package_name": "loadshaper",
    "title": "Producing Load Shape with Target Peak and Load Factor",
    "description": "Modifying a load shape to match specific peak and \n  load factor is a fundamental component for various power system \n  planning and operation studies. This package is an efficient tool \n  to modify a reference load shape while matching the desired peak\n  and load factor. The package offers both linear and non-linear method,\n  described in <https://rpubs.com/riazakhan94/load_shape_match_peak_energy>. \n  The user can control the shape of the final load shape by regulating \n  certain parameters. The package provides validation metrics for \n  assessing the derived load shape in terms of preserving time series \n  properties. It also offers powerful graphics, that allows the user to\n  visually assess the derived load shape.",
    "version": "1.1.1",
    "maintainer": "Md Riaz Ahmed Khan <mdriazahmed.khan@jacks.sdstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16603,
    "package_name": "locfit",
    "title": "Local Regression, Likelihood and Density Estimation",
    "description": "Local regression, likelihood and density estimation methods as described in the 1999 book by Loader.",
    "version": "1.5-9.12",
    "maintainer": "Andy Liaw <andy_liaw@merck.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16605,
    "package_name": "locits",
    "title": "Test of Stationarity and Localized Autocovariance",
    "description": "Provides test of second-order stationarity for time\n\tseries (for dyadic and arbitrary-n length data). Provides\n\tlocalized autocovariance, with confidence intervals,\n\tfor locally stationary (nonstationary) time series.\n\tSee Nason, G P (2013) \"A test for second-order stationarity and\n\tapproximate confidence intervals for localized autocovariance\n\tfor locally stationary time series.\" Journal of the Royal Statistical\n\tSociety, Series B, 75, 879-904.  <doi:10.1111/rssb.12015>.",
    "version": "1.7.8",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16606,
    "package_name": "locpol",
    "title": "Kernel Local Polynomial Regression",
    "description": "Computes local polynomial estimators for \n  the regression and also density. It comprises several \n  different utilities to handle kernel estimators.",
    "version": "0.9.0",
    "maintainer": "Bastiaan Quast <bquast@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16617,
    "package_name": "logNormReg",
    "title": "log Normal Linear Regression",
    "description": "Functions to fits simple linear regression models with log normal errors \n\t\tand identity link, i.e. taking the responses on the original scale. See\n\t\tMuggeo (2018) <doi:10.13140/RG.2.2.18118.16965>.",
    "version": "0.5-0",
    "maintainer": "Vito M. R. Muggeo <vito.muggeo@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16619,
    "package_name": "logbin",
    "title": "Relative Risk Regression Using the Log-Binomial Model",
    "description": "Methods for fitting log-link GLMs and GAMs to binomial data,\n    including EM-type algorithms with more stable convergence properties than standard methods.",
    "version": "2.0.6",
    "maintainer": "Mark W. Donoghoe <markdonoghoe@gmail.com>",
    "url": "https://github.com/mdonoghoe/logbin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16620,
    "package_name": "logconcens",
    "title": "Maximum Likelihood Estimation of a Log-Concave Density Based on\nCensored Data",
    "description": "Based on right or interval censored data, compute the maximum likelihood estimator of a (sub)probability density under the assumption that it is log-concave. For further information see Duembgen, Rufibach and Schuhmacher (2014) <doi:10.1214/14-EJS930>.",
    "version": "0.17-4",
    "maintainer": "Dominic Schuhmacher <dominic.schuhmacher@mathematik.uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16629,
    "package_name": "logicDT",
    "title": "Identifying Interactions Between Binary Predictors",
    "description": "A statistical learning method that tries to find the best set\n  of predictors and interactions between predictors for modeling binary or\n  quantitative response data in a decision tree. Several search algorithms\n  and ensembling techniques are implemented allowing for finetuning the\n  method to the specific problem. Interactions with quantitative\n  covariables can be properly taken into account by fitting local\n  regression models. Moreover, a variable importance measure for assessing\n  marginal and interaction effects is provided. Implements the\n  procedures proposed by Lau et al. (2024, <doi:10.1007/s10994-023-06488-6>).",
    "version": "1.0.5",
    "maintainer": "Michael Lau <michael.lau@hhu.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16649,
    "package_name": "logspline",
    "title": "Routines for Logspline Density Estimation",
    "description": "Contains routines for logspline density estimation.\n\tThe function oldlogspline() uses the same algorithm as the logspline package\n\tversion 1.0.x; i.e. the Kooperberg and Stone (1992) \n\talgorithm (with an improved interface).  The recommended routine logspline()\n\tuses an algorithm from Stone et al (1997)  <DOI:10.1214/aos/1031594728>.",
    "version": "2.1.22",
    "maintainer": "Charles Kooperberg <clk@fredhutch.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16654,
    "package_name": "lomb",
    "title": "Lomb-Scargle Periodogram",
    "description": "Computes the Lomb-Scargle Periodogram and actogram for evenly or unevenly sampled time series. Includes a randomization procedure to obtain exact p-values. Partially based on C original by Press et al. (Numerical Recipes) and the Python module Astropy. For more information see Ruf, T. (1999). The Lomb-Scargle periodogram in biological rhythm research: analysis of incomplete and unequally spaced time-series. Biological Rhythm Research, 30(2), 178-201.",
    "version": "2.5.0",
    "maintainer": "Thomas Ruf <Thomas.P.Ruf@me.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16660,
    "package_name": "longit",
    "title": "High Dimensional Longitudinal Data Analysis Using MCMC",
    "description": "High dimensional longitudinal data analysis with Markov Chain Monte Carlo(MCMC). \n             Currently support mixed effect regression with or without missing observations by considering \n             covariance structures. It provides estimates by missing at random and missing not at random assumptions.\n             In this R package, we present Bayesian approaches that statisticians and clinical \n             researchers can easily use. The functions' methodology is based on the book \"Bayesian Approaches in Oncology Using R and OpenBUGS\" by \n             Bhattacharjee A (2020) <doi:10.1201/9780429329449-14>.",
    "version": "0.1.0",
    "maintainer": "Atanu Bhattacharjee <atanustat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16662,
    "package_name": "longitudinalANAL",
    "title": "Longitudinal Data Analysis",
    "description": "Regression analysis of mixed sparse synchronous and asynchronous longitudinal covariates. Please cite the manuscripts corresponding to this package: Sun, Z. et al. (2023) <arXiv:2305.17715>  and  Liu, C. et al. (2023) <arXiv:2305.17662>.",
    "version": "0.2",
    "maintainer": "Zhuowei Sun <sunzw21@mails.jlu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16664,
    "package_name": "longitudinalcascade",
    "title": "Longitudinal Cascade",
    "description": "Creates a series of sets of graphics and statistics related to the longitudinal cascade, all included in a single object. The longitudinal cascade inputs longitudinal data to identify gaps in the HIV and related cascades by observing differences using time to event and survival methods. The stage definitions are set by the user, with default standard options. Outputs include graphics, datasets, and formal statistical tests.",
    "version": "0.3.2.6",
    "maintainer": "Noah Haber <noahhaber@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16666,
    "package_name": "longmemo",
    "title": "Statistics for Long-Memory Processes (Book Jan Beran), and\nRelated Functionality",
    "description": "Datasets and Functionality from\n  'Jan Beran' (1994). Statistics for Long-Memory Processes; Chapman & Hall.\n  Estimation of Hurst (and more) parameters for fractional Gaussian noise,\n  'fARIMA' and 'FEXP' models.",
    "version": "1.1-4",
    "maintainer": "Martin Maechler <maechler@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16684,
    "package_name": "lorad",
    "title": "Lowest Radial Distance Method of Marginal Likelihood Estimation",
    "description": "Estimates marginal likelihood from a posterior sample using the method described in Wang et al. (2023) <doi:10.1093/sysbio/syad007>, which does not require evaluation of any additional points and requires only the log of the unnormalized posterior density for each sampled parameter vector.",
    "version": "0.0.1.0",
    "maintainer": "Analisa Milkey <analisa.milkey@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16698,
    "package_name": "lpacf",
    "title": "Local Partial Autocorrelation Function Estimation for Locally\nStationary Wavelet Processes",
    "description": "Provides the method for computing the local partial autocorrelation function for locally stationary wavelet time series from Killick, Knight, Nason, Eckley (2020) <doi:10.1214/20-EJS1748>.",
    "version": "1.0.2",
    "maintainer": "Rebecca Killick <r.killick@lancs.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16700,
    "package_name": "lpc",
    "title": "Lassoed Principal Components for Testing Significance of\nFeatures",
    "description": "Implements the LPC method of Witten&Tibshirani(Annals of Applied Statistics 2008) for identification of significant genes in a microarray experiment.",
    "version": "1.0.2.1",
    "maintainer": "Daniela M Witten <dwitten@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16703,
    "package_name": "lpdensity",
    "title": "Local Polynomial Density Estimation and Inference",
    "description": "Without imposing stringent distributional assumptions or shape restrictions, nonparametric estimation has been popular in economics and other social sciences for counterfactual analysis, program evaluation, and policy recommendations. This package implements a novel density (and derivatives) estimator based on local polynomial regressions, documented in Cattaneo, Jansson and Ma (2022) <doi:10.18637/jss.v101.i02>: lpdensity() to construct local polynomial based density (and derivatives) estimator, and lpbwdensity() to perform data-driven bandwidth selection. ",
    "version": "2.5",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16707,
    "package_name": "lpl",
    "title": "Local Partial Likelihood Estimation and Simultaneous Confidence\nBand",
    "description": "Local partial likelihood estimation by Fan, Lin and Zhou(2006)<doi:10.1214/009053605000000796> and simultaneous confidence band is a set of tools to test the covariates-biomarker interaction for survival data. Test for the covariates-biomarker interaction using the bootstrap method and the asymptotic method with simultaneous confidence band (Liu, Jiang and Chen (2015)<doi:10.1002/sim.6563>).",
    "version": "0.13",
    "maintainer": "Bingshu E. Chen <bingshu.chen@queensu.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16715,
    "package_name": "lrgs",
    "title": "Linear Regression by Gibbs Sampling",
    "description": "Implements a Gibbs sampler to do linear regression with multiple covariates, multiple responses, Gaussian measurement errors on covariates and responses, Gaussian intrinsic scatter, and a covariate prior distribution which is given by either a Gaussian mixture of specified size or a Dirichlet process with a Gaussian base distribution. Described further in Mantz (2016) <DOI:10.1093/mnras/stv3008>.",
    "version": "0.5.4",
    "maintainer": "Adam Mantz <amantz@slac.stanford.edu>",
    "url": "https://github.com/abmantz/lrgs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16721,
    "package_name": "lsei",
    "title": "Solving Least Squares or Quadratic Programming Problems under\nEquality/Inequality Constraints",
    "description": "It contains functions that solve least squares linear\n\t     regression problems under linear equality/inequality\n\t     constraints. Functions for solving quadratic programming\n\t     problems are also available, which transform such problems\n\t     into least squares ones first. It is developed based on the\n\t     'Fortran' program of Lawson and Hanson (1974, 1995), which is\n\t     public domain and available at\n\t     <http://www.netlib.org/lawson-hanson/>.",
    "version": "1.3-0",
    "maintainer": "Yong Wang <yongwang@auckland.ac.nz>",
    "url": "https://www.stat.auckland.ac.nz/~yongwang/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16722,
    "package_name": "lsirm12pl",
    "title": "Latent Space Item Response Model",
    "description": "Analysis of dichotomous and continuous response data using latent factor by both 1PL LSIRM and 2PL LSIRM as described in Jeon et al. (2021) <doi:10.1007/s11336-021-09762-5>. It includes original 1PL LSIRM and 2PL LSIRM provided for binary response data and its extension for continuous response data. Bayesian model selection with spike-and-slab prior and method for dealing data with missing value under missing at random, missing completely at random are also supported. Various diagnostic plots are available to inspect the latent space and summary of estimated parameters.",
    "version": "1.3.9",
    "maintainer": "Gwanghee Kim <musagh08@yonsei.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16726,
    "package_name": "lsmeans",
    "title": "Least-Squares Means",
    "description": "Obtain least-squares means for linear, generalized linear, \n    and mixed models. Compute contrasts or linear functions of \n    least-squares means, and comparisons of slopes. \n    Plots and compact letter displays. Least-squares means were proposed in\n    Harvey, W (1960) \"Least-squares analysis of data with unequal subclass numbers\",\n    Tech Report ARS-20-8, USDA National Agricultural Library, and discussed\n    further in Searle, Speed, and Milliken (1980) \"Population marginal means \n    in the linear model: An alternative to least squares means\", \n    The American Statistician 34(4), 216-221 <doi:10.1080/00031305.1980.10483031>.\n    NOTE: lsmeans now relies primarily on code in the 'emmeans' package.\n    'lsmeans' will be archived in the near future.",
    "version": "2.30-2",
    "maintainer": "Russell Lenth <russell-lenth@uiowa.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16728,
    "package_name": "lspartition",
    "title": "Nonparametric Estimation and Inference Procedures using\nPartitioning-Based Least Squares Regression",
    "description": "Tools for statistical analysis using partitioning-based least squares regression as described in Cattaneo, Farrell and Feng (2020a, <doi:10.48550/arXiv.1804.04916>) and Cattaneo, Farrell and Feng (2020b, <doi:10.48550/arXiv.1906.00202>): lsprobust() for nonparametric point estimation of regression functions and their derivatives and for robust bias-corrected (pointwise and uniform) inference; lspkselect() for data-driven selection of the IMSE-optimal number of knots; lsprobust.plot() for regression plots with robust confidence intervals and confidence bands; lsplincom() for estimation and inference for linear combinations of regression functions from different groups.",
    "version": "0.5",
    "maintainer": "Yingjie Feng <fengyingjiepku@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16732,
    "package_name": "lss2",
    "title": "The Accelerated Failure Time Model to Right Censored Data Based\non Least-Squares Principle",
    "description": "Due to lack of proper inference procedure and software,\n        the ordinary linear regression model is seldom used in practice\n        for the analysis of right censored data. This paper presents an\n        S-Plus/R program that implements a recently developed inference\n        procedure (Jin, Lin and Ying, 2006) <doi:10.1093/biomet/93.1.147> \n        for the accelerated failure time model based on the least-squares\n        principle.",
    "version": "1.1",
    "maintainer": "Arvin Satwani <arvinsatwani@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16740,
    "package_name": "ltmle",
    "title": "Longitudinal Targeted Maximum Likelihood Estimation",
    "description": "Targeted Maximum Likelihood Estimation ('TMLE') of\n    treatment/censoring specific mean outcome or marginal structural model for\n    point-treatment and longitudinal data.",
    "version": "1.3-0",
    "maintainer": "Joshua Schwab <jschwab77@berkeley.edu>",
    "url": "https://github.com/joshuaschwab/ltmle,\nhttp://joshuaschwab.github.io/ltmle/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16741,
    "package_name": "ltsa",
    "title": "Linear Time Series Analysis",
    "description": "Methods of developing linear time series modelling.\n Methods are given for loglikelihood computation, forecasting\n  and simulation.",
    "version": "1.4.6.1",
    "maintainer": "A.I. McLeod <aimcleod@uwo.ca>",
    "url": "http://www.stats.uwo.ca/faculty/aim",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16743,
    "package_name": "ltxsparklines",
    "title": "Lightweight Sparklines for a LaTeX Document",
    "description": "Sparklines are small plots (about one line of text high),\n  made popular by Edward Tufte.  This package is the interface from R\n  to the LaTeX package sparklines by Andreas Loeffer and Dan Luecking\n  (<http://www.ctan.org/pkg/sparklines>).  It can work with Sweave or\n  knitr or other engines that produce TeX.  The package can be used to\n  plot vectors, matrices, data frames, time series (in ts or zoo format).",
    "version": "1.1.3",
    "maintainer": "Boris Veytsman <borisv@lk.net>",
    "url": "https://github.com/borisveytsman/ltxsparklines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16768,
    "package_name": "lwqs",
    "title": "Lagged Weighted Quantile Sum Regression",
    "description": "Wrapper functions for the implementation of lagged weighted quantile sum regression, as per 'Gennings et al' (2020) <doi:10.1016/j.envres.2020.109529>. ",
    "version": "0.5.0",
    "maintainer": "Paul Curtin <paul.curtin@mssm.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16774,
    "package_name": "mAr",
    "title": "Multivariate AutoRegressive Analysis",
    "description": "R functions for the estimation and eigen-decomposition of multivariate autoregressive models.",
    "version": "1.2-0",
    "maintainer": "S. M. Barbosa <susana.barbosa@fc.up.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16776,
    "package_name": "mBvs",
    "title": "Bayesian Variable Selection Methods for Multivariate Data",
    "description": "Bayesian variable selection methods for data with multivariate responses and multiple covariates. The package contains implementations of multivariate Bayesian variable selection methods for continuous data (Lee et al., Biometrics, 2017 <doi:10.1111/biom.12557>) and zero-inflated count data (Lee et al., Biostatistics, 2020 <doi:10.1093/biostatistics/kxy067>).",
    "version": "1.92",
    "maintainer": "Kyu Ha Lee <klee15239@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16785,
    "package_name": "mHMMbayes",
    "title": "Multilevel Hidden Markov Models Using Bayesian Estimation",
    "description": "An implementation of the multilevel (also known as mixed or random \n    effects) hidden Markov model using Bayesian estimation in R. The multilevel \n    hidden Markov model (HMM) is a generalization of the well-known hidden\n    Markov model, for the latter see Rabiner (1989) <doi:10.1109/5.18626>. The \n    multilevel HMM is tailored to accommodate (intense) longitudinal data of \n    multiple individuals simultaneously, see e.g., de Haan-Rietdijk et al. \n    <doi:10.1080/00273171.2017.1370364>. Using a multilevel framework, we allow \n    for heterogeneity in the model parameters (transition probability matrix and \n    conditional distribution), while estimating one overall HMM. The model can \n    be fitted on multivariate data with either a categorical, normal, or Poisson \n    distribution, and include individual level covariates (allowing for e.g., \n    group comparisons on model parameters). Parameters are estimated using \n    Bayesian estimation utilizing the forward-backward recursion within a hybrid \n    Metropolis within Gibbs sampler. Missing data (NA) in the dependent \n    variables is accommodated assuming MAR. The package also includes various \n    visualization options, a function to simulate data, and a function to obtain \n    the most likely hidden state sequence for each individual using the Viterbi \n    algorithm. ",
    "version": "1.1.1",
    "maintainer": "Emmeke Aarts <e.aarts@uu.nl>",
    "url": "https://CRAN.R-project.org/package=mHMMbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16790,
    "package_name": "mRMRe",
    "title": "Parallelized Minimum Redundancy, Maximum Relevance (mRMR)",
    "description": "Computes mutual information matrices from continuous, categorical \n  and survival variables, as well as feature selection with minimum redundancy, \n  maximum relevance (mRMR) and a new ensemble mRMR technique. Published in\n  De Jay et al. (2013) <doi:10.1093/bioinformatics/btt383>.",
    "version": "2.1.2.2",
    "maintainer": "Benjamin Haibe-Kains <benjamin.haibe.kains@utoronto.ca>",
    "url": "https://www.pmgenomics.ca/bhklab/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16800,
    "package_name": "maSAE",
    "title": "Mandallaz' Model-Assisted Small Area Estimators",
    "description": "An S4 implementation of the unbiased extension of\n    the model- assisted synthetic-regression estimator proposed by\n    Mandallaz (2013) <DOI:10.1139/cjfr-2012-0381>, Mandallaz et al. (2013)\n    <DOI:10.1139/cjfr-2013-0181> and Mandallaz (2014)\n    <DOI:10.1139/cjfr-2013-0449>.  It yields smaller variances than the\n    standard bias correction, the generalised regression estimator.",
    "version": "2.0.3",
    "maintainer": "Andreas Dominik Cullmann <fvafrcu@mailbox.org>",
    "url": "https://gitlab.com/fvafrCU/maSAE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16806,
    "package_name": "maclogp",
    "title": "Measures of Uncertainty for Model Selection",
    "description": "Following the common types of measures of uncertainty for parameter estimation, two measures of uncertainty were proposed for model selection, see Liu, Li and Jiang (2020) <doi:10.1007/s11749-020-00737-9>. The first measure is a kind of model confidence set that relates to the variation of model selection, called Mac. The second measure focuses on error of model selection, called LogP. They are all computed via bootstrapping. This package provides functions to compute these two measures. Furthermore, a similar model confidence set adapted from Bayesian Model Averaging can also be computed using this package. ",
    "version": "0.1.1",
    "maintainer": "Yuanyuan Li <yynli9696@gmail.com>",
    "url": "https://github.com/YuanyuanLi96/maclogp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16810,
    "package_name": "macrocol",
    "title": "Colombian Macro-Financial Time Series Generator",
    "description": "This repository aims to contribute to the econometric models' production\n\twith Colombian data, by providing a set of web-scrapping functions \n\tof some of the main macro-financial indicators. All the sources are public and\n\tfree, but the advantage of these functions is that they directly download \n\tand harmonize the information in R's environment. No need to import or download\n\tadditional files. You only need an internet connection!",
    "version": "0.1.0",
    "maintainer": "Pedro Alejandro Cabra-Acela <jando2797@gmail.com>",
    "url": "<https://github.com/pedroCabraAcela/Scrapping-Colombian-Macrodata>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16840,
    "package_name": "maicChecks",
    "title": "Exact Matching and Matching-Adjusted Indirect Comparison (MAIC)",
    "description": "The second version (0.2.0) contains implementation for exact matching which is an alternative to propensity score matching (see Glimm & Yau (2025)). The initial version (0.1.2) contains a collection of easy-to-implement tools for checking whether a MAIC can be conducted, as well as an alternative way of calculating weights (see Glimm & Yau (2021) <doi:10.1002/pst.2210>.)",
    "version": "0.2.0",
    "maintainer": "Lillian Yau <maicChecks@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16841,
    "package_name": "maicplus",
    "title": "Matching Adjusted Indirect Comparison",
    "description": "Facilitates performing matching adjusted indirect comparison\n (MAIC) analysis where the endpoint of interest is either time-to-event\n (e.g. overall survival) or binary (e.g. objective tumor response). The method\n is described by Signorovitch et al (2012) <doi:10.1016/j.jval.2012.05.004>.",
    "version": "0.1.2",
    "maintainer": "Isaac Gravestock <isaac.gravestock@roche.com>",
    "url": "https://github.com/hta-pharma/maicplus/,\nhttps://hta-pharma.github.io/maicplus/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16849,
    "package_name": "makedummies",
    "title": "Create Dummy Variables from Categorical Data",
    "description": "Create dummy variables from categorical data.\n    This package can convert categorical data (factor and ordered) into\n    dummy variables and handle multiple columns simultaneously.\n    This package enables to select whether a dummy variable for base group\n    is included (for principal component analysis/factor analysis) or\n    excluded (for regression analysis) by an option.\n    'makedummies' function accepts 'data.frame', 'matrix', and\n    'tbl' (tibble) class (by 'tibble' package).\n    'matrix' class data is automatically converted to 'data.frame' class.",
    "version": "1.2.1",
    "maintainer": "Toshiaki Ara <toshiaki.ara@gmail.com>",
    "url": "https://github.com/toshi-ara/makedummies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16866,
    "package_name": "maltese",
    "title": "Machine Learning For Time Series",
    "description": "Tools to transform a time series into a machine learning-friendly",
    "version": "0.1.3",
    "maintainer": "",
    "url": "https://github.com/bearloga/maltese",
    "exports": [],
    "topics": ["forecasting", "machine-learning", "r", "r-package", "rstats", "time-series"],
    "score": "NA",
    "stars": 49
  },
  {
    "id": 16872,
    "package_name": "manet",
    "title": "Multiple Allocation Model for Actor-Event Networks",
    "description": "Mixture model with overlapping clusters for binary actor-event data. Parameters are estimated in a Bayesian framework. Model and inference are described in Ranciati, Vinciotti, Wit (2017) Modelling actor-event network data via a mixture model under overlapping clusters. Submitted.",
    "version": "2.0",
    "maintainer": "Veronica Vinciotti <veronica.vinciotti@brunel.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16879,
    "package_name": "mantar",
    "title": "Missingness Alleviation for Network Analysis",
    "description": "Provides functionality for estimating cross-sectional network structures representing partial correlations in R, while accounting for missing values in the data. Networks are estimated via neighborhood selection, i.e., node-wise multiple regression, with model selection guided by information criteria. Missing data can be handled primarily via multiple imputation or a maximum likelihood-based approach; deletion techniques are available but secondary <doi:10.31234/osf.io/qpj35>.",
    "version": "0.1.0",
    "maintainer": "Kai Jannik Nehler <nehler@psych.uni-frankfurt.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16929,
    "package_name": "maptpx",
    "title": "MAP Estimation of Topic Models",
    "description": "Maximum a posteriori (MAP) estimation for topic models (i.e., Latent Dirichlet Allocation) in text analysis,\n\tas described in Taddy (2012) 'On estimation and selection for topic models'.  Previous versions of this code were included as part of the 'textir' package.  If you want to take advantage of openmp parallelization, uncomment the relevant flags in src/MAKEVARS before compiling.",
    "version": "1.9-7",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "url": "http://taddylab.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16934,
    "package_name": "mar1s",
    "title": "Multiplicative AR(1) with Seasonal Processes",
    "description": "Multiplicative AR(1) with Seasonal is a stochastic\n  process model built on top of AR(1). The package provides the\n  following procedures for MAR(1)S processes: fit, compose, decompose,\n  advanced simulate and predict.",
    "version": "2.1.1",
    "maintainer": "Andrey Paramonov <paramon@acdlabs.ru>",
    "url": "https://github.com/aparamon/mar1s",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16936,
    "package_name": "marble",
    "title": "Robust Marginal Bayesian Variable Selection for Gene-Environment\nInteractions",
    "description": "Recently, multiple marginal variable selection methods have been developed and shown to be effective in Gene-Environment interactions studies. We propose a novel marginal Bayesian variable selection method for Gene-Environment interactions studies. In particular, our marginal Bayesian method is robust to data contamination and outliers in the outcome variables. With the incorporation of spike-and-slab priors, we have implemented the Gibbs sampler based on Markov Chain Monte Carlo. The core algorithms of the package have been developed in 'C++'.",
    "version": "0.0.3",
    "maintainer": "Xi Lu <xilu@ksu.edu>",
    "url": "https://github.com/xilustat/marble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16939,
    "package_name": "marcox",
    "title": "Marginal Hazard Ratio Estimation in Clustered Failure Time Data",
    "description": "Estimation of marginal hazard ratios in clustered failure time data. It implements the weighted generalized estimating equation approach based on a semiparametric marginal proportional hazards model (See Niu, Y. Peng, Y.(2015). \"A new estimating equation approach for marginal hazard ratio estimation\"), accounting for within-cluster correlations. 5 different correlation structures are supported. The package is designed for researchers in biostatistics and epidemiology who require accurate and efficient estimation methods for survival analysis in clustered data settings.",
    "version": "1.0.0",
    "maintainer": "Junyi Chen <2655088079@qq.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16941,
    "package_name": "marg",
    "title": "Approximate Marginal Inference for Regression-Scale Models",
    "description": "Implements likelihood inference based on higher order approximations for linear nonnormal regression models.",
    "version": "1.2-4",
    "maintainer": "Alessandra R. Brazzale <alessandra.brazzale@unipd.it>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16945,
    "package_name": "marginme",
    "title": "Estimation of Relative Risks, Risk Differences, and Marginal\nEffects from Mixed Models Using Marginal Standardization",
    "description": "Functionality to estimate relative risks, risk differences, and \n  partial effects from mixed model. Marginalisation over random effect terms is accomplished using Markov Chain Monte Carlo.",
    "version": "0.1.0",
    "maintainer": "Sam Watson <S.I.Watson@bham.ac.uk>",
    "url": "https://github.com/samuel-watson/marginme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16946,
    "package_name": "margins",
    "title": "Marginal Effects for Model Objects",
    "description": "An R port of the margins command from 'Stata', which can be used to\n    calculate marginal (or partial) effects from model objects.",
    "version": "0.3.28",
    "maintainer": "Ben Bolker <bolker@mcmaster.ca>",
    "url": "https://github.com/bbolker/margins",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16947,
    "package_name": "marima",
    "title": "Multivariate ARIMA and ARIMA-X Analysis",
    "description": "Multivariate ARIMA and ARIMA-X estimation using Spliid's \n    algorithm (marima()) and simulation (marima.sim()).",
    "version": "2.2",
    "maintainer": "Henrik Spliid <hspl@dtu.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16953,
    "package_name": "marked",
    "title": "Mark-Recapture Analysis for Survival and Abundance Estimation",
    "description": "Functions for fitting various models to capture-recapture data\n    including mixed-effects Cormack-Jolly-Seber(CJS) and multistate models and\n    the multi-variate state model structure for survival\n    estimation and POPAN structured Jolly-Seber models for abundance estimation.\n    There are also Hidden Markov model (HMM) implementations of CJS and multistate\n    models with and without state uncertainty and a simulation capability for HMM\n    models.",
    "version": "1.2.8",
    "maintainer": "Jeff Laake <jefflaake@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16959,
    "package_name": "markovMSM",
    "title": "Methods for Checking the Markov Condition in Multi-State\nSurvival Data",
    "description": "The inference in multi-state models is traditionally performed under\n             a Markov assumption that claims that past and future of the process\n             are independent given the present state. In this package, we \n             consider tests of the Markov assumption that are applicable to \n             general multi-state models. Three approaches using existing \n             methodology are considered: a simple method based on including\n             covariates depending on the history in Cox models for the transition\n             intensities; methods based on measuring the discrepancy of the \n             non-Markov estimators of the transition probabilities to the \n             Markov Aalen-Johansen estimators; and, finally, methods that were\n             developed by considering summaries from families of log-rank \n             statistics where patients are grouped by the state occupied of the\n             process at a particular time point (see Soutinho G, Meira-Machado L\n             (2021) <doi:10.1007/s00180-021-01139-7> and Titman AC, Putter H\n             (2020) <doi:10.1093/biostatistics/kxaa030>).",
    "version": "0.1.3",
    "maintainer": "Gustavo Soutinho <gustavosoutinho@sapo.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16964,
    "package_name": "marlod",
    "title": "Marginal Modeling for Exposure Data with Values Below the LOD",
    "description": "Functions of marginal mean and quantile regression models are used to analyze environmental exposure and biomonitoring data with repeated measurements and non-detects (i.e., values below the limit of detection (LOD)), as well as longitudinal exposure data that include non-detects and time-dependent covariates. For more details see Chen IC, Bertke SJ, Curwin BD (2021) <doi:10.1038/s41370-021-00345-1>, Chen IC, Bertke SJ, Estill CF (2024) <doi:10.1038/s41370-024-00640-7>, Chen IC, Bertke SJ, Dahm MM (2024) <doi:10.1093/annweh/wxae068>, and Chen IC (2025) <doi:10.1038/s41370-025-00752-8>.",
    "version": "0.2.2",
    "maintainer": "I-Chen Chen <flecsh@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16976,
    "package_name": "mase",
    "title": "Model-Assisted Survey Estimators",
    "description": "A set of model-assisted survey estimators and corresponding\n    variance estimators for single stage, unequal probability, without replacement\n    sampling designs.  All of the estimators can be written as a generalized \n    regression estimator with the Horvitz-Thompson, ratio, post-stratified, and \n    regression estimators summarized by Sarndal et al. (1992, ISBN:978-0-387-40620-6).\n    Two of the estimators employ a statistical learning model as the assisting model:\n    the elastic net regression estimator, which is an extension of the lasso regression\n    estimator given by McConville et al. (2017) <doi:10.1093/jssam/smw041>, and the \n    regression tree estimator described in McConville and Toth (2017) <arXiv:1712.05708>. \n    The variance estimators which approximate the joint inclusion probabilities can\n    be found in Berger and Tille (2009) <doi:10.1016/S0169-7161(08)00002-3> and the\n    bootstrap variance estimator is presented in Mashreghi et al. (2016) \n    <doi:10.1214/16-SS113>.",
    "version": "0.1.5.2",
    "maintainer": "Kelly McConville <kmcconville@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16978,
    "package_name": "mashr",
    "title": "Multivariate Adaptive Shrinkage",
    "description": "Implements the multivariate adaptive shrinkage (mash)\n    method of Urbut et al (2019) <DOI:10.1038/s41588-018-0268-8> for\n    estimating and testing large numbers of effects in many conditions\n    (or many outcomes). Mash takes an empirical Bayes approach to\n    testing and effect estimation; it estimates patterns of similarity\n    among conditions, then exploits these patterns to improve accuracy\n    of the effect estimates. The core linear algebra is implemented in\n    C++ for fast model fitting and posterior computation.",
    "version": "0.2.79",
    "maintainer": "Peter Carbonetto <peter.carbonetto@gmail.com>",
    "url": "https://github.com/stephenslab/mashr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16988,
    "package_name": "mastif",
    "title": "Mast Inference and Forecasting",
    "description": "Analyzes production and dispersal of seeds dispersed from trees and recovered in seed traps.  Motivated by long-term inventory plots where seed collections are used to infer seed production by each individual plant. ",
    "version": "2.3",
    "maintainer": "James S. Clark <jimclark@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 16993,
    "package_name": "matchMulti",
    "title": "Optimal Multilevel Matching using a Network Algorithm",
    "description": "Performs multilevel matches for data with cluster-\n\tlevel treatments and individual-level outcomes using a network \n\toptimization algorithm.  Functions for checking balance at the \n\tcluster and individual levels are also provided, as are methods \n\tfor permutation-inference-based outcome analysis.  Details in \n\tPimentel et al. (2018) <doi:10.1214/17-AOAS1118>.  The optmatch \n\tpackage, which is useful for running many of the provided \n\tfunctions, may be downloaded from Github at \n\t<https://github.com/markmfredrickson/optmatch> if not available on \n\tCRAN.",
    "version": "1.1.14",
    "maintainer": "Sam Pimentel <spi@berkeley.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17011,
    "package_name": "matrans",
    "title": "Model Averaging-Assisted Optimal Transfer Learning",
    "description": "Transfer learning, as a prevailing technique in computer sciences, aims to improve the performance of a target model by leveraging auxiliary information from heterogeneous source data. We provide novel tools for multi-source transfer learning under statistical models based on model averaging strategies, including linear regression models, partially linear models. Unlike existing transfer learning approaches, this method integrates the auxiliary information through data-driven weight assignments to avoid negative transfer. This is the first package for transfer learning based on the optimal model averaging frameworks, providing efficient implementations for practitioners in multi-source data modeling. The details are described in Hu and Zhang (2023) <https://jmlr.org/papers/v24/23-0030.html>.",
    "version": "0.2.0",
    "maintainer": "Xiaonan Hu <xiaonanhu@cnu.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17017,
    "package_name": "matrixProfile",
    "title": "Matrix Profile",
    "description": "A simple and the early stage package for matrix profile based on the paper of Chin-Chia Michael Yeh, Yan Zhu, Liudmila Ulanova, Nurjahan Begum, Yifei Ding, Hoang Anh Dau, Diego Furtado Silva, Abdullah Mueen, and Eamonn Keogh (2016) <DOI:10.1109/ICDM.2016.0179>. This package calculates all-pairs-similarity for a given window size for time series data.",
    "version": "0.5.0",
    "maintainer": "Donghwan Kim <donhkim9714@korea.ac.kr>",
    "url": "https://github.com/ainsuotain/matrixprofile",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17035,
    "package_name": "maxEff",
    "title": "Additional Predictor with Maximum Effect Size",
    "description": "Methods of selecting one from many numeric predictors\n       for a regression model, to ensure that the additional\n       predictor has the maximum effect size.",
    "version": "0.2.0",
    "maintainer": "Tingting Zhan <tingtingzhan@gmail.com>",
    "url": "https://github.com/tingtingzhan/maxEff",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17037,
    "package_name": "maxRgain",
    "title": "Maximizing Polyclonal Selection Gains Using Integer Programming",
    "description": "Implements an Integer Programming-based method for optimising genetic gain in polyclonal selection, where the goal is to select a group of genotypes that jointly meet multi-trait selection criteria. The method uses predictors of genotypic effects obtained from the fitting of mixed models. Its application is demonstrated with grapevine data, but is applicable to other species and breeding contexts. For more details see Surgy et al. (2025) <doi:10.1007/s00122-025-04885-0>. ",
    "version": "1.0.2",
    "maintainer": "Sónia Surgy <soniasurgy@isa.ulisboa.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17039,
    "package_name": "maxbootR",
    "title": "Efficient Bootstrap Methods for Block Maxima",
    "description": "Implements state-of-the-art block bootstrap methods for extreme value \n  statistics based on block maxima. Includes disjoint blocks, sliding blocks, \n  relying on a circular transformation of blocks. \n  Fast C++ backends (via 'Rcpp') ensure scalability for large time series.",
    "version": "1.0.0",
    "maintainer": "Torben Staud <torben.staud@gmail.com>",
    "url": "https://torbenstaud.github.io/maxbootR/,\nhttps://github.com/torbenstaud/maxbootR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17040,
    "package_name": "maxcombo",
    "title": "The Group Sequential Max-Combo Test for Comparing Survival\nCurves",
    "description": "Functions for comparing survival curves using the max-combo test at a single timepoint or repeatedly at successive respective timepoints while controlling type I error (i.e., the group sequential setting), as published by Prior (2020) <doi:10.1177/0962280220931560>.  The max-combo test is a generalization of the weighted log-rank test, which itself is a generalization of the log-rank test, which is a commonly used statistical test for comparing survival curves, e.g., during or after a clinical trial as part of an effort to determine if a new drug or therapy is more effective at delaying undesirable outcomes than an established drug or therapy or a placebo.",
    "version": "1.0",
    "maintainer": "Tom Prior <tomjamesprior@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17045,
    "package_name": "maxnet",
    "title": "Fitting 'Maxent' Species Distribution Models with 'glmnet'",
    "description": "Procedures to fit species distributions models from occurrence records and environmental variables, using 'glmnet' for model fitting. Model structure is the same as for the 'Maxent' Java package, version 3.4.0, with the same feature types and regularization options.  See the 'Maxent' website <http://biodiversityinformatics.amnh.org/open_source/maxent> for more details.",
    "version": "0.1.4",
    "maintainer": "Steven Phillips <mrmaxent@gmail.com>",
    "url": "https://github.com/mrmaxent/maxnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17047,
    "package_name": "maxstat",
    "title": "Maximally Selected Rank Statistics",
    "description": "Maximally selected rank statistics with\n several p-value approximations.",
    "version": "0.7-26",
    "maintainer": "Torsten Hothorn <Torsten.Hothorn@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17065,
    "package_name": "mbr",
    "title": "Mass Balance Reconstruction",
    "description": "Mass-balance-adjusted Regression algorithm for streamflow reconstruction at sub-annual resolution (e.g., seasonal or monthly). The algorithm implements a penalty term to minimize the differences between the total sub-annual flows and the annual flow. The method is described in Nguyen et al (2020) <DOI:10.1002/essoar.10504791.1>.",
    "version": "0.0.1",
    "maintainer": "Hung Nguyen <ntthung@gmail.com>",
    "url": "https://github.com/ntthung/mbr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17066,
    "package_name": "mbrdr",
    "title": "Model-Based Response Dimension Reduction",
    "description": "Functions for model-based response dimension reduction. Usual dimension reduction methods in multivariate regression focus on the reduction of predictors, not responses.  The response dimension reduction is theoretically founded in Yoo and Cook (2008) <doi:10.1016/j.csda.2008.07.029>. Later, three model-based response dimension reduction approaches are proposed in Yoo (2016) <doi:10.1080/02331888.2017.1410152> and Yoo (2019) <doi:10.1016/j.jkss.2019.02.001>. The method by Yoo and Cook (2008) is based on non-parametric ordinary least squares, but the model-based approaches are done through maximum likelihood estimation. For two model-based response dimension reduction methods called principal fitted response reduction and unstructured principal fitted response reduction, chi-squared tests are provided for determining the dimension of the response subspace.",
    "version": "1.1.1",
    "maintainer": "Jae Keun Yoo <peter.yoo@ewha.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17067,
    "package_name": "mbreaks",
    "title": "Estimation and Inference for Structural Breaks in Linear\nRegression Models",
    "description": "Functions provide comprehensive treatments for estimating, inferring, testing and model selecting in linear regression models with structural breaks. The tests, estimation methods, inference and information criteria implemented are discussed in Bai and Perron (1998) \"Estimating and Testing Linear Models with Multiple Structural Changes\" <doi:10.2307/2998540>.",
    "version": "1.0.1",
    "maintainer": "Linh Nguyen <nguye535@purdue.edu>",
    "url": "https://github.com/RoDivinity/mbreaks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17068,
    "package_name": "mbrglm",
    "title": "Median Bias Reduction in Binomial-Response GLMs",
    "description": "Fit generalized linear models with binomial responses using  a median modified score approach (Kenne Pagui et al., 2016, <https://arxiv.org/abs/1604.04768>) to median bias reduction. This method respects equivariance under reparameterizations for each parameter component and also solves the infinite estimates problem (data separation).",
    "version": "0.0.1",
    "maintainer": "Euloge C. Kenne Pagui <kenne@stat.unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17069,
    "package_name": "mbsts",
    "title": "Multivariate Bayesian Structural Time Series",
    "description": "Tools for data analysis with multivariate Bayesian structural time series (MBSTS) models.  Specifically, the package provides facilities for implementing general structural time series models, flexibly adding on different time series components (trend, season, cycle, and regression), simulating them, fitting them to multivariate correlated time series data, conducting feature selection on the regression component.",
    "version": "3.0",
    "maintainer": "Ning Ning <patricianing@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17078,
    "package_name": "mcbiopi",
    "title": "Matrix Computation Based Identification of Prime Implicants",
    "description": "Computes the prime implicants or a minimal disjunctive normal form for a\n  logic expression presented by a truth table or a logic tree. Has been particularly \n  developed for logic expressions resulting from a logic regression analysis, i.e.\n  logic expressions typically consisting of up to 16 literals, where the prime implicants \n  are typically composed of a maximum of 4 or 5 literals.",
    "version": "1.1.6",
    "maintainer": "Holger Schwender <holger.schw@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17087,
    "package_name": "mcemGLM",
    "title": "Maximum Likelihood Estimation for Generalized Linear Mixed\nModels",
    "description": "Maximum likelihood estimation for generalized linear mixed models via Monte Carlo EM.\n    For a description of the algorithm see Brian S. Caffo, Wolfgang Jank and Galin L. Jones (2005)\n\t<DOI:10.1111/j.1467-9868.2005.00499.x>.",
    "version": "1.1.3",
    "maintainer": "Felipe Acosta Archila <acosta.felipe@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17090,
    "package_name": "mcgf",
    "title": "Markov Chain Gaussian Fields Simulation and Parameter Estimation",
    "description": "Simulating and estimating (regime-switching) Markov chain Gaussian \n    fields with covariance functions of the Gneiting class (Gneiting 2002) \n    <doi:10.1198/016214502760047113>. It supports parameter estimation by \n    weighted least squares and maximum likelihood methods, and produces Kriging \n    forecasts and intervals for existing and new locations.",
    "version": "1.1.1",
    "maintainer": "Tianxia Jia <tianxia.jia@ucalgary.ca>",
    "url": "https://github.com/tianxia-jia/mcgf,\nhttps://tianxia-jia.github.io/mcgf/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17091,
    "package_name": "mcgibbsit",
    "title": "Warnes and Raftery's 'MCGibbsit' MCMC Run Length and Convergence\nDiagnostic",
    "description": "\n  Implementation of Warnes & Raftery's MCGibbsit run-length and \n  convergence diagnostic for a set of (not-necessarily independent) \n  Markov Chain Monte Carlo (MCMC) samplers.  It combines the quantile \n  estimate error-bounding approach of the Raftery and Lewis MCMC run\n  length diagnostic `gibbsit` with the between verses within chain \n  approach of the Gelman and Rubin MCMC convergence diagnostic.",
    "version": "1.2.2",
    "maintainer": "Gregory R. Warnes <greg@warnes.net>",
    "url": "https://github.com/r-gregmisc/mcgibbsit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17101,
    "package_name": "mcmc",
    "title": "Markov Chain Monte Carlo",
    "description": "Simulates continuous distributions of random vectors using\n    Markov chain Monte Carlo (MCMC).  Users specify the distribution by an\n    R function that evaluates the log unnormalized density.  Algorithms\n    are random walk Metropolis algorithm (function metrop), simulated\n    tempering (function temper), and morphometric random walk Metropolis\n    (Johnson and Geyer, 2012, <doi:10.1214/12-AOS1048>,\n    function morph.metrop),\n    which achieves geometric ergodicity by change of variable.",
    "version": "0.9-8",
    "maintainer": "Charles J. Geyer <geyer@umn.edu>",
    "url": "http://www.stat.umn.edu/geyer/mcmc/,\nhttps://github.com/cjgeyer/mcmc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17102,
    "package_name": "mcmcderive",
    "title": "Derive MCMC Parameters",
    "description": "Generates derived parameter(s) from Monte Carlo Markov Chain\n    (MCMC) samples using R code. This allows Bayesian models to be fitted\n    without the inclusion of derived parameters which add unnecessary\n    clutter and slow model fitting. For more information on MCMC samples\n    see Brooks et al. (2011) <isbn:978-1-4200-7941-8>.",
    "version": "0.1.2",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://github.com/poissonconsulting/mcmcderive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17103,
    "package_name": "mcmcensemble",
    "title": "Ensemble Sampler for Affine-Invariant MCMC",
    "description": "Provides ensemble samplers for\n    affine-invariant Monte Carlo Markov Chain, which allow a faster\n    convergence for badly scaled estimation problems. Two samplers are\n    proposed: the 'differential.evolution' sampler from ter Braak and\n    Vrugt (2008) <doi:10.1007/s11222-008-9104-9> and the 'stretch' sampler\n    from Goodman and Weare (2010) <doi:10.2140/camcos.2010.5.65>.",
    "version": "3.2.0",
    "maintainer": "Hugo Gruson <hugo.gruson+R@normalesup.org>",
    "url": "https://hugogruson.fr/mcmcensemble/,\nhttps://github.com/Bisaloo/mcmcensemble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17104,
    "package_name": "mcmcr",
    "title": "Manipulate MCMC Samples",
    "description": "Functions and classes to store, manipulate and summarise\n    Monte Carlo Markov Chain (MCMC) samples. For more information see\n    Brooks et al. (2011) <isbn:978-1-4200-7941-8>.",
    "version": "0.6.2",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://github.com/poissonconsulting/mcmcr,\nhttps://poissonconsulting.github.io/mcmcr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17106,
    "package_name": "mcmcse",
    "title": "Monte Carlo Standard Errors for MCMC",
    "description": "Provides tools for computing Monte Carlo standard errors (MCSE) in\n    Markov chain Monte Carlo (MCMC) settings (survey in <doi:10.1201/b10905>,\n    Chapter 7). MCSE computation for expectation and quantile estimators is\n    supported as well as multivariate estimations. The package also provides\n    functions for computing effective sample size and for plotting Monte Carlo\n    estimates versus sample size.",
    "version": "1.5-1",
    "maintainer": "Dootika Vats <dootika@iitk.ac.in>",
    "url": "https://github.com/dvats/mcmcse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17108,
    "package_name": "mcmsupply",
    "title": "Estimating Public and Private Sector Contraceptive Market Supply\nShares",
    "description": "Family Planning programs and initiatives typically use nationally representative surveys to estimate key indicators of a country’s family planning progress. However, in recent years, routinely collected family planning services data (Service Statistics) have been used as a supplementary data source to bridge gaps in the surveys. The use of service statistics comes with the caveat that adjustments need to be made for missing private sector contributions to the contraceptive method supply chain. Evaluating the supply source of modern contraceptives often relies on Demographic Health Surveys (DHS), where many countries do not have recent data beyond 2015/16. Fortunately, in the absence of recent surveys we can rely on statistical model-based estimates and projections to fill the knowledge gap. We present a Bayesian, hierarchical, penalized-spline model with multivariate-normal spline coefficients, to account for across method correlations, to produce country-specific,annual estimates for the proportion of modern contraceptive methods coming from the public and private sectors. This package provides a quick and convenient way for users to access the DHS modern contraceptive supply share data at national and subnational administration levels, estimate, evaluate and plot annual estimates with uncertainty for a sample of low- and middle-income countries. Methods for the estimation of method supply shares at the national level are described in Comiskey, Alkema, Cahill (2022) <arXiv:2212.03844>.",
    "version": "1.1.1",
    "maintainer": "Hannah Comiskey <hannah.comiskey.2015@mumail.ie>",
    "url": "https://hannahcomiskey.github.io/mcmsupply/,\nhttps://hannahcomiskey.github.io/mcmsupply/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17111,
    "package_name": "mcount",
    "title": "Marginalized Count Regression Models",
    "description": "Implementation of marginalized models for zero-inflated count data. This package provides a tool to implement an estimation algorithm for the marginalized count models, which\n  directly makes inference on the effect of each covariate on the marginal mean of the outcome. \n  The method involves the marginalized zero-inflated Poisson model described \n  in Long et al. (2014) <doi:10.1002/sim.6293>.",
    "version": "1.0.0",
    "maintainer": "Zhengyang Zhou <zhengyang.zhou@unthsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17112,
    "package_name": "mcp",
    "title": "Regression with Multiple Change Points",
    "description": "Flexible and informed regression with Multiple Change Points. 'mcp' can infer change points in means, variances, autocorrelation structure, and any combination of these, as well as the parameters of the segments in between. All parameters are estimated with uncertainty and prediction intervals are supported - also near the change points. 'mcp' supports hypothesis testing via Savage-Dickey density ratio, posterior contrasts, and cross-validation. 'mcp' is described in Lindeløv (submitted) <doi:10.31219/osf.io/fzqxv> and generalizes the approach described in Carlin, Gelfand, & Smith (1992) <doi:10.2307/2347570> and Stephens (1994) <doi:10.2307/2986119>.",
    "version": "0.3.4",
    "maintainer": "Jonas Kristoffer Lindeløv <jonas@lindeloev.dk>",
    "url": "https://lindeloev.github.io/mcp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17126,
    "package_name": "mcvis",
    "title": "Multi-Collinearity Visualization",
    "description": "Visualize the relationship between linear regression variables and causes of multi-collinearity. \n    Implements the method in Lin et. al. (2020) <doi:10.1080/10618600.2020.1779729>. ",
    "version": "1.0.8",
    "maintainer": "Kevin Wang <kevin.wang09@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17131,
    "package_name": "mda",
    "title": "Mixture and Flexible Discriminant Analysis",
    "description": "Mixture and flexible discriminant analysis, multivariate\n        adaptive regression splines (MARS), BRUTO, and vector-response smoothing splines.\n\tHastie, Tibshirani and Friedman (2009) \"Elements of Statistical Learning (second edition, chap 12)\" Springer, New York. ",
    "version": "0.5-5",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17143,
    "package_name": "mdmb",
    "title": "Model Based Treatment of Missing Data",
    "description": "\n    Contains model-based treatment of missing data for regression \n    models with missing values in covariates or the dependent \n    variable using maximum likelihood or Bayesian estimation \n    (Ibrahim et al., 2005; <doi:10.1198/016214504000001844>;\n    Luedtke, Robitzsch, & West, 2020a, 2020b;\n    <doi:10.1080/00273171.2019.1640104><doi:10.1037/met0000233>).\n    The regression model can be nonlinear (e.g., interaction \n    effects, quadratic effects or B-spline functions). \n    Multilevel models with missing data in predictors are\n    available for Bayesian estimation. Substantive-model compatible \n    multiple imputation can be also conducted.",
    "version": "1.9-22",
    "maintainer": "Alexander Robitzsch <robitzsch@ipn.uni-kiel.de>",
    "url": "https://github.com/alexanderrobitzsch/mdmb,\nhttps://sites.google.com/site/alexanderrobitzsch2/software",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17145,
    "package_name": "mdpeer",
    "title": "Graph-Constrained Regression with Enhanced Regularization\nParameters Selection",
    "description": "Provides graph-constrained regression methods in which\n    regularization parameters are selected automatically via estimation of\n    equivalent Linear Mixed Model formulation. 'riPEER' (ridgified Partially\n    Empirical Eigenvectors for Regression) method employs a penalty term being\n    a linear combination of graph-originated and ridge-originated penalty terms,\n    whose two regularization parameters are ML estimators from corresponding\n    Linear Mixed Model solution; a graph-originated penalty term allows imposing\n    similarity between coefficients based on graph information given whereas\n    additional ridge-originated penalty term facilitates parameters estimation:\n    it reduces computational issues arising from singularity in a graph-originated\n    penalty matrix and yields plausible results in situations when graph information\n    is not informative. 'riPEERc' (ridgified Partially Empirical Eigenvectors\n    for Regression with constant) method utilizes addition of a diagonal matrix\n    multiplied by a predefined (small) scalar to handle the non-invertibility of\n    a graph Laplacian matrix. 'vrPEER' (variable reducted PEER) method performs\n    variable-reduction procedure to handle the non-invertibility of a graph\n    Laplacian matrix.",
    "version": "1.0.1",
    "maintainer": "Marta Karas <marta.karass@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17156,
    "package_name": "measuRing",
    "title": "Detection and Control of Tree-Ring Widths on Scanned Image\nSections",
    "description": "Identification of ring borders on scanned image sections from dendrochronological samples. Processing of image reflectances to produce gray matrices and time series of smoothed gray values. Luminance data is plotted on segmented images for users to perform both: visual identification of ring borders or control of automatic detection. Routines to visually include/exclude ring borders on the R graphical devices, or automatically detect ring borders using a linear detection algorithm. This algorithm detects ring borders according to positive/negative extreme values in the smoothed time-series of gray values. Most of the in-package routines can be recursively implemented using the multiDetect() function.",
    "version": "0.5.2",
    "maintainer": "Wilson Lara <wilarhen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17160,
    "package_name": "meboot",
    "title": "Maximum Entropy Bootstrap for Time Series",
    "description": "Maximum entropy density based dependent data bootstrap.\n  An algorithm is provided to create a population of time series (ensemble)\n  without assuming stationarity. The reference paper (Vinod, H.D., 2004\n  <DOI:10.1016/j.jempfin.2003.06.002>) explains how the algorithm satisfies\n  the ergodic theorem and the central limit theorem.",
    "version": "1.4-9.5",
    "maintainer": "Fred Viole <ovvo.open.source@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17167,
    "package_name": "mederrRank",
    "title": "Bayesian Methods for Identifying the Most Harmful Medication\nErrors",
    "description": "Two distinct but related statistical approaches to the problem of identifying the combinations of medication error characteristics that are more likely to result in harm are implemented in this package: 1) a Bayesian hierarchical model with optimal Bayesian ranking on the log odds of harm, and 2) an empirical Bayes model that estimates the ratio of the observed count of harm to the count that would be expected if error characteristics and harm were independent. In addition, for the Bayesian hierarchical model, the package provides functions to assess the sensitivity of results to different specifications of the random effects distributions.",
    "version": "0.1.0",
    "maintainer": "Sergio Venturini <sergio.venturini@unicatt.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17174,
    "package_name": "mediationsens",
    "title": "Simulation-Based Sensitivity Analysis for Causal Mediation\nStudies",
    "description": "Simulation-based sensitivity analysis for causal mediation studies. It numerically and graphically evaluates the sensitivity of causal mediation analysis results \n to the presence of unmeasured pretreatment confounding. The proposed method has primary advantages over existing methods. \n First, using an unmeasured pretreatment confounder conditional associations with the treatment, mediator, and outcome as \n sensitivity parameters, the method enables users to intuitively assess sensitivity in reference to prior knowledge about the \n strength of a potential unmeasured pretreatment confounder. Second, the method accurately reflects the influence of unmeasured \n pretreatment confounding on the efficiency of estimation of the causal effects. Third, the method can be implemented in \n different causal mediation analysis approaches, including regression-based, simulation-based, and propensity score-based \n methods. It is applicable to both randomized experiments and observational studies.",
    "version": "0.0.3",
    "maintainer": "Xu Qin <xuqin@pitt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17183,
    "package_name": "meerva",
    "title": "Analysis of Data with Measurement Error Using a Validation\nSubsample",
    "description": "Sometimes data for analysis are obtained using more convenient or less expensive means yielding \"surrogate\" variables for what could be obtained more accurately, albeit with less convenience; or less conveniently or at more expense yielding \"reference\" variables, thought of as being measured without error. Analysis of the surrogate variables measured with error generally yields biased estimates when the objective is to make inference about the reference variables. Often it is thought that ignoring the measurement error in surrogate variables only biases effects toward the null hypothesis, but this need not be the case. Measurement errors may bias parameter estimates either toward or away from the null hypothesis.    If one has a data set with surrogate variable data from the full sample, and also reference variable data from a randomly selected subsample, then one can assess the bias introduced by measurement error in parameter estimation, and use this information to derive improved estimates based upon all available data. Formulaically these estimates based upon the reference variables from the validation subsample combined with the surrogate variables from the whole sample can be interpreted as starting with the estimate from reference variables in the validation subsample, and \"augmenting\" this with additional information from the surrogate variables. This suggests the term \"augmented\" estimate.    The meerva package calculates these augmented estimates in the regression setting when there is a randomly selected subsample with both surrogate and reference variables. Measurement errors may be differential or non-differential, in any or all predictors (simultaneously) as well as outcome. The augmented estimates derive, in part, from the multivariate correlation between regression model parameter estimates from the reference variables and the surrogate variables, both from the validation subset. Because the validation subsample is chosen at random any biases imposed by measurement error, whether non-differential or differential, are reflected in this correlation and these correlations can be used to derive estimates for the reference variables using data from the whole sample.    The main functions in the package are meerva.fit which calculates estimates for a dataset, and meerva.sim.block which simulates multiple datasets as described by the user, and analyzes these datasets, storing the regression coefficient estimates for inspection.  The augmented estimates, as well as how measurement error may arise in practice, is described in more detail by Kremers WK (2021) <arXiv:2106.14063> and is an extension of the works by Chen Y-H, Chen H. (2000) <doi:10.1111/1467-9868.00243>, Chen Y-H. (2002) <doi:10.1111/1467-9868.00324>, Wang X, Wang Q (2015) <doi:10.1016/j.jmva.2015.05.017> and Tong J, Huang J, Chubak J, et al. (2020) <doi:10.1093/jamia/ocz180>.",
    "version": "0.2-2",
    "maintainer": "Walter K Kremers <kremers.walter@mayo.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17200,
    "package_name": "memisc",
    "title": "Management of Survey Data and Presentation of Analysis Results",
    "description": "An infrastructure for the management of survey data including\n        value labels, definable missing values, recoding of variables,\n        production of code books, and import of (subsets of) 'SPSS' and\n        'Stata' files is provided. Further, the package allows to produce\n        tables and data frames of arbitrary descriptive statistics and\n        (almost) publication-ready tables of regression model\n        estimates, which can be exported to 'LaTeX' and HTML.",
    "version": "0.99.31.8.3",
    "maintainer": "Martin Elff <memisc@elff.eu>",
    "url": "https://melff.github.io/memisc/,https://github.com/melff/memisc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17210,
    "package_name": "merTools",
    "title": "Tools for Analyzing Mixed Effect Regression Models",
    "description": "Provides methods for extracting results from mixed-effect model\n    objects fit with the 'lme4' package. Allows construction of prediction intervals\n    efficiently from large scale linear and generalized linear mixed-effects models.\n    This method draws from the simulation framework used in the Gelman and Hill (2007) textbook:\n    Data Analysis Using Regression and Multilevel/Hierarchical Models.",
    "version": "0.6.3",
    "maintainer": "Jared E. Knowles <jared@civilytics.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17216,
    "package_name": "mermboost",
    "title": "Gradient Boosting for Generalized Additive Mixed Models",
    "description": "Provides a novel framework to estimate mixed models via gradient \n    boosting. The implemented functions are based on the 'mboost' and 'lme4' packages, \n    and the family range is therefore determined by 'lme4'. A correction mechanism \n    for cluster-constant covariates is implemented, as well as estimation of the \n    covariance of random effects. These methods are described in \n    the accompanying publication; see <doi:10.1007/s11222-025-10612-y> for details.",
    "version": "0.1.1",
    "maintainer": "Lars Knieper <lars.knieper@uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17220,
    "package_name": "mesonet",
    "title": "Download and Process Oklahoma Mesonet Data",
    "description": "A collection of functions to download and process weather data from\n  the Oklahoma Mesonet <https://mesonet.org>. Functions are available for downloading station metadata,\n  downloading Mesonet time series (MTS) files, importing MTS files into R, and \n  converting soil temperature change measurements into soil matric potential and\n  volumetric soil moisture.",
    "version": "0.0.2",
    "maintainer": "Phillip D. Alderman <phillip.alderman@okstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17230,
    "package_name": "meta.shrinkage",
    "title": "Meta-Analyses for Simultaneously Estimating Individual Means",
    "description": "Implement meta-analyses for simultaneously estimating individual means with shrinkage,\n isotonic regression and pretests. Include our original implementation of the isotonic regression via the pool-adjacent-violators algorithm (PAVA) algorithm.\n For the pretest estimator, the confidence interval for individual means are provided.\n Methodologies were published in \n Taketomi et al. (2021) <doi:10.3390/axioms10040267>, Taketomi et al. (2022) <doi:10.3390/a15010026>, \n Taketomi et al. (2023-) (under review).",
    "version": "0.1.4",
    "maintainer": "Nanami Taketomi <nnmamikrn@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17231,
    "package_name": "meta4diag",
    "title": "Meta-Analysis for Diagnostic Test Studies",
    "description": "Bayesian inference analysis for bivariate meta-analysis of diagnostic test studies using integrated nested Laplace approximation with INLA. A purpose built graphic user interface is available. The installation of R package INLA is compulsory for successful usage. The INLA package can be obtained from <https://www.r-inla.org>. We recommend the testing version, which can be downloaded by running: install.packages(\"INLA\", repos=c(getOption(\"repos\"), INLA=\"https://inla.r-inla-download.org/R/testing\"), dep=TRUE).",
    "version": "2.1.1",
    "maintainer": "Jingyi Guo <jingyi.guo@ntnu.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17248,
    "package_name": "metaSVR",
    "title": "Support Vector Regression with Metaheuristic Algorithms\nOptimization",
    "description": "Provides a hybrid modeling framework combining Support Vector Regression (SVR) with metaheuristic optimization algorithms, including the Archimedes Optimization Algorithm (AO) (Hashim et al. (2021) <doi:10.1007/s10489-020-01893-z>), Coot Bird Optimization (CBO) (Naruei & Keynia (2021) <doi:10.1016/j.eswa.2021.115352>), and their hybrid (AOCBO), as well as several others such as Harris Hawks Optimization (HHO) (Heidari et al. (2019) <doi:10.1016/j.future.2019.02.028>), Gray Wolf Optimizer (GWO) (Mirjalili et al. (2014) <doi:10.1016/j.advengsoft.2013.12.007>), Ant Lion Optimization (ALO) (Mirjalili (2015) <doi:10.1016/j.advengsoft.2015.01.010>), and Enhanced Harris Hawk Optimization with Coot Bird Optimization (EHHOCBO) (Cui et al. (2023) <doi:10.32604/cmes.2023.026019>). The package enables automatic tuning of SVR hyperparameters (cost, gamma, and epsilon) to enhance prediction performance. Suitable for regression tasks in domains such as renewable energy forecasting and hourly data prediction. For more details about implementation and parameter bounds see: Setiawan et al. (2021) <doi:10.1016/j.procs.2020.12.003> and Liu et al. (2018) <doi:10.1155/2018/6076475>.  ",
    "version": "0.1.0",
    "maintainer": "Rechtiana Putri Arini <rparini17@gmail.com>",
    "url": "https://github.com/rechtianaputri/metaSVR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17250,
    "package_name": "metaSurvival",
    "title": "Meta-Analysis of a Single Survival Curve",
    "description": "To assess a summary survival curve from survival probabilities and number of at-risk patients collected at various points in time in various studies, and to test the between-strata heterogeneity.",
    "version": "0.1.0",
    "maintainer": "Shubhram Pandey <shubhram1992@gmail.com>",
    "url": "https://github.com/shubhrampandey/metaSurvival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17259,
    "package_name": "metabup",
    "title": "Bayesian Meta-Analysis Using Basic Uncertain Pooling",
    "description": "Contains functions that allow Bayesian meta-analysis (1) with binomial data, counts(y) and total counts (n) or, (2) with user-supplied point estimates and associated variances.   Case (1) provides an analysis based on the logit transformation of the sample proportion. This methodology is also appropriate for combining data from sample surveys and related sources. The functions can  calculate the corresponding similarity matrix. More details can be found in Cahoy and Sedransk (2023), Cahoy and Sedransk (2022)  <doi:10.1007/s42519-018-0027-2>, Evans and Sedransk (2001) <doi:10.1093/biomet/88.3.643>, and Malec and Sedransk (1992) <doi:10.1093/biomet/79.3.593>.",
    "version": "0.1.3",
    "maintainer": "Dexter Cahoy <dexter.cahoy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17273,
    "package_name": "metagam",
    "title": "Meta-Analysis of Generalized Additive Models",
    "description": "Meta-analysis of generalized additive\n    models and generalized additive mixed models. A typical use case is\n    when data cannot be shared across locations, and an overall meta-analytic\n    fit is sought. 'metagam' provides functionality for removing individual\n    participant data from models computed using the 'mgcv' and 'gamm4' packages such\n    that the model objects can be shared without exposing individual data.\n    Furthermore, methods for meta-analysing these fits are provided. The implemented\n    methods are described in Sorensen et al. (2020), <doi:10.1016/j.neuroimage.2020.117416>,\n    extending previous works by Schwartz and Zanobetti (2000)\n    and Crippa et al. (2018) <doi:10.6000/1929-6029.2018.07.02.1>.",
    "version": "0.4.1",
    "maintainer": "Oystein Sorensen <oystein.sorensen@psykologi.uio.no>",
    "url": "https://lifebrain.github.io/metagam/,\nhttps://github.com/Lifebrain/metagam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17287,
    "package_name": "metamedian",
    "title": "Meta-Analysis of Medians",
    "description": "Implements several methods to meta-analyze studies that report the \n    sample median of the outcome. The methods described by \n    McGrath et al. (2019) <doi:10.1002/sim.8013>, \n    Ozturk and Balakrishnan (2020) <doi:10.1002/sim.8738>, and \n    McGrath et al. (2020a) <doi:10.1002/bimj.201900036> can be \n    applied to directly meta-analyze the median or difference of medians between \n    groups. Additionally, a number of methods (e.g., McGrath et al. (2020b) \n    <doi:10.1177/0962280219889080>, Cai et al. (2021) \n    <doi:10.1177/09622802211047348>, and McGrath et al. (2023) \n    <doi:10.1177/09622802221139233>) are implemented to estimate \n    study-specific (difference of) means and their standard errors in order to \n    estimate the pooled (difference of) means. Methods for meta-analyzing median \n    survival times (McGrath et al. (2025) <doi:10.48550/arXiv.2503.03065>) are \n    also implemented. See McGrath et al. (2024) <doi:10.1002/jrsm.1686> for a \n    detailed guide on using the package.",
    "version": "1.2.1",
    "maintainer": "Sean McGrath <sean.mcgrath514@gmail.com>",
    "url": "https://github.com/stmcg/metamedian,\nhttps://doi.org/10.1002/jrsm.1686",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17296,
    "package_name": "metapack",
    "title": "Bayesian Meta-Analysis and Network Meta-Analysis",
    "description": "Contains functions performing Bayesian inference for meta-analytic and network meta-analytic models through Markov chain Monte Carlo algorithm. Currently, the package implements Hui Yao, Sungduk Kim, Ming-Hui Chen, Joseph G. Ibrahim, Arvind K. Shah, and Jianxin Lin (2015) <doi:10.1080/01621459.2015.1006065> and Hao Li, Daeyoung Lim, Ming-Hui Chen, Joseph G. Ibrahim, Sungduk Kim, Arvind K. Shah, Jianxin Lin (2021) <doi:10.1002/sim.8983>. For maximal computational efficiency, the Markov chain Monte Carlo samplers for each model, written in C++, are fine-tuned. This software has been developed under the auspices of the National Institutes of Health and Merck & Co., Inc., Kenilworth, NJ, USA.",
    "version": "0.3",
    "maintainer": "Daeyoung Lim <Daeyoung.Lim@fda.hhs.gov>",
    "url": "https://events.stat.uconn.edu/metapack/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17311,
    "package_name": "metatest",
    "title": "Fit and Test Metaregression Models",
    "description": "Fits and tests meta regression models and generates a\n number of useful test statistics: next to t- and z-tests, the likelihood ratio,\n bartlett corrected likelihood ratio and permutation tests are performed on\n the model coefficients.",
    "version": "1.0-5",
    "maintainer": "Ingmar Visser <i.visser@uva.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17342,
    "package_name": "metools",
    "title": "Macroeconomics Tools",
    "description": "Provides a number of functions to facilitate the handling and production of reports using time series data.\n    The package was developed to be understandable for beginners, so some functions aim to transform processes that would be\n    complex into functions with a few lines. The main advantage of using the 'metools' package is the ease of producing reports and\n    working with time series using a few lines of code, so the code is clean and easy to understand/maintain. \n    Learn more about the 'metools' at <https://metoolsr.wordpress.com>.",
    "version": "1.0.0",
    "maintainer": "João Victor Gomes de Araujo Santana <jvg.santana@gmail.com>",
    "url": "https://metoolsr.wordpress.com,https://github.com/jvg0mes/metools,https://jvg0mes.github.io/metoolsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17354,
    "package_name": "mexhaz",
    "title": "Mixed Effect Excess Hazard Models",
    "description": "Fit flexible (excess) hazard regression models with the possibility of including non-proportional effects of covariables and of adding a random effect at the cluster level (corresponding to a shared frailty). A detailed description of the package functionalities is provided in Charvat and Belot (2021) <doi: 10.18637/jss.v098.i14>.",
    "version": "2.6",
    "maintainer": "Hadrien Charvat  <h.charvat.ef@juntendo.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17356,
    "package_name": "mfGARCH",
    "title": "Mixed-Frequency GARCH Models",
    "description": "Estimating GARCH-MIDAS (MIxed-DAta-Sampling) models (Engle, Ghysels, Sohn, 2013, <doi:10.1162/REST_a_00300>) and related statistical inference, accompanying the paper \"Two are better than one: Volatility forecasting using multiplicative component GARCH models\" by Conrad and Kleen (2020, <doi:10.1002/jae.2742>). The GARCH-MIDAS model decomposes the conditional variance of (daily) stock returns into a short- and long-term component, where the latter may depend on an exogenous covariate sampled at a lower frequency. ",
    "version": "0.2.1",
    "maintainer": "Onno Kleen <r@onnokleen.de>",
    "url": "https://github.com/onnokleen/mfGARCH/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17360,
    "package_name": "mfp",
    "title": "Multivariable Fractional Polynomials",
    "description": "Multivariable Fractional Polynomial algorithm for model-building. Fractional polynomials are used to represent curvature in regression models. A key reference is Royston and Altman, 1994.",
    "version": "1.5.5.1",
    "maintainer": "Georg Heinze <georg.heinze@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17361,
    "package_name": "mfp2",
    "title": "Multivariable Fractional Polynomial Models with Extensions",
    "description": "Multivariable fractional polynomial algorithm simultaneously selects variables and functional forms in both generalized linear models and Cox proportional hazard models. Key references are Royston and Altman (1994) <doi:10.2307/2986270> and Royston and Sauerbrei (2008, ISBN:978-0-470-02842-1). In addition, it can model a sigmoid relationship between variable x and an outcome variable y using the approximate cumulative distribution transformation proposed by Royston (2014) <doi:10.1177/1536867X1401400206>. This feature distinguishes it from a standard fractional polynomial function, which lacks the ability to achieve such modeling.",
    "version": "1.0.1",
    "maintainer": "Edwin Kipruto <edwin.kipruto@uniklinik-freiburg.de>",
    "url": "https://github.com/EdwinKipruto/mfp2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17366,
    "package_name": "mgcViz",
    "title": "Visualisations for Generalized Additive Models",
    "description": "Extension of the 'mgcv' package, providing visual tools for Generalized Additive Models that exploit the additive structure of such models, scale to large data sets and can be used in conjunction with a wide range of response distributions. The focus is providing visual methods for better understanding the model output and for aiding model checking and development beyond simple exponential family regression. The graphical framework is based on the layering system provided by 'ggplot2'.",
    "version": "0.2.1",
    "maintainer": "Matteo Fasiolo <matteo.fasiolo@gmail.com>",
    "url": "https://github.com/mfasiolo/mgcViz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17372,
    "package_name": "mglmn",
    "title": "Model Averaging for Multivariate GLM with Null Models",
    "description": "Tools for univariate and multivariate generalized linear models with model averaging and null model technique.",
    "version": "0.1.0",
    "maintainer": "Masatoshi Katabuchi <mattocci27@gmail.com>",
    "url": "https://github.com/mattocci27/mglmn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17373,
    "package_name": "mgm",
    "title": "Estimating Time-Varying k-Order Mixed Graphical Models",
    "description": "Estimation of k-Order time-varying Mixed Graphical Models and mixed VAR(p) models via elastic-net regularized neighborhood regression. For details see Haslbeck & Waldorp (2020) <doi:10.18637/jss.v093.i08>.",
    "version": "1.2-15",
    "maintainer": "Jonas Haslbeck  <jonashaslbeck@gmail.com>",
    "url": "https://www.jstatsoft.org/article/view/v093i08",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17380,
    "package_name": "mhazard",
    "title": "Nonparametric and Semiparametric Methods for Multivariate\nFailure Time Data",
    "description": "Nonparametric survival function estimates and semiparametric\n    regression for the multivariate failure time data with right-censoring.\n    For nonparametric survival function estimates, the Volterra, Dabrowska,\n    and Prentice-Cai estimates for bivariate failure time data may be\n    computed as well as the Dabrowska estimate for the trivariate failure\n    time data. Bivariate marginal hazard rate regression can be fitted for\n    the bivariate failure time data. Functions are also provided to compute\n    (bootstrap) confidence intervals and plot the estimates of the bivariate\n    survival function. For details, see \"The Statistical Analysis of\n    Multivariate Failure Time Data: A Marginal Modeling Approach\", Prentice,\n    R., Zhao, S. (2019, ISBN: 978-1-4822-5657-4), CRC Press.",
    "version": "0.2.3",
    "maintainer": "Eric Bair <eric.bair@sciome.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17382,
    "package_name": "mhsmm",
    "title": "Inference for Hidden Markov and Semi-Markov Models",
    "description": "Parameter estimation and prediction for hidden Markov and semi-Markov models for data with multiple observation sequences.  Suitable for equidistant time series data, with multivariate and/or missing data. Allows user defined emission distributions.",
    "version": "0.4.21",
    "maintainer": "Jared O'Connell <jaredoconnell@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17385,
    "package_name": "mi",
    "title": "Missing Data Imputation and Model Checking",
    "description": "The mi package provides functions for data manipulation, imputing missing values in an approximate Bayesian framework, diagnostics of the models used to generate the imputations, confidence-building mechanisms to validate some of the assumptions of the imputation algorithm, and functions to analyze multiply imputed data sets with the appropriate degree of sampling uncertainty.",
    "version": "1.2",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "url": "https://sites.stat.columbia.edu/gelman/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17387,
    "package_name": "miCoPTCM",
    "title": "Promotion Time Cure Model with Mis-Measured Covariates",
    "description": "Fits Semiparametric Promotion Time Cure Models, taking into account (using a \n\t\t\t corrected score approach or the SIMEX algorithm) or not the measurement error\n\t\t\t in the covariates, using a backfitting approach to maximize the likelihood.",
    "version": "1.1",
    "maintainer": "Aurelie Bertrand <aurelie.bertrand@uclouvain.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17399,
    "package_name": "miRecSurv",
    "title": "Left-Censored Recurrent Events Survival Models",
    "description": "Fitting recurrent events survival models for\n    left-censored data with multiple imputation of the number of previous episodes.\n    See Hernández-Herrera G, Moriña D, Navarro A. (2020) <arXiv:2007.15031>.",
    "version": "1.0.2",
    "maintainer": "David Moriña <dmorina@ub.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17424,
    "package_name": "micompr",
    "title": "Multivariate Independent Comparison of Observations",
    "description": "A procedure for comparing multivariate samples associated with\n    different groups. It uses principal component analysis to convert\n    multivariate observations into a set of linearly uncorrelated statistical\n    measures, which are then compared using a number of statistical methods. The\n    procedure is independent of the distributional properties of samples and\n    automatically selects features that best explain their differences, avoiding\n    manual selection of specific points or summary statistics. It is appropriate\n    for comparing samples of time series, images, spectrometric measures or\n    similar multivariate observations. This package is described in Fachada et\n    al. (2016) <doi:10.32614/RJ-2016-055>.",
    "version": "1.3.0",
    "maintainer": "Nuno Fachada <faken@fakenmc.com>",
    "url": "https://github.com/nunofachada/micompr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17451,
    "package_name": "microsynth",
    "title": "Synthetic Control Methods with Micro- And Meso-Level Data",
    "description": "A generalization of the 'Synth' package that is\n    designed for data at a more granular level (e.g., micro-level).\n    Provides functions to construct weights (including propensity\n    score-type weights) and run analyses for synthetic control methods\n    with micro- and meso-level data; see Robbins, Saunders, and Kilmer\n    (2017) <doi:10.1080/01621459.2016.1213634> and Robbins and Davenport\n    (2021) <doi:10.18637/jss.v097.i02>.",
    "version": "2.0.51",
    "maintainer": "Michael Robbins <mrobbins@rand.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17453,
    "package_name": "micss",
    "title": "Modified Iterative Cumulative Sum of Squares Algorithm",
    "description": "Companion package of Carrion-i-Silvestre & Sansó (2023): \n  \"Generalized Extreme Value Approximation to the CUMSUMQ Test for Constant \n  Unconditional Variance in Heavy-Tailed Time Series\". It implements the Modified \n  Iterative Cumulative Sum of Squares Algorithm, which is an extension of \n  the Iterative Cumulative Sum of Squares (ICSS) Algorithm of Inclan and Tiao (1994), and it checks for changes in the \n  unconditional variance of a time series controlling for the tail index of \n  the underlying distribution. The fourth order moment is estimated non-parametrically\n  to avoid the size problems when the innovations are non-Gaussian (see, Sansó et al., 2004). \n  Critical values and p-values are generated using a Generalized Extreme Value distribution approach.\n  References\n  Carrion-i-Silvestre J.J & Sansó A (2023) <https://www.ub.edu/irea/working_papers/2023/202309.pdf>.\n  Inclan C & Tiao G.C (1994) <doi:10.1080/01621459.1994.10476824>,\n  Sansó A & Aragó V & Carrion-i-Silvestre J.L (2004) <https://dspace.uib.es/xmlui/bitstream/handle/11201/152078/524035.pdf>.",
    "version": "0.2.0",
    "maintainer": "Andreu Sansó <andreu.sanso@uib.eu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17454,
    "package_name": "micvar",
    "title": "Order Selection in Vector Autoregression by Mean Square\nInformation Criteria",
    "description": "Implements order selection for Vector Autoregressive (VAR) models using the Mean Square Information Criterion (MIC). Unlike standard methods such as AIC and BIC, MIC is likelihood-free. This method consistently estimates VAR order and has robust performance under model misspecification. For more details, see Hellstern and Shojaie (2025) <doi:10.48550/arXiv.2511.19761>.",
    "version": "0.1.0",
    "maintainer": "Michael Hellstern <mikeh1@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17456,
    "package_name": "midas2",
    "title": "Bayesian Platform Design with Subgroup Efficacy\nExploration(MIDAS-2)",
    "description": "The rapid screening of effective and optimal therapies from large numbers of candidate combinations, as well as exploring subgroup efficacy, remains challenging, which necessitates innovative, integrated, and efficient trial designs(Yuan, Y., et al. (2016) <doi:10.1002/sim.6971>). MIDAS-2 package enables quick and continuous screening of promising combination strategies and exploration of their subgroup effects within a unified platform design framework. We used a regression model to characterize the efficacy pattern in subgroups. Information borrowing was applied through Bayesian hierarchical model to improve trial efficiency considering the limited sample size in subgroups(Cunanan, K. M., et al. (2019) <doi:10.1177/1740774518812779>). MIDAS-2 provides an adaptive drug screening and subgroup exploring framework to accelerate immunotherapy development in an efficient, accurate, and integrated fashion(Wathen, J. K., & Thall, P. F. (2017) <doi: 10.1177/1740774517692302>).",
    "version": "1.1.0",
    "maintainer": "Su Liwen <cpusullivan@163.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17459,
    "package_name": "midasr",
    "title": "Mixed Data Sampling Regression",
    "description": "Methods and tools for mixed frequency time series data analysis.\n    Allows estimation, model selection and forecasting for MIDAS regressions.",
    "version": "0.9",
    "maintainer": "Vaidotas Zemlys-Balevičius <zemlys@gmail.com>",
    "url": "http://mpiktas.github.io/midasr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17466,
    "package_name": "miebl",
    "title": "Performance Criteria Modeler for Discrete Trial Training",
    "description": "Provides a tool for computing probabilities and other quantities that are relevant in selecting performance criteria for discrete trial training. The main function, miebl(), computes Bayesian and frequentist probabilities and bounds for each of n possible performance criterion choices when attempting to determine a student's true mastery level by counting their number of successful attempts at displaying learning among n trials. The reporting function miebl_re() takes output from miebl() and prepares it into a brief report for a specific criterion. miebl_cp() combines 2 to 5 distributions of true mastery level given performance criterion in one plot for comparison. Ramos (2025) <doi:10.1007/s40617-025-01058-9>.",
    "version": "0.1.0",
    "maintainer": "Mark Ramos <mlr6219@psu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17471,
    "package_name": "migraph",
    "title": "Inferential Methods for Multimodal and Other Networks",
    "description": "A set of tools for testing networks.\n   It includes functions for univariate and multivariate \n   conditional uniform graph and quadratic assignment procedure testing,\n   and network regression.\n   The package is a complement to \n   'Multimodal Political Networks' (2021, ISBN:9781108985000),\n   and includes various datasets used in the book.\n   Built on the 'manynet' package, all functions operate with matrices, \n   edge lists, and 'igraph', 'network', and 'tidygraph' objects,\n   and on one-mode and two-mode (bipartite) networks.",
    "version": "1.5.6",
    "maintainer": "James Hollway <james.hollway@graduateinstitute.ch>",
    "url": "https://stocnet.github.io/migraph/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17493,
    "package_name": "mind",
    "title": "Multivariate Model Based Inference for Domains",
    "description": "Allows users to produce estimates and MSE for multivariate variables using Linear Mixed Model. The package follows the approach of Datta, Day and Basawa (1999) <doi:10.1016/S0378-3758(98)00147-5>.",
    "version": "1.1.0",
    "maintainer": "Andrea Fasulo <fasulo@istat.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17522,
    "package_name": "minque",
    "title": "Various Linear Mixed Model Analyses",
    "description": "This package offers three important components: (1) to construct a use-defined linear mixed model, (2) to employ one of linear mixed model approaches: minimum norm quadratic unbiased estimation (MINQUE) (Rao, 1971) for variance component estimation and random effect prediction; and (3) to employ a jackknife resampling technique to conduct various statistical tests. In addition, this package provides the function for model or data evaluations.This R package offers fast computations for large data sets analyses for various irregular data structures.",
    "version": "2.0.0",
    "maintainer": "Jixiang Wu <jixiang.wu@sdstate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17536,
    "package_name": "misPRIME",
    "title": "Partial Replacement Imputation Estimation for Missing Covariates",
    "description": "Partial Replacement Imputation Estimation (PRIME) can overcome problems caused by missing covariates in additive partially linear model. PRIME conducts imputation and regression simultaneously with known and unknown model structure. More details can be referred to \n    Zishu Zhan, Xiangjie Li and Jingxiao Zhang. (2022) <arXiv:2205.14994>.",
    "version": "0.1.0",
    "maintainer": "Zishu Zhan <zishu927@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17540,
    "package_name": "miscFuncs",
    "title": "Miscellaneous Useful Functions Including LaTeX Tables, Kalman\nFiltering, QQplots with Simulation-Based Confidence Intervals,\nLinear Regression Diagnostics and Development Tools",
    "description": "Implementing various things including functions for LaTeX tables,\n    the Kalman filter, QQ-plots with simulation-based confidence intervals, linear regression diagnostics, web scraping, development tools, relative risk and odds\n    rati, GARCH(1,1) Forecasting.",
    "version": "1.5-10",
    "maintainer": "Benjamin M. Taylor <benjamin.taylor.software@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17542,
    "package_name": "miscTools",
    "title": "Miscellaneous Tools and Utilities",
    "description": "Miscellaneous small tools and utilities.\n   Many of them facilitate the work with matrices,\n   e.g. inserting rows or columns, creating symmetric matrices,\n   or checking for semidefiniteness.\n   Other tools facilitate the work with regression models,\n   e.g. extracting the standard errors,\n   obtaining the number of (estimated) parameters,\n   or calculating R-squared values.",
    "version": "0.6-28",
    "maintainer": "Arne Henningsen <arne.henningsen@gmail.com>",
    "url": "https://github.com/arne-henningsen/miscTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17547,
    "package_name": "mispr",
    "title": "Multiple Imputation with Sequential Penalized Regression",
    "description": "Generates multivariate imputations using sequential regression with L2 penalty. For more details see Zahid and Heumann (2018) <doi:10.1177/0962280218755574>.",
    "version": "1.0.0",
    "maintainer": "Faisal Maqbool Zahid <faisalmz99@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17562,
    "package_name": "missingHE",
    "title": "Missing Outcome Data in Health Economic Evaluation",
    "description": "Contains a suite of functions for health economic evaluations with missing outcome data. \n  The package can fit different types of statistical models under a fully Bayesian approach using the software 'JAGS' (which should be installed locally and which is loaded in 'missingHE' via the 'R' package 'R2jags'). \n  Three classes of models can be fitted under a variety of missing data assumptions: selection models, pattern mixture models and hurdle models.\n  In addition to model fitting, 'missingHE' provides a set of specialised functions to assess model convergence and fit, and to summarise the statistical and economic results using different types of measures and graphs. \n  The methods implemented are described in Mason (2018) <doi:10.1002/hec.3793>, Molenberghs (2000) <doi:10.1007/978-1-4419-0300-6_18> and Gabrio (2019) <doi:10.1002/sim.8045>.",
    "version": "1.5.1",
    "maintainer": "Andrea Gabrio <a.gabrio@maastrichtuniversity.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17563,
    "package_name": "missoNet",
    "title": "Joint Sparse Regression & Network Learning with Missing Data",
    "description": "Simultaneously estimates sparse regression coefficients and\n    response network structure in multivariate models with missing data.\n    Unlike traditional approaches requiring imputation, handles\n    missingness natively through unbiased estimating equations (MCAR/MAR\n    compatible). Employs dual L1 regularization with automated selection\n    via cross-validation or information criteria. Includes parallel\n    computation, warm starts, adaptive grids, publication-ready\n    visualizations, and prediction methods.  Ideal for genomics,\n    neuroimaging, and multi-trait studies with incomplete high-dimensional\n    outcomes. See Zeng et al. (2025) <doi:10.48550/arXiv.2507.05990>.",
    "version": "1.5.1",
    "maintainer": "Yixiao Zeng <yixiao.zeng@mail.mcgill.ca>",
    "url": "https://github.com/yixiao-zeng/missoNet,\nhttps://arxiv.org/abs/2507.05990",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17578,
    "package_name": "mixAK",
    "title": "Multivariate Normal Mixture Models and Mixtures of Generalized\nLinear Mixed Models Including Model Based Clustering",
    "description": "Contains a mixture of statistical methods including the MCMC methods to analyze normal mixtures. Additionally, model based clustering methods are implemented to perform classification based on (multivariate) longitudinal (or otherwise correlated) data. The basis for such clustering is a mixture of multivariate generalized linear mixed models. The package is primarily related to the publications Komárek (2009, Comp. Stat. and Data Anal.) <doi:10.1016/j.csda.2009.05.006> and Komárek and Komárková (2014, J. of Stat. Soft.) <doi:10.18637/jss.v059.i12>. It also implements methods published in Komárek and Komárková (2013, Ann. of Appl. Stat.) <doi:10.1214/12-AOAS580>, Hughes, Komárek, Bonnett, Czanner, García-Fiñana (2017, Stat. in Med.) <doi:10.1002/sim.7397>, Jaspers, Komárek, Aerts (2018, Biom. J.) <doi:10.1002/bimj.201600253> and Hughes, Komárek, Czanner, García-Fiñana (2018, Stat. Meth. in Med. Res) <doi:10.1177/0962280216674496>.",
    "version": "5.8",
    "maintainer": "Arnošt Komárek <arnost.komarek@mff.cuni.cz>",
    "url": "https://msekce.karlin.mff.cuni.cz/~komarek/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17584,
    "package_name": "mixPHM",
    "title": "Mixtures of Proportional Hazard Models",
    "description": "Fits multiple variable mixtures of various parametric proportional hazard models using the EM-Algorithm. Proportionality restrictions can be imposed on the latent groups and/or on the variables. Several survival distributions can be specified. Missing values and censored values are allowed. Independence is assumed over the single variables.",
    "version": "0.7-2",
    "maintainer": "Patrick Mair <mair@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17598,
    "package_name": "mixedMem",
    "title": "Tools for Discrete Multivariate Mixed Membership Models",
    "description": "Fits mixed membership models with discrete multivariate data (with\n    or without repeated measures) following the general framework of Erosheva et al\n    (2004). This package uses a Variational EM approach by approximating the\n    posterior distribution of latent memberships and selecting hyperparameters\n    through a pseudo-MLE procedure. Currently supported data types are\n    Bernoulli, multinomial and rank (Plackett-Luce). The extended GoM model with \n    fixed stayers from Erosheva et al (2007) is now also supported. See Airoldi et al \n    (2014) for other examples of mixed membership models.",
    "version": "1.1.2",
    "maintainer": "Y. Samuel Wang <ysamuelwang@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17599,
    "package_name": "mixedbiastest",
    "title": "Bias Diagnostic for Linear Mixed Models",
    "description": "Provides a function to perform bias diagnostics on linear mixed models fitted with lmer() from the 'lme4' package. Implements permutation tests for assessing the bias of fixed effects, as described in Karl and Zimmerman (2021) <doi:10.1016/j.jspi.2020.06.004>. Karl and Zimmerman (2020) <doi:10.17632/tmynggddfm.1> provide R code for implementing the test using 'mvglmmRank' output. Development of this package was assisted by 'GPT o1-preview' for code structure and documentation.",
    "version": "1.0.2",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17605,
    "package_name": "mixl",
    "title": "Simulated Maximum Likelihood Estimation of Mixed Logit Models\nfor Large Datasets",
    "description": "Specification and estimation of multinomial logit\n    models.  Large datasets and complex models are supported, with an\n    intuitive syntax.  Multinomial Logit Models, Mixed models, random\n    coefficients and Hybrid Choice are all supported.  For more\n    information, see Molloy et al. (2021) <https://www.research-collection.ethz.ch/handle/20.500.11850/477416>.",
    "version": "1.3.5",
    "maintainer": "Daniel Heimgartner <d.heimgartners@gmail.com>",
    "url": "https://github.com/joemolloy/fast-mixed-mnl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17606,
    "package_name": "mixlm",
    "title": "Mixed Model ANOVA and Statistics for Education",
    "description": "The main functions perform mixed models analysis by least squares\n    or REML by adding the function r() to formulas of lm() and glm(). A collection of\n    text-book statistics for higher education is also included, e.g. modifications\n    of the functions lm(), glm() and associated summaries from the package 'stats'.",
    "version": "1.4.3",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://github.com/khliland/mixlm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17607,
    "package_name": "mixmeta",
    "title": "An Extended Mixed-Effects Framework for Meta-Analysis",
    "description": "A collection of functions to perform various meta-analytical models\n  through a unified mixed-effects framework, including standard univariate\n  fixed and random-effects meta-analysis and meta-regression, and non-standard\n  extensions such as multivariate, multilevel, longitudinal, and dose-response\n  models.",
    "version": "1.2.2",
    "maintainer": "Antonio Gasparrini <antonio.gasparrini@lshtm.ac.uk>",
    "url": "https://github.com/gasparrini/mixmeta,\nhttp://www.ag-myresearch.com/package-mixmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17609,
    "package_name": "mixpoissonreg",
    "title": "Mixed Poisson Regression for Overdispersed Count Data",
    "description": "Fits mixed Poisson regression models (Poisson-Inverse Gaussian or Negative-Binomial) on data sets with response variables being count data. The models can have varying precision parameter, where a linear regression structure (through a link function) is assumed to hold on the precision parameter. The Expectation-Maximization algorithm for both these models (Poisson Inverse Gaussian and Negative Binomial) is an important contribution of this package. Another important feature of this package is the set of functions to perform global and local influence analysis. See Barreto-Souza and Simas (2016) <doi:10.1007/s11222-015-9601-6> for further details.  ",
    "version": "1.0.0",
    "maintainer": "Alexandre B. Simas <alexandre.impa@gmail.com>",
    "url": "https://github.com/vpnsctl/mixpoissonreg/,\nhttps://vpnsctl.github.io/mixpoissonreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17623,
    "package_name": "mlVAR",
    "title": "Multi-Level Vector Autoregression",
    "description": "Estimates the multi-level vector autoregression model on time-series data.\n             Three network structures are obtained: temporal networks, contemporaneous\n             networks and between-subjects networks.",
    "version": "0.5.2",
    "maintainer": "Sacha Epskamp <mail@sachaepskamp.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17626,
    "package_name": "mlbgameday",
    "title": "Tools to Gather Data from Major League Baseball Advanced Media",
    "description": "Multi-core processing of data from Major League Baseball Advanced Media <http://gd2.mlb.com/components/game/mlb/>. Additional tools to parallel process large data sets and write them to a database.",
    "version": "0.2.1",
    "maintainer": "",
    "url": "https://github.com/keberwein/mlbgameday",
    "exports": [],
    "topics": ["baseball", "database", "etl", "mlb-gameday", "mlbam", "parallel-processing", "statistics"],
    "score": "NA",
    "stars": 42
  },
  {
    "id": 17646,
    "package_name": "mlmhelpr",
    "title": "Multilevel/Mixed Model Helper Functions",
    "description": "A collection of miscellaneous helper function for running multilevel/mixed models in 'lme4'. This package aims to provide functions to compute common tasks when estimating multilevel models such as computing the intraclass correlation and design effect, centering variables, estimating the proportion of variance explained at each level, pseudo-R squared, random intercept and slope reliabilities, tests for homogeneity of variance at level-1, and cluster robust and bootstrap standard errors. The tests and statistics reported in the package are from Raudenbush & Bryk (2002, ISBN:9780761919049), Hox et al. (2018, ISBN:9781138121362), and Snijders & Bosker (2012, ISBN:9781849202015). ",
    "version": "0.1.1",
    "maintainer": "Louis Rocconi <lrocconi@utk.edu>",
    "url": "https://github.com/lrocconi/mlmhelpr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17650,
    "package_name": "mlmtools",
    "title": "Multi-Level Model Assessment Kit",
    "description": "Multilevel models (mixed effects models) are the statistical tool of choice for analyzing multilevel data (Searle et al, 2009). These models account for the correlated nature of observations within higher level units by adding group-level error terms that augment the singular residual error of a standard OLS regression. Multilevel and mixed effects models often require specialized data pre-processing and further post-estimation derivations and graphics to gain insight into model results. The package presented here, 'mlmtools', is a suite of pre- and post-estimation tools for multilevel models in 'R'. Package implements post-estimation tools designed to work with models estimated using 'lme4''s (Bates et al., 2014) lmer() function, which fits linear mixed effects regression models. Searle, S. R., Casella, G., & McCulloch, C. E. (2009, ISBN:978-0470009598). Bates, D., Mächler, M., Bolker, B., & Walker, S. (2014) <doi:10.18637/jss.v067.i01>.",
    "version": "1.0.2",
    "maintainer": "Laura Jamison <lj5yn@virginia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17653,
    "package_name": "mlogitBMA",
    "title": "Bayesian Model Averaging for Multinomial Logit Models",
    "description": "Provides a modified function bic.glm of the BMA package that can be applied to multinomial logit (MNL) data. The data is converted to binary logit using the Begg & Gray approximation. The package also contains functions for maximum likelihood estimation of MNL. ",
    "version": "0.1-9",
    "maintainer": "Hana Sevcikova <hanas@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17657,
    "package_name": "mlr",
    "title": "Machine Learning in R",
    "description": "Interface to a large number of classification and regression",
    "version": "2.19.3",
    "maintainer": "",
    "url": "https://github.com/mlr-org/mlr",
    "exports": [],
    "topics": ["classification", "clustering", "cran", "data-science", "feature-selection", "hyperparameters-optimization", "imbalance-correction", "learners", "machine-learning", "mlr", "multilabel-classification", "predictive-modeling", "r", "r-package", "regression", "stacking", "statistics", "survival-analysis", "tuning", "tutorial"],
    "score": "NA",
    "stars": 1676
  },
  {
    "id": 17692,
    "package_name": "mlrpro",
    "title": "Stepwise Regression with Assumptions Checking",
    "description": "The stepwise regression with assumptions checking and the possible Box-Cox transformation.",
    "version": "0.1.2",
    "maintainer": "Thidarat Thongsri <k.th.thidarat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17693,
    "package_name": "mlrv",
    "title": "Long-Run Variance Estimation in Time Series Regression",
    "description": "Plug-in and difference-based long-run covariance matrix estimation for time series regression. Two applications of hypothesis testing are also provided. The first one is for testing for structural stability in coefficient functions. The second one is aimed at detecting long memory in time series regression. Lujia Bai and Weichi Wu (2024)<doi:10.3150/23-BEJ1680> Zhou Zhou and Wei Biao Wu(2010)<doi:10.1111/j.1467-9868.2010.00743.x> Jianqing Fan and Wenyang Zhang<doi:10.1214/aos/1017939139> Lujia Bai and Weichi Wu(2024)<doi:10.1093/biomet/asae013> Dimitris N. Politis, Joseph P. Romano, Michael Wolf(1999)<doi:10.1007/978-1-4612-1554-7> Weichi Wu and Zhou Zhou(2018)<doi:10.1214/17-AOS1582>.",
    "version": "0.1.2",
    "maintainer": "Lujia Bai <bailujia98@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17694,
    "package_name": "mlsbm",
    "title": "Efficient Estimation of Bayesian SBMs & MLSBMs",
    "description": "Fit Bayesian stochastic block models (SBMs) and multi-level stochastic block models (MLSBMs) using efficient Gibbs sampling implemented in 'Rcpp'. The models assume symmetric, non-reflexive graphs (no self-loops) with unweighted, binary edges. Data are input as a symmetric binary adjacency matrix (SBMs), or list of such matrices (MLSBMs). ",
    "version": "0.99.2",
    "maintainer": "Carter Allen <carter.allen12@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17703,
    "package_name": "mlts",
    "title": "Multilevel Latent Time Series Models with 'R' and 'Stan'",
    "description": "Fit multilevel manifest or latent time-series models, including popular Dynamic Structural Equation Models (DSEM).\n  The models can be set up and modified with user-friendly functions and are fit to the data using 'Stan' for Bayesian inference.\n  Path models and formulas for user-defined models can be easily created with functions using 'knitr'. \n  Asparouhov, Hamaker, & Muthen (2018) <doi:10.1080/10705511.2017.1406803>.",
    "version": "2.0.1",
    "maintainer": "Kenneth Koslowski <kenneth.koslowski@uni-leipzig.de>",
    "url": "https://github.com/munchfab/mlts",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17711,
    "package_name": "mmb",
    "title": "Arbitrary Dependency Mixed Multivariate Bayesian Models",
    "description": "Supports Bayesian models with full and partial (hence\n    arbitrary) dependencies between random variables. Discrete and continuous\n    variables are supported, and conditional joint probabilities and probability\n    densities are estimated using Kernel Density Estimation (KDE). The full\n    general form, which implements an extension to Bayes' theorem, as well as\n    the simple form, which is just a Bayesian network, both support regression\n    through segmentation and KDE and estimation of probability or relative\n    likelihood of discrete or continuous target random variables. This package\n    also provides true statistical distance measures based on Bayesian models.\n    Furthermore, these measures can be facilitated on neighborhood searches,\n    and to estimate the similarity and distance between data points.\n    Related work is by Bayes (1763) <doi:10.1098/rstl.1763.0053>\n    and by Scutari (2010) <doi:10.18637/jss.v035.i03>.",
    "version": "0.13.3",
    "maintainer": "Sebastian Hönel <sebastian.honel@lnu.se>",
    "url": "https://github.com/MrShoenel/R-mmb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17715,
    "package_name": "mmcmcBayes",
    "title": "Multistage MCMC Method for Detecting DMRs",
    "description": "Implements differential methylation region (DMR) detection using a\n    multistage Markov chain Monte Carlo (MCMC) algorithm based on the\n    alpha-skew generalized normal (ASGN) distribution. Version 0.2.0 removes\n    the Anderson-Darling test stage, improves computational efficiency of the\n    core ASGN and multistage MCMC routines, and adds convenience functions for\n    summarizing and visualizing detected DMRs. The methodology is based on\n    Yang (2025) <https://www.proquest.com/docview/3218878972>.",
    "version": "0.2.0",
    "maintainer": "Zhexuan Yang <zky5198@psu.edu>",
    "url": "https://github.com/zyang1919/mmcmcBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17720,
    "package_name": "mmeta",
    "title": "Multivariate Meta-Analysis",
    "description": "Multiple 2 by 2 tables often arise in meta-analysis which combines statistical evidence from multiple studies. Two risks within the same study are possibly correlated because they share some common factors such as environment and population structure. This package implements a set of novel Bayesian approaches for multivariate meta analysis when the risks within the same study are independent or correlated. The exact posterior inference of odds ratio, relative risk, and risk difference given either a single 2 by 2 table or multiple 2 by 2 tables is provided. Luo, Chen, Su, Chu, (2014) <doi:10.18637/jss.v056.i11>, Chen, Luo, (2011) <doi:10.1002/sim.4248>, Chen, Chu, Luo, Nie, Chen, (2015) <doi:10.1177/0962280211430889>, Chen, Luo, Chu, Su, Nie, (2014) <doi:10.1080/03610926.2012.700379>, Chen, Luo, Chu, Wei, (2013) <doi:10.1080/19466315.2013.791483>.",
    "version": "3.0.2",
    "maintainer": "Bingyu Zhang <bingyuz7@sas.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17721,
    "package_name": "mmiCATs",
    "title": "Cluster Adjusted t Statistic Applications",
    "description": "Simulation results detailed in Esarey and Menger (2019) <doi:10.1017/psrm.2017.42>\n    demonstrate that cluster adjusted t statistics (CATs) are an effective method\n    for correcting standard errors in scenarios with a small number of clusters.\n    The 'mmiCATs' package offers a suite of tools for working with CATs. The\n    mmiCATs() function initiates a 'shiny' web application, facilitating\n    the analysis of data utilizing CATs, as implemented in the cluster.im.glm()\n    function from the 'clusterSEs' package. Additionally, the pwr_func_lmer()\n    function is designed to simplify the process of conducting simulations to\n    compare mixed effects models with CATs models. For educational purposes, the\n    CloseCATs() function launches a 'shiny' application card game, aimed at enhancing\n    users' understanding of the conditions under which CATs should be preferred\n    over random intercept models.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "url": "https://github.com/mightymetrika/mmiCATs",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17722,
    "package_name": "mmibain",
    "title": "Bayesian Informative Hypotheses Evaluation Web Applications",
    "description": "Researchers often have expectations about the relations between means\n    of different groups or standardized regression coefficients; using informative\n    hypothesis testing to incorporate these expectations into the analysis through\n    order constraints increases statistical power\n    Vanbrabant and Rosseel (2020) <doi:10.4324/9780429273872-14>. Another valuable\n    tool, the Bayes factor, can evaluate evidence for multiple hypotheses without\n    concerns about multiple testing, and can be used in Bayesian updating\n    Hoijtink, Mulder, van Lissa & Gu (2019) <doi:10.1037/met0000201>. The 'bain'\n    R package enables informative hypothesis testing using the Bayes factor. The\n    'mmibain' package provides 'shiny' web applications based on 'bain'. The\n    RepliCrisis() function launches a 'shiny' card game to simulate the evaluation\n    of replication studies while the mmibain() function launches a 'shiny'\n    application to fit Bayesian informative hypotheses evaluation models from\n    'bain'.",
    "version": "0.2.0",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "url": "https://github.com/mightymetrika/mmibain",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17725,
    "package_name": "mmmgee",
    "title": "Simultaneous Inference for Multiple Linear Contrasts in GEE\nModels",
    "description": "Provides global hypothesis tests, multiple testing procedures and simultaneous confidence intervals for multiple linear contrasts of regression\n\tcoefficients in a single generalized estimating equation (GEE) model or across multiple GEE models. GEE models are fit by a modified version of the 'geeM' package.",
    "version": "1.20",
    "maintainer": "Robin Ristl <robin.ristl@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17727,
    "package_name": "mmodely",
    "title": "Modeling Multivariate Origins Determinants - Evolutionary\nLineages in Ecology",
    "description": "\n Perform multivariate modeling of evolved traits, with special attention to\n understanding the interplay of the multi-factorial determinants of their origins\n in complex ecological settings (Stephens, 2007 <doi:10.1016/j.tree.2006.12.003>).\n This software primarily concentrates on phylogenetic regression analysis, enabling\n implementation of tree transformation averaging and visualization functionality.\n Functions additionally support information theoretic approaches\n (Grueber, 2011 <doi:10.1111/j.1420-9101.2010.02210.x>;\n Garamszegi, 2011 <doi:10.1007/s00265-010-1028-7>)\n such as  model averaging and selection of phylogenetic models.\n Accessory functions are also implemented for coef standardization (Cade 2015), \n selection uncertainty, and variable importance (Burnham & Anderson 2000).\n There are other numerous functions for visualizing confounded variables,\n plotting phylogenetic trees, as well as reporting and exporting modeling results.\n Lastly, as challenges to ecology are inherently multifarious, and therefore often\n multi-dataset, this package features several functions to support the identification,\n interpolation, merging, and updating of missing data and outdated nomenclature.",
    "version": "0.2.5",
    "maintainer": "David M Schruth <dschruth@anthropoidea.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17738,
    "package_name": "mniw",
    "title": "The Matrix-Normal Inverse-Wishart Distribution",
    "description": "Density evaluation and random number generation for the Matrix-Normal Inverse-Wishart (MNIW) distribution, as well as the the Matrix-Normal, Matrix-T, Wishart, and Inverse-Wishart distributions.  Core calculations are implemented in a portable (header-only) C++ library, with matrix manipulations using the 'Eigen' library for linear algebra.  Also provided is a Gibbs sampler for Bayesian inference on a random-effects model with multivariate normal observations.",
    "version": "1.0.2",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "https://github.com/mlysy/mniw/, https://mlysy.github.io/mniw/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17761,
    "package_name": "modQR",
    "title": "Multiple-Output Directional Quantile Regression",
    "description": "Contains basic tools for performing \n  multiple-output quantile regression and computing\n  regression quantile contours by means of directional\n  regression quantiles. In the location case, one can thus\n  obtain halfspace depth contours in two to six dimensions.\n  Hallin, M., Paindaveine, D. and Šiman, M. (2010)\n  Multivariate quantiles and multiple-output regression quantiles:\n  from L1 optimization to halfspace depth. Annals of Statistics 38, 635-669\n  For more references about the method, see Help pages.",
    "version": "0.1.3",
    "maintainer": "Pavel Boček <bocek@utia.cas.cz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17768,
    "package_name": "modelObj",
    "title": "A Model Object Framework for Regression Analysis",
    "description": "A utility library to facilitate the generalization of statistical methods built on a regression framework. Package developers can use 'modelObj' methods to initiate a regression analysis without concern for the details of the regression model and the method to be used to obtain parameter estimates. The specifics of the regression step are left to the user to define when calling the function. The user of a function developed within the 'modelObj' framework creates as input a 'modelObj' that contains the model and the R methods to be used to obtain parameter estimates and to obtain predictions.  In this way, a user can easily go from linear to non-linear models within the same package.  ",
    "version": "4.3",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17770,
    "package_name": "modelSelection",
    "title": "High-Dimensional Model Selection",
    "description": "Model selection and averaging for regression, generalized linear models, generalized additive models, graphical models and mixtures, focusing on Bayesian model selection and information criteria (Bayesian information criterion etc.). See Rossell (2025) <doi:10.5281/zenodo.17119597> (see the URL field below for its URL) for a hands-on book describing the methods, examples and suggested citations if you use the package.",
    "version": "1.0.4",
    "maintainer": "David Rossell <rosselldavid@gmail.com>",
    "url": "https://github.com/davidrusi/modelSelection,\nhttps://github.com/davidrusi/modelSelection-book",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17773,
    "package_name": "modelbpp",
    "title": "Model BIC Posterior Probability",
    "description": "Fits the neighboring models of a fitted\n  structural equation model and assesses the model\n  uncertainty of the fitted model based on BIC posterior\n  probabilities, using the method presented in\n  Wu, Cheung, and Leung (2020)\n  <doi:10.1080/00273171.2019.1574546>.",
    "version": "0.1.6",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "url": "https://sfcheung.github.io/modelbpp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17774,
    "package_name": "modelc",
    "title": "A Linear Model to 'SQL' Compiler",
    "description": "This is a cross-platform linear model to 'SQL' compiler. It generates 'SQL' from linear and generalized linear models. Its interface consists of a single function, modelc(), which takes the output of lm() or glm() functions (or any object which has the same signature) and outputs a 'SQL' character vector representing the predictions on the scale of the response variable as described in Dunn & Smith (2018) <doi:10.1007/978-1-4419-0118-7> and originating in Nelder & Wedderburn (1972) <doi:10.2307/2344614>. The resultant 'SQL' can be included in a 'SELECT' statement and returns output similar to that of the glm.predict() or lm.predict() predictions, assuming numeric types are represented in the database using sufficient precision. Currently log and identity link functions are supported.",
    "version": "1.0.0.0",
    "maintainer": "Hugo Saavedra <analytics+hugo@sparkfish.com>",
    "url": "https://github.com/sparkfish/modelc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17787,
    "package_name": "modeltime.gluonts",
    "title": "'GluonTS' Deep Learning",
    "description": "",
    "version": "0.3.1.9000",
    "maintainer": "",
    "url": "https://github.com/business-science/modeltime.gluonts",
    "exports": [],
    "topics": ["deep-learning", "forecasting", "gluonts", "gluonts-deep-learning", "modeltime", "r-package", "tidymodels", "time-series"],
    "score": "NA",
    "stars": 43
  },
  {
    "id": 17788,
    "package_name": "modeltime.h2o",
    "title": "Modeltime 'H2O' Machine Learning",
    "description": "",
    "version": "0.1.2.9000",
    "maintainer": "",
    "url": "https://github.com/business-science/modeltime.h2o",
    "exports": [],
    "topics": ["deep-learning", "forecast", "forecasting", "h2o", "machine-learning", "modeltime", "r", "r-package", "tidymodels", "time-series", "time-series-analysis", "timeseries"],
    "score": "NA",
    "stars": 44
  },
  {
    "id": 17792,
    "package_name": "modelwordcloud",
    "title": "Model Word Clouds",
    "description": "Makes a word cloud of text, sized by the frequency of the word, and colored either by user-specified colors or colored by the strength of the coefficient of that text derived from a regression model.",
    "version": "0.1",
    "maintainer": "Peter Hurford <peter.hurford@datarobot.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17794,
    "package_name": "moderate.mediation",
    "title": "Causal Moderated Mediation Analysis",
    "description": "Causal moderated mediation analysis using the methods proposed by Qin and Wang (2023) <doi:10.3758/s13428-023-02095-4>. Causal moderated mediation analysis is crucial for investigating how, for whom, and where a treatment is effective by assessing the heterogeneity of mediation mechanism across individuals and contexts. This package enables researchers to estimate and test the conditional and moderated mediation effects, assess their sensitivity to unmeasured pre-treatment confounding, and visualize the results. The package is built based on the quasi-Bayesian Monte Carlo method, because it has relatively better performance at small sample sizes, and its running speed is the fastest. The package is applicable to a treatment of any scale, a binary or continuous mediator, a binary or continuous outcome, and one or more moderators of any scale. ",
    "version": "0.0.12",
    "maintainer": "Xu Qin <xuqin@pitt.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17795,
    "package_name": "modernBoot",
    "title": "Modern Resampling Methods: Bootstraps, Wild, Block, Permutation,\nand Selection Guidance",
    "description": "Implements modern resampling and permutation methods for robust \n    statistical inference without restrictive parametric assumptions. Provides \n    bias-corrected and accelerated (BCa) bootstrap (Efron and Tibshirani (1993) \n    <doi:10.1201/9780429246593>), wild bootstrap for heteroscedastic regression \n    (Liu (1988) <doi:10.1214/aos/1176351062>, Davidson and Flachaire (2008) \n    <doi:10.1016/j.jeconom.2008.08.003>), block bootstrap for time series \n    (Politis and Romano (1994) <doi:10.1080/01621459.1994.10476870>), and \n    permutation-based multiple testing correction (Westfall and Young (1993) \n    <ISBN:0-471-55761-7>). Methods handle non-normal data, \n    heteroscedasticity, time series correlation, and multiple comparisons.",
    "version": "0.1.1",
    "maintainer": "Ibrahim Kholil Rakib <ikrakib1010@gmail.com>",
    "url": "https://github.com/ikrakib/modernBoot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17797,
    "package_name": "moderndive",
    "title": "Tidyverse-Friendly Introductory Linear Regression",
    "description": "Datasets and wrapper functions for tidyverse-friendly introductory linear regression, used in \"Statistical Inference via Data Science: A ModernDive into R and the Tidyverse\" available at <https://moderndive.com/>.",
    "version": "0.7.0",
    "maintainer": "Albert Y. Kim <albert.ys.kim@gmail.com>",
    "url": "https://moderndive.github.io/moderndive/,\nhttps://github.com/moderndive/moderndive/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17801,
    "package_name": "modgo",
    "title": "Mock Data Generation",
    "description": "Generation of synthetic data from a real dataset using the combination of rank normal inverse transformation with the calculation of correlation matrix <doi:10.1055/a-2048-7692>. Completely artificial data may be generated through the use of Generalized Lambda Distribution and Generalized Poisson Distribution <doi:10.1201/9781420038040>. Quantitative, binary, ordinal categorical, and survival data may be simulated. Functionalities are offered to generate synthetic data sets according to user's needs.",
    "version": "1.0.1",
    "maintainer": "Georgios Koliopanos <george.koliopanos@cardio-care.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17803,
    "package_name": "modifiedmk",
    "title": "Modified Versions of Mann Kendall and Spearman's Rho Trend Tests",
    "description": "Power of non-parametric Mann-Kendall test and Spearman’s Rho test is highly influenced by serially correlated data. To address this issue, trend tests may be applied on the modified versions of the time series data by  Block Bootstrapping (BBS), Prewhitening (PW) , Trend Free Prewhitening (TFPW), Bias Corrected Prewhitening and Variance Correction Approach by calculating effective sample size.\n    Mann, H. B. (1945).<doi:10.1017/CBO9781107415324.004>.\n    Kendall, M. (1975). Multivariate analysis. Charles Griffin&Company Ltd,. \n    sen, P. K. (1968).<doi:10.2307/2285891>.\n    Önöz, B., & Bayazit, M. (2012) <doi:10.1002/hyp.8438>.\n    Hamed, K. H. (2009).<doi:10.1016/j.jhydrol.2009.01.040>.\n    Yue, S., & Wang, C. Y. (2002) <doi:10.1029/2001WR000861>.\n    Yue, S., Pilon, P., Phinney, B., & Cavadias, G. (2002) <doi:10.1002/hyp.1095>.\n    Hamed, K. H., & Ramachandra Rao, A. (1998) <doi:10.1016/S0022-1694(97)00125-X>.\n    Yue, S., & Wang, C. Y. (2004) <doi:10.1023/B:WARM.0000043140.61082.60>.",
    "version": "1.6",
    "maintainer": "Sandeep Kumar Patakamuri <sandeep.patakamuri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17817,
    "package_name": "mombf",
    "title": "Model Selection with Bayesian Methods and Information Criteria",
    "description": "Model selection and averaging for regression and mixtures, inclusing Bayesian model selection and information criteria (BIC, EBIC, AIC, GIC).",
    "version": "3.5.4",
    "maintainer": "David Rossell <rosselldavid@gmail.com>",
    "url": "https://github.com/davidrusi/mombf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17826,
    "package_name": "mondate",
    "title": "Keep Track of Dates in Terms of Months",
    "description": "Keep track of dates in terms of fractional calendar months \n  per Damien Laker \"Time Calculations for Annualizing Returns: the Need for Standardization\", \n  The Journal of Performance Measurement, 2008.\n  Model dates as of close of business.\n  Perform date arithmetic in units of \"months\" and \"years\".\n  Allow \"infinite\" dates to model \"ultimate\" time.",
    "version": "1.0",
    "maintainer": "Dan Murphy <chiefmurphy@gmail.com>",
    "url": "https://www.R-project.org, https://github.com/chiefmurph/mondate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17833,
    "package_name": "monobin",
    "title": "Monotonic Binning for Credit Rating Models",
    "description": "Performs monotonic binning of numeric risk factor in credit rating models (PD, LGD, EAD) \n\tdevelopment. All functions handle both binary and continuous target variable. \n\tFunctions that use isotonic regression in the first stage of binning process have an additional \n\tfeature for correction of minimum percentage of observations and minimum target rate per bin. \t\n\tAdditionally, monotonic trend can be identified based on raw data or, if known in advance,\n\tforced by functions' argument. Missing values and other possible special values are treated \n\tseparately from so-called complete cases.",
    "version": "0.2.4",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "url": "https://github.com/andrija-djurovic/monobin",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17839,
    "package_name": "monomvn",
    "title": "Estimation for MVN and Student-t Data with Monotone Missingness",
    "description": "Estimation of multivariate normal (MVN) and student-t data of \n arbitrary dimension where the pattern of missing data is monotone.\n See Pantaleo and Gramacy (2010) <doi:10.48550/arXiv.0907.2135>.\n Through the use of parsimonious/shrinkage regressions \n (plsr, pcr, lasso, ridge,  etc.), where standard regressions fail, \n the package can handle a nearly arbitrary amount of missing data. \n The current version supports maximum likelihood inference and \n a full Bayesian approach employing scale-mixtures for Gibbs sampling.\n Monotone data augmentation extends this Bayesian approach to arbitrary \n missingness patterns.  A fully functional standalone interface to the \n Bayesian lasso (from Park & Casella), Normal-Gamma (from Griffin & Brown),\n Horseshoe (from Carvalho, Polson, & Scott), and ridge regression \n with model selection via Reversible Jump, and student-t errors \n (from Geweke) is also provided.",
    "version": "1.9-21",
    "maintainer": "Robert B. Gramacy <rbg@vt.edu>",
    "url": "https://bobby.gramacy.com/r_packages/monomvn/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17840,
    "package_name": "monoreg",
    "title": "Bayesian Monotonic Regression Using a Marked Point Process\nConstruction",
    "description": "An extended version of the nonparametric Bayesian monotonic regression procedure described in Saarela & Arjas (2011) <DOI:10.1111/j.1467-9469.2010.00716.x>, allowing for multiple additive monotonic components in the linear predictor, and time-to-event outcomes through case-base sampling. The extension and its applications, including estimation of absolute risks, are described in Saarela & Arjas (2015) <DOI:10.1111/sjos.12125>. The package also implements the nonparametric ordinal regression model described in Saarela, Rohrbeck & Arjas <DOI:10.1214/22-BA1310>.",
    "version": "2.1",
    "maintainer": "Olli Saarela <olli.saarela@utoronto.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17841,
    "package_name": "monotone",
    "title": "Performs Monotone Regression",
    "description": "The monotone package contains a fast up-and-down-blocks implementation for the pool-adjacent-violators algorithm \n  for simple linear ordered monotone regression, including two spin-off functions\n  for unimodal and bivariate monotone regression (see <doi:10.18637/jss.v102.c01>).",
    "version": "0.1.2",
    "maintainer": "Frank Busing <busing@fsw.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17843,
    "package_name": "monreg",
    "title": "Nonparametric Monotone Regression",
    "description": "Estimates monotone regression and variance functions in a nonparametric model, based on Dette, Holger, Neumeyer, and Pilz (2006) <doi:10.3150/bj/1151525131>.",
    "version": "0.1.4.1",
    "maintainer": "Scott Kostyshak <scott.kostyshak@gmail.com>",
    "url": "https://gitlab.com/scottkosty/monreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17854,
    "package_name": "moose",
    "title": "Mean Squared Out-of-Sample Error Projection",
    "description": "Projects mean squared out-of-sample error for a linear regression based upon the methodology developed in Rohlfs (2022) <doi:10.48550/arXiv.2209.01493>.  It consumes as inputs the lm object from an estimated OLS regression (based on the \"training sample\") and a data.frame of out-of-sample cases (the \"test sample\") that have non-missing values for the same predictors. The test sample may or may not include data on the outcome variable; if it does, that variable is not used. The aim of the exercise is to project what what mean squared out-of-sample error can be expected given the predictor values supplied in the test sample. Output consists of a list of three elements: the projected mean squared out-of-sample error, the projected out-of-sample R-squared, and a vector of out-of-sample \"hat\" or \"leverage\" values, as defined in the paper.",
    "version": "0.0.1",
    "maintainer": "Chris Rohlfs <car2228@columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17857,
    "package_name": "morepls",
    "title": "Interpretation Tools for Partial Least Squares Regression",
    "description": "Various kinds of plots (observations, variables, correlations, weights, regression coefficients and Variable Importance in the Projection) and aids to interpretation (coefficients, Q2, correlations, redundancies) for partial least squares regressions computed with the 'pls' package, following Tenenhaus (1998, ISBN:2-7108-0735-1).",
    "version": "0.2.1",
    "maintainer": "Nicolas Robette <nicolas.robette@uvsq.fr>",
    "url": "https://framagit.org/nicolas-robette/morepls,\nhttps://nicolas-robette.frama.io/morepls/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17864,
    "package_name": "morse",
    "title": "Modelling Reproduction and Survival Data in Ecotoxicology",
    "description": "Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival and reproduction Data. Among\n   others, it facilitates Bayesian inference of the general unified\n   threshold model of survival (GUTS). See our companion paper \n   Baudrot and Charles (2021) <doi:10.21105/joss.03200>,\n   as well as complementary details in Baudrot et al. (2018)\n   <doi:10.1021/acs.est.7b05464> and Delignette-Muller et al.\n   (2017) <doi:10.1021/acs.est.6b05326>.",
    "version": "3.3.4",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "url": "https://gitlab.in2p3.fr/mosaic-software/morse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17865,
    "package_name": "morseDR",
    "title": "Bayesian Inference of Binary, Count and Continuous Data in\nToxicology",
    "description": "Advanced methods for a valuable quantitative environmental\n    risk assessment using Bayesian inference of several type of\n    toxicological data. 'binary' (e.g., survival, mobility), 'count'\n    (e.g., reproduction) and 'continuous' (e.g., growth as length,\n    weight).  Estimation procedures can be used without a deep knowledge\n    of their underlying probabilistic model or inference methods. Rather,\n    they were designed to behave as well as possible without requiring a\n    user to provide values for some obscure parameters. That said, models\n    can also be used as a first step to tailor new models for more\n    specific situations.",
    "version": "0.1.2",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17866,
    "package_name": "morseTKTD",
    "title": "Bayesian Inference of TKTD Models",
    "description": "Advanced methods for a valuable quantitative environmental risk \n   assessment using Bayesian inference of survival Data with toxicokinetics\n   toxicodynamics (TKTD) models. Among others, it facilitates Bayesian inference of \n   the general unified threshold model of survival (GUTS). See models description\n   in Jager et al. (2011) <doi:10.1021/es103092a> and implementation\n   using Bayesian inference in Baudrot and Charles (2019)\n   <doi:10.1038/s41598-019-47698-0>.",
    "version": "0.1.3",
    "maintainer": "Virgile Baudrot <virgile.baudrot@qonfluens.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17879,
    "package_name": "mosum",
    "title": "Moving Sum Based Procedures for Changes in the Mean",
    "description": "Implementations of MOSUM-based statistical procedures and algorithms for detecting multiple changes in the mean. This comprises the MOSUM procedure for estimating multiple mean changes from Eichinger and Kirch (2018) <doi:10.3150/16-BEJ887> and the multiscale algorithmic extension from Cho and Kirch (2022) <doi:10.1007/s10463-021-00811-5>, as well as the bootstrap procedure for generating confidence intervals about the locations of change points as proposed in Cho and Kirch (2022) <doi:10.1016/j.csda.2022.107552>. See also Meier, Kirch and Cho (2021) <doi:10.18637/jss.v097.i08> which accompanies the R package.",
    "version": "1.2.7",
    "maintainer": "Haeran Cho <haeran.cho@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17925,
    "package_name": "mpr",
    "title": "Multi-Parameter Regression (MPR)",
    "description": "Fitting Multi-Parameter Regression (MPR) models to right-censored survival data. These are flexible parametric regression models which extend standard models, for example, proportional hazards. See Burke & MacKenzie (2016) <doi:10.1111/biom.12625> and Burke et al (2020) <doi:10.1111/rssc.12398>.",
    "version": "1.0.6",
    "maintainer": "Kevin Burke <kevin.burke@ul.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17936,
    "package_name": "mratios",
    "title": "Ratios of Coefficients in the General Linear Model",
    "description": "Performs (simultaneous) inferences for ratios of linear combinations of coefficients in the general linear model, linear mixed model, and for quantiles in a one-way layout. Multiple comparisons and simultaneous confidence interval estimations can be performed for ratios of treatment means in the normal one-way layout with homogeneous and heterogeneous treatment variances, according to Dilba et al. (2007) <https://cran.r-project.org/doc/Rnews/Rnews_2007-1.pdf> and Hasler and Hothorn (2008) <doi:10.1002/bimj.200710466>. Confidence interval estimations for ratios of linear combinations of linear model parameters like in (multiple) slope ratio and parallel line assays can be carried out. Moreover, it is possible to calculate the sample sizes required in comparisons with a control based on relative margins. For the simple two-sample problem, functions for a t-test for ratio-formatted hypotheses and the corresponding confidence interval are provided assuming homogeneous or heterogeneous group variances.",
    "version": "1.4.4",
    "maintainer": "Frank Schaarschmidt <schaarschmidt@biostat.uni-hannover.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17937,
    "package_name": "mrbayes",
    "title": "Bayesian Summary Data Models for Mendelian Randomization Studies",
    "description": "Bayesian estimation of inverse variance weighted (IVW),\n    Burgess et al. (2013) <doi:10.1002/gepi.21758>, and MR-Egger, Bowden\n    et al. (2015) <doi:10.1093/ije/dyv080>, summary data models for\n    Mendelian randomization analyses.",
    "version": "0.5.2",
    "maintainer": "Tom Palmer <remlapmot@hotmail.com>",
    "url": "https://github.com/okezie94/mrbayes,\nhttps://okezie94.github.io/mrbayes/,\nhttps://mrcieu.r-universe.dev/mrbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17942,
    "package_name": "mreg",
    "title": "Fits Regression Models When the Outcome is Partially Missing",
    "description": "Implements the methods described in Bond S, Farewell V, 2006, Exact Likelihood Estimation for a Negative Binomial Regression Model with Missing Outcomes, Biometrics.",
    "version": "1.2.1",
    "maintainer": "Simon Bond <simon.bond7@nhs.net>",
    "url": "https://github.com/shug0131/mreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17946,
    "package_name": "mrfse",
    "title": "Markov Random Field Structure Estimator",
    "description": "Three algorithms for estimating a Markov random field structure.Two of them are an exact version and a simulated annealing version of a penalized maximum conditional likelihood method similar to the Bayesian Information Criterion. These algorithm are described in Frondana (2016) <doi:10.11606/T.45.2018.tde-02022018-151123>.The third one is a greedy algorithm, described in Bresler (2015) <doi:10.1145/2746539.2746631).",
    "version": "0.4.2",
    "maintainer": "Rodrigo Carvalho <rodrigorsdc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17967,
    "package_name": "msaeHB",
    "title": "Multivariate Small Area Estimation using Hierarchical Bayesian\nMethod",
    "description": "Implements area level of multivariate small area estimation using Hierarchical Bayesian method under Normal and T distribution. The 'rjags' package is employed to obtain parameter estimates. For the reference, see Rao and Molina (2015) <doi:10.1002/9781118735855>.",
    "version": "0.1.0",
    "maintainer": "Novia Permatasari <novia.permatasari@bps.go.id>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17970,
    "package_name": "msce",
    "title": "Hazard of Multi-Stage Clonal Expansion Models",
    "description": "Functions to calculate hazard and survival function of Multi-Stage Clonal Expansion Models used in cancer epidemiology. For the Two-Stage Clonal Expansion Model an exact solution is implemented assuming piecewise constant parameters, see Heidenreich, Luebeck, Moolgavkar (1997) <doi:10.1111/j.1539-6924.1997.tb00878.x>. Numerical solutions are provided for its extensions, see also Little, Vineis, Li (2008) <doi:10.1016/j.jtbi.2008.05.027>.",
    "version": "1.0.2",
    "maintainer": "Cristoforo Simonetto <cristoforo.simonetto@ph.tum.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17978,
    "package_name": "msde",
    "title": "Bayesian Inference for Multivariate Stochastic Differential\nEquations",
    "description": "Implements an MCMC sampler for the posterior distribution of arbitrary time-homogeneous multivariate stochastic differential equation (SDE) models with possibly latent components.  The package provides a simple entry point to integrate user-defined models directly with the sampler's C++ code, and parallelizes large portions of the calculations when compiled with 'OpenMP'.",
    "version": "1.0.5",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17987,
    "package_name": "msir",
    "title": "Model-Based Sliced Inverse Regression",
    "description": "An R package for dimension reduction based on finite Gaussian mixture modeling of inverse regression.",
    "version": "1.4",
    "maintainer": "Luca Scrucca <luca.scrucca@unibo.it>",
    "url": "https://mclust-org.github.io/msir/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17989,
    "package_name": "msltrend",
    "title": "Improved Techniques to Estimate Trend, Velocity and Acceleration\nfrom Sea Level Records",
    "description": "Analysis of annual average ocean water level time series\n    from long (minimum length 80 years) individual records, providing improved\n    estimates of trend (mean sea level) and associated real-time velocities and\n    accelerations. Improved trend estimates are based on Singular Spectrum Analysis\n    methods. Various gap-filling options are included to accommodate incomplete time\n    series records. The package also contains a forecasting module to consider the\n    implication of user defined quantum of sea level rise between the end of the\n    available historical record and the year 2100. A wide range of screen and pdf\n    plotting options are available in the package.",
    "version": "1.0",
    "maintainer": "Phil J Watson <philwatson.slr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 17990,
    "package_name": "msm",
    "title": "Multi-State Markov and Hidden Markov Models in Continuous Time",
    "description": "Functions for fitting continuous-time Markov and hidden\n    Markov multi-state models to longitudinal data.  Designed for\n    processes observed at arbitrary times in continuous time (panel data)\n    but some other observation schemes are supported. Both Markov\n    transition rates and the hidden Markov output process can be modelled\n    in terms of covariates, which may be constant or piecewise-constant\n    in time.",
    "version": "1.8.2",
    "maintainer": "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>",
    "url": "https://github.com/chjackson/msm, https://chjackson.github.io/msm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18009,
    "package_name": "mtarm",
    "title": "Bayesian Estimation of Multivariate Threshold Autoregressive\nModels",
    "description": "Estimation, inference and forecasting using the Bayesian approach for multivariate threshold autoregressive (TAR) models in which the distribution used to describe the noise process belongs to the class of Gaussian variance mixtures.",
    "version": "0.1.7",
    "maintainer": "Luis Hernando Vanegas <lhvanegasp@unal.edu.co>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18016,
    "package_name": "mtsdi",
    "title": "Multivariate Time Series Data Imputation",
    "description": "This is an EM algorithm based method for imputation of missing values in multivariate normal time series. The imputation algorithm accounts for both spatial and temporal correlation structures. Temporal patterns can be modeled using an ARIMA(p,d,q), optionally with seasonal components, a non-parametric cubic spline or generalized additive models with exogenous covariates. This algorithm is specially tailored for climate data with missing measurements from several monitors along a given region.",
    "version": "0.3.7",
    "maintainer": "Washington Junger <wjunger@ims.uerj.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18018,
    "package_name": "mtvc",
    "title": "Multiple Counting Process Structure for Survival Analysis",
    "description": "Counting process structure is fundamental to model time varying covariates.\n    This package restructures dataframes in the counting process format for one or more variables.\n    F. W. Dekker, et al. (2008) <doi:10.1038/ki.2008.328>.",
    "version": "1.1.0",
    "maintainer": "Elia Gonzato <elia.gonzato@outlook.it>",
    "url": "https://github.com/egonzato/mtvc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18024,
    "package_name": "muhaz",
    "title": "Hazard Function Estimation in Survival Analysis",
    "description": "Produces a smooth estimate of the hazard\n  function for censored data.",
    "version": "1.2.6.4",
    "maintainer": "David Winsemius <dwinsemius@comcast.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18026,
    "package_name": "mulSEM",
    "title": "Some Multivariate Analyses using Structural Equation Modeling",
    "description": "A set of functions for some multivariate analyses utilizing a\n             structural equation modeling (SEM) approach through the 'OpenMx' package.\n             These analyses include canonical correlation analysis (CANCORR),\n             redundancy analysis (RDA), and multivariate principal component regression (MPCR).\n             It implements procedures discussed in Gu and Cheung (2023) <doi:10.1111/bmsp.12301>,\n             Gu, Yung, and Cheung (2019) <doi:10.1080/00273171.2018.1512847>, and\n             Gu et al. (2023) <doi:10.1080/00273171.2022.2141675>.",
    "version": "1.0",
    "maintainer": "Mike Cheung <mikewlcheung@nus.edu.sg>",
    "url": "https://github.com/mikewlcheung/mulsem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18027,
    "package_name": "mulTree",
    "title": "Performs MCMCglmm on Multiple Phylogenetic Trees",
    "description": "Allows to run a MCMCglmm on multiple phylogenetic trees to take into account phylogenetic uncertainty.",
    "version": "1.3.7",
    "maintainer": "Thomas Guillerme <guillert@tcd.ie>",
    "url": "https://github.com/TGuillerme/mulTree",
    "exports": [],
    "topics": ["bayesian", "comparative-genomics", "mcmcglmm", "multiple-trees", "phylogenetics", "r"],
    "score": "NA",
    "stars": 12
  },
  {
    "id": 18033,
    "package_name": "multDM",
    "title": "Multivariate Version of the Diebold-Mariano Test",
    "description": "Allows to perform the multivariate version of the Diebold-Mariano test for equal predictive ability of multiple forecast comparison. Main reference: Mariano, R.S., Preve, D. (2012) <doi:10.1016/j.jeconom.2012.01.014>. ",
    "version": "1.1.5",
    "maintainer": "Krzysztof Drachal <kdrachal@wne.uw.edu.pl>",
    "url": "https://CRAN.R-project.org/package=multDM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18053,
    "package_name": "multiMarker",
    "title": "Latent Variable Model to Infer Food Intake from Multiple\nBiomarkers",
    "description": "A latent variable model based on factor analytic and mixture of experts models, designed to infer food intake from multiple biomarkers data. The model is framed within a Bayesian hierarchical framework, which provides flexibility to adapt to different biomarker distributions and facilitates inference on food intake from biomarker data alone, along with the associated uncertainty. Details are in D'Angelo, et al. (2020) <arXiv:2006.02995>.",
    "version": "1.0.1",
    "maintainer": "Silvia D'Angelo <silvia.dangelo@ucd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18065,
    "package_name": "multibreakeR",
    "title": "Tests for a Structural Change in Multivariate Time Series",
    "description": "Flexible implementation of a structural change point detection algorithm for multivariate time series.\n    It authorizes inclusion of trends, exogenous variables, and break test on the intercept or on the full vector autoregression system.\n    Bai, Lumsdaine, and Stock (1998) <doi:10.1111/1467-937X.00051>.",
    "version": "0.1.0",
    "maintainer": "Loic Marechal <loic.marechal@unil.ch>",
    "url": "https://github.com/loicym/multibreakeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18073,
    "package_name": "multid",
    "title": "Multivariate Difference Between Two Groups",
    "description": "Estimation of multivariate differences between two groups (e.g., multivariate sex differences) with regularized regression methods and predictive approach. See Ilmarinen et al. (2023) <doi:10.1177/08902070221088155>. Deconstructing difference score correlations (e.g., gender-equality paradox), see Ilmarinen & Lönnqvist (2024) <doi:10.1037/pspp0000508>.\n    Includes also tools that help in understanding difference score reliability, conditional intra-class correlations, tail-dependency, and heterogeneity of variance estimates. Package development was supported by the Academy of Finland research grant 338891.",
    "version": "1.0.2",
    "maintainer": "Ville-Juhani Ilmarinen <vj.ilmarinen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18076,
    "package_name": "multifamm",
    "title": "Multivariate Functional Additive Mixed Models",
    "description": "An implementation for multivariate functional additive mixed\n    models (multiFAMM), see Volkmann et al. (2021, <arXiv:2103.06606>). It builds on developed methods for univariate sparse \n    functional regression models and multivariate functional principal component\n    analysis. This package contains the function to run a multiFAMM and some\n    convenience functions useful when working with large models. An additional \n    package on GitHub contains more convenience functions to reproduce the \n    analyses of the corresponding paper \n    (<https://github.com/alexvolkmann/multifammPaper>).",
    "version": "0.1.1",
    "maintainer": "Alexander Volkmann <alexandervolkmann8@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18077,
    "package_name": "multifear",
    "title": "Multiverse Analyses for Conditioning Data",
    "description": "A suite of functions for performing analyses, based on a multiverse approach, for conditioning data. Specifically, given the appropriate data, the functions are able to perform t-tests, analyses of variance, and mixed models for the provided data and return summary statistics and plots. The function is also able to return for all those tests p-values, confidence intervals, and Bayes factors. The methods are described in Lonsdorf, Gerlicher, Klingelhofer-Jens, & Krypotos (2022) <doi:10.1016/j.brat.2022.104072>.",
    "version": "0.1.4",
    "maintainer": "Angelos-Miltiadis Krypotos <amkrypotos@gmail.com>",
    "url": "https://github.com/AngelosPsy/multifear",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18088,
    "package_name": "multilevelcoda",
    "title": "Estimate Bayesian Multilevel Models for Compositional Data",
    "description": "Implement Bayesian multilevel modelling for compositional data. \n             Compute multilevel compositional data and \n             perform log-ratio transforms at between and within-person levels, \n             fit Bayesian multilevel models for compositional predictors and outcomes, \n             and run post-hoc analyses such as isotemporal substitution models.\n             References: \n             Le, Stanford, Dumuid, and Wiley (2025) <doi:10.1037/met0000750>,\n             Le, Dumuid, Stanford, and Wiley (2025) <doi:10.1080/00273171.2025.2565598>.",
    "version": "1.3.3",
    "maintainer": "Flora Le <floralebui@gmail.com>",
    "url": "https://florale.github.io/multilevelcoda/,\nhttps://github.com/florale/multilevelcoda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18089,
    "package_name": "multilevelmediation",
    "title": "Utility Functions for Multilevel Mediation Analysis",
    "description": "The ultimate goal is to support 2-2-1, 2-1-1, and 1-1-1 models for\n multilevel mediation, the option of a moderating variable for either the a, b,\n or both paths, and covariates. Currently the 1-1-1 model is supported\n and several options of random effects; the initial code for bootstrapping was\n evaluated in simulations by Falk, Vogel, Hammami, and Miočević (2024) <doi:10.3758/s13428-023-02079-4>.\n Support for Bayesian estimation using 'brms' comprises ongoing work. Currently\n only continuous mediators and outcomes are supported. Factors for any\n predictors must be numerically represented.",
    "version": "0.4.1",
    "maintainer": "Carl F. Falk <carl.falk@mcgill.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18095,
    "package_name": "multimediate",
    "title": "Causal Mediation Analysis in Presence of Multiple Mediators\nUncausally Related",
    "description": "Estimates key quantities in causal mediation analysis - including \n    average causal mediation effects (indirect effects), average direct \n    effects, total effects, and proportions mediated - in the presence of \n    multiple uncausally related mediators. Methods are described by \n    Jérolon et al., (2021) <doi:10.1515/ijb-2019-0088> and extended to \n    accommodate survival outcomes as described by Domingo-Relloso et al., \n    (2024) <doi:10.1101/2024.02.16.24302923>.",
    "version": "0.1.4",
    "maintainer": "Samara Kiihl <samarak@unicamp.br>",
    "url": "https://samarafk.github.io/multimediate/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18103,
    "package_name": "multinma",
    "title": "Bayesian Network Meta-Analysis of Individual and Aggregate Data",
    "description": "Network meta-analysis and network meta-regression models for \n    aggregate data, individual patient data, and mixtures of both individual \n    and aggregate data using multilevel network meta-regression as described by\n    Phillippo et al. (2020) <doi:10.1111/rssa.12579>. Models are estimated in a\n    Bayesian framework using 'Stan'.",
    "version": "0.8.1",
    "maintainer": "David M. Phillippo <david.phillippo@bristol.ac.uk>",
    "url": "https://dmphillippo.github.io/multinma/,\nhttps://github.com/dmphillippo/multinma",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18109,
    "package_name": "multipleNCC",
    "title": "Weighted Cox-Regression for Nested Case-Control Data",
    "description": "Fit Cox proportional hazard models with a weighted \n  partial likelihood. It handles one or multiple endpoints, additional matching \n  and makes it possible to reuse controls for other endpoints \n  Stoer NC and Samuelsen SO (2016) <doi:10.32614/rj-2016-030>.",
    "version": "1.2-5",
    "maintainer": "Nathalie C. Stoer <nast@fhi.no>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18110,
    "package_name": "multipleOutcomes",
    "title": "Asymptotic Covariance Matrix of Regression Models for Multiple\nOutcomes",
    "description": "Regression models can be fitted for multiple outcomes simultaneously. This package computes estimates of parameters across fitted models and returns the matrix of asymptotic covariance. Various applications of this package, including CUPED (Controlled Experiments Utilizing Pre-Experiment Data), multiple comparison adjustment, are illustrated. ",
    "version": "0.4",
    "maintainer": "Han Zhang <zhangh.ustc@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18119,
    "package_name": "multisite.accuracy",
    "title": "Estimation of Accuracy in Multisite Machine-Learning Models",
    "description": "The effects of the site may severely bias the accuracy of a multisite machine-learning model, even if the analysts removed them when fitting the model in the 'training set' and applying the model in the 'test set' (Solanes et al., Neuroimage 2023, 265:119800). This simple R package estimates the accuracy of a multisite machine-learning model unbiasedly, as described in (Solanes et al., Psychiatry Research: Neuroimaging 2021, 314:111313). It currently supports the estimation of sensitivity, specificity, balanced accuracy (for binary or multinomial variables), the area under the curve, correlation, mean squarer error, and hazard ratio for binomial, multinomial, gaussian, and survival (time-to-event) outcomes.",
    "version": "1.3",
    "maintainer": "Joaquim Radua <quimradua@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18123,
    "package_name": "multitaper",
    "title": "Spectral Analysis Tools using the Multitaper Method",
    "description": "Implements multitaper spectral analysis using discrete prolate spheroidal sequences (Slepians) and sine tapers. It includes an adaptive weighted multitaper spectral estimate, a coherence estimate, Thomson's Harmonic F-test, and complex demodulation. The Slepians sequences are generated efficiently using a tridiagonal matrix solution, and jackknifed confidence intervals are available for most estimates. This package is an implementation of the method described in D.J. Thomson (1982) \"Spectrum estimation and harmonic analysis\" <doi:10.1109/PROC.1982.12433>.",
    "version": "1.0-17",
    "maintainer": "Karim Rahim <karim.rahim@queensu.ca>",
    "url": "https://github.com/krahim/multitaper/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18129,
    "package_name": "multiview",
    "title": "Cooperative Learning for Multi-View Analysis",
    "description": "Cooperative learning combines the usual squared error loss of predictions with an agreement penalty to encourage the predictions from different data views to agree. By varying the weight of the agreement penalty, we get a continuum of solutions that include the well-known early and late fusion approaches. Cooperative learning chooses the degree of agreement (or fusion) in an adaptive manner, using a validation set or cross-validation to estimate test set prediction error. In the setting of cooperative regularized linear regression, the method combines the lasso penalty with the agreement penalty (Ding, D., Li, S., Narasimhan, B., Tibshirani, R. (2021) <doi:10.1073/pnas.2202113119>).",
    "version": "0.8",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18142,
    "package_name": "murphydiagram",
    "title": "Murphy Diagrams for Forecast Comparisons",
    "description": "Data and code for the paper by Ehm, Gneiting, Jordan and\n    Krueger ('Of Quantiles and Expectiles: Consistent Scoring Functions, Choquet\n    Representations, and Forecast Rankings', JRSS-B, 2016 <DOI:10.1111/rssb.12154>).",
    "version": "0.12.2",
    "maintainer": "Fabian Krueger <Fabian.Krueger83@gmail.com>",
    "url": "https://sites.google.com/site/fk83research/code",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18158,
    "package_name": "mvDFA",
    "title": "Multivariate Detrended Fluctuation Analysis",
    "description": "This R package provides an implementation of multivariate extensions of a well-known fractal analysis technique, Detrended Fluctuations Analysis (DFA; Peng et al., 1995<doi:10.1063/1.166141>), for multivariate time series: multivariate DFA (mvDFA). Several coefficients are implemented that take into account the correlation structure of the multivariate time series to varying degrees. These coefficients may be used to analyze long memory and changes in the dynamic structure that would by univariate DFA. Therefore, this R package aims to extend and complement the original univariate DFA (Peng et al., 1995) for estimating the scaling properties of nonstationary time series.",
    "version": "0.0.4",
    "maintainer": "Julien Patrick Irmer <jirmer@psych.uni-frankfurt.de>",
    "url": "https://github.com/jpirmer/mvDFA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18159,
    "package_name": "mvGPS",
    "title": "Causal Inference using Multivariate Generalized Propensity Score",
    "description": "\n    Methods for estimating and utilizing the multivariate generalized\n    propensity score (mvGPS) for multiple continuous exposures described in\n    Williams, J.R, and Crespi, C.M. (2020) <arxiv:2008.13767>. The methods allow\n    estimation of a dose-response surface relating the joint distribution of multiple\n    continuous exposure variables to an outcome. Weights are constructed assuming a\n    multivariate normal density for the marginal and conditional distribution of\n    exposures given a set of confounders. Confounders can be different for different\n    exposure variables. The weights are designed to achieve balance across all\n    exposure dimensions and can be used to estimate dose-response surfaces.",
    "version": "1.2.2",
    "maintainer": "Justin Williams <williazo@ucla.edu>",
    "url": "https://github.com/williazo/mvGPS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18160,
    "package_name": "mvLSW",
    "title": "Multivariate, Locally Stationary Wavelet Process Estimation",
    "description": "Tools for analysing multivariate time series with wavelets. This includes: simulation of a multivariate locally stationary wavelet (mvLSW) process from a multivariate evolutionary wavelet spectrum (mvEWS); estimation of the mvEWS, local coherence and local partial coherence. See Park, Eckley and Ombao (2014) <doi:10.1109/TSP.2014.2343937> for details.",
    "version": "1.2.5",
    "maintainer": "Daniel Grose <dan.grose@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18161,
    "package_name": "mvLSWimpute",
    "title": "Imputation Methods for Multivariate Locally Stationary Time\nSeries",
    "description": "Implementation of imputation techniques based on locally stationary wavelet time series forecasting methods from Wilson, R. E. et al. (2021) <doi:10.1007/s11222-021-09998-2>.",
    "version": "0.1.1",
    "maintainer": "Matt Nunes <nunesrpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18170,
    "package_name": "mvSUSY",
    "title": "Multivariate Surrogate Synchrony",
    "description": "Multivariate Surrogate Synchrony ('mvSUSY') estimates the synchrony within datasets that contain more than two time series. 'mvSUSY' was developed from Surrogate Synchrony ('SUSY') with respect to implementing surrogate controls, and extends synchrony estimation to multivariate data. 'mvSUSY' works as described in Meier & Tschacher (2021).",
    "version": "0.1.0",
    "maintainer": "Wolfgang Tschacher <wolfgang.tschacher@unibe.ch>",
    "url": "https://wtschacher.github.io/mvSUSY/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18183,
    "package_name": "mvglmmRank",
    "title": "Multivariate Generalized Linear Mixed Models for Ranking Sports\nTeams",
    "description": "Maximum likelihood estimates are obtained via an EM algorithm with either a first-order or a fully exponential Laplace approximation as documented by Broatch and Karl (2018) <doi:10.48550/arXiv.1710.05284>,\n    Karl, Yang, and Lohr (2014) <doi:10.1016/j.csda.2013.11.019>, and by \n\tKarl (2012) <doi:10.1515/1559-0410.1471>. Karl and Zimmerman <doi:10.1016/j.jspi.2020.06.004> use this package to illustrate how the home field effect estimator from a mixed model can be biased under nonrandom scheduling. ",
    "version": "1.2-4",
    "maintainer": "Andrew T. Karl <akarl@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18186,
    "package_name": "mvinfluence",
    "title": "Influence Measures and Diagnostic Plots for Multivariate Linear\nModels",
    "description": "Computes regression deletion diagnostics for multivariate linear models and provides some associated\n\tdiagnostic plots.  The diagnostic measures include hat-values (leverages), generalized Cook's distance, and\n\tgeneralized squared 'studentized' residuals.  Several types of plots to detect influential observations are\n\tprovided.",
    "version": "0.9.2",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "url": "https://github.com/friendly/mvinfluence,\nhttps://friendly.github.io/mvinfluence/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18188,
    "package_name": "mvmeta",
    "title": "Multivariate and Univariate Meta-Analysis and Meta-Regression",
    "description": "Collection of functions to perform fixed and random-effects multivariate and univariate meta-analysis and meta-regression.",
    "version": "1.0.3",
    "maintainer": "Antonio Gasparrini <antonio.gasparrini@lshtm.ac.uk>",
    "url": "http://www.ag-myresearch.com/package-mvmeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18190,
    "package_name": "mvna",
    "title": "Nelson-Aalen Estimator of the Cumulative Hazard in Multistate\nModels",
    "description": "Computes the Nelson-Aalen estimator of the cumulative transition hazard for arbitrary Markov multistate models <ISBN:978-0-387-68560-1>. ",
    "version": "2.0.1",
    "maintainer": "Arthur Allignol <arthur.allignol@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18198,
    "package_name": "mvord",
    "title": "Multivariate Ordinal Regression Models",
    "description": "A flexible framework for fitting multivariate\n    ordinal regression models with composite likelihood methods. Methodological details are given in Hirk, Hornik, Vana (2020) <doi:10.18637/jss.v093.i04>.",
    "version": "1.2.6",
    "maintainer": "Laura Vana <laura.vana@tuwien.ac.at>",
    "url": "https://github.com/lauravana/mvord",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18205,
    "package_name": "mvst",
    "title": "Bayesian Inference for the Multivariate Skew-t Model",
    "description": "Estimates the multivariate skew-t and nested models, as described in the\n    articles Liseo, B., Parisi, A. (2013). Bayesian inference for the multivariate skew-normal\n    model: a population Monte Carlo approach. Comput. Statist. Data Anal.\n    <doi:10.1016/j.csda.2013.02.007> and in Parisi, A., Liseo, B. (2017). Objective Bayesian\n    analysis for the multivariate skew-t model. Statistical Methods & Applications\n    <doi: 10.1007/s10260-017-0404-0>.",
    "version": "1.1.1",
    "maintainer": "Antonio Parisi <antonio.parisi@uniroma2.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18208,
    "package_name": "mvtsplot",
    "title": "Multivariate Time Series Plot",
    "description": "A function for plotting multivariate time series data.",
    "version": "1.0-5",
    "maintainer": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "url": "https://github.com/rdpeng/mvtsplot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18209,
    "package_name": "mvtweedie",
    "title": "Estimate Diet Proportions Using Multivariate Tweedie Model",
    "description": "Defines predict function that transforms output from a Tweedie\n    Generalized Linear Mixed Model (using 'glmmTMB'),\n    Generalized Additive Model (using 'mgcv'), or\n    spatio-temporal Generalized Linear Mixed Model (using package 'tinyVAST'),\n    and returns predicted proportions (and standard errors) across a\n    grouping variable from an equivalent multivariate-logit Tweedie model.\n    These predicted proportions can then be used for standard plotting\n    and diagnostics.  See Thorson et al. 2022 <doi:10.1002/ecy.3637>.",
    "version": "1.2.0",
    "maintainer": "James Thorson <James.Thorson@noaa.gov>",
    "url": "https://james-thorson-noaa.github.io/mvtweedie/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18228,
    "package_name": "mycor",
    "title": "Automatic Correlation and Regression Test in a 'data.frame'",
    "description": "Perform correlation and linear regression test\n    among the numeric fields in a data.frame automatically\n    and make plots using pairs or lattice::parallelplot.",
    "version": "0.1.1",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "url": "https://github.com/cardiomoon/mycor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18264,
    "package_name": "nanotime",
    "title": "Nanosecond-Resolution Time Support for R",
    "description": "Full 64-bit resolution date and time functionality with\n nanosecond granularity is provided, with easy transition to and from\n the standard 'POSIXct' type. Three additional classes offer interval,\n period and duration functionality for nanosecond-resolution timestamps.",
    "version": "0.3.12",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "url": "https://github.com/eddelbuettel/nanotime,\nhttps://eddelbuettel.github.io/nanotime/,\nhttps://dirk.eddelbuettel.com/code/nanotime.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18271,
    "package_name": "nardl",
    "title": "Nonlinear Cointegrating Autoregressive Distributed Lag Model",
    "description": "Computes the nonlinear cointegrating autoregressive distributed lag model with automatic bases aic and bic lags selection of independent variables proposed by (Shin, Yu & Greenwood-Nimmo, 2014 <doi:10.1007/978-1-4899-8008-3_9>).",
    "version": "0.1.6",
    "maintainer": "Taha Zaghdoudi <zedtaha@gmail.com>",
    "url": "https://github.com/zedtaha/nardl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18272,
    "package_name": "narfima",
    "title": "Neural AutoRegressive Fractionally Integrated Moving Average\nModel",
    "description": "Methods and tools for forecasting univariate time series using the NARFIMA (Neural AutoRegressive Fractionally Integrated Moving Average) model. It combines neural networks with fractional differencing to capture both nonlinear patterns and long-term dependencies. The NARFIMA model supports seasonal adjustment, Box-Cox transformations, optional exogenous variables, and the computation of prediction intervals. In addition to the NARFIMA model, this package provides alternative forecasting models including NARIMA (Neural ARIMA), NBSTS (Neural Bayesian Structural Time Series), and NNaive (Neural Naive) for performance comparison across different modeling approaches. The methods are based on algorithms introduced by Chakraborty et al. (2025) <doi:10.48550/arXiv.2509.06697>.",
    "version": "0.1.0",
    "maintainer": "Donia Besher <donia.a.besher@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18308,
    "package_name": "ncappc",
    "title": "NCA Calculations and Population Model Diagnosis",
    "description": "A flexible tool that can perform\n    (i) traditional non-compartmental analysis (NCA) and\n    (ii) Simulation-based posterior predictive checks for population\n    pharmacokinetic (PK) and/or pharmacodynamic (PKPD) models using \n    NCA metrics. The methods are described in Acharya et al. (2016) \n    <doi:10.1016/j.cmpb.2016.01.013>.",
    "version": "1.0.0",
    "maintainer": "Andrew C. Hooker <andrew.hooker@uu.se>",
    "url": "https://github.com/UUPharmacometrics/ncappc,\nhttps://uupharmacometrics.github.io/ncappc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18312,
    "package_name": "ncdfCF",
    "title": "Easy Access to NetCDF Files with CF Metadata Conventions",
    "description": "Network Common Data Form ('netCDF') files are widely used for \n    scientific data. Library-level access in R is provided through packages \n    'RNetCDF' and 'ncdf4'. Package 'ncdfCF' is built on top of 'RNetCDF' and \n    makes the data and its attributes available as a set of R6 classes that are \n    informed by the Climate and Forecasting Metadata Conventions. Access to the \n    data uses standard R subsetting operators and common function forms.",
    "version": "0.7.0",
    "maintainer": "Patrick Van Laake <patrick@vanlaake.net>",
    "url": "https://github.com/R-CF/ncdfCF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18339,
    "package_name": "negligible",
    "title": "A Collection of Functions for Negligible Effect/Equivalence\nTesting",
    "description": "Researchers often want to evaluate whether there is a negligible\n    relationship among variables. The 'negligible' package provides functions that \n    are useful for conducting negligible effect testing (also called\n    equivalence testing). For example, there are functions for evaluating the \n    equivalence of means or the presence of a negligible association \n    (correlation or  regression). Beribisky, N., Mara, C., & Cribbie, R. A. (2020) <doi:10.20982/tqmp.16.4.p424>.\n    Beribisky, N., Davidson, H., Cribbie, R. A. (2019) <doi:10.7717/peerj.6853>.\n    Shiskina, T., Farmus, L., & Cribbie, R. A. (2018) <doi:10.20982/tqmp.14.3.p167>.\n    Mara, C. & Cribbie, R. A. (2017) <doi:10.1080/00220973.2017.1301356>.\n    Counsell, A. & Cribbie, R. A. (2015) <doi:10.1111/bmsp.12045>.\n    van Wieringen, K. & Cribbie, R. A. (2014) <doi:10.1111/bmsp.12015>.\n    Goertzen, J. R. & Cribbie, R. A. (2010) <doi:10.1348/000711009x475853>.\n    Cribbie, R. A., Gruman, J. & Arpin-Cribbie, C. (2004) <doi:10.1002/jclp.10217>.",
    "version": "0.1.11",
    "maintainer": "Robert Cribbie <cribbie@yorku.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18387,
    "package_name": "netie",
    "title": "Antigen T Cell Interaction Estimation",
    "description": "The Bayesian hierarchical model named antigen-T cell interaction estimation is to estimate the history of the immune pressure on the evolution of the tumor clones.The model is based on the estimation result from Andrew Roth (2014) <doi:10.1038/nmeth.2883>.",
    "version": "1.0",
    "maintainer": "Tianshi Lu <tianshi.lu@utsouthwestern.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18396,
    "package_name": "netregR",
    "title": "Regression of Network Responses",
    "description": "Regress network responses (both directed and undirected) onto covariates of interest that may be actor-, relation-, or network-valued. In addition, compute principled variance estimates of the coefficients assuming that the errors are jointly exchangeable. Missing data is accommodated. Additionally implements building and inversion of covariance matrices under joint exchangeability, and  generates random covariance matrices from this class. For more detail on methods, see Marrs, Fosdick, and McCormick (2017) <arXiv:1701.05530>.",
    "version": "1.0.1",
    "maintainer": "Frank W. Marrs <frank.marrs@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18399,
    "package_name": "nets",
    "title": "Network Estimation for Time Series",
    "description": "Sparse VAR estimation based on LASSO.",
    "version": "0.9.1",
    "maintainer": "Christian Brownlees <christian.brownlees@upf.edu>",
    "url": "https://github.com/ctbrownlees/R-Package-nets",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18400,
    "package_name": "netseer",
    "title": "Graph Prediction from a Graph Time Series",
    "description": "Predicting the structure of a graph including new nodes and edges using\n    a time series of graphs. Flux balance analysis, a linear and integer programming \n    technique used in biochemistry is used with time series prediction methods to \n    predict the graph structure at a future time point \n    Kandanaarachchi (2025) <doi:10.48550/arXiv.2507.05806>.",
    "version": "0.1.2",
    "maintainer": "Sevvandi Kandanaarachchi <sevvandik@gmail.com>",
    "url": "https://sevvandi.github.io/netseer/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18418,
    "package_name": "neuRosim",
    "title": "Simulate fMRI Data",
    "description": "Generates functional Magnetic Resonance Imaging (fMRI) \n             time series or 4D data. Some high-level \n             functions are created for fast data generation with only \n             a few arguments and a diversity of functions to define \n             activation and noise. For more advanced users it is possible \n             to use the low-level functions and manipulate the arguments.\n             See Welvaert et al. (2011) <doi:10.18637/jss.v044.i10>.",
    "version": "0.2-14",
    "maintainer": "Karsten Tabelow <karsten.tabelow@wias-berlin.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18422,
    "package_name": "neuroUp",
    "title": "Plan Sample Size for Task fMRI Research using Bayesian Updating",
    "description": "Calculate the precision in mean differences (raw or Cohen's D) and \n    correlation coefficients for different sample sizes. Uses permutations of the\n    collected functional magnetic resonance imaging (fMRI) region of interest data.\n    Method described in Klapwijk, Jongerling, Hoijtink and Crone (2024) \n    <doi:10.31234/osf.io/cz32t>.",
    "version": "0.3.1",
    "maintainer": "Eduard Klapwijk <et.klapwijk@gmail.com>",
    "url": "https://eduardklap.github.io/neuroUp/,\nhttps://github.com/eduardklap/neuroUp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18431,
    "package_name": "neutroSurvey",
    "title": "Neutrosophic Survey Data Analysis",
    "description": "Apply neutrosophic regression type estimator and performs neutrosophic interval analysis including metric calculations for survey data.",
    "version": "0.1.0",
    "maintainer": "Pankaj Das <pankaj.iasri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18434,
    "package_name": "nevada",
    "title": "Network-Valued Data Analysis",
    "description": "A flexible statistical framework for network-valued data analysis. \n    It leverages the complexity of the space of distributions on graphs by using \n    the permutation framework for inference as implemented in the 'flipr' package. \n    Currently, only the two-sample testing problem is covered and generalization \n    to k samples and regression will be added in the future as well. It is a \n    4-step procedure where the user chooses a suitable representation of the \n    networks, a suitable metric to embed the representation into a metric space, \n    one or more test statistics to target specific aspects of the distributions \n    to be compared and a formula to compute the permutation p-value. Two types \n    of inference are provided: a global test answering whether there is a \n    difference between the distributions that generated the two samples and a \n    local test for localizing differences on the network structure. The latter \n    is assumed to be shared by all networks of both samples. References: Lovato, \n    I., Pini, A., Stamm, A., Vantini, S. (2020) \"Model-free two-sample test for \n    network-valued data\" <doi:10.1016/j.csda.2019.106896>; Lovato, I., Pini, A., \n    Stamm, A., Taquet, M., Vantini, S. (2021) \"Multiscale null hypothesis \n    testing for network-valued data: Analysis of brain networks of patients with \n    autism\" <doi:10.1111/rssc.12463>.",
    "version": "0.2.0",
    "maintainer": "Aymeric Stamm <aymeric.stamm@cnrs.fr>",
    "url": "https://astamm.github.io/nevada/,\nhttps://github.com/astamm/nevada/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18439,
    "package_name": "newTestSurvRec",
    "title": "Statistical Tests to Compare Curves with Recurrent Events",
    "description": "Implements the routines to compare the survival curves with recurrent events, including the estimations of survival curves. The first model is a  model for recurrent event, when the data are correlated or not  correlated. It was proposed by Wang and Chang (1999) <doi:10.2307/2669690>. In the independent case, the survival function can be  estimated by the generalization of the limit product model of Pena (2001) <doi:10.1198/016214501753381922>.",
    "version": "1.0.2",
    "maintainer": "Carlos Martinez <cmmm7031@gmail.com>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18467,
    "package_name": "nhs.predict",
    "title": "Breast Cancer Survival and Therapy Benefits",
    "description": "Calculate Overall Survival or Recurrence-Free Survival for breast cancer patients, using 'NHS Predict'. \n            The time interval for the estimation can be set up to 15 years, with default at 10.\n            Incremental therapy benefits are estimated for hormone therapy, chemotherapy, trastuzumab, and bisphosphonates.\n            An additional function, suited for SCAN audits, features a more user-friendly version of the code, with fewer inputs, but necessitates\n            the correct standardised inputs.\n            This work is not affiliated with the development of 'NHS Predict' and its underlying statistical model.\n            Details on 'NHS Predict' can be found at: <doi:10.1186/bcr2464>.\n            The web version of 'NHS Predict': <https://breast.predict.nhs.uk/>.\n            A small dataset of 50 fictional patient observations is provided for the purpose of running examples with the main two functions,\n            and an additional dataset is provided for running example with the dedicated SCAN function.",
    "version": "1.4.0",
    "maintainer": "Giovanni Tramonti <giovanni.tramonti@ed.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18474,
    "package_name": "nicheROVER",
    "title": "Niche Region and Niche Overlap Metrics for Multidimensional\nEcological Niches",
    "description": "Implementation of a probabilistic method to calculate 'nicheROVER' (_niche_ _r_egion and niche _over_lap) metrics using multidimensional niche indicator data (e.g., stable isotopes, environmental variables, etc.). The niche region is defined as the joint probability density function of the multidimensional niche indicators at a user-defined probability alpha (e.g., 95%).  Uncertainty is accounted for in a Bayesian framework, and the method can be extended to three or more indicator dimensions.  It provides directional estimates of niche overlap, accounts for species-specific distributions in multivariate niche space, and produces unique and consistent bivariate projections of the multivariate niche region.  The article by Swanson et al. (2015) <doi:10.1890/14-0235.1> provides a detailed description of the methodology.  See the package vignette for a worked example using fish stable isotope data.",
    "version": "1.1.2",
    "maintainer": "Martin Lysy <mlysy@uwaterloo.ca>",
    "url": "https://github.com/mlysy/nicheROVER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18475,
    "package_name": "nichetools",
    "title": "Complementary Package to 'nicheROVER' and 'SIBER'",
    "description": "Provides functions complementary to packages 'nicheROVER' and 'SIBER'\n        allowing the user to extract Bayesian estimates from data objects created \n        by the packages 'nicheROVER' and 'SIBER'. Please see the following \n        publications for detailed methods on 'nicheROVER' and 'SIBER' \n        Hansen et al. (2015) <doi:10.1890/14-0235.1>, \n        Jackson et al. (2011) <doi:10.1111/j.1365-2656.2011.01806.x>, and\n        Layman et al. (2007) <doi:10.1890/0012-9658(2007)88[42:CSIRPF]2.0.CO;2>,\n        respectfully.",
    "version": "0.3.2",
    "maintainer": "Benjamin L. Hlina <benjamin.hlina@gmail.com>",
    "url": "https://benjaminhlina.github.io/nichetools/,\nhttps://github.com/benjaminhlina/nichetools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18483,
    "package_name": "nimble",
    "title": "MCMC, Particle Filtering, and Programmable Hierarchical Modeling",
    "description": "A system for writing hierarchical statistical models largely\n    compatible with 'BUGS' and 'JAGS', writing nimbleFunctions to operate models\n    and do basic R-style math, and compiling both models and nimbleFunctions via\n    custom-generated C++. 'NIMBLE' includes default methods for MCMC, Laplace\n    Approximation, deterministic nested approximations, Monte Carlo Expectation\n    Maximization, and some other tools.\n    The nimbleFunction system makes it easy to do things like implement new MCMC\n    samplers from R, customize the assignment of samplers to different parts of\n    a model from R, and compile the new samplers automatically via C++ alongside\n    the samplers 'NIMBLE' provides. 'NIMBLE' extends the 'BUGS'/'JAGS' language\n    by making it extensible: New distributions and functions can be added,\n    including as calls to external compiled code. Although most people think\n    of MCMC as the main goal of the 'BUGS'/'JAGS' language for writing models,\n    one can use 'NIMBLE' for writing arbitrary other kinds of model-generic\n    algorithms as well. A full User Manual is available at <https://r-nimble.org>.",
    "version": "1.4.0",
    "maintainer": "Christopher Paciorek <paciorek@stat.berkeley.edu>",
    "url": "https://r-nimble.org, https://github.com/nimble-dev/nimble",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18487,
    "package_name": "nimbleHMC",
    "title": "Hamiltonian Monte Carlo and Other Gradient-Based MCMC Sampling\nAlgorithms for 'nimble'",
    "description": "Provides gradient-based MCMC sampling algorithms for use with the MCMC engine provided by the 'nimble' package.  This includes two versions of Hamiltonian Monte Carlo (HMC) No-U-Turn (NUTS) sampling, and (under development) Langevin samplers.  The `NUTS_classic` sampler implements the original HMC-NUTS algorithm as described in Hoffman and Gelman (2014) <doi:10.48550/arXiv.1111.4246>.  The `NUTS` sampler is a modern version of HMC-NUTS sampling matching the HMC sampler available in version 2.32.2 of Stan (Stan Development Team, 2023). In addition, convenience functions are provided for generating and modifying MCMC configuration objects which employ HMC sampling. Functionality of the 'nimbleHMC' package is described further in Turek, et al (2024) <doi: 10.21105/joss.06745>.",
    "version": "0.2.4",
    "maintainer": "Daniel Turek <danielturek@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18489,
    "package_name": "nimbleNoBounds",
    "title": "Transformed Distributions for Improved MCMC Efficiency",
    "description": "A collection of common univariate bounded probability distributions transformed to the unbounded real line, for the purpose of increased MCMC efficiency.",
    "version": "1.0.3",
    "maintainer": "David Pleydell <david.pleydell@inrae.fr>",
    "url": "https://github.com/DRJP/nimbleNoBounds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18498,
    "package_name": "nixmass",
    "title": "Snow Water Equivalent Modeling with the 'Delta.snow' and\n'HS2SWE' Models and Empirical Regression Models",
    "description": "Snow water equivalent is modeled with the process based\n    models 'delta.snow' and 'HS2SWE' and empirical regression, which use\n    relationships between density and diverse at-site parameters. The\n    methods are described in Winkler et al. (2021)\n    <doi:10.5194/hess-25-1165-2021>, Magnusson et al. (2025)\n    <doi:10.1016/j.coldregions.2025.104435>, Guyennon et al. (2019)\n    <doi:10.1016/j.coldregions.2019.102859>, Pistocchi (2016)\n    <doi:10.1016/j.ejrh.2016.03.004>, Jonas et al. (2009)\n    <doi:10.1016/j.jhydrol.2009.09.021> and Sturm et al. (2010)\n    <doi:10.1175/2010JHM1202.1>.",
    "version": "1.3.1",
    "maintainer": "Harald Schellander <harald.schellander@geosphere.at>",
    "url": "https://haraldschellander.github.io/nixmass/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18499,
    "package_name": "nixtlar",
    "title": "A Software Development Kit for 'Nixtla''s 'TimeGPT'",
    "description": "A Software Development Kit for working with 'Nixtla''s 'TimeGPT', a foundation\n    model for time series forecasting. 'API' is an acronym for 'application\n    programming interface'; this package allows users to interact with\n    'TimeGPT' via the 'API'. You can set and validate 'API' keys and generate forecasts\n    via 'API' calls. It is compatible with 'tsibble' and base R. For more details \n    visit <https://docs.nixtla.io/>.",
    "version": "0.6.2",
    "maintainer": "Mariana Menchero <mariana@nixtla.io>",
    "url": "https://nixtla.github.io/nixtlar/, https://docs.nixtla.io/,\nhttps://github.com/Nixtla/nixtlar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18506,
    "package_name": "nlist",
    "title": "Lists of Numeric Atomic Objects",
    "description": "Create and manipulate numeric list ('nlist') objects.  An\n    'nlist' is an S3 list of uniquely named numeric objects.  An numeric\n    object is an integer or double vector, matrix or array.  An 'nlists'\n    object is a S3 class list of 'nlist' objects with the same names,\n    dimensionalities and typeofs.  Numeric list objects are of interest\n    because they are the raw data inputs for analytic engines such as\n    'JAGS', 'STAN' and 'TMB'.  Numeric lists objects, which are useful for\n    storing multiple realizations of of simulated data sets, can be\n    converted to coda::mcmc and coda::mcmc.list objects.",
    "version": "0.4.0",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://github.com/poissonconsulting/nlist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18507,
    "package_name": "nlive",
    "title": "Automated Estimation of Sigmoidal and Piecewise Linear Mixed\nModels",
    "description": "Estimation of relatively complex nonlinear mixed-effects models, including the Sigmoidal Mixed Model and the Piecewise Linear Mixed Model with abrupt or smooth transition, through a single intuitive line of code and with automated generation of starting values. ",
    "version": "0.8.0",
    "maintainer": "Maude Wagner <maude_wagner@rush.edu>",
    "url": "https://github.com/MaudeWagner/nlive",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18519,
    "package_name": "nlmm",
    "title": "Generalized Laplace Mixed-Effects Models",
    "description": "Provides functions to fit linear mixed models\n\tbased on convolutions of the generalized Laplace (GL) distribution.\n\tThe GL mixed-effects model includes four special cases with normal random\n\teffects and normal errors (NN), normal random effects and Laplace errors (NL),\n\tLaplace random effects and normal errors (LN), and Laplace random effects\n\tand Laplace errors (LL). The methods are described in Geraci and Farcomeni (2020,\n\tStatistical Methods in Medical Research) <doi:10.1177/0962280220903763>.",
    "version": "1.1.1",
    "maintainer": "Marco Geraci <marco.geraci@uniroma1.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18527,
    "package_name": "nlraa",
    "title": "Nonlinear Regression for Agricultural Applications",
    "description": "Additional nonlinear regression functions using self-start (SS) algorithms. One of the functions is the Beta growth function proposed by Yin et al. (2003) <doi:10.1093/aob/mcg029>. There are several other functions with breakpoints (e.g. linear-plateau, plateau-linear, exponential-plateau, plateau-exponential, quadratic-plateau, plateau-quadratic and bilinear), a non-rectangular hyperbola and a bell-shaped curve. Twenty eight (28) new self-start (SS) functions in total. This package also supports the publication 'Nonlinear regression Models and applications in agricultural research' by Archontoulis and Miguez (2015) <doi:10.2134/agronj2012.0506>, a book chapter with similar material <doi:10.2134/appliedstatistics.2016.0003.c15> and a publication by Oddi et. al. (2019) in Ecology and Evolution <doi:10.1002/ece3.5543>. The function 'nlsLMList' uses 'nlsLM' for fitting, but it is otherwise almost identical to 'nlme::nlsList'.In addition, this release of the package provides functions for conducting simulations for 'nlme' and 'gnls' objects as well as bootstrapping. These functions are intended to work with the modeling framework of the 'nlme' package. It also provides four vignettes with extended examples.",
    "version": "1.9.10",
    "maintainer": "Fernando Miguez <femiguez@iastate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18531,
    "package_name": "nls.multstart",
    "title": "Robust Non-Linear Regression using AIC Scores",
    "description": "Non-linear least squares regression with the Levenberg-Marquardt algorithm using multiple starting values for increasing the chance that the minimum found is the global minimum.",
    "version": "2.0.0",
    "maintainer": "Daniel Padfield <d.padfield@exeter.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18532,
    "package_name": "nls2",
    "title": "Non-Linear Regression with Brute Force",
    "description": "Adds brute force and multiple starting values to nls.",
    "version": "0.3-4",
    "maintainer": "G. Grothendieck <ggrothendieck@gmail.com>",
    "url": "https://github.com/ggrothendieck/nls2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18533,
    "package_name": "nlsMicrobio",
    "title": "Nonlinear Regression in Predictive Microbiology",
    "description": "Data sets and nonlinear regression models dedicated to predictive microbiology.",
    "version": "1.0-0",
    "maintainer": "Aurelie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "url": "https://github.com/lbbe-software/nlsMicrobio",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18537,
    "package_name": "nlsmsn",
    "title": "Fitting Nonlinear Models with Scale Mixture of Skew-Normal\nDistributions",
    "description": "Fit univariate non-linear scale mixture of skew-normal(NL-SMSN) regression, details in Garay, Lachos and Abanto-Valle (2011) <doi:10.1016/j.jkss.2010.08.003> and Lachos, Bandyopadhyay and Garay (2011) <doi:10.1016/j.spl.2011.03.019>.",
    "version": "0.0-6",
    "maintainer": "Marcos Prates <marcosop@est.ufmg.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18540,
    "package_name": "nlstools",
    "title": "Tools for Nonlinear Regression Analysis",
    "description": "Several tools for assessing the quality of fit of a gaussian nonlinear model are provided.",
    "version": "2.1-0",
    "maintainer": "Aurelie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "url": "https://github.com/lbbe-software/nlstools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18542,
    "package_name": "nltm",
    "title": "Non-Linear Transformation Models",
    "description": "Fits a non-linear transformation model ('nltm') for\n        analyzing survival data, see Tsodikov (2003) <doi:10.1111/1467-9868.00414>. The\n        class of 'nltm' includes the following currently supported\n        models: Cox proportional hazard, proportional hazard cure,\n        proportional odds, proportional hazard - proportional hazard\n        cure, proportional hazard - proportional odds cure, Gamma\n        frailty, and proportional hazard - proportional odds.",
    "version": "1.4.6",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "url": "https://github.com/mclements/nltm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18543,
    "package_name": "nlts",
    "title": "Nonlinear Time Series Analysis",
    "description": "R functions for (non)linear time series analysis with an emphasis on nonparametric autoregression and order estimation, and tests for linearity / additivity.",
    "version": "1.0-2",
    "maintainer": "Ottar N. Bjornstad <onb1@psu.edu>",
    "url": "http://ento.psu.edu/directory/onb1",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18565,
    "package_name": "nnfor",
    "title": "Time Series Forecasting with Neural Networks",
    "description": "Automatic time series modelling with neural networks. \n    Allows fully automatic, semi-manual or fully manual specification of networks. For details of the\n\tspecification methodology see: (i) Crone and Kourentzes (2010) <doi:10.1016/j.neucom.2010.01.017>;\n\tand (ii) Kourentzes et al. (2014) <doi:10.1016/j.eswa.2013.12.011>.",
    "version": "0.9.9",
    "maintainer": "Nikolaos Kourentzes <nikolaos@kourentzes.com>",
    "url": "https://kourentzes.com/forecasting/2019/01/16/tutorial-for-the-nnfor-r-package/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18567,
    "package_name": "nnlasso",
    "title": "Non-Negative Lasso and Elastic Net Penalized Generalized Linear\nModels",
    "description": "Estimates of coefficients of lasso penalized linear regression and generalized linear models subject to non-negativity constraints on the parameters using multiplicative iterative algorithm. Entire regularization path for a sequence of lambda values can be obtained. Functions are available for creating plots of regularization path, cross validation and estimating coefficients at a given lambda value. There is also provision for obtaining standard error of coefficient estimates.",
    "version": "0.3",
    "maintainer": "Baidya Nath Mandal <mandal.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18581,
    "package_name": "noia",
    "title": "Implementation of the Natural and Orthogonal InterAction (NOIA)\nModel",
    "description": "The NOIA model, as described extensively in Alvarez-Castro & Carlborg (2007), is a framework facilitating the estimation of genetic effects and genotype-to-phenotype maps. This package provides the basic tools to perform linear and multilinear regressions from real populations (provided the phenotype and the genotype of every individuals), estimating the genetic effects from different reference points, the genotypic values, and the decomposition of genetic variances in a multi-locus, 2 alleles system. This package is presented in Le Rouzic & Alvarez-Castro (2008). ",
    "version": "0.97.3",
    "maintainer": "Arnaud Le Rouzic <arnaud.le-rouzic@universite-paris-saclay.fr>",
    "url": "https://github.com/lerouzic/noia",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18587,
    "package_name": "noisysbmGGM",
    "title": "Noisy Stochastic Block Model for GGM Inference",
    "description": "Greedy Bayesian algorithm to fit the noisy stochastic block model to an observed sparse graph. Moreover, a graph inference procedure to recover Gaussian Graphical Model (GGM) from real data. This procedure comes with a control of the false discovery rate. The method is described in the article \"Enhancing the Power of Gaussian Graphical Model Inference by Modeling the Graph Structure\" by Kilian, Rebafka, and Villers (2024) <arXiv:2402.19021>.",
    "version": "0.1.2.3",
    "maintainer": "Valentin Kilian <valentin.kilian@ens-rennes.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18597,
    "package_name": "nomogramEx",
    "title": "Extract Equations from a Nomogram",
    "description": "\n  A nomogram can not be easily applied,\n    because it is difficult to calculate the points or even the survival probability.\n  The package, including a function of nomogramEx(),\n    is to extract the polynomial equations to calculate the points of each variable,\n    and the survival probability corresponding to the total points.",
    "version": "3.0",
    "maintainer": "Zhicheng Du<dgdzc@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18598,
    "package_name": "nomogramFormula",
    "title": "Calculate Total Points and Probabilities for Nomogram",
    "description": "\n  A nomogram, which can be carried out in 'rms' package, provides a \n    graphical explanation of a prediction process. However, it is not very easy\n    to draw straight lines, read points and probabilities accurately. Even, it \n    is hard for users to calculate total points and probabilities for all \n    subjects.\n  This package provides formula_rd() and formula_lp() functions to fit the \n    formula of total points with raw data and linear predictors respectively by\n    polynomial regression. Function points_cal() will help you calculate the \n    total points. prob_cal() can be used to calculate the probabilities after\n    lrm(), cph() or psm() regression. \n  For more complex condition, interaction or restricted cubic spine, TotalPoints.rms() \n    can be used.",
    "version": "1.2.0.0",
    "maintainer": "Jing Zhang<zj391120@163.com>",
    "url": "https://github.com/yikeshu0611/nomogramFormula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18602,
    "package_name": "noncomplyR",
    "title": "Bayesian Analysis of Randomized Experiments with Non-Compliance",
    "description": "Functions for Bayesian analysis of data from randomized experiments with non-compliance. The functions are based on the models described in Imbens and Rubin (1997) <doi:10.1214/aos/1034276631>. Currently only two types of outcome models are supported: binary outcomes and normally distributed outcomes. Models can be fit with and without the exclusion restriction and/or the strong access monotonicity assumption. Models are fit using the data augmentation algorithm as described in Tanner and Wong (1987) <doi:10.2307/2289457>.",
    "version": "1.0",
    "maintainer": "Scott Coggeshall <sscogges@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18605,
    "package_name": "nonlinearICP",
    "title": "Invariant Causal Prediction for Nonlinear Models",
    "description": "Performs 'nonlinear Invariant Causal Prediction' to estimate the \n    causal parents of a given target variable from data collected in\n    different experimental or environmental conditions, extending\n    'Invariant Causal Prediction' from Peters, Buehlmann and Meinshausen (2016), \n    <arXiv:1501.01332>, to nonlinear settings. For more details, see C. Heinze-Deml, \n    J. Peters and N. Meinshausen: 'Invariant Causal Prediction for Nonlinear Models', \n    <arXiv:1706.08576>.",
    "version": "0.1.2.1",
    "maintainer": "Christina Heinze-Deml <heinzedeml@stat.math.ethz.ch>",
    "url": "https://github.com/christinaheinze/nonlinearICP-and-CondIndTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18615,
    "package_name": "nonparametric.bayes",
    "title": "Project Code - Nonparametric Bayes",
    "description": "Basic implementation of a Gibbs sampler for a Chinese Restaurant Process along with some visual aids to help understand how the sampling works. This is developed as part of a postgraduate school project for an Advanced Bayesian Nonparametric course. It is inspired by Tamara Broderick's presentation on Nonparametric Bayesian statistics given at the Simons institute.",
    "version": "0.0.1",
    "maintainer": "Erik-Cristian Seulean <erikseulean@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18617,
    "package_name": "nonsmooth",
    "title": "Nonparametric Methods for Smoothing Nonsmooth Data",
    "description": "Nonparametric methods for smoothing regression function data with change-points, utilizing range kernels for iterative and anisotropic smoothing methods. For further details, see the paper by John R.J. Thompson (2024) <doi:10.1080/02664763.2024.2352759>.",
    "version": "1.0.0",
    "maintainer": "John R.J. Thompson <john.thompson@ubc.ca>",
    "url": "https://github.com/jrjthompson/R-package-nonsmooth/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18618,
    "package_name": "nonstat",
    "title": "Detecting Nonstationarity in Time Series",
    "description": "Provides a nonvisual procedure for screening time series for nonstationarity in the context of intensive longitudinal designs, such as ecological momentary assessments. The method combines two diagnostics: one for detecting trends (based on the split R-hat statistic from Bayesian convergence diagnostics) and one for detecting changes in variance (a novel extension inspired by Levene's test). This approach allows researchers to efficiently and reproducibly detect violations of the stationarity assumption, especially when visual inspection of many individual time series is impractical. The procedure is suitable for use in all areas of research where time series analysis is central. For a detailed description of the method and its validation through simulations and empirical application, see Zitzmann, S., Lindner, C., Lohmann, J. F., & Hecht, M. (2024) \"A Novel Nonvisual Procedure for Screening for Nonstationarity in Time Series as Obtained from Intensive Longitudinal Designs\" <https://www.researchgate.net/publication/384354932_A_Novel_Nonvisual_Procedure_for_Screening_for_Nonstationarity_in_Time_Series_as_Obtained_from_Intensive_Longitudinal_Designs>.",
    "version": "0.0.6",
    "maintainer": "Martin Hecht <martin.hecht@hsu-hh.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18637,
    "package_name": "nortsTest",
    "title": "Assessing Normality of Stationary Process",
    "description": "Despite that several tests for normality in stationary processes have been proposed in the literature, consistent implementations of these tests in programming languages are limited. Seven normality test are implemented. The asymptotic Lobato & Velasco's, asymptotic Epps, Psaradakis and  Vávra, Lobato & Velasco's and Epps sieve bootstrap approximations, El bouch et al., and the random projections tests for univariate stationary process. Some other diagnostics such as, unit root test for  stationarity, seasonal tests for seasonality, and arch effect test for volatility; are also performed. Additionally, the El bouch test performs normality tests for bivariate time series. The package also offers residual diagnostic for linear time series models developed in several packages.",
    "version": "1.1.3",
    "maintainer": "Asael Alonzo Matamoros <asael.alonzo@gmail.com>",
    "url": "https://github.com/asael697/nortsTest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18655,
    "package_name": "npROCRegression",
    "title": "Kernel-Based Nonparametric ROC Regression Modelling",
    "description": "Implements several nonparametric regression approaches for the inclusion of covariate information on the receiver operating characteristic (ROC) framework.",
    "version": "1.0-7",
    "maintainer": "Maria Xose Rodriguez-Alvarez <mxrodriguez@uvigo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18663,
    "package_name": "npbr",
    "title": "Nonparametric Boundary Regression",
    "description": "A variety of functions for the best known and most innovative approaches to nonparametric boundary estimation. The selected methods are concerned with empirical, smoothed, unrestricted as well as constrained fits under both separate and multiple shape constraints. They cover robust approaches to outliers  as well as data envelopment techniques based on piecewise polynomials, splines, local linear fitting, extreme values and kernel smoothing. The package also seamlessly allows for Monte Carlo comparisons among these different estimation methods.  Its use is illustrated via a number of empirical applications and simulated examples.",
    "version": "1.8",
    "maintainer": "Thibault Laurent <thibault.laurent@univ-tlse1.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18671,
    "package_name": "nph",
    "title": "Planning and Analysing Survival Studies under Non-Proportional\nHazards",
    "description": "Piecewise constant hazard functions are used to flexibly model survival distributions with non-proportional hazards and \n\tto simulate data from the specified distributions. A function to calculate weighted log-rank tests for the comparison of two\n\thazard functions is included. Also, a function to calculate a test using the maximum of a set of test statistics from weighted\n\tlog-rank tests (MaxCombo test) is provided. This test utilizes the asymptotic multivariate normal joint distribution of the\n\tseparate test statistics. The correlation is estimated from the data. These methods are described in Ristl et al. (2021) \n\t<doi:10.1002/pst.2062>.\n\tFinally, a function is provided for the estimation and inferential statistics of various parameters that quantify the difference\n\tbetween two survival curves. Eligible parameters are differences in survival probabilities, log survival probabilities,\n\tcomplementary log log (cloglog) transformed survival probabilities, quantiles of the survival functions, log transformed quantiles,\n\trestricted mean survival times, as well as an average hazard ratio, the Cox model score statistic (logrank statistic), and the\n\tCox-model hazard ratio. Adjustments for multiple testing and simultaneous confidence intervals are calculated using a multivariate\n\tnormal approximation to the set of selected parameters.",
    "version": "2.1",
    "maintainer": "Robin Ristl <robin.ristl@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18672,
    "package_name": "nphPower",
    "title": "Sample Size Calculation under Non-Proportional Hazards",
    "description": "Performs combination tests and sample size calculation for \n   fixed design with survival endpoints using combination tests under either\n   proportional hazards or non-proportional hazards. The combination tests \n   include maximum weighted log-rank test and projection test. The sample \n   size calculation procedure is very flexible, allowing for user-defined\n   hazard ratio function and considering various trial conditions like \n   staggered entry, drop-out etc. The sample size calculation also applies to \n   various cure models such as proportional hazards cure model, cure model with\n   (random) delayed treatments effects. Trial simulation function is also provided \n   to facilitate the empirical power calculation. The references for \n   projection test and maximum weighted logrank test include Brendel et al. (2014)\n   <doi:10.1111/sjos.12059> and Cheng and He (2021) <arXiv:2110.03833>. The \n   references for sample size calculation under proportional hazard include\n   Schoenfeld (1981) <doi:10.1093/biomet/68.1.316> and Freedman (1982) <doi:10.1002/sim.4780010204>.\n   The references for calculation under non-proportional hazards include \n   Lakatos (1988) <doi:10.2307/2531910> and Cheng and He (2023) <doi:10.1002/bimj.202100403>.",
    "version": "1.1.0",
    "maintainer": "Huan Cheng <hcheng1118@hotmail.com>",
    "url": "https://github.com/hcheng99/nphPower",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18675,
    "package_name": "npiv",
    "title": "Nonparametric Instrumental Variables Estimation and Inference",
    "description": "Implements methods introduced in Chen, Christensen, and Kankanala (2024) <doi:10.1093/restud/rdae025> for estimating and constructing uniform confidence bands for nonparametric structural functions using instrumental variables, including data-driven choice of tuning parameters. All methods in this package apply to nonparametric regression as a special case. ",
    "version": "0.1.3",
    "maintainer": "Timothy Christensen <timothy.christensen@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18686,
    "package_name": "npreg",
    "title": "Nonparametric Regression via Smoothing Splines",
    "description": "Multiple and generalized nonparametric regression using smoothing spline ANOVA models and generalized additive models, as described in Helwig (2020) <doi:10.4135/9781526421036885885>. Includes support for Gaussian and non-Gaussian responses, smoothers for multiple types of predictors (including random intercepts), interactions between smoothers of mixed types, eight different methods for smoothing parameter selection, and flexible tools for diagnostics, inference, and prediction.",
    "version": "1.1.0",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18687,
    "package_name": "npregderiv",
    "title": "Nonparametric Estimation of the Derivatives of a Regression\nFunction",
    "description": "Estimating the first and second derivatives of a regression function by the method of Wang and Lin (2015) <http://www.jmlr.org/papers/v16/wang15b.html>.",
    "version": "1.0",
    "maintainer": "Olga Savchuk <olga.y.savchuk@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18688,
    "package_name": "npregfast",
    "title": "Nonparametric Estimation of Regression Models with\nFactor-by-Curve Interactions",
    "description": "A method for obtaining nonparametric estimates of regression models\n    with or without factor-by-curve interactions using local polynomial kernel\n    smoothers or splines. Additionally, a parametric model (allometric model) can be\n    estimated.",
    "version": "1.6.0",
    "maintainer": "Marta Sestelo <sestelo@uvigo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18689,
    "package_name": "nprobust",
    "title": "Nonparametric Robust Estimation and Inference Methods using\nLocal Polynomial Regression and Kernel Density Estimation",
    "description": "Tools for data-driven statistical analysis using local polynomial regression and kernel density estimation methods as described in Calonico, Cattaneo and Farrell (2018, <doi:10.1080/01621459.2017.1285776>): 'lprobust()' for local polynomial point estimation and robust bias-corrected inference, 'lpbwselect()' for local polynomial bandwidth selection, 'kdrobust()' for kernel density point estimation and robust bias-corrected inference, 'kdbwselect()' for kernel density bandwidth selection, and 'nprobust.plot()' for plotting results. The main methodological and numerical features of this package are described in Calonico, Cattaneo and Farrell (2019, <doi:10.18637/jss.v091.i08>).",
    "version": "0.5.0",
    "maintainer": "Sebastian Calonico <scalonico@ucdavis.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18691,
    "package_name": "nprotreg",
    "title": "Nonparametric Rotations for Sphere-Sphere Regression",
    "description": "Fits sphere-sphere regression models by estimating locally weighted\n    rotations. Simulation of sphere-sphere data according to non-rigid rotation\n    models. Provides methods for bias reduction applying iterative procedures\n    within a Newton-Raphson learning scheme. Cross-validation is exploited to select\n    smoothing parameters. See Marco Di Marzio, Agnese Panzera & Charles C. Taylor\n    (2018) <doi:10.1080/01621459.2017.1421542>.",
    "version": "1.1.1",
    "maintainer": "Giovanni Lafratta <giovanni.lafratta@unich.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18695,
    "package_name": "npsurv",
    "title": "Nonparametric Survival Analysis",
    "description": "Non-parametric survival analysis of exact and\n\t     interval-censored observations. The methods implemented\n\t     are developed by Wang (2007)\n\t     <doi:10.1111/j.1467-9868.2007.00583.x>, Wang (2008)\n\t     <doi:10.1016/j.csda.2007.10.018>, Wang and Taylor (2013)\n\t     <doi:10.1007/s11222-012-9341-9> and Wang and Fani (2018)\n\t     <doi:10.1007/s11222-017-9724-z>.",
    "version": "0.5-0",
    "maintainer": "Yong Wang <yongwang@auckland.ac.nz>",
    "url": "https://www.stat.auckland.ac.nz/~yongwang/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18696,
    "package_name": "npsurvSS",
    "title": "Sample Size and Power Calculation for Common Non-Parametric\nTests in Survival Analysis",
    "description": "A number of statistical tests have been proposed to compare two \n  survival curves, including the difference in (or ratio of) t-year \n  survival, difference in (or ratio of) p-th percentile survival, difference in\n  (or ratio of) restricted mean survival time, and the weighted log-rank test. \n  Despite the multitude of options, the convention in survival studies is to assume\n  proportional hazards and to use the unweighted log-rank test for design and \n  analysis. This package provides sample size and power \n  calculation for all of the above statistical tests with allowance for \n  flexible accrual, censoring, and survival (eg. Weibull, piecewise-exponential, \n  mixture cure). It is the companion R package to the paper by Yung and Liu (2020)\n  <doi:10.1111/biom.13196>. Specific to the weighted log-rank test, users may \n  specify which approximations they wish to use to estimate the large-sample mean \n  and variance. The default option has been shown to provide substantial\n  improvement over the conventional sample size and power equations based on Schoenfeld \n  (1981) <doi:10.1093/biomet/68.1.316>.",
    "version": "1.1.0",
    "maintainer": "Godwin Yung <godwin.y.yung@gmail.com>",
    "url": "https://github.com/godwinyyung/npsurvSS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18697,
    "package_name": "nptest",
    "title": "Nonparametric Bootstrap and Permutation Tests",
    "description": "Robust nonparametric bootstrap and permutation tests for location, correlation, and regression problems, as described in Helwig (2019a) <doi:10.1002/wics.1457> and Helwig (2019b) <doi:10.1016/j.neuroimage.2019.116030>. Univariate and multivariate tests are supported. For each problem, exact tests and Monte Carlo approximations are available. Five different nonparametric bootstrap confidence intervals are implemented. Parallel computing is implemented via the 'parallel' package.",
    "version": "1.1",
    "maintainer": "Nathaniel E. Helwig <helwig@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18704,
    "package_name": "nsarfima",
    "title": "Methods for Fitting and Simulating Non-Stationary ARFIMA Models",
    "description": "Routines for fitting and simulating data under autoregressive fractionally integrated moving average (ARFIMA) models, without the constraint of covariance stationarity. Two fitting methods are implemented, a pseudo-maximum likelihood method and a minimum distance estimator. Mayoral, L. (2007) <doi:10.1111/j.1368-423X.2007.00202.x>. Beran, J. (1995) <doi:10.1111/j.2517-6161.1995.tb02054.x>.",
    "version": "0.2.0.0",
    "maintainer": "Benjamin Groebe <ben.groebe@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18705,
    "package_name": "nscancor",
    "title": "Non-Negative and Sparse CCA",
    "description": "Two implementations of canonical correlation analysis\n        (CCA) that are based on iterated regression. By choosing the\n        appropriate regression algorithm for each data domain, it is\n        possible to enforce sparsity, non-negativity or other kinds of\n        constraints on the projection vectors. Multiple canonical\n        variables are computed sequentially using a generalized\n        deflation scheme, where the additional correlation not\n        explained by previous variables is maximized. nscancor() is\n        used to analyze paired data from two domains, and has the same\n        interface as cancor() from the 'stats' package (plus some extra\n        parameters). mcancor() is appropriate for analyzing data from\n        three or more domains. See\n        <https://sigg-iten.ch/learningbits/2014/01/20/canonical-correlation-analysis-under-constraints/>\n        and Sigg et al. (2007) <doi:10.1109/MLSP.2007.4414315> for more\n        details.",
    "version": "0.7.0-6",
    "maintainer": "Christian Sigg <christian@sigg-iten.ch>",
    "url": "https://sigg-iten.ch/research/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18753,
    "package_name": "o2plsda",
    "title": "Multiomics Data Integration",
    "description": "Provides functions to do 'O2PLS-DA' analysis for multiple omics data integration.\n            The algorithm came from \"O2-PLS, a two-block (X±Y) latent variable regression (LVR) method with an integral OSC filter\" \n            which published by Johan Trygg and Svante Wold at 2003 <doi:10.1002/cem.775>. \n            'O2PLS' is a bidirectional multivariate regression method that aims to separate the covariance between\n            two data sets (it was recently extended to multiple data sets) (Löfstedt and Trygg, 2011 <doi:10.1002/cem.1388>; Löfstedt et al., 2012 <doi:10.1016/j.aca.2013.06.026>) \n            from the systematic sources of variance being specific for each data set separately. ",
    "version": "0.0.26",
    "maintainer": "Kai Guo <guokai8@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18755,
    "package_name": "oHMMed",
    "title": "HMMs with Ordered Hidden States and Emission Densities",
    "description": "Inference using a class of Hidden Markov models\n    (HMMs) called 'oHMMed'(ordered HMM with emission densities \n    <doi:10.1186/s12859-024-05751-4>): The 'oHMMed' algorithms identify \n    the number of comparably homogeneous regions within observed sequences\n    with autocorrelation patterns. These are modelled as discrete hidden\n    states; the observed data points are then realisations of continuous\n    probability distributions with state-specific means that enable\n    ordering of these distributions. The observed sequence is labelled\n    according to the hidden states, permitting only neighbouring states\n    that are also neighbours within the ordering of their associated\n    distributions. The parameters that characterise these state-specific\n    distributions are then inferred. Relevant for application to genomic\n    sequences, time series, or any other sequence data with serial\n    autocorrelation.",
    "version": "1.0.2",
    "maintainer": "Michal Majka <michalmajka@hotmail.com>",
    "url": "https://github.com/LynetteCaitlin/oHMMed,\nhttps://lynettecaitlin.github.io/oHMMed/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18756,
    "package_name": "oRaklE",
    "title": "Multi-Horizon Electricity Demand Forecasting in High Resolution",
    "description": "Advanced forecasting algorithms for long-term energy demand at the \n  national or regional level. The methodology is based on Grandón et al. (2024) \n  <doi:10.1016/j.apenergy.2023.122249>; Zimmermann & Ziel (2024) \n  <doi:10.1016/j.apenergy.2025.125444>. Real-time data, including power demand, weather conditions, and macroeconomic indicators, are provided through automated API integration with various institutions. The modular approach maintains transparency on the various model selection processes and encompasses the ability to be adapted to individual needs. 'oRaklE' tries to help facilitating robust decision-making in energy management and planning.",
    "version": "1.0.2",
    "maintainer": "Johannes Schwenzer <schwenzer@europa-uni.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18763,
    "package_name": "oaxaca",
    "title": "Blinder-Oaxaca Decomposition",
    "description": "An implementation of the Blinder-Oaxaca decomposition for linear regression models.",
    "version": "0.1.5",
    "maintainer": "Marek Hlavac <mhlavac@alumni.princeton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18781,
    "package_name": "oce",
    "title": "Analysis of Oceanographic Data",
    "description": "Supports the analysis of Oceanographic data, including 'ADCP'\n    measurements, measurements made with 'argo' floats, 'CTD' measurements,\n    sectional data, sea-level time series, coastline and topographic data, etc.\n    Provides specialized functions for calculating seawater properties such as\n    potential temperature in either the 'UNESCO' or 'TEOS-10' equation of state.\n    Produces graphical displays that conform to the conventions of the\n    Oceanographic literature. This package is discussed extensively by\n    Kelley (2018) \"Oceanographic Analysis with R\" <doi:10.1007/978-1-4939-8844-0>.",
    "version": "1.8-3",
    "maintainer": "Dan Kelley <Dan.Kelley@Dal.Ca>",
    "url": "https://dankelley.github.io/oce/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18802,
    "package_name": "oddnet",
    "title": "Anomaly Detection in Temporal Networks",
    "description": "Anomaly detection in dynamic, temporal networks. The package \n    'oddnet' uses a feature-based method to identify anomalies. First, it computes \n    many features for each network. Then it models the features using time series \n    methods. Using time series residuals it detects anomalies. This way, the \n    temporal dependencies are accounted for when identifying anomalies \n    (Kandanaarachchi, Hyndman 2022) <arXiv:2210.07407>.",
    "version": "0.1.1",
    "maintainer": "Sevvandi Kandanaarachchi <sevvandik@gmail.com>",
    "url": "https://sevvandi.github.io/oddnet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18806,
    "package_name": "oddsratio",
    "title": "Odds Ratio Calculation for GAM(M)s & GLM(M)s",
    "description": "Simplified odds ratio calculation of GAM(M)s & GLM(M)s.",
    "version": "2.0.2.9000",
    "maintainer": "",
    "url": "https://github.com/pat-s/oddsratio",
    "exports": [],
    "topics": ["odds-ratio", "probability", "statistics"],
    "score": "NA",
    "stars": 30
  },
  {
    "id": 18807,
    "package_name": "oddstream",
    "title": "Outlier Detection in Data Streams",
    "description": "We proposes a framework that provides real time support for early detection of\n    anomalous series within a large collection of streaming time series data. By definition, anomalies\n    are rare in comparison to a system's typical behaviour. We define an anomaly as an observation that\n    is very unlikely given the forecast distribution. The algorithm first forecasts a boundary for the\n    system's typical behaviour using a representative sample of the typical behaviour of the system. An\n    approach based on extreme value theory is used for this boundary prediction process. Then a sliding\n    window is used to test for anomalous series within the newly arrived collection of series. Feature\n    based representation of time series is used as the input to the model. To cope with concept drift,\n    the forecast boundary for the system's typical behaviour is updated periodically.  More details\n    regarding the algorithm can be found in Talagala, P. D., Hyndman, R. J., Smith-Miles, K., et al.\n    (2019) <doi:10.1080/10618600.2019.1617160>.",
    "version": "0.5.0",
    "maintainer": "Priyanga Dilini Talagala <pritalagala@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18808,
    "package_name": "odeGUTS",
    "title": "Solve ODE for GUTS-RED-SD and GUTS-RED-IT Using Compiled Code",
    "description": "Allows performing forwards prediction for the General Unified \n          Threshold model of Survival using compiled ode code. This package \n          was created to avoid dependency with the 'morse' package that requires \n          the installation of 'JAGS'. This package is based on functions from \n          the 'morse' package v3.3.1: Virgile Baudrot, Sandrine Charles, \n          Marie Laure Delignette-Muller, Wandrille Duchemin, Benoit Goussen, \n          Nils Kehrein, Guillaume Kon-Kam-King, Christelle Lopes, Philippe Ruiz, \n          Alexander Singer and Philippe Veber (2021) <https://CRAN.R-project.org/package=morse>.",
    "version": "1.1.0",
    "maintainer": "Carlo Romoli <carlo.romoli@ibacon.com>",
    "url": "https://github.com/bgoussen/odeGUTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18810,
    "package_name": "odiffr",
    "title": "Fast Pixel-by-Pixel Image Comparison Using 'odiff'",
    "description": "R bindings to 'odiff', a blazing-fast pixel-by-pixel image\n    comparison tool <https://github.com/dmtrKovalenko/odiff>. Supports PNG,\n    JPEG, WEBP, and TIFF with configurable thresholds, antialiasing detection,\n    and region ignoring. Requires system installation of 'odiff'. Ideal for\n    visual regression testing in automated workflows.",
    "version": "0.5.1",
    "maintainer": "Ben Wolstenholme <odiffr@benwolst.dev>",
    "url": "https://github.com/BenWolst/odiffr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18830,
    "package_name": "offlineChange",
    "title": "Detect Multiple Change Points from Time Series",
    "description": "Detect the number and locations of change points. The locations can be either exact or in terms of ranges, \n            depending on the available computational resource. The method is based on Jie Ding, Yu Xiang, Lu Shen, Vahid Tarokh (2017) <doi:10.1109/TSP.2017.2711558>.",
    "version": "0.0.4",
    "maintainer": "Jiahuan Ye <jiahuanye431@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18835,
    "package_name": "ohenery",
    "title": "Modeling of Ordinal Random Variables via Softmax Regression",
    "description": "Supports the modeling of ordinal random variables, \n    like the outcomes of races, via Softmax regression,\n    under the Harville <doi:10.1080/01621459.1973.10482425> and\n    Henery <doi:10.1111/j.2517-6161.1981.tb01153.x> models.",
    "version": "0.1.4",
    "maintainer": "Steven E. Pav <shabbychef@gmail.com>",
    "url": "https://github.com/shabbychef/ohenery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18836,
    "package_name": "ohoegdm",
    "title": "Ordinal Higher-Order Exploratory General Diagnostic Model for\nPolytomous Data",
    "description": "Perform a Bayesian estimation of the ordinal exploratory \n    Higher-order General Diagnostic Model (OHOEGDM) for Polytomous Data \n    described by Culpepper, S. A. and Balamuta, J. J. (2021) <doi:10.1080/00273171.2021.1985949>.",
    "version": "0.1.1",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "url": "https://github.com/tmsalab/ohoegdm,\nhttps://tmsalab.github.io/ohoegdm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18850,
    "package_name": "olr",
    "title": "Optimal Linear Regression",
    "description": "The olr function systematically evaluates multiple linear regression models by exhaustively fitting all possible combinations of independent variables against the specified dependent variable. It selects the model that yields the highest adjusted R-squared (by default) or R-squared, depending on user preference. In model evaluation, both R-squared and adjusted R-squared are key metrics: R-squared measures the proportion of variance explained but tends to increase with the addition of predictors—regardless of relevance—potentially leading to overfitting. Adjusted R-squared compensates for this by penalizing model complexity, providing a more balanced view of fit quality. The goal of olr is to identify the most suitable model that captures the underlying structure of the data while avoiding unnecessary complexity. By comparing both metrics, it offers a robust evaluation framework that balances predictive power with model parsimony. Example Analogy: Imagine a gardener trying to understand what influences plant growth (the dependent variable). They might consider variables like sunlight, watering frequency, soil type, and nutrients (independent variables). Instead of manually guessing which combination works best, the olr function automatically tests every possible combination of predictors and identifies the most effective model—based on either the highest R-squared or adjusted R-squared value. This saves the user from trial-and-error modeling and highlights only the most meaningful variables for explaining the outcome. A Python version is also available at <https://pypi.org/project/olr>.",
    "version": "1.2",
    "maintainer": "Mathew Fok <quiksilver67213@yahoo.com>",
    "url": "https://github.com/MatHatter/olr_r, https://pypi.org/project/olr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18880,
    "package_name": "oncomsm",
    "title": "Bayesian Multi-State Models for Early Oncology",
    "description": "Implements methods to fit a parametric Bayesian multi-state model\n    to tumor response data.\n    The model can be used to sample from the predictive distribution to impute\n    missing data and calculate probability of success for custom decision\n    criteria in early clinical trials during an ongoing trial.\n    The inference is implemented using 'stan'.",
    "version": "0.1.4",
    "maintainer": "Kevin Kunzmann <kevin.kunzmann@boehringer-ingelheim.com>",
    "url": "https://boehringer-ingelheim.github.io/oncomsm/,\nhttps://github.com/Boehringer-Ingelheim/oncomsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18884,
    "package_name": "oneinfl",
    "title": "Estimates OIPP and OIZTNB Regression Models",
    "description": "Estimates one-inflated positive Poisson (OIPP) and\n    one-inflated zero-truncated negative binomial (OIZTNB) regression\n    models. A suite of ancillary statistical tools are also provided,\n    including: estimation of positive Poisson (PP) and zero-truncated\n    negative binomial (ZTNB) models; marginal effects and their standard\n    errors; diagnostic likelihood ratio and Wald tests; plotting;\n    predicted counts and expected responses; and random variate\n    generation. The models and tools, as well as four applications, are\n    shown in Godwin, R. T. (2024). \"One-inflated zero-truncated count\n    regression models\" arXiv preprint <doi:10.48550/arXiv.2402.02272>.",
    "version": "1.0.2",
    "maintainer": "Ryan T. Godwin <ryan.godwin@umanitoba.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18890,
    "package_name": "onlineBcp",
    "title": "Online Bayesian Methods for Change Point Analysis",
    "description": "It implements the online Bayesian methods for change point analysis. It can\n            also perform missing data imputation with methods from 'VIM'. The reference \n            is Yigiter A, Chen J, An L, Danacioglu N (2015) <doi:10.1080/02664763.2014.1001330>. \n            The link to the package is <https://CRAN.R-project.org/package=onlineBcp>.",
    "version": "0.1.8",
    "maintainer": "Hongyan Xu <hxu@augusta.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18894,
    "package_name": "onlineforecast",
    "title": "Forecast Modelling for Online Applications",
    "description": "A framework for fitting adaptive forecasting models. Provides a way to use forecasts as input to models, e.g. weather forecasts for energy related forecasting. The models can be fitted recursively and can easily be setup for updating parameters when new data arrives. See the included vignettes, the website <https://onlineforecasting.org> and the paper \"onlineforecast: An R package for adaptive and recursive forecasting\" <https://journal.r-project.org/articles/RJ-2023-031/>.",
    "version": "1.0.2",
    "maintainer": "Peder Bacher <pbac@dtu.dk>",
    "url": "https://onlineforecasting.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18896,
    "package_name": "onls",
    "title": "Orthogonal Nonlinear Least-Squares Regression",
    "description": "Fits two-dimensional data by means of orthogonal nonlinear least-squares using Levenberg-Marquardt minimization and provides functionality for fit diagnostics and plotting. Delivers the same results as the 'ODRPACK' Fortran implementation described in Boggs et al. (1989) <doi:10.1145/76909.76913>, but is implemented in pure R.",
    "version": "0.1-4",
    "maintainer": "Andrej-Nikolai Spiess <draspiess@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18923,
    "package_name": "openEBGM",
    "title": "EBGM Disproportionality Scores for Adverse Event Data Mining",
    "description": "An implementation of DuMouchel's (1999) <doi:10.1080/00031305.1999.10474456>\n  Bayesian data mining method for the market basket problem.\n  Calculates Empirical Bayes Geometric Mean (EBGM) and posterior quantile scores\n  using the Gamma-Poisson Shrinker (GPS) model to find unusually large cell\n  counts in large, sparse contingency tables. Can be used to find unusually high\n  reporting rates of adverse events associated with products. In general, can be\n  used to mine any database where the co-occurrence of two variables or items is\n  of interest. Also calculates relative and proportional reporting ratios.\n  Builds on the work of the 'PhViD' package, from which much of the code is\n  derived. Some of the added features include stratification to adjust for\n  confounding variables and data squashing to improve computational efficiency.\n  Includes an implementation of the EM algorithm for hyperparameter estimation\n  loosely derived from the 'mederrRank' package.",
    "version": "0.9.1",
    "maintainer": "John Ihrie <John.Ihrie@fda.hhs.gov>",
    "url": "https://journal.r-project.org/archive/2017/RJ-2017-063/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18934,
    "package_name": "openair",
    "title": "Tools for the Analysis of Air Pollution Data",
    "description": "Tools to analyse, interpret and understand air pollution\n    data. Data are typically regular time series and air quality\n    measurement, meteorological data and dispersion model output can be\n    analysed. The package is described in Carslaw and Ropkins (2012,\n    <doi:10.1016/j.envsoft.2011.09.008>) and subsequent papers.",
    "version": "2.19.0",
    "maintainer": "David Carslaw <david.carslaw@york.ac.uk>",
    "url": "https://openair-project.github.io/openair/,\nhttps://github.com/openair-project/openair",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 18972,
    "package_name": "optband",
    "title": "'surv' Object Confidence Bands Optimized by Area",
    "description": "Given a certain coverage level, obtains simultaneous confidence\n    bands for the survival and cumulative hazard functions such that the area\n    between is minimized. Produces an approximate solution based on local time\n    arguments.",
    "version": "0.2.2",
    "maintainer": "Sam Tracy <seasamgo@gmail.com>",
    "url": "https://github.com/seasamgo/optband",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19006,
    "package_name": "optmatch",
    "title": "Functions for Optimal Matching",
    "description": "Distance based bipartite matching using minimum cost flow, oriented\n    to matching of treatment and control groups in observational studies ('Hansen'\n    and 'Klopfer' 2006 <doi:10.1198/106186006X137047>). Routines are provided to\n    generate distances from generalised linear models (propensity score\n    matching), formulas giving variables on which to limit matched distances,\n    stratified or exact matching directives, or calipers, alone or in\n    combination.",
    "version": "0.10.8",
    "maintainer": "Josh Errickson <jerrick@umich.edu>",
    "url": "http://optmat.ch, https://github.com/markmfredrickson/optmatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19019,
    "package_name": "ordDisp",
    "title": "Separating Location and Dispersion in Ordinal Regression Models",
    "description": "Estimate location-shift models or rating-scale models accounting for response styles (RSRS) for the regression analysis of ordinal responses.",
    "version": "2.1.2",
    "maintainer": "Moritz Berger <moritz.berger@imbie.uni-bonn.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19020,
    "package_name": "ordbetareg",
    "title": "Ordered Beta Regression Models with 'brms'",
    "description": "Implements ordered beta regression models, which are for modeling continuous variables with upper and lower bounds, such as\n   survey sliders, dose-response relationships and indexes. For more information, see\n   Kubinec (2023) <doi:10.31235/osf.io/2sx6y>. The package is a front-end to the R package 'brms', which \n   facilitates a range of regression specifications, including hierarchical, dynamic and\n   multivariate modeling.",
    "version": "0.8",
    "maintainer": "Robert Kubinec <bobkubinec@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19032,
    "package_name": "ordgam",
    "title": "Additive Model for Ordinal Data using Laplace P-Splines",
    "description": "Additive proportional odds model for ordinal data using Laplace P-splines. The combination of Laplace approximations and P-splines enable fast and flexible inference in a Bayesian framework. Specific approximations are proposed to account for the asymmetry in the marginal posterior distributions of non-penalized parameters. For more details, see Lambert and Gressani (2023) <doi:10.1177/1471082X231181173> ; Preprint: <arXiv:2210.01668>).",
    "version": "0.9.1",
    "maintainer": "Philippe Lambert <p.lambert@uliege.be>",
    "url": "<https://github.com/plambertULiege/ordgam>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19034,
    "package_name": "ordinal",
    "title": "Regression Models for Ordinal Data",
    "description": "Implementation of cumulative link (mixed) models also known\n    as ordered regression models, proportional odds models, proportional\n    hazards models for grouped survival times and ordered logit/probit/...\n    models. Estimation is via maximum likelihood and mixed models are fitted\n    with the Laplace approximation and adaptive Gauss-Hermite quadrature.\n    Multiple random effect terms are allowed and they may be nested, crossed or\n    partially nested/crossed. Restrictions of symmetry and equidistance can be\n    imposed on the thresholds (cut-points/intercepts). Standard model\n    methods are available (summary, anova, drop-methods, step,\n    confint, predict etc.) in addition to profile methods and slice\n    methods for visualizing the likelihood function and checking\n    convergence.",
    "version": "2025.12-29",
    "maintainer": "Rune Haubo Bojesen Christensen <rune.haubo@gmail.com>",
    "url": "https://github.com/runehaubo/ordinal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19035,
    "package_name": "ordinalCont",
    "title": "Ordinal Regression Analysis for Continuous Scales",
    "description": "A regression framework for response variables which are continuous\n    self-rating scales such as the Visual Analog Scale (VAS) used in pain\n    assessment, or the Linear Analog Self-Assessment (LASA) scales in quality\n    of life studies. These scales measure subjects' perception of an intangible\n    quantity, and cannot be handled as ratio variables because of their inherent\n    non-linearity. We treat them as ordinal variables, measured on a continuous\n    scale. A function (the g function) connects the scale with an underlying\n    continuous latent variable. The link function is the inverse of the CDF of the\n    assumed underlying distribution of the latent variable. A variety of\n    link functions are currently implemented. Such models are described in Manuguerra et al (2020) <doi:10.18637/jss.v096.i08>.",
    "version": "2.0.2",
    "maintainer": "Maurizio Manuguerra <maurizio.manuguerra@mq.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19038,
    "package_name": "ordinalNet",
    "title": "Penalized Ordinal Regression",
    "description": "Fits ordinal regression models with elastic net penalty.\n    Supported model families include cumulative probability, stopping ratio, \n    continuation ratio, and adjacent category. These families are a subset of \n    vector glm's which belong to a model class we call the elementwise link \n    multinomial-ordinal (ELMO) class. Each family in this class links a vector \n    of covariates to a vector of class probabilities. Each of these families \n    has a parallel form, which is appropriate for ordinal response data, as \n    well as a nonparallel form that is appropriate for an unordered categorical\n    response, or as a more flexible model for ordinal data. The parallel model\n    has a single set of coefficients, whereas the nonparallel model has a set of\n    coefficients for each response category except the baseline category. It is \n    also possible to fit a model with both parallel and nonparallel terms, which \n    we call the semi-parallel model. The semi-parallel model has the flexibility \n    of the nonparallel model, but the elastic net penalty shrinks it toward the \n    parallel model. For details, refer to Wurm, Hanlon, and Rathouz (2021) \n    <doi:10.18637/jss.v099.i06>.",
    "version": "2.13",
    "maintainer": "Michael Wurm <wurm@uwalumni.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19039,
    "package_name": "ordinalRR",
    "title": "Analysis of Repeatability and Reproducibility Studies with\nOrdinal Measurements",
    "description": "Implements Bayesian data analyses of balanced repeatability and reproducibility studies with ordinal measurements. Model fitting is based on MCMC posterior sampling with 'rjags'. Function ordinalRR() directly carries out the model fitting, and this function has the flexibility to allow the user to specify key aspects of the model, e.g., fixed versus random effects. Functions for preprocessing data and for the numerical and graphical display of a fitted model are also provided. There are also functions for displaying the model at fixed (user-specified) parameters and for simulating a hypothetical data set at a fixed (user-specified) set of parameters for a random-effects rater population. For additional technical details, refer to Culp, Ryan, Chen, and Hamada (2018) and cite this Technometrics paper when referencing any aspect of this work. The demo of this package reproduces results from the Technometrics paper.",
    "version": "1.1",
    "maintainer": "Ken Ryan <kjryan@mail.wvu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19041,
    "package_name": "ordinalbayes",
    "title": "Bayesian Ordinal Regression for High-Dimensional Data",
    "description": "Provides a function for fitting various penalized Bayesian\n    cumulative link ordinal response models when the number of parameters\n    exceeds the sample size. These models have been described in \n    Zhang and Archer (2021) <doi:10.1186/s12859-021-04432-w>.",
    "version": "0.1.2",
    "maintainer": "Kellie J. Archer <archer.43@osu.edu>",
    "url": "https://github.com/kelliejarcher/ordinalbayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19042,
    "package_name": "ordinalgmifs",
    "title": "Ordinal Regression for High-Dimensional Data",
    "description": "Provides a function for fitting cumulative link, adjacent category, forward and backward continuation ratio, and stereotype ordinal response models when the number of parameters exceeds the sample size, using the the generalized monotone incremental forward stagewise method.",
    "version": "1.0.8",
    "maintainer": "Kellie J. Archer <archer.43@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19043,
    "package_name": "ordinalpattern",
    "title": "Tests Based on Ordinal Patterns",
    "description": "Ordinal patterns describe the dynamics of a time series by looking at the ranks of subsequent observations. By comparing ordinal patterns of two times series, Schnurr (2014) <doi:10.1007/s00362-013-0536-8> defines a robust and non-parametric dependence measure: the ordinal pattern coefficient. Functions to calculate this and a method to detect a change in the pattern coefficient proposed in Schnurr and Dehling (2017) <doi:10.1080/01621459.2016.1164706> are provided. Furthermore, the package contains a function for calculating the ordinal pattern frequencies. Generalized ordinal patterns as proposed by Schnurr and Fischer (2022) <doi:10.1016/j.csda.2022.107472> are also considered.",
    "version": "0.2.7",
    "maintainer": "Angelika Silbernagel <silbernagel@hsu-hh.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19060,
    "package_name": "ormPlot",
    "title": "Advanced Plotting of Ordinal Regression Models",
    "description": "An extension to the Regression Modeling Strategies package that\n    facilitates plotting ordinal regression  model predictions together with\n    confidence intervals for each dependent variable level. \n    It also adds a functionality to plot the  model summary as a modifiable \n    object.",
    "version": "0.3.6",
    "maintainer": "Richard Meitern <richard.meitern@ut.ee>",
    "url": "https://doi.org/10.1186/s12889-019-8072-7",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19077,
    "package_name": "osd",
    "title": "Orthogonal Signal Deconvolution for Spectra Deconvolution in\nGC-MS and GCxGC-MS Data",
    "description": "Compound deconvolution for chromatographic data, including gas chromatography - mass spectrometry (GC-MS) and comprehensive gas chromatography - mass spectrometry (GCxGC-MS). The package includes functions to perform independent component analysis - orthogonal signal deconvolution (ICA-OSD), independent component regression (ICR), multivariate curve resolution (MCR-ALS) and orthogonal signal deconvolution (OSD) alone.",
    "version": "0.1",
    "maintainer": "Xavier Domingo-Almenara <xavier.domingo@urv.cat>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19103,
    "package_name": "otrKM",
    "title": "Optimal Treatment Regimes in Survival Contexts with\nKaplan-Meier-Like Estimators",
    "description": "Provide methods for estimating optimal treatment regimes in survival contexts \n    with Kaplan-Meier-like estimators when no unmeasured confounding assumption is \n    satisfied (Jiang, R., Lu, W., Song, R., and Davidian, M. (2017) <doi:10.1111/rssb.12201>) \n    and when no unmeasured confounding assumption fails to hold and a binary instrument \n    is available (Xia, J., Zhan, Z., Zhang, J. (2022) <arXiv:2210.05538>).",
    "version": "0.2.1",
    "maintainer": "Junwen Xia <xiajunwen@ruc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19105,
    "package_name": "otsfeatures",
    "title": "Ordinal Time Series Analysis",
    "description": "An implementation of several functions for feature extraction in \n    ordinal time series datasets. Specifically, some of the features proposed by\n    Weiss (2019) <doi:10.1080/01621459.2019.1604370> can be computed.  \n    These features can be used to perform inferential tasks or to feed machine\n    learning algorithms for ordinal time series, among others. The package also includes some\n    interesting datasets containing financial time series. Practitioners from a \n    broad variety of fields could benefit from the general framework provided \n    by 'otsfeatures'.",
    "version": "1.0.0",
    "maintainer": "Angel Lopez-Oriona <oriona38@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19119,
    "package_name": "outliers.ts.oga",
    "title": "Efficient Outlier Detection for Large Time Series Databases",
    "description": "Programs for detecting and cleaning outliers in single time series and in time series from homogeneous and heterogeneous databases using an Orthogonal Greedy Algorithm (OGA) for saturated linear regression models. The programs implement the procedures presented in the paper entitled \"Efficient Outlier Detection for Large Time Series Databases\" by Pedro Galeano, Daniel Peña and Ruey S. Tsay (2025), working paper, Universidad Carlos III de Madrid. Version 1.1.1 contains some improvements in parallelization with respect to version 1.0.1.",
    "version": "1.1.1",
    "maintainer": "Pedro Galeano <pedro.galeano@uc3m.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19122,
    "package_name": "outreg",
    "title": "Regression Table for Publication",
    "description": "Create regression tables for publication.\n    Currently supports 'lm', 'glm', 'survreg', and 'ivreg' outputs.",
    "version": "0.2.2",
    "maintainer": "Kota Mori <kmori05@gmail.com>",
    "url": "https://github.com/kota7/outreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19124,
    "package_name": "overdisp",
    "title": "Overdispersion in Count Data Multiple Regression Analysis",
    "description": "Detection of overdispersion in count data for multiple regression analysis.\n    Log-linear count data regression is one of the most popular techniques for predictive \n    modeling where there is a non-negative discrete quantitative dependent variable. In \n    order to ensure the inferences from the use of count data models are appropriate, \n    researchers may choose between the estimation of a Poisson model and a negative binomial\n    model, and the correct decision for prediction from a count data estimation is directly\n    linked to the existence of overdispersion of the dependent variable, conditional to the \n    explanatory variables. Based on the studies of Cameron and Trivedi (1990)\n    <doi:10.1016/0304-4076(90)90014-K> and Cameron and Trivedi (2013, ISBN:978-1107667273), \n    the overdisp() command is a contribution to researchers, providing a fast and secure \n    solution for the detection of overdispersion in count data. Another advantage is that \n    the installation of other packages is unnecessary, since the command runs in the basic \n    R language.",
    "version": "0.1.2",
    "maintainer": "Rafael Freitas Souza <fsrafael@usp.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19137,
    "package_name": "oxcAAR",
    "title": "Interface to 'OxCal' Radiocarbon Calibration",
    "description": "A set of tools that enables using 'OxCal' from within R. 'OxCal' (<https://c14.arch.ox.ac.uk/oxcal.html>) is a standard archaeological tool intended to provide 14C calibration and analysis of archaeological and environmental chronological information. 'OxcAAR' allows simple calibration with 'Oxcal' and plotting of the results as well as the execution of sophisticated ('OxCal') code and the import of the results of bulk analysis and complex Bayesian sequential calibration.",
    "version": "1.1.2",
    "maintainer": "Hinz Martin <martin.hinz@ufg.uni-kiel.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19142,
    "package_name": "p3state.msm",
    "title": "Analyzing Survival Data from an Illness-Death Model",
    "description": "Contains functions for data preparation,\n\tprediction of transition probabilities,\n\testimating semi-parametric regression models\n\tand for implementing nonparametric estimators\n\tfor other quantities. See Meira-Machado and\n\tRoca-Pardiñas (2011) <doi:10.18637/jss.v038.i03>.",
    "version": "1.3.3",
    "maintainer": "Gustavo Soutinho <gustavosoutinho@sapo.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19149,
    "package_name": "pEPA",
    "title": "Tests of Equal Predictive Accuracy for Panels of Forecasts",
    "description": "Allows to perform the tests of equal predictive accuracy for panels of forecasts. Main references: Qu et al. (2024) <doi:10.1016/j.ijforecast.2023.08.001> and Akgun et al. (2024) <doi:10.1016/j.ijforecast.2023.02.001>. ",
    "version": "1.2",
    "maintainer": "Krzysztof Drachal <kdrachal@wne.uw.edu.pl>",
    "url": "https://CRAN.R-project.org/package=pEPA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19150,
    "package_name": "pGPx",
    "title": "Pseudo-Realizations for Gaussian Process Excursions",
    "description": "Computes pseudo-realizations from the posterior distribution of a Gaussian Process (GP) with the method described in Azzimonti et al. (2016) <doi:10.1137/141000749>. The realizations are obtained from simulations of the field at few well chosen points that minimize the expected distance in measure between the true excursion set of the field and the approximate one. Also implements a R interface for (the main function of) Distance Transform of sampled Functions (<https://cs.brown.edu/people/pfelzens/dt/index.html>).",
    "version": "0.1.4",
    "maintainer": "Dario Azzimonti <dario.azzimonti@gmail.com>",
    "url": "https://doi.org/10.1137/141000749",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19158,
    "package_name": "pRSR",
    "title": "Test of Periodicity using Response Surface Regression",
    "description": "Tests periodicity in short time series using response surface regression.",
    "version": "3.1.1",
    "maintainer": "M. S. Islam <shahed-sta@sust.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19164,
    "package_name": "pa",
    "title": "Performance Attribution for Equity Portfolios",
    "description": "It provides tools for conducting performance attribution for equity portfolios. The package uses two methods: the Brinson method and a regression-based analysis.",
    "version": "1.2-4",
    "maintainer": "Yang Lu <yang.lu2014@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19206,
    "package_name": "pairwiseComparisons",
    "title": "Multiple Pairwise Comparison Tests",
    "description": "Multiple pairwise comparison tests on tidy data for",
    "version": "3.2.0",
    "maintainer": "Indrajeet Patil <patilindrajeet.science@gmail.com>",
    "url": "https://github.com/IndrajeetPatil/pairwiseComparisons",
    "exports": [],
    "topics": ["bayes-factor", "pairwise-comparison-tests", "parametric", "robust", "statistics"],
    "score": "NA",
    "stars": 46
  },
  {
    "id": 19213,
    "package_name": "palasso",
    "title": "Sparse Regression with Paired Covariates",
    "description": "Implements sparse regression with paired covariates (<doi:10.1007/s11634-019-00375-6>). The paired lasso is designed for settings where each covariate in one set forms a pair with a covariate in the other set (one-to-one correspondence). For the optional correlation shrinkage, install ashr (<https://github.com/stephens999/ashr>) and CorShrink (<https://github.com/kkdey/CorShrink>) from GitHub (see README).",
    "version": "1.0.0",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@uni.lu>",
    "url": "https://github.com/rauschenberger/palasso,\nhttps://rauschenberger.github.io/palasso/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19217,
    "package_name": "paleoTS",
    "title": "Analyze Paleontological Time-Series",
    "description": "Facilitates analysis of paleontological sequences of trait values.  \n    Functions are provided to fit, using maximum likelihood, simple \n    evolutionary models (including unbiased random walks, directional \n    evolution,stasis, Ornstein-Uhlenbeck, covariate-tracking) and \n    complex models (punctuation, mode shifts).",
    "version": "0.6.2",
    "maintainer": "Gene Hunt <hunte@si.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19235,
    "package_name": "pam",
    "title": "Fast and Efficient Processing of PAM Data",
    "description": "Processing Chlorophyll Fluorescence & P700 Absorbance data generated by WALZ hardware. Four models are provided for the regression of Pi curves, which can be compared with each other in order to select the most suitable model for the data set. Control plots ensure the successful verification of each regression. Bundled output of alpha, ETRmax, Ik etc. enables fast and reliable further processing of the data. ",
    "version": "1.0.6",
    "maintainer": "Julien Böhm <julien.boehm@uni-rostock.de>",
    "url": "https://github.com/biotoolbox/pam",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19236,
    "package_name": "pamm",
    "title": "Power Analysis for Random Effects in Mixed Models",
    "description": "Simulation functions to assess or explore the power of a dataset to estimates significant random effects (intercept or slope) in a mixed model. The functions are based on the \"lme4\" and \"lmerTest\" packages.",
    "version": "1.122",
    "maintainer": "Julien Martin <julien.martin@uottawa.ca>",
    "url": "https://github.com/JulienGAMartin/pamm_R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19238,
    "package_name": "pamr",
    "title": "Pam: Prediction Analysis for Microarrays",
    "description": "Some functions for sample classification in microarrays.",
    "version": "1.57",
    "maintainer": "Balasubramanian Narasimhan <naras@stanford.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19239,
    "package_name": "pan",
    "title": "Multiple Imputation for Multivariate Panel or Clustered Data",
    "description": "It provides functions and examples for maximum likelihood estimation for\n              generalized linear mixed models and Gibbs sampler for multivariate linear\n              mixed models with incomplete data, as described in Schafer JL (1997)\n              \"Imputation of missing covariates under a multivariate linear mixed model\".\n              Technical report 97-04, Dept. of Statistics, The Pennsylvania State University.",
    "version": "1.9",
    "maintainer": "Jing hua Zhao <jinghuazhao@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19241,
    "package_name": "pandemics",
    "title": "Monitoring a Developing Pandemic with Available Data",
    "description": "Full dynamic system to describe and forecast the spread and the severity\n    of a developing pandemic, based on available data. These data are number of infections, \n    hospitalizations, deaths and recoveries notified each day. The system consists of three \n    transitions, infection-infection, infection-hospital and hospital-death/recovery. \n    The intensities of these transitions are dynamic and estimated using non-parametric local\n    linear estimators. The package can be used to provide forecasts and survival indicators \n    such as the median time spent in hospital and the probability that a patient who has been\n    in hospital for a number of days can leave it alive. Methods are described in Gámiz, \n    Mammen, Martínez-Miranda, and Nielsen (2024) <doi:10.48550/arXiv.2308.09918> and \n    <doi:10.48550/arXiv.2308.09919>. ",
    "version": "0.1.0",
    "maintainer": "María Dolores Martínez-Miranda <mmiranda@ugr.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19246,
    "package_name": "panelPomp",
    "title": "Inference for Panel Partially Observed Markov Processes",
    "description": "Data analysis based on panel partially-observed Markov process (PanelPOMP) models. To implement such models, simulate them and fit them to panel data, 'panelPomp' extends some of the facilities provided for time series data by the 'pomp' package. Implemented methods include filtering (panel particle filtering) and maximum likelihood estimation (Panel Iterated Filtering) as proposed in Breto, Ionides and King (2020) \"Panel Data Analysis via Mechanistic Models\" <doi:10.1080/01621459.2019.1604367>.",
    "version": "1.7.0.0",
    "maintainer": "Jesse Wheeler <jeswheel@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19247,
    "package_name": "panelSUR",
    "title": "Two-Way Error Component SUR Systems Estimation on Unbalanced\nPanel Data",
    "description": "Generalized Least Squares (GLS) estimation of Seemingly Unrelated Regression\n             (SUR) systems on unbalanced panel in the one/two-way cases also taking into \n             account the possibility of cross equation restrictions.  Methodological details\n             can be found in Biørn (2004) <doi:10.1016/j.jeconom.2003.10.023> and Platoni,\n             Sckokai, Moro (2012) <doi:10.1080/07474938.2011.607098>.",
    "version": "0.1.0",
    "maintainer": "Laura Barbieri <laura.barbieri@unicatt.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19252,
    "package_name": "panelr",
    "title": "Regression Models and Utilities for Repeated Measures and Panel Data",
    "description": "Provides an object type and associated tools for storing and",
    "version": "1.0.0.9000",
    "maintainer": "",
    "url": "https://github.com/jacob-long/panelr",
    "exports": [],
    "topics": ["r", "r-package", "social-science", "statistics"],
    "score": "NA",
    "stars": 99
  },
  {
    "id": 19254,
    "package_name": "panelvar",
    "title": "Panel Vector Autoregression",
    "description": "We extend two general methods of moment estimators to panel vector \n    autoregression models (PVAR) with p lags of endogenous variables, predetermined \n    and strictly exogenous variables. This general PVAR model contains the first \n    difference GMM estimator by Holtz-Eakin et al. (1988) <doi:10.2307/1913103>, \n    Arellano and Bond (1991) <doi:10.2307/2297968> and the system GMM estimator \n    by Blundell and Bond (1998) <doi:10.1016/S0304-4076(98)00009-8>. We also \n    provide specification tests (Hansen overidentification test, lag selection \n    criterion and stability test of the PVAR polynomial) and classical structural \n    analysis for PVAR models such as orthogonal and generalized impulse response \n    functions, bootstrapped confidence intervals for impulse response analysis and \n    forecast error variance decompositions.",
    "version": "0.5.6",
    "maintainer": "Robert Ferstl <robert.ferstl@ur.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19265,
    "package_name": "parTimeROC",
    "title": "Parametric Time-Dependent Receiver Operating Characteristic",
    "description": "Producing the time-dependent receiver operating characteristic (ROC) curve through parametric approaches. Tools for\n    generating random data, fitting, predicting and check goodness of fit are prepared. The methods are\n    developed from the theoretical framework of proportional hazard model and copula functions.\n    Using this package, users can now simulate parametric time-dependent ROC and run experiment\n    to understand the behavior of the curve under different scenario.",
    "version": "0.2.0",
    "maintainer": "Faiz Azhar <faiz.azhar241@gmail.com>",
    "url": "https://github.com/FaizAzhar/parTimeROC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19286,
    "package_name": "paramsim",
    "title": "Parameterized Simulation",
    "description": "This function obtains a Random Number Generator (RNG) or collection of RNGs that replicate the required parameter(s) of a distribution for a time series of data. Consider the case of reproducing a time series data set of size 20 that uses an autoregressive (AR) model with phi = 0.8 and standard deviation equal to 1. When one checks the arima.sin() function's estimated parameters, it's possible that after a single trial or a few more, one won't find the precise parameters. This enables one to look for the ideal RNG setting for a simulation that will accurately duplicate the desired parameters.",
    "version": "0.1.0",
    "maintainer": "Daniel James <futathesis@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19295,
    "package_name": "parglm",
    "title": "Parallel GLM",
    "description": "Provides a parallel estimation method for generalized \n  linear models without compiling with a multithreaded LAPACK or BLAS.",
    "version": "0.1.7",
    "maintainer": "Benjamin Christoffersen <boennecd@gmail.com>",
    "url": "https://github.com/boennecd/parglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19299,
    "package_name": "parmsurvfit",
    "title": "Parametric Models for Survival Data",
    "description": "Executes simple parametric models for right-censored \n    survival data.  Functionality emulates capabilities in 'Minitab', including\n    fitting right-censored data, assessing fit, plotting survival functions,\n    and summary statistics and probabilities.",
    "version": "0.1.0",
    "maintainer": "Ashley Jacobson <ashleypjacobson@gmail.com>",
    "url": "https://github.com/apjacobson/parmsurvfit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19312,
    "package_name": "partR2",
    "title": "Partitioning R2 in GLMMs",
    "description": "Partitioning the R2 of GLMMs into variation explained by each \n    predictor and combination of predictors using semi-partial (part) R2 and\n    inclusive R2. Methods are based on the R2 for GLMMs described in\n    Nakagawa & Schielzeth (2013) and Nakagawa, Johnson & Schielzeth (2017).",
    "version": "0.9.2",
    "maintainer": "Martin A. Stoffel <martin.adam.stoffel@gmail.com>",
    "url": "https://github.com/mastoffel/partR2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19323,
    "package_name": "partsm",
    "title": "Periodic Autoregressive Time Series Models",
    "description": "Basic functions to fit and predict periodic autoregressive time series models. These models are discussed in the book P.H. Franses (1996) \"Periodicity and Stochastic Trends in Economic Time Series\", Oxford University Press. Data set analyzed in that book is also provided. NOTE: the package was orphaned during several years. It is now only maintained, but no major enhancements are expected, and the maintainer cannot provide any support. ",
    "version": "1.1-4",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "url": "https://github.com/MatthieuStigler/partsm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19324,
    "package_name": "parttime",
    "title": "Partial Datetime Handling",
    "description": "\n    Datetimes and timestamps are invariably an imprecise notation, with any\n    partial representation implying some amount of uncertainty. To handle this,\n    'parttime' provides classes for embedding partial missingness as a central\n    part of its datetime classes. This central feature allows for more ergonomic\n    use of datetimes for challenging datetime computation, including\n    calculations of overlapping date ranges, imputations, and more thoughtful\n    handling of ambiguity that arises from uncertain time zones. This package was\n    developed first and foremost with pharmaceutical applications in mind, but\n    aims to be agnostic to application to accommodate general use cases just as\n    conveniently.",
    "version": "0.1.2",
    "maintainer": "Doug Kelkhoff <doug.kelkhoff@gmail.com>",
    "url": "https://dgkf.github.io/parttime/, https://github.com/dgkf/parttime",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19334,
    "package_name": "pastaPlot",
    "title": "Spaghetti-Plot Fixed and Random Effects of Linear Mixed Models",
    "description": "Plot both fixed and random effects of linear mixed models, multilevel models in a single spaghetti plot. The package allows to visualize the effect of a predictor on a criterion between different levels of a grouping variable. Additionally, confidence intervals can be displayed for fixed effects. Calculation of predicted values of random effects allows only models with one random intercept and/or one random slope to be plotted. Confidence intervals and predicted values of fixed effects are computed using the 'ggpredict' function from the 'ggeffects' package. Lüdecke, D. (2018) <doi:10.21105/joss.00638>.",
    "version": "0.1.0",
    "maintainer": "Jan-Felix Palnau <jan.palnau@mailbox.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19336,
    "package_name": "pastclim",
    "title": "Manipulate Time Series of Climate Reconstructions",
    "description": "Methods to easily extract and manipulate climate\n  reconstructions for ecological and anthropological analyses, as described\n  in Leonardi et al. (2023) <doi:10.1111/ecog.06481>. The package includes datasets\n  of palaeoclimate reconstructions, present observations, and future projections \n  from multiple climate models.",
    "version": "2.2.0",
    "maintainer": "Andrea Manica <am315@cam.ac.uk>",
    "url": "https://github.com/EvolEcolGroup/pastclim,\nhttps://evolecolgroup.github.io/pastclim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19355,
    "package_name": "paths",
    "title": "An Imputation Approach to Estimating Path-Specific Causal\nEffects",
    "description": "In causal mediation analysis with multiple causally ordered mediators, a set of path-specific\n  effects are identified under standard ignorability assumptions. This package implements an imputation\n  approach to estimating these effects along with a set of bias formulas for conducting sensitivity analysis\n  (Zhou and Yamamoto <doi:10.31235/osf.io/2rx6p>). It contains two main functions: paths() for estimating \n  path-specific effects and sens() for conducting sensitivity analysis. Estimation uncertainty is quantified \n  using the nonparametric bootstrap.",
    "version": "0.1.2",
    "maintainer": "Xiang Zhou <xiang_zhou@fas.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19396,
    "package_name": "pbkrtest",
    "title": "Parametric Bootstrap, Kenward-Roger and Satterthwaite Based\nMethods for Test in Mixed Models",
    "description": "Computes p-values based on (a) Satterthwaite or\n    Kenward-Rogers degree of freedom methods and (b) parametric bootstrap\n    for mixed effects models as implemented in the 'lme4'\n    package. Implements parametric bootstrap test for generalized linear\n    mixed models as implemented in 'lme4' and generalized linear\n    models. The package is documented in the paper by Halekoh and\n    Højsgaard, (2012, <doi:10.18637/jss.v059.i09>).  Please see\n    'citation(\"pbkrtest\")' for citation details.",
    "version": "0.5.5",
    "maintainer": "Søren Højsgaard <sorenh@math.aau.dk>",
    "url": "https://people.math.aau.dk/~sorenh/software/pbkrtest/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19397,
    "package_name": "pblm",
    "title": "Bivariate Additive Marginal Regression for Categorical Responses",
    "description": "Bivariate additive categorical regression via penalized maximum likelihood. \n             Under a multinomial framework, the method fits bivariate models where both \n             responses are nominal, ordinal, or a mix of the two. Partial proportional \n             odds models are supported, with flexible (non-)uniform association structures. \n             Various logit types and parametrizations can be specified for both marginals \n             and the association, including Dale’s model. The association structure can \n             be regularized using polynomial-type penalty terms. Additive effects are \n             modeled using P-splines. Standard methods such as summary(), residuals(), \n             and predict() are available.   ",
    "version": "0.1-12",
    "maintainer": "Marco Enea <marco.enea@unipa.it>",
    "url": "https://github.com/MarcoEnea/pblm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19401,
    "package_name": "pboost",
    "title": "Profile Boosting Framework for Parametric Models",
    "description": "\n    A profile boosting framework for feature selection in parametric models.\n    It offers a unified interface pboost() and several wrapped models, including linear model, generalized linear models, quantile regression, Cox proportional hazards model, beta regression.\n    An S3 interface EBIC() is provided as the stopping rule for the profile boosting by default.",
    "version": "0.2.1",
    "maintainer": "Zengchao Xu <zengc.xu@aliyun.com>",
    "url": "https://github.com/paradoxical-rhapsody/pboost",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19405,
    "package_name": "pcFactorStan",
    "title": "Stan Models for the Paired Comparison Factor Model",
    "description": "Provides convenience functions and pre-programmed\n    Stan models related to the paired comparison factor model. Its purpose\n    is to make fitting paired comparison data using Stan easy. This\n    package is described in Pritikin (2020) <doi:10.1016/j.heliyon.2020.e04821>.",
    "version": "1.5.4",
    "maintainer": "Joshua N. Pritikin <jpritikin@pobox.com>",
    "url": "https://github.com/jpritikin/pcFactorStan",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19426,
    "package_name": "pch",
    "title": "Piecewise Constant Hazard Models for Censored and Truncated Data",
    "description": "Piecewise constant hazard models for survival data. \n    The package allows for right-censored, left-truncated, and interval-censored data.",
    "version": "2.2",
    "maintainer": "Paolo Frumento <paolo.frumento@unipi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19429,
    "package_name": "pcmabc",
    "title": "Approximate Bayesian Computations for Phylogenetic Comparative\nMethods",
    "description": "Fits by ABC, the parameters of a stochastic process modelling the phylogeny and evolution of a suite of traits following the tree. The user may define an arbitrary Markov process for the trait and phylogeny. Importantly, trait-dependent speciation models are handled and fitted to data. See K. Bartoszek, P. Lio' (2019) <doi:10.5506/APhysPolBSupp.12.25>. The suggested geiger package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/geiger/>, suggested to take latest version. Otherwise its required code is present in the pcmabc package. The suggested distory package can be obtained from CRAN's archive <https://cran.r-project.org/src/contrib/Archive/distory/>, suggested to take latest version. ",
    "version": "1.1.3",
    "maintainer": "Krzysztof Bartoszek <krzbar@protonmail.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19430,
    "package_name": "pcnetmeta",
    "title": "Patient-Centered Network Meta-Analysis",
    "description": "Performs Bayesian arm-based network meta-analysis for datasets with binary, continuous, and count outcomes\n (Zhang et al., 2014 <doi:10.1177/1740774513498322>;\n  Lin et al., 2017 <doi:10.18637/jss.v080.i05>).",
    "version": "2.8",
    "maintainer": "Lifeng Lin <lifenglin@arizona.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19433,
    "package_name": "pcsstools",
    "title": "Tools for Regression Using Pre-Computed Summary Statistics",
    "description": "Defines functions to describe regression models using only\n    pre-computed summary statistics (i.e. means, variances, and covariances)\n    in place of individual participant data.\n    Possible models include linear models for linear combinations, products, \n    and logical combinations of phenotypes.\n    Implements methods presented in \n    Wolf et al. (2021) <doi:10.3389/fgene.2021.745901>\n    Wolf et al. (2020) <doi:10.1142/9789811215636_0063> and \n    Gasdaska et al. (2019) <doi:10.1142/9789813279827_0036>.",
    "version": "0.1.2",
    "maintainer": "Jack Wolf <jackwolf910@gmail.com>",
    "url": "https://github.com/jackmwolf/pcsstools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19438,
    "package_name": "pcv",
    "title": "Procrustes Cross-Validation",
    "description": "Implements Procrustes cross-validation method for Principal Component Analysis, Principal Component Regression and Partial Least Squares regression models. S. Kucheryavskiy (2023) <doi:10.1016/j.aca.2023.341096>.",
    "version": "1.1.0",
    "maintainer": "Sergey Kucheryavskiy <svkucheryavski@gmail.com>",
    "url": "https://github.com/svkucheryavski/pcv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19439,
    "package_name": "pcvr",
    "title": "Plant Phenotyping and Bayesian Statistics",
    "description": "Analyse common types of plant phenotyping data, provide a simplified interface\n\tto longitudinal growth modeling and select Bayesian statistics,\n\tand streamline use of 'PlantCV' output.\n\tSeveral Bayesian methods and reporting guidelines for Bayesian methods are described in\n\tKruschke (2018) <doi:10.1177/2515245918771304>,\n\tKruschke (2013) <doi:10.1037/a0029146>, and Kruschke (2021) <doi:10.1038/s41562-021-01177-7>.",
    "version": "1.3.1",
    "maintainer": "Josh Sumner <jsumner@danforthcenter.org>",
    "url": "https://github.com/danforthcenter/pcvr, https://plantcv.org/,\nhttps://danforthcenter.github.io/pcvr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19444,
    "package_name": "pdc",
    "title": "Permutation Distribution Clustering",
    "description": "Permutation Distribution Clustering is a clustering method for time series. Dissimilarity of time series is formalized as the divergence between their permutation distributions. The permutation distribution was proposed as measure of the complexity of a time series.",
    "version": "1.0.3",
    "maintainer": "Andreas M. Brandmaier <brandmaier@mpib-berlin.mpg.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19449,
    "package_name": "pdfetch",
    "title": "Fetch Economic and Financial Time Series Data from Public\nSources",
    "description": "Download economic and financial time series from public sources, \n  including the St Louis Fed's FRED system, Yahoo Finance, the US Bureau of Labor Statistics, \n  the US Energy Information Administration, the World Bank, Eurostat, the European Central Bank,\n  the Bank of England, the UK's Office of National Statistics, Deutsche Bundesbank, and INSEE.",
    "version": "0.3.3",
    "maintainer": "Abiel Reinhart <abielr@gmail.com>",
    "url": "https://github.com/abielr/pdfetch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19458,
    "package_name": "pdt",
    "title": "Permutation Distancing Test",
    "description": "Permutation (randomisation) test for single-case phase design data with \n    two phases (e.g., pre- and post-treatment). Correction for dependency of observations \n    is done through stepwise resampling the time series while varying \n    the distance between observations. The required distance 0,1,2,3.. is determined \n    based on repeated dependency testing while stepwise increasing the distance.\n    In preparation: Vroegindeweij et al. \"A Permutation distancing test \n    for single-case observational AB phase design data: A Monte Carlo simulation study\".",
    "version": "0.0.2",
    "maintainer": "Jan Houtveen <janhoutveen@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19463,
    "package_name": "peacots",
    "title": "Periodogram Peaks in Correlated Time Series",
    "description": "Calculates the periodogram of a time series, maximum-likelihood fits an Ornstein-Uhlenbeck state space (OUSS) null model and evaluates the statistical significance of periodogram peaks against the OUSS null hypothesis. The OUSS is a parsimonious model for stochastically fluctuating variables with linear stabilizing forces, subject to uncorrelated measurement errors. Contrary to the classical white noise null model for detecting cyclicity, the OUSS model can account for temporal correlations typically occurring in ecological and geological time series. Citation: Louca, Stilianos and Doebeli, Michael (2015) <doi:10.1890/14-0126.1>.",
    "version": "1.3.2",
    "maintainer": "Stilianos Louca <louca.research@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19467,
    "package_name": "pec",
    "title": "Prediction Error Curves for Risk Prediction Models in Survival\nAnalysis",
    "description": "Validation of risk predictions obtained from survival models and\n    competing risk models based on censored data using inverse weighting and\n    cross-validation. Most of the 'pec' functionality has been moved to 'riskRegression'.",
    "version": "2025.06.24",
    "maintainer": "Thomas A. Gerds <tag@biostat.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19492,
    "package_name": "pema",
    "title": "Penalized Meta-Analysis",
    "description": "Conduct penalized meta-analysis, see Van Lissa, Van Erp, & Clapper\n    (2023) <doi:10.31234/osf.io/6phs5>. In meta-analysis, there are\n    often between-study differences. These can be coded as moderator variables,\n    and controlled for using meta-regression. However, if the number of\n    moderators is large relative to the number of studies, such an analysis may\n    be overfit. Penalized meta-regression is useful in these cases, because\n    it shrinks the regression slopes of irrelevant moderators towards zero.",
    "version": "0.1.5",
    "maintainer": "Caspar J van Lissa <c.j.vanlissa@tilburguniversity.edu>",
    "url": "https://github.com/cjvanlissa/pema,\nhttps://cjvanlissa.github.io/pema/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19495,
    "package_name": "pemultinom",
    "title": "L1-Penalized Multinomial Regression with Statistical Inference",
    "description": "We aim for fitting a multinomial regression model with Lasso penalty and doing statistical inference (calculating confidence intervals of coefficients and p-values for individual variables). It implements 1) the coordinate descent algorithm to fit an l1-penalized multinomial regression model (parameterized with a reference level); 2) the debiasing approach to obtain the inference results, which is described in \"Tian, Y., Rusinek, H., Masurkar, A. V., & Feng, Y. (2024). L1‐Penalized Multinomial Regression: Estimation, Inference, and Prediction, With an Application to Risk Factor Identification for Different Dementia Subtypes. Statistics in Medicine, 43(30), 5711-5747.\"",
    "version": "0.1.1",
    "maintainer": "Ye Tian <ye.t@columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19497,
    "package_name": "penMSM",
    "title": "Estimating Regularized Multi-state Models Using L1 Penalties",
    "description": "Structured fusion Lasso penalized estimation of multi-state models with the penalty applied to absolute effects and absolute effect differences (i.e., effects on transition-type specific hazard rates).",
    "version": "0.99",
    "maintainer": "Holger Reulen <hreulen@uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19498,
    "package_name": "penalized",
    "title": "L1 (Lasso and Fused Lasso) and L2 (Ridge) Penalized Estimation\nin GLMs and in the Cox Model",
    "description": "Fitting possibly high dimensional penalized\n        regression models. The penalty structure can be any combination\n        of an L1 penalty (lasso and fused lasso), an L2 penalty (ridge) and a\n        positivity constraint on the regression coefficients. The\n        supported regression models are linear, logistic and Poisson\n        regression and the Cox Proportional Hazards model.\n        Cross-validation routines allow optimization of the tuning\n        parameters.",
    "version": "0.9-53",
    "maintainer": "Jelle Goeman <j.j.goeman@lumc.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19502,
    "package_name": "penaltyLearning",
    "title": "Penalty Learning",
    "description": "Implementations of algorithms from \n Learning Sparse Penalties for Change-point Detection\n using Max Margin Interval Regression, by\n Hocking, Rigaill, Vert, Bach\n <http://proceedings.mlr.press/v28/hocking13.html>\n published in proceedings of ICML2013.",
    "version": "2024.9.3",
    "maintainer": "Toby Dylan Hocking <toby.hocking@r-project.org>",
    "url": "https://github.com/tdhock/penaltyLearning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19503,
    "package_name": "pencal",
    "title": "Penalized Regression Calibration (PRC) for the Dynamic\nPrediction of Survival",
    "description": "Computes penalized regression calibration (PRC), a\n    statistical method for the dynamic prediction of survival when many\n    longitudinal predictors are available. See Signorelli (2024)\n    <doi:10.32614/RJ-2024-014> and Signorelli et al. (2021)\n    <doi:10.1002/sim.9178> for details.",
    "version": "2.3.0",
    "maintainer": "Mirko Signorelli <msignorelli.rpackages@gmail.com>",
    "url": "https://mirkosignorelli.github.io/r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19512,
    "package_name": "pensim",
    "title": "Simulation of High-Dimensional Data and Parallelized Repeated\nPenalized Regression",
    "description": "Simulation of continuous, correlated high-dimensional data with \n    time to event or binary response, and parallelized functions for Lasso, \n    Ridge, and Elastic Net penalized regression with repeated starts and \n    two-dimensional tuning of the Elastic Net.",
    "version": "1.3.6",
    "maintainer": "Levi Waldron <lwaldron.research@gmail.com>",
    "url": "https://waldronlab.io/pensim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19516,
    "package_name": "peopleanalyticsdata",
    "title": "Data Sets for Keith McNulty's Handbook of Regression Modeling in\nPeople Analytics",
    "description": "Data sets for statistical inference modeling related to People Analytics.  \n  Contains various data sets from the book 'Handbook of Regression Modeling in People Analytics' \n  by Keith McNulty (2020).",
    "version": "0.2.1",
    "maintainer": "Keith McNulty <keith.mcnulty@gmail.com>",
    "url": "http://peopleanalytics-regression-book.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19520,
    "package_name": "peperr",
    "title": "Parallelised Estimation of Prediction Error",
    "description": "Designed for prediction error estimation\n        through resampling techniques, possibly accelerated by parallel\n        execution on a compute cluster. Newly developed model fitting\n        routines can be easily incorporated. Methods used in the package are detailed in\n        Porzelius Ch., Binder H. and Schumacher M. (2009) <doi:10.1093/bioinformatics/btp062>\n        and were used, for instance, in\n        Porzelius Ch., Schumacher M. and Binder H. (2011) <doi:10.1007/s00180-011-0236-6>.",
    "version": "1.6",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://github.com/fbertran/peperr/,\nhttps://fbertran.github.io/peperr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19524,
    "package_name": "perARMA",
    "title": "Periodic Time Series Analysis",
    "description": "Identification, model fitting and estimation for time series with periodic structure.\n    Additionally, procedures for simulation of periodic processes\n    and real data sets are included.\n    Hurd, H. L., Miamee, A. G. (2007) <doi:10.1002/9780470182833>\n    Box, G. E. P., Jenkins, G. M., Reinsel, G. (1994)  <doi:10.1111/jtsa.12194>\n    Brockwell, P. J., Davis, R. A. (1991, ISBN:978-1-4419-0319-8) \n    Bretz, F., Hothorn, T., Westfall, P. (2010, ISBN: 9780429139543) \n    Westfall, P. H., Young, S. S. (1993, ISBN:978-0-471-55761-6)\n    Bloomfield, P., Hurd, H. L.,Lund, R. (1994) \n    <doi:10.1111/j.1467-9892.1994.tb00181.x>\n    Dehay, D., Hurd, H. L. (1994, ISBN:0-7803-1023-3)\n    Vecchia, A. (1985) <doi:10.1080/00401706.1985.10488076>\n    Vecchia, A. (1985) <doi:10.1111/j.1752-1688.1985.tb00167.x>\n    Jones, R., Brelsford, W. (1967) <doi:10.1093/biomet/54.3-4.403>\n    Makagon, A. (1999)\n    <https://www.math.uni.wroc.pl/~pms/files/19.2/Article/19.2.5.pdf>\n    Sakai, H. (1989) <doi:10.1111/j.1467-9892.1991.tb00069.x>\n    Gladyshev, E. G. (1961) \n    <https://www.mathnet.ru/php/archive.phtml?wshow=paper&jrnid=dan&paperid=24851>\n    Ansley (1979) <doi:10.1093/biomet/66.1.59>\n    Hurd, H. L., Gerr, N. L. (1991) <doi:10.1111/j.1467-9892.1991.tb00088.x>.",
    "version": "1.7",
    "maintainer": "Karolina Marek <karolina.marek10@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19538,
    "package_name": "permuco",
    "title": "Permutation Tests for Regression, (Repeated Measures)\nANOVA/ANCOVA and Comparison of Signals",
    "description": "Functions to compute p-values based on permutation tests. Regression, ANOVA and ANCOVA, omnibus F-tests, marginal unilateral and bilateral t-tests are available. Several methods to handle nuisance variables are implemented (Kherad-Pajouh, S., & Renaud, O. (2010) <doi:10.1016/j.csda.2010.02.015> ; Kherad-Pajouh, S., & Renaud, O. (2014) <doi:10.1007/s00362-014-0617-3> ; Winkler, A. M., Ridgway, G. R., Webster, M. A., Smith, S. M., & Nichols, T. E. (2014) <doi:10.1016/j.neuroimage.2014.01.060>). An extension for the comparison of signals issued from experimental conditions (e.g. EEG/ERP signals) is provided. Several corrections for multiple testing are possible, including the cluster-mass statistic (Maris, E., & Oostenveld, R. (2007) <doi:10.1016/j.jneumeth.2007.03.024>) and the threshold-free cluster enhancement (Smith, S. M., & Nichols, T. E. (2009) <doi:10.1016/j.neuroimage.2008.03.061>). ",
    "version": "1.1.3",
    "maintainer": "Jaromil Frossard <jaromil.frossard@gmail.com>",
    "url": "https://github.com/jaromilfrossard/permuco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19541,
    "package_name": "permutes",
    "title": "Permutation Tests for Time Series Data",
    "description": "Helps you determine the analysis window to use when analyzing densely-sampled\n    time-series data, such as EEG data, using permutation testing (Maris & Oostenveld, 2007)\n    <doi:10.1016/j.jneumeth.2007.03.024>. These permutation tests can help identify the timepoints\n    where significance of an effect begins and ends, and the results can be plotted in various\n    types of heatmap for reporting. Mixed-effects models are supported using an implementation of\n    the approach by Lee & Braun (2012) <doi:10.1111/j.1541-0420.2011.01675.x>.",
    "version": "2.8",
    "maintainer": "Cesko C. Voeten <cvoeten@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19544,
    "package_name": "perry",
    "title": "Resampling-Based Prediction Error Estimation for Regression\nModels",
    "description": "Tools that allow developers to write functions for prediction\n    error estimation with minimal programming effort and assist users with\n    model selection in regression problems.",
    "version": "0.3.1",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19545,
    "package_name": "perryExamples",
    "title": "Examples for Integrating Prediction Error Estimation into\nRegression Models",
    "description": "Examples for integrating package 'perry' for prediction error estimation into regression models.",
    "version": "0.1.1",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19552,
    "package_name": "perspectev",
    "title": "Permutation of Species During Turnover Events",
    "description": "Provides a robust framework for analyzing the extent to which differential survival with respect to higher level trait variation is reducible to lower level variation. In addition to its primary test, it also provides functions for simulation-based power analysis, reading in common data set formats, and visualizing results. Temporarily contains an edited version of function hr.mcp() from package 'wild1', written by Glen Sargeant. For tutorial see: http://evolve.zoo.ox.ac.uk/Evolve/Perspectev.html.",
    "version": "1.1",
    "maintainer": "Kenneth B. Hoehn <perspectev@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19560,
    "package_name": "pesel",
    "title": "Automatic Estimation of Number of Principal Components in PCA",
    "description": "Automatic estimation of number of principal components in PCA\n    with PEnalized SEmi-integrated Likelihood (PESEL). See Piotr Sobczyk, Malgorzata Bogdan, Julie Josse\n    \"Bayesian dimensionality reduction with PCA using penalized semi-integrated likelihood\"\n    (2017) <doi:10.1080/10618600.2017.1340302>.",
    "version": "0.7.5",
    "maintainer": "Piotr Sobczyk <pj.sobczyk@gmail.com>",
    "url": "https://github.com/psobczyk/pesel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19565,
    "package_name": "pexm",
    "title": "Loading a JAGS Module for the Piecewise Exponential Distribution",
    "description": "Load the Just Another Gibbs Sampling (JAGS) module 'pexm'. The module provides the tools to work with the Piecewise Exponential (PE) distribution in a Bayesian model with the corresponding Markov Chain Monte Carlo algorithm (Gibbs Sampling) implemented via JAGS. Details about the module implementation can be found in Mayrink et al. (2021) <doi:10.18637/jss.v100.i08>.",
    "version": "1.1.3",
    "maintainer": "Vinicius Mayrink <vdinizm@gmail.com>",
    "url": "https://github.com/vdinizm/pexm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19566,
    "package_name": "pez",
    "title": "Phylogenetics for the Environmental Sciences",
    "description": "Eco-phylogenetic and community phylogenetic analyses.\n    Keeps community ecological and phylogenetic data matched up and\n    comparable using 'comparative.comm' objects. Wrappers for common\n    community phylogenetic indices ('pez.shape', 'pez.evenness',\n    'pez.dispersion', and 'pez.dissimilarity' metrics). Implementation\n    of Cavender-Bares (2004) correlation of phylogenetic and\n    ecological matrices ('fingerprint.regression'). Phylogenetic\n    Generalised Linear Mixed Models (PGLMMs; 'pglmm') following Ives &\n    Helmus (2011) and Rafferty & Ives (2013). Simulation of null\n    assemblages, traits, and phylogenies ('scape', 'sim.meta.comm').",
    "version": "1.2-5",
    "maintainer": "William D. Pearse <will.pearse@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19579,
    "package_name": "pglm",
    "title": "Panel Generalized Linear Models",
    "description": "Estimation of panel models for glm-like models:\n             this includes binomial models (logit and probit), count models (poisson and negbin)\n\t     and ordered models (logit and probit), as described in:\n             Baltagi (2013) Econometric Analysis of Panel Data, ISBN-13:978-1-118-67232-7,\n\t     Hsiao (2014) Analysis of Panel Data  <doi:10.1017/CBO9781139839327> and\n\t     Croissant and Millo (2018), Panel Data Econometrics with R, ISBN:978-1-118-94918-4.",
    "version": "0.2-3",
    "maintainer": "Yves Croissant <yves.croissant@univ-reunion.fr>",
    "url": "https://cran.r-project.org/package=pglm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19584,
    "package_name": "ph2bayes",
    "title": "Bayesian Single-Arm Phase II Designs",
    "description": "An implementation of Bayesian single-arm phase II design methods for binary outcome based on posterior\n  probability (Thall and Simon (1994) <doi:10.2307/2533377>) and predictive probability (Lee and Liu (2008) <doi:10.1177/1740774508089279>).",
    "version": "0.0.2",
    "maintainer": "Kengo Nagashima <nshi1201@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19585,
    "package_name": "ph2bye",
    "title": "Phase II Clinical Trial Design Using Bayesian Methods",
    "description": "Calculate the Bayesian posterior/predictive probability and\n    determine the sample size and stopping boundaries for single-arm Phase II design.",
    "version": "0.1.4",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "url": "https://allen.shinyapps.io/BayesDesign/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19586,
    "package_name": "ph2mult",
    "title": "Phase II Clinical Trial Design for Multinomial Endpoints",
    "description": "Provide multinomial design methods under intersection-union test (IUT) and union-intersection test (UIT) scheme for Phase II trial. The design types include : Minimax (minimize the maximum sample size), Optimal (minimize the expected sample size), Admissible (minimize the Bayesian risk) and Maxpower (maximize the exact power level).",
    "version": "0.1.1",
    "maintainer": "Yalin Zhu <yalin.zhu@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19601,
    "package_name": "phase12designs",
    "title": "Comprehensive Tools for Running Model-Assisted Phase I/II Trial\nSimulations",
    "description": "Provides a comprehensive set of tools to simulate, evaluate, and compare model-assisted designs for early-phase (Phase I/II) clinical trials, including:\n  - BOIN12 (Bayesian optimal interval phase 1/11 trial design; Lin et al. (2020) <doi:10.1200/PO.20.00257>),\n  - BOIN-ET (Takeda, K., Taguri, M., & Morita, S. (2018) <doi:10.1002/pst.1864>),\n  - EffTox (Thall, P. F., & Cook, J. D. (2004) <doi:10.1111/j.0006-341X.2004.00218.x>),\n  - Ji3+3 (Joint i3+3 design; Lin, X., & Ji, Y. (2020) <doi:10.1080/10543406.2020.1818250>),\n  - PRINTE (probability intervals of toxicity and efficacy design; Lin, X., & Ji, Y. (2021) <doi:10.1177/0962280220977009>),\n  - STEIN (simple toxicity and efficacy interval design; Lin, R., & Yin, G. (2017) <doi:10.1002/sim.7428>),\n  - TEPI (toxicity and efficacy probability interval design; Li, D. H., Whitmore, J. B., Guo, W., & Ji, Y. (2017) <doi:10.1158/1078-0432.CCR-16-1125>),\n  - uTPI (utility-based toxicity Probability interval design; Shi, H., Lin, R., & Lin, X. (2024) <doi:10.1002/sim.8922>).\n Includes flexible simulation parameters that allow\n researchers to efficiently compute operating characteristics under\n various fixed and random trial scenarios and export the results.",
    "version": "0.3.1",
    "maintainer": "Angela Cao <cao.t.angela@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19602,
    "package_name": "phase1PRMD",
    "title": "Personalized Repeated Measurement Design for Phase I Clinical\nTrials",
    "description": "Implements Bayesian phase I repeated measurement design that \n    accounts for multidimensional toxicity endpoints and longitudinal efficacy \n    measure from multiple treatment cycles. The package provides flags to \n    fit a variety of model-based phase I design, including 1 stage models with\n    or without individualized dose modification, 3-stage models with or without\n    individualized dose modification, etc. Functions are provided to recommend \n    dosage selection based on the data collected in the available patient cohorts \n    and to simulate trial characteristics given design parameters. \n    Yin, Jun, et al. (2017) <doi:10.1002/sim.7134>. ",
    "version": "1.0.2",
    "maintainer": "Lu Zhang <luzhangstat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19603,
    "package_name": "phase1RMD",
    "title": "Repeated Measurement Design for Phase I Clinical Trial",
    "description": "Implements our Bayesian phase I repeated measurement design that accounts for multidimensional toxicity endpoints from multiple treatment cycles. The package also provides a novel design to account for both multidimensional toxicity endpoints and early-stage efficacy endpoints in the phase I design. For both designs, functions are provided to recommend the next dosage selection based on the data collected in the available patient cohorts and to simulate trial characteristics given design parameters. Yin, Jun, et al. (2017) <doi:10.1002/sim.7134>.",
    "version": "1.0.9",
    "maintainer": "Jun Yin <vivien.jyin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19614,
    "package_name": "phenoCDM",
    "title": "Continuous Development Models for Incremental Time-Series\nAnalysis",
    "description": "Using the Bayesian state-space approach, we developed a continuous development model to quantify dynamic incremental changes in the response variable. While the model was originally developed for daily changes in forest green-up, the model can be used to predict any similar process. The CDM can capture both timing and rate of nonlinear processes. Unlike statics methods, which aggregate variations into a single metric, our dynamic model tracks the changing impacts over time. The CDM accommodates nonlinear responses to variation in predictors, which changes throughout development. ",
    "version": "0.1.3",
    "maintainer": "Bijan Seyednasrollah <bijan.s.nasr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19616,
    "package_name": "phenocamr",
    "title": "Facilitates 'PhenoCam' Data Access and Time Series\nPost-Processing",
    "description": "\n    Programmatic interface to the 'PhenoCam' web services (<https://phenocam.nau.edu/webcam>).\n    Allows for easy downloading of 'PhenoCam' data directly to your R workspace\n    or your computer and provides post-processing routines for consistent and easy\n    timeseries outlier detection, smoothing and estimation of phenological transition dates.\n    Methods for this package are described in detail in Hufkens et. al (2018) <doi:10.1111/2041-210X.12970>.",
    "version": "1.1.5",
    "maintainer": "Koen Hufkens <koen.hufkens@gmail.com>",
    "url": "https://github.com/bluegreen-labs/phenocamr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19618,
    "package_name": "phenology",
    "title": "Tools to Manage a Parametric Function that Describes Phenology\nand More",
    "description": "Functions used to fit and test the phenology of species based on counts. Based on Girondot, M. (2010) <doi:10.3354/esr00292> for the phenology function, Girondot, M. (2017) <doi:10.1016/j.ecolind.2017.05.063> for the convolution of negative binomial, Girondot, M. and Rizzo, A. (2015) <doi:10.2993/etbi-35-02-337-353.1> for Bayesian estimate, Pfaller JB, ..., Girondot M (2019) <doi:10.1007/s00227-019-3545-x> for tag-loss estimate, Hancock J, ..., Girondot M (2019) <doi:10.1016/j.ecolmodel.2019.04.013> for nesting history, Laloe J-O, ..., Girondot M, Hays GC (2020) <doi:10.1007/s00227-020-03686-x> for aggregating several seasons.",
    "version": "2025.11.12",
    "maintainer": "Marc Girondot <marc.girondot@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19626,
    "package_name": "philentropy",
    "title": "Similarity and Distance Quantification Between Probability Functions",
    "description": "Computes 46 optimized distance and similarity measures for comparing probability functions (Drost (2018) <doi:10.21105/joss.00765>). These comparisons between probability functions have their foundations in a broad range of scientific disciplines from mathematics to ecology. The aim of this package is to provide a core framework for clustering, classification, statistical inference, goodness-of-fit, non-parametric statistics, information theory, and machine learning tasks that are based on comparing univariate or multivariate probability functions.",
    "version": "0.10.0",
    "maintainer": "Hajk-Georg Drost <hajk-georg.drost@tuebingen.mpg.de>",
    "url": "https://github.com/drostlab/philentropy",
    "exports": [],
    "topics": ["distance-measures", "distance-quantification", "information-theory", "jensen-shannon-divergence", "parametric-distributions", "r", "similarity-measures", "statistics"],
    "score": "NA",
    "stars": 147
  },
  {
    "id": 19629,
    "package_name": "phoenics",
    "title": "Pathways Longitudinal and Differential Analysis in Metabolomics",
    "description": "Perform a differential analysis at pathway level based on \n             metabolite quantifications and information on pathway metabolite \n             composition. The method, described in Guilmineau et al (2025)\n             <doi:10.1186/s12859-025-06118-z> is based on a Principal Component \n             Analysis step and on a linear mixed model.\n  Automatic query of metabolic pathways is also implemented. ",
    "version": "0.6",
    "maintainer": "Remi Servien <remi.servien@inrae.fr>",
    "url": "https://forge.inrae.fr/panoramics/phoenics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19646,
    "package_name": "photobiologySun",
    "title": "Data for Sunlight Spectra",
    "description": "Data for the extraterrestrial solar spectral irradiance and ground \n    level solar spectral irradiance and irradiance. In addition data for \n    shade light under vegetation and irradiance time series from different\n    broadband sensors.  Part of the \n    'r4photobiology' suite, Aphalo P. J. (2015) <doi:10.19232/uv4pb.2015.1.14>.",
    "version": "0.5.1",
    "maintainer": "Pedro J. Aphalo <pedro.aphalo@helsinki.fi>",
    "url": "https://docs.r4photobiology.info/photobiologySun/,\nhttps://github.com/aphalo/photobiologySun",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19669,
    "package_name": "phylolm.hp",
    "title": "Hierarchical Partitioning of R2 for Phylogenetic Linear\nRegression",
    "description": "Conducts hierarchical partitioning to calculate individual contributions of phylogenetic tree and  predictors (groups) towards total R2  for phylogenetic linear regression models.",
    "version": "0.0-4",
    "maintainer": "Jiangshan Lai <lai@njfu.edu.cn>",
    "url": "https://github.com/laijiangshan/phylolm.hp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19671,
    "package_name": "phylopairs",
    "title": "Comparative Analyses of Lineage-Pair Traits",
    "description": "Facilitates the testing of causal relationships among lineage-pair traits in a phylogenetically informed context. Lineage-pair traits are characters that are defined for pairs of lineages instead of individual taxa. Examples include the strength of reproductive isolation, range overlap, competition coefficient, diet niche similarity, and relative hybrid fitness. Users supply a lineage-pair dataset and a phylogeny. 'phylopairs' calculates a covariance matrix for the pairwise-defined data and provides built-in models to test for relationships among variables while taking this covariance into account. Bayesian sampling is run through built-in 'Stan' programs via the 'rstan' package. The various models and methods that this package makes available are described in Anderson et al. (In Review), Coyne and Orr (1989) <doi:10.1111/j.1558-5646.1989.tb04233.x>, Fitzpatrick (2002) <doi:10.1111/j.0014-3820.2002.tb00860.x>, and Castillo (2007) <doi:10.1002/ece3.3093>.",
    "version": "0.1.1",
    "maintainer": "Sean A. S. Anderson <sean.as.anderson@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19691,
    "package_name": "piar",
    "title": "Price Index Aggregation",
    "description": "Most price indexes are made with a two-step procedure, where\n    period-over-period elementary indexes are first calculated for a collection\n    of elementary aggregates at each point in time, and then aggregated according\n    to a price index aggregation structure. These indexes can then be chained\n    together to form a time series that gives the evolution of prices with\n    respect to a fixed base period. This package contains a collection of\n    functions that revolve around this work flow, making it easy to build\n    standard price indexes, and implement the methods described by\n    Balk (2008, <doi:10.1017/CBO9780511720758>), von der Lippe (2007,\n    <doi:10.3726/978-3-653-01120-3>), and the CPI manual (2020,\n    <doi:10.5089/9781484354841.069>) for bilateral price indexes.",
    "version": "0.8.3",
    "maintainer": "Steve Martin <marberts@protonmail.com>",
    "url": "https://marberts.github.io/piar/, https://github.com/marberts/piar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19780,
    "package_name": "plac",
    "title": "A Pairwise Likelihood Augmented Cox Estimator for Left-Truncated\nData",
    "description": "A semi-parametric estimation method for the Cox model\n    with left-truncated data using augmented information\n    from the marginal of truncation times.",
    "version": "0.1.3",
    "maintainer": "Fan Wu <fannwu@umich.edu>",
    "url": "https://github.com/942kid/plac",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19781,
    "package_name": "placeholder",
    "title": "Does not matter.",
    "description": "",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/jolars/TSAsolutions",
    "exports": [],
    "topics": ["r", "statistics", "time-series", "time-series-analysis"],
    "score": "NA",
    "stars": 17
  },
  {
    "id": 19793,
    "package_name": "plaqr",
    "title": "Partially Linear Additive Quantile Regression",
    "description": "Estimation, prediction, thresholding, transformation, and plotting for partially linear additive quantile regression.  Intuitive functions for fitting and plotting partially linear additive quantile regression models.  Uses and works with functions from the 'quantreg' package.",
    "version": "2.0",
    "maintainer": "Adam Maidman <maidm004@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19796,
    "package_name": "plasso",
    "title": "Cross-Validated Post-Lasso",
    "description": "Provides tools for cross-validated Lasso and Post-Lasso estimation.\n  Built on top of the 'glmnet' package by Friedman, Hastie and Tibshirani (2010) <doi:10.18637/jss.v033.i01>,\n  the main function plasso() extends the standard 'glmnet' output with coefficient paths\n  for Post-Lasso models, while cv.plasso() performs cross-validation for both Lasso\n  and Post-Lasso models and different ways to select the penalty parameter lambda as discussed in Knaus (2021) <doi:10.1111/rssa.12623>.",
    "version": "0.1.3",
    "maintainer": "Michael C. Knaus <michael.knaus@uni-tuebingen.de>",
    "url": "https://github.com/MCKnaus/plasso",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19805,
    "package_name": "plde",
    "title": "Penalized Log-Density Estimation Using Legendre Polynomials",
    "description": "We present a penalized log-density estimation method using Legendre polynomials with lasso penalty to adjust estimate's smoothness. Re-expressing the logarithm of the density estimator via a linear combination of Legendre polynomials, we can estimate parameters by maximizing the penalized log-likelihood function. Besides, we proposed an implementation strategy that builds on the coordinate decent algorithm, together with the Bayesian information criterion (BIC).",
    "version": "0.1.2",
    "maintainer": "JungJun Lee <ljjoj@korea.ac.kr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19820,
    "package_name": "plm",
    "title": "Linear Models for Panel Data",
    "description": "A set of estimators for models and (robust) covariance matrices, and tests for panel data\n             econometrics, including within/fixed effects, random effects, between, first-difference, \n             nested random effects as well as instrumental-variable (IV) and Hausman-Taylor-style models,\n             panel generalized method of moments (GMM) and general FGLS models,\n             mean groups (MG), demeaned MG, and common correlated effects (CCEMG) and pooled (CCEP) estimators\n             with common factors, variable coefficients and limited dependent variables models.\n             Test functions include model specification, serial correlation, cross-sectional dependence,\n             panel unit root and panel Granger (non-)causality. Typical references are general econometrics \n             text books such as Baltagi (2021), Econometric Analysis of Panel Data (<doi:10.1007/978-3-030-53953-5>),\n             Hsiao (2014), Analysis of Panel Data (<doi:10.1017/CBO9781139839327>), and Croissant and Millo (2018), \n             Panel Data Econometrics with R (<doi:10.1002/9781119504641>).",
    "version": "2.6-7",
    "maintainer": "Kevin Tappe <kevin.tappe@bwi.uni-stuttgart.de>",
    "url": "https://cran.r-project.org/package=plm,\nhttps://github.com/ycroissant/plm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19835,
    "package_name": "plotMCMC",
    "title": "MCMC Diagnostic Plots",
    "description": "Markov chain Monte Carlo diagnostic plots. The purpose of the\n  package is to combine existing tools from the 'coda' and 'lattice' packages,\n  and make it easy to adjust graphical details.",
    "version": "2.0.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19838,
    "package_name": "plotSEMM",
    "title": "Graphing Nonlinear Relations Among Latent Variables from\nStructural Equation Mixture Models",
    "description": "Contains a graphical user interface to generate the diagnostic\n    plots proposed by Bauer (2005; <doi:10.1207/s15328007sem1204_1>), \n    Pek & Chalmers (2015; <doi:10.1080/10705511.2014.937790>), and\n    Pek, Chalmers, R. Kok, & Losardo (2015; <doi:10.3102/1076998615589129>) to investigate\n    nonlinear bivariate relationships in latent regression models using structural\n    equation mixture models (SEMMs).",
    "version": "2.4",
    "maintainer": "Phil Chalmers <rphilip.chalmers@gmail.com>",
    "url": "https://github.com/philchalmers/plotSEMM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19843,
    "package_name": "plotfunctions",
    "title": "Various Functions to Facilitate Visualization of Data and\nAnalysis",
    "description": "When analyzing data, plots are a helpful tool for visualizing data and interpreting statistical models. This package provides a set of simple tools for building plots incrementally, starting with an empty plot region, and adding bars, data points, regression lines, error bars, gradient legends, density distributions in the margins, and even pictures. The package builds further on R graphics by simply combining functions and settings in order to reduce the amount of code to produce for the user. As a result, the package does not use formula input or special syntax, but can be used in combination with default R plot functions. Note: Most of the functions were part of the package 'itsadug', which is now split in two packages: 1. the package 'itsadug', which contains the core functions for visualizing and evaluating nonlinear regression models, and 2. the package 'plotfunctions', which contains more general plot functions.",
    "version": "1.5",
    "maintainer": "Jacolien van Rij <j.c.van.rij@rug.nl>",
    "url": "https://jacolienvanrij.com/tutorials.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19845,
    "package_name": "plotlsirm",
    "title": "Plot Toolkit for Latent Space Item Response Models",
    "description": "Provides publication‑quality and interactive plots for exploring \n             the posterior output of Latent Space Item Response Models, including \n             Posterior Interaction Profiles, radar charts, 2‑D latent maps, and \n             item‑similarity heat maps. The methods implemented in this package \n             are based on work by Jeon, M., Jin, I. H., Schweinberger, M., Baugh, S. (2021) <doi:10.1007/s11336-021-09762-5>.",
    "version": "0.1.3",
    "maintainer": "Jinwen Luo <jevanluo@ucla.edu>",
    "url": "https://github.com/jevanluo/plotlsirm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19861,
    "package_name": "plpoisson",
    "title": "Prediction Limits for Poisson Distribution",
    "description": "Prediction limits for the Poisson distribution\n  are produced from both frequentist and Bayesian viewpoints. Limiting results\n  are provided in a Bayesian setting with uniform, Jeffreys and gamma as prior\n  distributions. More details on the methodology are discussed in Bejleri and\n  Nandram (2018) <doi:10.1080/03610926.2017.1373814> and Bejleri, Sartore and\n  Nandram (2021) <doi:10.1007/s42952-021-00157-x>.",
    "version": "0.3.1",
    "maintainer": "Luca Sartore <drwolf85@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19862,
    "package_name": "pls",
    "title": "Partial Least Squares and Principal Component Regression",
    "description": "Multivariate regression methods\n\tPartial Least Squares Regression (PLSR), Principal Component\n\tRegression (PCR) and Canonical Powered Partial Least Squares (CPPLS).",
    "version": "2.8-5",
    "maintainer": "Kristian Hovde Liland <kristian.liland@nmbu.no>",
    "url": "https://github.com/khliland/pls",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19863,
    "package_name": "plsRbeta",
    "title": "Partial Least Squares Regression for Beta Regression Models",
    "description": "Provides Partial least squares Regression for (weighted) beta regression models (Bertrand 2013,  <https://ojs-test.apps.ocp.math.cnrs.fr/index.php/J-SFdS/article/view/215>) and k-fold cross-validation of such models using various criteria. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available.",
    "version": "0.3.2",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/plsRbeta/,\nhttps://github.com/fbertran/plsRbeta/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19864,
    "package_name": "plsRcox",
    "title": "Partial Least Squares Regression for Cox Models and Related\nTechniques",
    "description": "Provides Partial least squares Regression and various regular, sparse or kernel, techniques for fitting Cox models in high dimensional settings <doi:10.1093/bioinformatics/btu660>, Bastien, P., Bertrand, F., Meyer N., Maumy-Bertrand, M. (2015), Deviance residuals-based sparse PLS and sparse kernel PLS regression for censored data, Bioinformatics, 31(3):397-404. Cross validation criteria were studied in <doi:10.48550/arXiv.1810.02962>, Bertrand, F., Bastien, Ph. and Maumy-Bertrand, M. (2018), Cross validating extensions of kernel, sparse or regular partial least squares regression models to censored data.",
    "version": "1.8.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/plsRcox/,\nhttps://github.com/fbertran/plsRcox/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19865,
    "package_name": "plsRglm",
    "title": "Partial Least Squares Regression for Generalized Linear Models",
    "description": "Provides (weighted) Partial least squares Regression for generalized linear models and repeated k-fold cross-validation of such models using various criteria <doi:10.48550/arXiv.1810.01005>. It allows for missing data in the explanatory variables. Bootstrap confidence intervals constructions are also available.",
    "version": "1.6.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/plsRglm/,\nhttps://github.com/fbertran/plsRglm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19867,
    "package_name": "plsdepot",
    "title": "Partial Least Squares (PLS) Data Analysis Methods",
    "description": "Different methods for PLS analysis of one or two data tables such as Tucker's Inter-Battery, NIPALS, SIMPLS, SIMPLS-CA, PLS Regression, and PLS Canonical Analysis. The main reference for this software is the awesome book (in French) 'La Regression PLS: Theorie et Pratique' by Michel Tenenhaus.",
    "version": "0.3.0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://github.com/gastonstat/plsdepot/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19868,
    "package_name": "plsdof",
    "title": "Degrees of Freedom and Statistical Inference for Partial Least\nSquares Regression",
    "description": "The plsdof package provides Degrees of Freedom estimates\n        for Partial Least Squares (PLS) Regression. Model selection for\n        PLS is based on various information criteria (aic, bic, gmdl)\n        or on cross-validation. Estimates for the mean and covariance\n        of the PLS regression coefficients are available. They allow\n        the construction of approximate confidence intervals and the\n        application of test procedures (Kramer and Sugiyama \n        2012 <doi:10.1198/jasa.2011.tm10107>).\n        Further, cross-validation procedures for Ridge Regression and \n        Principal Components Regression are available.",
    "version": "0.4-0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://github.com/fbertran/plsdof/,\nhttps://fbertran.github.io/plsdof/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19893,
    "package_name": "pmcalibration",
    "title": "Calibration Curves for Clinical Prediction Models",
    "description": "Fit calibrations curves for clinical prediction models and calculate several associated \n  metrics (Eavg, E50, E90, Emax). Ideally predicted probabilities from a prediction model \n  should align with observed probabilities. Calibration curves relate predicted probabilities \n  (or a transformation thereof) to observed outcomes via a flexible non-linear smoothing function. \n  'pmcalibration' allows users to choose between several smoothers (regression splines, generalized \n  additive models/GAMs, lowess, loess). Both binary and time-to-event outcomes are supported. \n  See Van Calster et al. (2016) <doi:10.1016/j.jclinepi.2015.12.005>; \n  Austin and Steyerberg (2019) <doi:10.1002/sim.8281>; \n  Austin et al. (2020) <doi:10.1002/sim.8570>.",
    "version": "0.2.0",
    "maintainer": "Stephen Rhodes <steverho89@gmail.com>",
    "url": "https://github.com/stephenrho/pmcalibration",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19911,
    "package_name": "pmsampsize",
    "title": "Sample Size for Development of a Prediction Model",
    "description": "Computes the minimum sample size required for the development of a new multivariable prediction model using the criteria proposed by Riley et al. (2018) <doi: 10.1002/sim.7992>. pmsampsize can be used to calculate the minimum sample size for the development of models with continuous, binary or survival (time-to-event) outcomes. Riley et al. (2018) <doi: 10.1002/sim.7992> lay out a series of criteria the sample size should meet. These aim to minimise the overfitting and to ensure precise estimation of key parameters in the prediction model.",
    "version": "1.1.3",
    "maintainer": "Joie Ensor <j.ensor@bham.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19912,
    "package_name": "pmsesampling",
    "title": "Sample Size Determination for Accurate Predictive Linear\nRegression",
    "description": "Provides analytic and simulation tools to estimate the minimum\n    sample size required for achieving a target prediction mean-squared error\n    (PMSE) or a specified proportional PMSE reduction (pPMSEr) in linear\n    regression models.  Functions implement the criteria of Ma (2023) \n    <https://digital.wpi.edu/downloads/0g354j58c>, support covariance-matrix \n    handling, and include helpers for root-finding and diagnostic plotting.",
    "version": "0.1.1",
    "maintainer": "Louis Chen <chenaters@gmail.com>",
    "url": "https://github.com/Chenaters/pmsesampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19924,
    "package_name": "poLCA",
    "title": "Polytomous Variable Latent Class Analysis",
    "description": "Latent class analysis and latent class regression models \n    for polytomous outcome variables.  Also known as latent structure analysis.",
    "version": "1.6.0.1",
    "maintainer": "Drew Linzer <drew@votamatic.org>",
    "url": "https://github.com/dlinzer/poLCA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19933,
    "package_name": "pointRes",
    "title": "Analyzing Pointer Years and Components of Resilience",
    "description": "Functions to calculate and plot event and pointer years as well as resilience indices. Designed for dendroecological applications, but also suitable to analyze patterns in other ecological time series.",
    "version": "2.0.2",
    "maintainer": "Marieke van der Maaten-Theunissen <marieke.theunissen@tu-dresden.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19963,
    "package_name": "polspline",
    "title": "Polynomial Spline Routines",
    "description": "Routines for the polynomial spline fitting routines\n  hazard regression, hazard estimation with flexible tails, logspline,\n  lspec, polyclass, and polymars, by C. Kooperberg and co-authors.",
    "version": "1.1.25",
    "maintainer": "Charles Kooperberg <clk@fredhutch.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19968,
    "package_name": "polySegratioMM",
    "title": "Bayesian Mixture Models for Marker Dosage in Autopolyploids",
    "description": "Fits Bayesian mixture models to estimate marker dosage for dominant markers in autopolyploids using JAGS (1.0 or greater) as outlined in Baker et al \"Bayesian estimation of marker dosage in sugarcane and other autopolyploids\" (2010, <doi:10.1007/s00122-010-1283-z>). May be used in conjunction with polySegratio for simulation studies and comparison with standard methods.",
    "version": "0.6-4",
    "maintainer": "Peter Baker <p.baker1@uq.edu.au>",
    "url": "https://github.com/petebaker/polysegratiomm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19970,
    "package_name": "polyapost",
    "title": "Simulating from the Polya Posterior",
    "description": "Simulate via Markov chain Monte Carlo (hit-and-run algorithm)\n    a Dirichlet distribution conditioned to satisfy a finite set of linear\n    equality and inequality constraints (hence to lie in a convex polytope\n    that is a subset of the unit simplex).",
    "version": "1.7-1",
    "maintainer": "Glen Meeden <gmeeden@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19975,
    "package_name": "polykde",
    "title": "Polyspherical Kernel Density Estimation",
    "description": "Kernel density estimation on the polysphere, (hyper)sphere, and\n    circle. Includes functions for density estimation, regression estimation,\n    ridge estimation, bandwidth selection, kernels, samplers, and homogeneity\n    tests. Companion package to García-Portugués and Meilán-Vila (2025)\n    <doi:10.1080/01621459.2025.2521898> and García-Portugués and Meilán-Vila\n    (2023) <doi:10.1007/978-3-031-32729-2_4>.",
    "version": "1.1.7",
    "maintainer": "Eduardo García-Portugués <edgarcia@est-econ.uc3m.es>",
    "url": "https://github.com/egarpor/polykde",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 19985,
    "package_name": "polywog",
    "title": "Bootstrapped Basis Regression with Oracle Model Selection",
    "description": "Routines for flexible functional form estimation via basis\n    regression, with model selection via the adaptive LASSO or SCAD to prevent\n    overfitting.",
    "version": "0.4-2",
    "maintainer": "Brenton Kenkel <brenton.kenkel@gmail.com>",
    "url": "https://github.com/brentonk/polywog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20029,
    "package_name": "portes",
    "title": "Portmanteau Tests for Time Series Models",
    "description": "Contains common univariate and multivariate portmanteau test statistics for time series models. These tests are based on using asymptotic distributions such as chi-square distribution and based on using the Monte Carlo significance tests. Also, it can be used to simulate from univariate and multivariate seasonal time series models.",
    "version": "6.0",
    "maintainer": "Esam Mahdi <emahdi2012@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20035,
    "package_name": "portvine",
    "title": "Vine Based (Un)Conditional Portfolio Risk Measure Estimation",
    "description": "Following Sommer (2022) <https://mediatum.ub.tum.de/1658240>\n    portfolio level risk estimates (e.g. Value at Risk, Expected\n    Shortfall) are estimated by modeling each asset univariately by an\n    ARMA-GARCH model and then their cross dependence via a Vine Copula\n    model in a rolling window fashion. One can even condition on\n    variables/time series at certain quantile levels to stress test the\n    risk measure estimates.",
    "version": "1.0.3",
    "maintainer": "Emanuel Sommer <emanuel_sommer@gmx.de>",
    "url": "https://github.com/EmanuelSommer/portvine,\nhttps://emanuelsommer.github.io/portvine/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20036,
    "package_name": "poset",
    "title": "Analysis of Partially Ordered Data",
    "description": "Win ratio approach to partially ordered data, such as multivariate ordinal \n  responses under product (consensus) or prioritized order. Two-sample tests and multiplicative\n  regression models are implemented (Mao, 2024, under revision).",
    "version": "1.0.0",
    "maintainer": "Lu Mao <lmao@biostat.wisc.edu>",
    "url": "https://sites.google.com/view/lmaowisc/,\nhttps://lmaowisc.github.io/poset/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20050,
    "package_name": "postpack",
    "title": "Utilities for Processing Posterior Samples Stored in\n'mcmc.lists'",
    "description": "The aim of 'postpack' is to provide the infrastructure for a standardized workflow for 'mcmc.list' objects.\n    These objects can be used to store output from models fitted with Bayesian inference using\n    'JAGS', 'WinBUGS', 'OpenBUGS', 'NIMBLE', 'Stan', or even custom MCMC algorithms. Although the 'coda' R package provides\n    some methods for these objects, it is somewhat limited in easily performing post-processing tasks for\n    specific nodes. Models are ever increasing in their complexity and the number of tracked nodes, and oftentimes\n    a user may wish to summarize/diagnose sampling behavior for only a small subset of nodes at a time\n    for a particular question or figure. Thus, many 'postpack' functions support performing tasks on a\n    subset of nodes, where the subset is specified with regular expressions. The functions in 'postpack'\n    streamline the extraction, summarization, and diagnostics of specific monitored nodes after model fitting.\n    Further, because there is rarely only ever one model under consideration, 'postpack' scales efficiently \n    to perform the same tasks on output from multiple models simultaneously, facilitating rapid assessment \n    of model sensitivity to changes in assumptions.",
    "version": "0.5.4",
    "maintainer": "Ben Staton <statonbe@gmail.com>",
    "url": "https://bstaton1.github.io/postpack/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20052,
    "package_name": "poth",
    "title": "Precision of Treatment Hierarchy (POTH)",
    "description": "Calculate POTH for treatment hierarchies from frequentist and Bayesian network meta-analysis. POTH quantifies the certainty in a treatment hierarchy. Subset POTH, POTH residuals, and best k treatments POTH can also be calculated to improve interpretation of treatment hierarchies.",
    "version": "0.3-0",
    "maintainer": "Augustine Wigle <amhwigle@uwaterloo.ca>",
    "url": "https://github.com/augustinewigle/poth",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20055,
    "package_name": "potts",
    "title": "Markov Chain Monte Carlo for Potts Models",
    "description": "Do Markov chain Monte Carlo (MCMC) simulation of Potts models\n   (Potts, 1952, <doi:10.1017/S0305004100027419>),\n   which are the multi-color generalization of Ising models\n   (so, as as special case, also simulates Ising models).\n   Use the Swendsen-Wang algorithm (Swendsen and Wang, 1987,\n   <doi:10.1103/PhysRevLett.58.86>) so MCMC is fast.\n   Do maximum composite likelihood estimation of parameters\n   (Besag, 1975, <doi:10.2307/2987782>,\n   Lindsay, 1988, <doi:10.1090/conm/080>).",
    "version": "0.5-11",
    "maintainer": "Charles J. Geyer <charlie@stat.umn.edu>",
    "url": "http://www.stat.umn.edu/geyer/mcmc/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20063,
    "package_name": "power4mome",
    "title": "Power Analysis for Moderation and Mediation",
    "description": "Power analysis and sample size determination\n  for moderation, mediation, and moderated mediation in models\n  fitted by structural equation modelling using the 'lavaan'\n  package by Rosseel (2012) <doi:10.18637/jss.v048.i02> or\n  by multiple regression. The package 'manymome' by\n  Cheung and Cheung (2024) <doi:10.3758/s13428-023-02224-z>\n  is used to specify the indirect paths or conditional\n  indirect paths to be tested.",
    "version": "0.1.1",
    "maintainer": "Shu Fai Cheung <shufai.cheung@gmail.com>",
    "url": "https://sfcheung.github.io/power4mome/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20064,
    "package_name": "powerCompRisk",
    "title": "Power Analysis Tool for Joint Testing Hazards with Competing\nRisks Data",
    "description": "A power analysis tool for jointly testing the cause-1 cause-specific hazard and the any-cause hazard with competing risks data.",
    "version": "1.0.1",
    "maintainer": "Eric Kawaguchi <erickawaguchi@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20065,
    "package_name": "powerEQTL",
    "title": "Power and Sample Size Calculation for Bulk Tissue and\nSingle-Cell eQTL Analysis",
    "description": "Power and sample size calculation for bulk tissue and single-cell eQTL analysis\n             based on ANOVA, simple linear regression, or linear mixed effects model. It can also calculate power/sample size \n             for testing the association of a SNP to a continuous type phenotype.\n             Please see the reference: Dong X, Li X, Chang T-W, Scherzer CR, Weiss ST, Qiu W. (2021) <doi:10.1093/bioinformatics/btab385>.",
    "version": "0.3.6",
    "maintainer": "Weiliang Qiu <weiliang.qiu@gmail.com>",
    "url": "https://github.com/sterding/powerEQTL,\nhttps://bwhbioinfo.shinyapps.io/powerEQTL/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20070,
    "package_name": "powerNLSEM",
    "title": "Simulation-Based Power Estimation (MSPE) for Nonlinear SEM",
    "description": "Model-implied simulation-based power estimation (MSPE) for\n    nonlinear (and linear) SEM, path analysis and regression analysis. A\n    theoretical framework is used to approximate the relation between\n    power and sample size for given type I error rates and effect sizes.\n    The package offers an adaptive search algorithm to find the optimal N for\n    given effect sizes and type I error rates. Plots can be used to visualize\n    the power relation to N for different parameters of interest (POI). \n    Theoretical justifications are given in Irmer et al. \n    (2024a) <doi:10.31219/osf.io/pe5bj> and detailed description\n    are given in Irmer et al. (2024b) <doi:10.3758/s13428-024-02476-3>.",
    "version": "0.1.2",
    "maintainer": "Julien Patrick Irmer <jpirmer@gmail.com>",
    "url": "https://github.com/jpirmer/powerNLSEM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20075,
    "package_name": "powerbrmsINLA",
    "title": "Bayesian Power Analysis Using 'brms' and 'INLA'",
    "description": "Provides tools for Bayesian power analysis and assurance calculations using the statistical frameworks of 'brms' and 'INLA'. Includes simulation-based approaches, support for multiple decision rules (direction, threshold, ROPE), sequential designs, and visualisation helpers. Methods are based on Kruschke (2014, ISBN:9780124058880) \"Doing Bayesian Data Analysis: A Tutorial with R, JAGS, and Stan\", O'Hagan & Stevens (2001) <doi:10.1177/0272989X0102100307> \"Bayesian Assessment of Sample Size for Clinical Trials of Cost-Effectiveness\", Kruschke (2018) <doi:10.1177/2515245918771304> \"Rejecting or Accepting Parameter Values in Bayesian Estimation\", Rue et al. (2009) <doi:10.1111/j.1467-9868.2008.00700.x> \"Approximate Bayesian inference for latent Gaussian models by using integrated nested Laplace approximations\", and Bürkner (2017) <doi:10.18637/jss.v080.i01> \"brms: An R Package for Bayesian Multilevel Models using Stan\".",
    "version": "1.1.1",
    "maintainer": "Tony Myers <admyers@aol.com>",
    "url": "https://github.com/Tony-Myers/powerbrmsINLA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20082,
    "package_name": "powerprior",
    "title": "Conjugate Power Priors for Bayesian Analysis of Normal Data",
    "description": "Implements conjugate power priors for efficient Bayesian analysis \n    of normal data. Power priors allow principled incorporation of historical \n    information while controlling the degree of borrowing through a discounting \n    parameter (Ibrahim and Chen (2000) <doi:10.1214/ss/1009212519>). This \n    package provides closed-form conjugate representations for both univariate \n    and multivariate normal data using Normal-Inverse-Chi-squared and \n    Normal-Inverse-Wishart distributions, eliminating the need for MCMC sampling. \n    The conjugate framework builds upon standard Bayesian methods described in \n    Gelman et al. (2013, ISBN:978-1439840955).",
    "version": "1.0.0",
    "maintainer": "Yusuke Yamaguchi <yamagubed@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20085,
    "package_name": "ppRep",
    "title": "Analysis of Replication Studies using Power Priors",
    "description": "Provides functionality for Bayesian analysis of replication studies using power prior approaches (Pawel et al., 2023) <doi:10.1007/s11749-023-00888-5>.",
    "version": "0.42.3",
    "maintainer": "Samuel Pawel <samuel.pawel@uzh.ch>",
    "url": "https://github.com/SamCH93/ppRep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20099,
    "package_name": "ppls",
    "title": "Penalized Partial Least Squares",
    "description": "Linear and nonlinear regression\n        methods based on Partial Least Squares and Penalization\n        Techniques. Model parameters are selected via cross-validation,\n        and confidence intervals ans tests for the regression\n        coefficients can be conducted via jackknifing. \n        The method is described and applied to simulated and experimental \n        data in Kraemer et al. (2008) <doi:10.1016/j.chemolab.2008.06.009>.",
    "version": "2.0.0",
    "maintainer": "Vincent Guillemot <vincent.guillemot@pasteur.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20100,
    "package_name": "ppmHR",
    "title": "Privacy-Protecting Hazard Ratio Estimation in Distributed Data\nNetworks",
    "description": "An implementation of the one-step privacy-protecting method for estimating the overall and site-specific hazard ratios using inverse probability weighted Cox models in distributed data network studies, as proposed by Shu, Yoshida, Fireman, and Toh (2019) <doi: 10.1177/0962280219869742>. This method only requires sharing of summary-level riskset tables instead of individual-level data. Both the conventional inverse probability weights and the stabilized weights are implemented. ",
    "version": "1.0",
    "maintainer": "Di Shu <shudi1991@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20107,
    "package_name": "ppseq",
    "title": "Design Clinical Trials using Sequential Predictive Probability\nMonitoring",
    "description": "Functions are available to calibrate designs over a range of posterior and predictive thresholds, to plot the various design options, and to obtain the operating characteristics of optimal accuracy and optimal efficiency designs.",
    "version": "0.2.5",
    "maintainer": "Emily C. Zabor <zabore2@ccf.org>",
    "url": "https://github.com/zabore/ppseq, https://www.emilyzabor.com/ppseq/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20112,
    "package_name": "pqrfe",
    "title": "Penalized Quantile Regression with Fixed Effects",
    "description": "Quantile regression with fixed effects is a general model for longitudinal data. Here we proposed to solve it by several methods. The estimation methods include three loss functions as check, asymmetric least square and asymmetric Huber functions; and three structures as simple regression, fixed effects and fixed effects with penalized intercepts by LASSO.    ",
    "version": "1.3",
    "maintainer": "Ian Meneghel Danilevicz <iandanilevicz@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20117,
    "package_name": "pracma",
    "title": "Practical Numerical Math Functions",
    "description": "\n    Provides a large number of functions from numerical analysis and\n    linear algebra, numerical optimization, differential equations,\n    time series, plus some well-known special mathematical functions.\n    Uses 'MATLAB' function names where appropriate to simplify porting.",
    "version": "2.4.6",
    "maintainer": "Hans W. Borchers <hwborchers@googlemail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20122,
    "package_name": "prais",
    "title": "Prais-Winsten Estimator for AR(1) Serial Correlation",
    "description": "The Prais-Winsten estimator (Prais & Winsten, 1954) takes into account AR(1) serial correlation of the errors in a linear regression model. The procedure recursively estimates the coefficients and the error autocorrelation of the specified model until sufficient convergence of the AR(1) coefficient is attained.",
    "version": "1.1.4",
    "maintainer": "Franz X. Mohr <franz.x.mohr@outlook.com>",
    "url": "https://github.com/franzmohr/prais",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20143,
    "package_name": "predict3d",
    "title": "Draw Three Dimensional Predict Plot Using Package 'rgl'",
    "description": "Draw 2 dimensional and three dimensional plot for multiple regression models using package 'ggplot2' and 'rgl'.\n   Supports linear models (lm), generalized linear models (glm) and local polynomial regression fittings (loess).  ",
    "version": "0.1.6",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "url": "https://github.com/cardiomoon/predict3d",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20148,
    "package_name": "predictmeans",
    "title": "Predicted Means for Linear and Semiparametric Models",
    "description": "Providing functions to diagnose and make inferences from various linear models, \n    such as those obtained from 'aov', 'lm', 'glm', 'gls', 'lme', 'lmer', 'glmmTMB' and 'semireg'. \n\tInferences include predicted means and standard errors, contrasts, multiple comparisons, \n\tpermutation tests, adjusted R-square and graphs.",
    "version": "1.1.1",
    "maintainer": "Dongwen Luo <dongwen.luo@agresearch.co.nz>",
    "url": "https://CRAN.R-project.org/package=predictmeans",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20156,
    "package_name": "predtoolsTS",
    "title": "Time Series Prediction Tools",
    "description": "Makes the time series prediction easier by automatizing this process\n  using four main functions: prep(), modl(), pred() and postp(). Features different\n  preprocessing methods to homogenize variance and to remove trend and seasonality.\n  Also has the potential to bring together different predictive models to make comparatives.\n  Features ARIMA and Data Mining Regression models (using caret).",
    "version": "0.1.1",
    "maintainer": "Alberto Vico Moreno <avm00016@red.ujaen.es>",
    "url": "https://github.com/avm00016/predtoolsTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20160,
    "package_name": "preference",
    "title": "2-Stage Preference Trial Design and Analysis",
    "description": "Design and analyze two-stage randomized trials with a continuous\n    outcome measure. The package contains functions to compute the required \n    sample size needed to detect a given preference, treatment, and selection \n    effect; alternatively, the package contains functions that can report the \n    study power given a fixed sample size. Finally, analysis functions are \n    provided to test each effect using either summary data (i.e. means, \n    variances) or raw study data <doi:10.18637/jss.v094.c02>.",
    "version": "1.1.6",
    "maintainer": "Michael Kane <michael.kane@yale.edu>",
    "url": "https://github.com/kaneplusplus/preference",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20166,
    "package_name": "prepost",
    "title": "Non-Parametric Bounds and Gibbs Sampler for Assessing Priming\nand Post-Treatment Bias",
    "description": "A set of tools to implement the non-parametric bounds and Bayesian methods for assessing post-treatment bias developed in Blackwell, Brown, Hill, Imai, and Yamamoto (2025) <doi:10.1017/pan.2025.3>.",
    "version": "0.3.0",
    "maintainer": "Matthew Blackwell <mblackwell@gmail.com>",
    "url": "https://github.com/mattblackwell/prepost,\nhttps://mattblackwell.github.io/prepost/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20193,
    "package_name": "prevalence",
    "title": "Tools for Prevalence Assessment Studies",
    "description": "The prevalence package provides Frequentist and Bayesian methods for prevalence assessment studies. IMPORTANT: the truePrev functions in the prevalence package call on JAGS (Just Another Gibbs Sampler), which therefore has to be available on the user's system. JAGS can be downloaded from <https://mcmc-jags.sourceforge.io/>.",
    "version": "0.4.1",
    "maintainer": "Brecht Devleesschauwer <brechtdv@gmail.com>",
    "url": "http://prevalence.cbra.be/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20198,
    "package_name": "priceR",
    "title": "Economics and Pricing Tools",
    "description": "Functions to aid in micro and macro economic analysis and handling of price and",
    "version": "1.0.3",
    "maintainer": "Steve Condylios <steve.condylios@gmail.com>",
    "url": "https://github.com/stevecondylios/priceR",
    "exports": [],
    "topics": ["cran", "data-science", "econometrics", "economics", "finance", "modeling", "r-programming", "statistics"],
    "score": "NA",
    "stars": 62
  },
  {
    "id": 20240,
    "package_name": "probe",
    "title": "Sparse High-Dimensional Linear Regression with PROBE",
    "description": "Implements an efficient and powerful Bayesian approach for sparse high-dimensional linear regression. It uses minimal prior assumptions on the parameters through plug-in empirical Bayes estimates of hyperparameters. An efficient Parameter-Expanded Expectation-Conditional-Maximization (PX-ECM) algorithm estimates maximum a posteriori (MAP) values of regression parameters and variable selection probabilities. The PX-ECM results in a robust computationally efficient coordinate-wise optimization, which adjusts for the impact of other predictor variables. The E-step is motivated by the popular two-group approach to multiple testing. The result is a PaRtitiOned empirical Bayes Ecm (PROBE) algorithm applied to sparse high-dimensional linear regression, implemented using one-at-a-time or all-at-once type optimization. More information can be found in McLain, Zgodic, and Bondell (2022) <arXiv:2209.08139>.",
    "version": "1.1",
    "maintainer": "Alexander McLain <mclaina@mailbox.sc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20257,
    "package_name": "prodlim",
    "title": "Product-Limit Estimation for Censored Event History Analysis",
    "description": "Fast and user friendly implementation of nonparametric estimators\n    for censored event history (survival) analysis. Kaplan-Meier and\n    Aalen-Johansen method.",
    "version": "2025.04.28",
    "maintainer": "Thomas A. Gerds <tag@biostat.ku.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20258,
    "package_name": "productivity",
    "title": "Indices of Productivity Using Data Envelopment Analysis (DEA)",
    "description": "\n  Levels and changes of productivity and profitability are measured with various indices.\n  The package contains the multiplicatively complete Färe-Primont, Fisher, Hicks-Moorsteen, \n  Laspeyres, Lowe, and Paasche indices, as well as the classic Malmquist productivity index.\n  Färe-Primont and Lowe indices verify the transitivity property and can therefore be used for \n  multilateral or multitemporal comparison.\n  Fisher, Hicks-Moorsteen, Laspeyres, Malmquist, and Paasche indices are not transitive and are \n  only to be used for binary comparison.\n  All indices can also be decomposed into different components, providing insightful information \n  on the sources of productivity and profitability changes.\n  In the use of Malmquist productivity index, the technological change index can be further \n  decomposed into bias technological change components.\n  The package also allows to prohibit technological regression (negative technological change). In \n  the case of the Fisher, Hicks-Moorsteen, Laspeyres, Paasche and the transitive Färe-Primont \n  and Lowe indices, it is furthermore possible to rule out technological change. \n  Deflated shadow prices can also be obtained. Besides, the package allows parallel computing as \n  an option, depending on the user's computer configuration. \n  All computations are carried out with the nonparametric Data Envelopment Analysis (DEA), and \n  several assumptions regarding returns to scale are available.\n  All DEA linear programs are implemented using 'lp_solve'.",
    "version": "1.1.0",
    "maintainer": "Yann Desjeux <yann.desjeux@inra.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20264,
    "package_name": "profileModel",
    "title": "Profiling Inference Functions for Various Model Classes",
    "description": "Provides tools that can be used to calculate, evaluate, plot and use for inference the profiles of *arbitrary* inference functions for *arbitrary* 'glm'-like fitted models with linear predictors. More information on the methods that are implemented can be found in Kosmidis (2008) <https://www.r-project.org/doc/Rnews/Rnews_2008-2.pdf>.",
    "version": "0.6.1",
    "maintainer": "Ioannis Kosmidis <ioannis.kosmidis@warwick.ac.uk>",
    "url": "https://github.com/ikosmidis/profileModel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20269,
    "package_name": "profoc",
    "title": "Probabilistic Forecast Combination Using CRPS Learning",
    "description": "Combine probabilistic forecasts using CRPS learning algorithms proposed in Berrisch, Ziel (2021) <doi:10.48550/arXiv.2102.00968> <doi:10.1016/j.jeconom.2021.11.008>. The package implements multiple online learning algorithms like Bernstein online aggregation; see Wintenberger (2014) <doi:10.48550/arXiv.1404.1356>. Quantile regression is also implemented for comparison purposes. Model parameters can be tuned automatically with respect to the loss of the forecast combination. Methods like predict(), update(), plot() and print() are available for convenience. This package utilizes the optim C++ library for numeric optimization <https://github.com/kthohr/optim>.",
    "version": "1.3.3",
    "maintainer": "Jonathan Berrisch <Jonathan@Berrisch.biz>",
    "url": "https://profoc.berrisch.biz, https://github.com/BerriJ/profoc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20286,
    "package_name": "prolsirm",
    "title": "Procrustes Matching for Latent Space Item Response Model",
    "description": "Procrustes matching of the posterior samples of person and item latent positions from latent space item response models. The methods implemented in this package are based on work by Borg, I., Groenen, P. (1997, ISBN:978-0-387-94845-4), Jeon, M., Jin, I. H., Schweinberger, M., Baugh, S. (2021) <doi:10.1007/s11336-021-09762-5>, and Andrew, D. M., Kevin M. Q., Jong Hee Park. (2011) <doi:10.18637/jss.v042.i09>.",
    "version": "0.1.1",
    "maintainer": "Jinwen Luo <jevanluo@ucla.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20301,
    "package_name": "propertee",
    "title": "Standardization-Based Effect Estimation with Optional Prior\nCovariance Adjustment",
    "description": "The Prognostic Regression Offsets with Propagation of\n    ERrors (for Treatment Effect Estimation) package facilitates\n    direct adjustment for experiments and observational studies that\n    is compatible with a range of study designs and covariance\n    adjustment strategies. It uses explicit specification of clusters,\n    blocks and treatment allocations to furnish probability of\n    assignment-based weights targeting any of several average\n    treatment effect parameters, and for standard error calculations\n    reflecting these design parameters. For covariance adjustment of\n    its Hajek and (one-way) fixed effects estimates, it enables\n    offsetting the outcome against predictions from a dedicated\n    covariance model, with standard error calculations propagating\n    error as appropriate from the covariance model.",
    "version": "1.0.3",
    "maintainer": "Josh Errickson <jerrick@umich.edu>",
    "url": "https://github.com/benbhansen-stats/propertee",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20337,
    "package_name": "proxysnps",
    "title": "Get proxy SNPs for a SNP in the 1000 Genomes Project",
    "description": "This package implements functions to query remote VCF files. You",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/slowkow/proxysnps",
    "exports": [],
    "topics": ["bioinformatics", "linkage-disequilibrium", "rstats", "snps", "statistics"],
    "score": "NA",
    "stars": 30
  },
  {
    "id": 20348,
    "package_name": "psbcGroup",
    "title": "Penalized Parametric and Semiparametric Bayesian Survival Models\nwith Shrinkage and Grouping Priors",
    "description": "Algorithms to implement various Bayesian penalized survival regression models including: semiparametric proportional hazards models with lasso priors (Lee et al., Int J Biostat, 2011 <doi:10.2202/1557-4679.1301>) and three  other shrinkage and group priors (Lee et al., Stat Anal Data Min, 2015 <doi:10.1002/sam.11266>); parametric accelerated failure time models with group/ordinary lasso prior (Lee et al. Comput Stat Data Anal, 2017 <doi:10.1016/j.csda.2017.02.014>).",
    "version": "1.7",
    "maintainer": "Kyu Ha Lee <klee@hsph.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20352,
    "package_name": "pscl",
    "title": "Political Science Computational Laboratory",
    "description": "Bayesian analysis of item-response theory (IRT) models,\n  roll call analysis; computing highest density regions; \n  maximum likelihood estimation of zero-inflated and hurdle models for count data;\n  goodness-of-fit measures for GLMs;\n  data sets used in writing\tand teaching; seats-votes curves.",
    "version": "1.5.9",
    "maintainer": "Simon Jackman <simon.jackman@sydney.edu.au>",
    "url": "https://github.com/atahk/pscl",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20354,
    "package_name": "psd",
    "title": "Adaptive, Sine-Multitaper Power Spectral Density and Cross\nSpectrum Estimation",
    "description": "Produces power spectral density estimates through iterative\n    refinement of the optimal number of sine-tapers at each frequency. This\n    optimization procedure is based on the method of Riedel and Sidorenko\n    (1995), which minimizes the Mean Square Error (sum of variance and bias)\n    at each frequency, but modified for computational stability. The same\n    procedure can now be used to calculate the cross spectrum (multivariate\n    analyses).",
    "version": "2.1.2",
    "maintainer": "Andrew J. Barbour <andy.barbour@gmail.com>",
    "url": "https://github.com/abarbour/psd,\nhttps://doi.org/10.1016/j.cageo.2013.09.015",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20355,
    "package_name": "psdr",
    "title": "Use Time Series to Generate and Compare Power Spectral Density",
    "description": "Functions that allow you to generate and compare power spectral density (PSD) \n\tplots given time series data. Fast Fourier Transform (FFT) is used to take a time series \n\tdata, analyze the oscillations, and then output the frequencies of these oscillations \n\tin the time series in the form of a PSD plot.Thus given a time series, the dominant \n\tfrequencies in the time series can be identified. Additional functions in this package \n\tallow the dominant frequencies of multiple groups of time series to be compared with each other. \n\tTo see example usage with the main functions of this package, please visit\n\tthis site: <https://yhhc2.github.io/psdr/articles/Introduction.html>. \n\tThe mathematical operations used to generate the PSDs are described in these sites:\n\t<https://www.mathworks.com/help/matlab/ref/fft.html>.\n\t<https://www.mathworks.com/help/signal/ug/power-spectral-density-estimates-using-fft.html>.",
    "version": "1.0.3",
    "maintainer": "Yong-Han Hank Cheng <yhhc@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20356,
    "package_name": "pseudo",
    "title": "Computes Pseudo-Observations for Modeling",
    "description": "Various functions for computing pseudo-observations for censored data regression. Computes pseudo-observations for modeling: competing risks based on the cumulative incidence function, survival function based on the restricted mean,  survival function based on the Kaplan-Meier estimator see Klein et al. (2008) <doi:10.1016/j.cmpb.2007.11.017>. ",
    "version": "1.4.3",
    "maintainer": "Kevin Rodrigues <kevin.asr@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20357,
    "package_name": "pseudoCure",
    "title": "A Pseudo-Observations Approach for Analyzing Survival Data with\na Cure Fraction",
    "description": "A collection of easy-to-use tools for regression analysis of survival data with a cure fraction proposed in Su et al. (2022) <doi:10.1177/09622802221108579>. The modeling framework is based on the Cox proportional hazards mixture cure model and the bounded cumulative hazard (promotion time cure) model. The pseudo-observations approach is utilized to assess covariate effects and embedded in the variable selection procedure. ",
    "version": "1.0.0",
    "maintainer": "Sy Han (Steven) Chiou <schiou@smu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20368,
    "package_name": "psm3mkv",
    "title": "Evaluate Partitioned Survival and State Transition Models",
    "description": "Fits and evaluates three-state partitioned survival analyses\n    (PartSAs) and Markov models (clock forward or clock reset) to\n    progression and overall survival data typically collected in oncology clinical trials. These model structures are typically considered in\n    cost-effectiveness modeling in advanced/metastatic cancer indications.\n    Muston (2024). \"Informing structural assumptions for three state oncology cost-effectiveness models through model efficiency and fit\". Applied Health Economics and Health Policy. ",
    "version": "0.3.2",
    "maintainer": "Dominic Muston <dominic.muston@merck.com>",
    "url": "https://merck.github.io/psm3mkv/, https://github.com/Merck/psm3mkv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20373,
    "package_name": "psp",
    "title": "Parameter Space Partitioning MCMC for Global Model Evaluation",
    "description": "Implements an n-dimensional parameter space partitioning algorithm for evaluating the global behaviour of formal computational models as described by Pitt, Kim, Navarro and Myung (2006) <doi:10.1037/0033-295X.113.1.57>.",
    "version": "1.0.2",
    "maintainer": "Lenard Dome <lenarddome@gmail.com>",
    "url": "https://github.com/lenarddome/psp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20407,
    "package_name": "ptmixed",
    "title": "Poisson-Tweedie Generalized Linear Mixed Model",
    "description": "Fits the Poisson-Tweedie generalized linear mixed model\n    described in Signorelli et al. (2021, <doi:10.1177/1471082X20936017>).\n    Likelihood approximation based on adaptive Gauss Hermite quadrature\n    rule.",
    "version": "1.1.3",
    "maintainer": "Mirko Signorelli <msignorelli.rpackages@gmail.com>",
    "url": "https://mirkosignorelli.github.io/r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20410,
    "package_name": "pttstability",
    "title": "Particle-Takens Stability",
    "description": "Includes a collection of functions presented in \"Measuring stability in ecological systems without static equilibria\" by Clark et al. (2022) <doi:10.1002/ecs2.4328> in Ecosphere.\n\tThese can be used to estimate the parameters of a stochastic state space model (i.e. a model where\n\ta time series is observed with error). The goal of this package is to estimate the variability\n\taround a deterministic process, both in terms of observation error - i.e. variability due to\n\timperfect observations that does not influence system state - and in terms of process noise - i.e.\n\tstochastic variation in the actual state of the process. Unlike classical methods for estimating\n\tvariability, this package does not necessarily assume that the deterministic state is fixed (i.e.\n\ta fixed-point equilibrium), meaning that variability around a dynamic trajectory can be estimated\n\t(e.g. stochastic fluctuations during predator-prey dynamics).",
    "version": "1.4",
    "maintainer": "Adam Clark <adam.tclark@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20413,
    "package_name": "ptw",
    "title": "Parametric Time Warping",
    "description": "Parametric Time Warping aligns patterns, i.e. it aims to\n        put corresponding features at the same locations. The algorithm\n        searches for an optimal polynomial describing the warping. It\n        is possible to align one sample to a reference, several samples\n        to the same reference, or several samples to several\n        references. One can choose between calculating individual\n        warpings, or one global warping for a set of samples and one\n        reference. Two optimization criteria are implemented: RMS (Root\n        Mean Square error) and WCC (Weighted Cross Correlation). Both\n\twarping of peak profiles and of peak lists are supported. A\n\tvignette for the latter is contained in the inst/doc directory\n\tof the source package - the vignette source can be found on\n\tthe package github site.",
    "version": "1.9-16",
    "maintainer": "Ron Wehrens <ron.wehrens@gmail.com>",
    "url": "https://github.com/rwehrens/ptw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20415,
    "package_name": "ptycho",
    "title": "Bayesian Variable Selection with Hierarchical Priors",
    "description": "\n  Bayesian variable selection for linear regression models using hierarchical\n  priors. There is a prior that combines information across responses and one\n  that combines information across covariates, as well as a standard spike and\n  slab prior for comparison. An MCMC samples from the marginal posterior\n  distribution for the 0-1 variables indicating if each covariate belongs to the\n  model for each response.",
    "version": "1.1-5",
    "maintainer": "Laurel Stell <lstell@stanford.edu>",
    "url": "http://web.stanford.edu/~lstell/ptycho/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20430,
    "package_name": "pumBayes",
    "title": "Bayesian Estimation of Probit Unfolding Models for Binary\nPreference Data",
    "description": "Bayesian estimation and analysis methods for Probit Unfolding Models (PUMs), a novel class of scaling models designed for binary preference data. These models allow for both monotonic and non-monotonic response functions. The package supports Bayesian inference for both static and dynamic PUMs using Markov chain Monte Carlo (MCMC) algorithms with minimal or no tuning. Key functionalities include posterior sampling, hyperparameter selection, data preprocessing, model fit evaluation, and visualization. The methods are particularly suited to analyzing voting data, such as from the U.S. Congress or Supreme Court, but can also be applied in other contexts where non-monotonic responses are expected. For methodological details, see Shi et al. (2025) <doi:10.48550/arXiv.2504.00423>.",
    "version": "1.0.1",
    "maintainer": "Skylar Shi <dshi98@uw.edu>",
    "url": "https://github.com/SkylarShiHub/pumBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20450,
    "package_name": "pvars",
    "title": "VAR Modeling for Heterogeneous Panels",
    "description": "Implements (1) panel cointegration rank tests, (2) estimators for panel\n    vector autoregressive (VAR) models, and (3) identification methods for panel\n    structural vector autoregressive (SVAR) models as described in the accompanying vignette.\n    The implemented functions allow to account for cross-sectional dependence\n    and for structural breaks in the deterministic terms of the VAR processes.\n    Among the large set of functions, particularly noteworthy are those that implement\n    (1) the correlation-augmented inverse normal test on the cointegration rank\n    by Arsova and Oersal (2021, <doi:10.1016/j.ecosta.2020.05.002>),\n    (2) the two-step estimator for pooled cointegrating vectors\n    by Breitung (2005, <doi:10.1081/ETC-200067895>), and\n    (3) the pooled identification based on independent component analysis\n    by Herwartz and Wang (2024, <doi:10.1002/jae.3044>).",
    "version": "1.1.1",
    "maintainer": "Lennart Empting <lennart.empting@vwl.uni-due.de>",
    "url": "https://github.com/Lenni89/pvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20459,
    "package_name": "pwlapprox2d",
    "title": "Approximates Univariate Continuous Functions Through Piecewise\nLinear Regression",
    "description": "Allows users to find a piecewise linear regression approximation to a given continuous univariate function within a specified error tolerance. Methods based on Warwicker and Rebennack (2025) \"Efficient continuous piecewise linear regression for linearising univariate non-linear functions\" <doi:10.1080/24725854.2023.2299809>.",
    "version": "0.1.0",
    "maintainer": "John Warwicker <john.warwicker@kit.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20464,
    "package_name": "pwr4exp",
    "title": "Power Analysis for Research Experiments",
    "description": "Provides tools for calculating statistical power for experiments \n    analyzed using linear mixed models. It supports standard designs, including \n    randomized block, split-plot, and Latin Square designs, while offering flexibility \n    to accommodate a variety of other complex study designs.",
    "version": "1.0.1",
    "maintainer": "Kai Wang <kai.wang@usys.ethz.ch>",
    "url": "https://github.com/an-ethz/pwr4exp,\nhttps://an-ethz.github.io/pwr4exp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20479,
    "package_name": "pyinit",
    "title": "Pena-Yohai Initial Estimator for Robust S-Regression",
    "description": "Deterministic Pena-Yohai initial estimator for robust S estimators\n    of regression. The procedure is described in detail in\n    Pena, D., & Yohai, V. (1999) <doi:10.2307/2670164>.",
    "version": "1.1.5",
    "maintainer": "David Kepplinger <david.kepplinger@gmail.com>",
    "url": "https://github.com/dakep/pyinit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20498,
    "package_name": "qbld",
    "title": "Quantile Regression for Binary Longitudinal Data",
    "description": "Implements the Bayesian quantile regression model for binary longitudinal data \n             (QBLD) developed in Rahman and Vossmeyer (2019) <DOI:10.1108/S0731-90532019000040B009>.\n             The model handles both fixed and random effects and implements both a blocked\n             and an unblocked Gibbs sampler for posterior inference.",
    "version": "1.0.3",
    "maintainer": "Ayush Agarwal<ayush.agarwal50@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20501,
    "package_name": "qbrms",
    "title": "Quick Bayesian Regression Models Using 'INLA' with 'brms' Syntax",
    "description": "Provides a 'brms'-like interface for fitting Bayesian regression models \n    using 'INLA' (Integrated Nested Laplace Approximations) and 'TMB' (Template Model Builder). \n    The package offers faster model fitting while maintaining familiar 'brms' syntax and \n    output formats. Supports fixed and mixed effects models, multiple probability distributions, \n    conditional effects plots, and posterior predictive checks with summary methods compatible \n    with 'brms'. 'TMB' integration provides fast ordinal regression capabilities. Implements \n    methods adapted from 'emmeans' for marginal means estimation and 'bayestestR' for Bayesian \n    inference assessment. Methods are based on Rue et al. (2009) <doi:10.1111/j.1467-9868.2008.00700.x>, \n    Kristensen et al. (2016) <doi:10.18637/jss.v070.i05>, Lenth (2016) <doi:10.18637/jss.v069.i01>, \n    Bürkner (2017) <doi:10.18637/jss.v080.i01>, Makowski et al. (2019) <doi:10.21105/joss.01541>, \n    and Kruschke (2014, ISBN:9780124058880).",
    "version": "1.0.1",
    "maintainer": "Tony Myers <admyers@aol.com>",
    "url": "https://github.com/Tony-Myers/qbrms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20504,
    "package_name": "qcauchyreg",
    "title": "Quantile Regression Quasi-Cauchy",
    "description": "Quasi-Cauchy quantile regression, proposed by de Oliveira, Ospina, Leiva, Figueroa-Zuniga and Castro (2023) <doi:10.3390/fractalfract7090667>. This regression model is useful for the case where you want to model data of a nature limited to the intervals [0,1], (0,1], [0,1) or (0,1) and you want to use a quantile approach.",
    "version": "1.0",
    "maintainer": "Jose Sergio Case de Oliveira <js_cdo@hotmail.com>",
    "url": "<https://www.r-project.org>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20507,
    "package_name": "qch",
    "title": "Query Composite Hypotheses",
    "description": "Provides functions for the joint analysis of Q sets of\n    p-values obtained for the same list of items. This joint analysis is\n    performed by querying a composite hypothesis, i.e. an arbitrary\n    complex combination of simple hypotheses, as described in Mary-Huard\n    et al. (2021) <doi:10.1093/bioinformatics/btab592> and De Walsche et\n    al.(2023) <doi:10.1101/2024.03.17.585412>. In this approach, the\n    Q-uplet of p-values associated with each item is distributed as a\n    multivariate mixture, where each of the 2^Q components corresponds to\n    a specific combination of simple hypotheses. The dependence between\n    the p-value series is considered using a Gaussian copula function. A\n    p-value for the composite hypothesis test is derived from the\n    posterior probabilities.",
    "version": "2.1.0",
    "maintainer": "Tristan Mary-Huard <tristan.mary-huard@agroparistech.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20512,
    "package_name": "qcpm",
    "title": "Quantile Composite Path Modeling",
    "description": "Implements the Quantile Composite-based Path Modeling approach \n\t(Davino and Vinzi, 2016 <doi:10.1007/s11634-015-0231-9>; \n\t Dolce et al., 2021 <doi:10.1007/s11634-021-00469-0>). The method complements the traditional PLS Path Modeling approach, analyzing the entire distribution of outcome variables and, therefore, overcoming the classical exploration of only average effects. It exploits quantile regression to investigate changes in the relationships among constructs and between constructs and observed variables.",
    "version": "0.4",
    "maintainer": "Giuseppe Lamberti <giuseppelamb@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20514,
    "package_name": "qcrlscR",
    "title": "Quality Control–based Robust LOESS Signal Correction",
    "description": "An R implementation of quality control–based robust LOESS(local\n    polynomial regression fitting) signal correction for metabolomics data\n    analysis, described in Dunn, W., Broadhurst, D., Begley, P. et al. (2011)\n    <doi:10.1038/nprot.2011.335>. The optimisation of LOESS's span parameter\n    using generalized cross-validation (GCV) is provided as an option. In\n    addition to signal correction, 'qcrlscR' includes some utility functions\n    like batch shifting and data filtering.",
    "version": "0.1.3",
    "maintainer": "Wanchang Lin <wanchanglin@hotmail.com>",
    "url": "https://github.com/wanchanglin/qcrlscR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20526,
    "package_name": "qgam",
    "title": "Smooth Additive Quantile Regression Models",
    "description": "Smooth additive quantile regression models, fitted using\n    the methods of Fasiolo et al. (2020) <doi:10.1080/01621459.2020.1725521>.\n    See Fasiolo at al. (2021) <doi:10.18637/jss.v100.i09> for an introduction to the package. Differently from\n    'quantreg', the smoothing parameters are estimated automatically by marginal\n    loss minimization, while the regression coefficients are estimated using either\n    PIRLS or Newton algorithm. The learning rate is determined so that the Bayesian\n    credible intervals of the estimated effects have approximately the correct\n    coverage. The main function is qgam() which is similar to gam() in 'mgcv', but\n    fits non-parametric quantile regression models.",
    "version": "2.0.0",
    "maintainer": "Matteo Fasiolo <matteo.fasiolo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20527,
    "package_name": "qgcomp",
    "title": "Quantile G-Computation",
    "description": "G-computation for a set of time-fixed exposures with\n    quantile-based basis functions, possibly under linearity and\n    homogeneity assumptions. This approach estimates a regression line\n    corresponding to the expected change in the outcome (on the link\n    basis) given a simultaneous increase in the quantile-based category\n    for all exposures. Works with continuous, binary, and right-censored\n    time-to-event outcomes.  Reference: Alexander P. Keil, Jessie P.\n    Buckley, Katie M. OBrien, Kelly K. Ferguson, Shanshan Zhao, and\n    Alexandra J. White (2019) A quantile-based g-computation approach to\n    addressing the effects of exposure mixtures; <doi:10.1289/EHP5838>.",
    "version": "2.18.7",
    "maintainer": "Alexander Keil <alex.keil@nih.gov>",
    "url": "https://github.com/alexpkeil1/qgcomp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20529,
    "package_name": "qgg",
    "title": "Statistical Tools for Quantitative Genetic Analyses",
    "description": "Provides an infrastructure for efficient processing of large-scale genetic and phenotypic data including core functions for: 1) fitting linear mixed models, 2) constructing marker-based genomic relationship matrices, 3) estimating genetic parameters (heritability and correlation), 4) performing genomic prediction and genetic risk profiling, and 5) single or multi-marker association analyses.\n    Rohde et al. (2019) <doi:10.1101/503631>.",
    "version": "1.1.6",
    "maintainer": "Peter Soerensen <peter.sorensen@r-qgg.org>",
    "url": "https://github.com/psoerensen/qgg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20535,
    "package_name": "qif",
    "title": "Quadratic Inference Function",
    "description": "Developed to perform the estimation and inference for regression \n    coefficient parameters in longitudinal marginal models using the method of \n    quadratic inference functions. Like generalized estimating equations, this \n    method is also a quasi-likelihood inference method. It has been showed that \n    the method gives consistent estimators of the regression coefficients even if \n    the correlation structure is misspecified, and it is more efficient than GEE \n    when the correlation structure is misspecified. Based on Qu, A., Lindsay, \n    B.G. and Li, B. (2000) <doi:10.1093/biomet/87.4.823>.",
    "version": "1.5",
    "maintainer": "Michael Kleinsasser <mkleinsa@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20540,
    "package_name": "qlcal",
    "title": "R Bindings to the Calendaring Functionality of 'QuantLib'",
    "description": "'QuantLib' bindings are provided for R using 'Rcpp' via an evolved version\n of the initial header-only 'Quantuccia' project offering an subset of 'QuantLib' (now\n maintained separately just for the calendaring subset). See the included file 'AUTHORS'\n for a full list of contributors to 'QuantLib' (and hence also 'Quantuccia').",
    "version": "0.0.17",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "url": "https://github.com/qlcal/qlcal-r,\nhttps://dirk.eddelbuettel.com/code/qlcal-r.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20551,
    "package_name": "qountstat",
    "title": "Statistical Analysis of Count Data and Quantal Data",
    "description": "Methods for statistical analysis of count data and quantal data.\n    For the analysis of count data an implementation of the Closure Principle Computational Approach Test (\"CPCAT\") is provided (Lehmann, R et al. (2016) <doi:10.1007/s00477-015-1079-4>), as well as an implementation of a \"Dunnett GLM\" approach using a Quasi-Poisson regression (Hothorn, L, Kluxen, F (2020) <doi:10.1101/2020.01.15.907881>).\n    For the analysis of quantal data an implementation of the Closure Principle Fisher–Freeman–Halton test (\"CPFISH\") is provided (Lehmann, R et al. (2018) <doi:10.1007/s00477-017-1392-1>). P-values and no/lowest observed (adverse) effect concentration values are calculated.\n    All implemented methods include further functions to evaluate the power and the minimum detectable difference using a bootstrapping approach.",
    "version": "0.1.1",
    "maintainer": "Benjamin Daniels <Benjamin.Daniels@uba.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20562,
    "package_name": "qrLMM",
    "title": "Quantile Regression for Linear Mixed-Effects Models",
    "description": "Quantile regression (QR) for Linear \n             Mixed-Effects Models via the asymmetric Laplace distribution (ALD). \n             It uses the Stochastic Approximation of the EM (SAEM) algorithm for \n             deriving exact maximum likelihood estimates and full inference results \n             for the fixed-effects and variance components. \n             It also provides graphical summaries for assessing the algorithm \n             convergence and fitting results.",
    "version": "2.3",
    "maintainer": "Christian E. Galarza <cgalarza88@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20563,
    "package_name": "qrNLMM",
    "title": "Quantile Regression for Nonlinear Mixed-Effects Models",
    "description": "Quantile regression (QR) for Nonlinear\n             Mixed-Effects Models via the asymmetric Laplace distribution (ALD). \n             It uses the Stochastic Approximation of the EM (SAEM) algorithm for \n             deriving exact maximum likelihood estimates and full inference result\n             is \n             for the fixed-effects and variance components.\n             It also provides prediction and graphical summaries for assessing the algorithm\n             convergence and fitting results.",
    "version": "4.0",
    "maintainer": "Christian E. Galarza <cgalarza88@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20566,
    "package_name": "qrcm",
    "title": "Quantile Regression Coefficients Modeling",
    "description": "Parametric modeling of quantile regression coefficient functions.",
    "version": "3.2",
    "maintainer": "Paolo Frumento <paolo.frumento@unipi.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20567,
    "package_name": "qrcmNP",
    "title": "Nonlinear and Penalized Quantile Regression Coefficients\nModeling",
    "description": "Nonlinear and Penalized parametric modeling of quantile regression coefficient functions. Sottile G, Frumento P, Chiodi M and Bottai M (2020) <doi:10.1177/1471082X19825523>.",
    "version": "0.2.2",
    "maintainer": "Gianluca Sottile <gianluca.sottile@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20572,
    "package_name": "qrjoint",
    "title": "Joint Estimation in Linear Quantile Regression",
    "description": "Joint estimation of quantile specific intercept and slope parameters in a linear regression setting.",
    "version": "2.0-11",
    "maintainer": "Surya Tokdar <surya.tokdar@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20575,
    "package_name": "qrmix",
    "title": "Quantile Regression Mixture Models",
    "description": "Implements the robust algorithm for fitting finite mixture models based on quantile regression proposed by Emir et al., 2017 (unpublished).",
    "version": "0.9.0",
    "maintainer": "Maria de los Angeles Resa <maria@stat.columbia.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20591,
    "package_name": "qte",
    "title": "Quantile Treatment Effects",
    "description": "Provides several methods for computing the Quantile Treatment Effect (QTE) and Quantile Treatment Effect on the Treated (QTT). The main cases covered are (i) Treatment is randomly assigned, (ii) Treatment is as good as randomly assigned after conditioning on some covariates (also called conditional independence or selection on observables) using the methods developed in Firpo (2007) <doi:10.1111/j.1468-0262.2007.00738.x>, (iii) Identification is based on a Difference in Differences assumption (several varieties are available in the package e.g. Athey and Imbens (2006) <doi:10.1111/j.1468-0262.2006.00668.x> Callaway and Li (2019) <doi:10.3982/QE935>, Callaway, Li, and Oka (2018) <doi:10.1016/j.jeconom.2018.06.008>).",
    "version": "1.3.1",
    "maintainer": "Brantly Callaway <brantly.callaway@uga.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20604,
    "package_name": "qtlnet",
    "title": "Causal Inference of QTL Networks",
    "description": "Functions to Simultaneously Infer Causal Graphs and Genetic Architecture.\n  Includes acyclic and cyclic graphs for data from an experimental cross with a modest number (<10) of phenotypes driven by\n  a few genetic loci (QTL).\n  Chaibub Neto E, Keller MP, Attie AD, Yandell BS (2010)\n  Causal Graphical Models in Systems Genetics: a unified framework for joint inference of causal network and genetic architecture for correlated phenotypes.\n  Annals of Applied Statistics 4: 320-339.\n  <doi:10.1214/09-AOAS288>.",
    "version": "1.5.4",
    "maintainer": "Brian S. Yandell <brian.yandell@wisc.edu>",
    "url": "http://www.stat.wisc.edu/~yandell/sysgen",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20608,
    "package_name": "quadVAR",
    "title": "Quadratic Vector Autoregression",
    "description": "Estimate quadratic vector autoregression models with the\n    strong hierarchy using the Regularization Algorithm under Marginality\n    Principle (RAMP) by Hao et al. (2018)\n    <doi:10.1080/01621459.2016.1264956>, compare the performance with\n    linear models, and construct networks with partial derivatives.",
    "version": "0.1.2",
    "maintainer": "Jingmeng Cui <jingmeng.cui@outlook.com>",
    "url": "https://github.com/Sciurus365/quadVAR,\nhttps://sciurus365.github.io/quadVAR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20617,
    "package_name": "quadrupen",
    "title": "Sparsity by Worst-Case Quadratic Penalties",
    "description": "Fits classical sparse regression models with\n    efficient active set algorithms by solving quadratic problems as described by \n    Grandvalet, Chiquet and Ambroise (2017) <doi:10.48550/arXiv.1210.2077>. Also provides a few\n    methods for model selection purpose (cross-validation, stability selection).",
    "version": "0.2-13",
    "maintainer": "Julien Chiquet <julien.chiquet@inrae.fr>",
    "url": "https://github.com/jchiquet/quadrupenCRAN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20626,
    "package_name": "quantCurves",
    "title": "Estimate Quantiles Curves",
    "description": "Non-parametric methods as local normal regression, polynomial local regression and penalized cubic B-splines regression are used to estimate quantiles curves. See Fan and Gijbels (1996) <doi:10.1201/9780203748725> and Perperoglou et al.(2019) <doi:10.1186/s12874-019-0666-3>.",
    "version": "1.0.0",
    "maintainer": "Sandie Ferrigno <sandie.ferrigno@univ-lorraine.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20627,
    "package_name": "quantbayes",
    "title": "Bayesian Quantification of Evidence Sufficiency",
    "description": "Implements the Quantification Evidence Standard algorithm for computing\n    Bayesian evidence sufficiency from binary evidence matrices. It provides\n    posterior estimates, credible intervals, percentiles, and optional visual\n    summaries. The method is universal, reproducible, and independent of\n    any specific clinical or rule based framework. For details see The Quantitative Omics Epidemiology Group et al. (2025) <doi:10.64898/2025.12.02.25341503>.",
    "version": "0.1.0",
    "maintainer": "Dylan Lawless <admin@switzerlandomics.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20636,
    "package_name": "quantification",
    "title": "Quantification of Qualitative Survey Data",
    "description": "Provides different functions for quantifying qualitative survey data. It supports the Carlson-Parkin method, the regression approach, the balance approach and the conditional expectations method.",
    "version": "0.2.0",
    "maintainer": "Joachim Zuckarelli <joachim@zuckarelli.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20643,
    "package_name": "quantoptr",
    "title": "Algorithms for Quantile- And Mean-Optimal Treatment Regimes",
    "description": "Estimation methods for optimal treatment regimes under three different criteria, namely marginal quantile, marginal mean, and mean absolute difference. For the first two criteria, both one-stage and two-stage estimation method are implemented. A doubly robust estimator for estimating the quantile-optimal treatment regime is also included. ",
    "version": "0.1.3",
    "maintainer": "Yu Zhou <zhou0269@umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20644,
    "package_name": "quantreg",
    "title": "Quantile Regression",
    "description": "Estimation and inference methods for models for conditional quantile functions: \n  Linear and nonlinear parametric and non-parametric (total variation penalized) models \n  for conditional quantiles of a univariate response and several methods for handling\n  censored survival data.  Portfolio selection methods based on expected shortfall\n  risk are also now included. See Koenker, R. (2005) Quantile Regression, Cambridge U. Press,\n  <doi:10.1017/CBO9780511754098> and Koenker, R. et al. (2017) Handbook of Quantile Regression, \n  CRC Press, <doi:10.1201/9781315120256>. ",
    "version": "6.1",
    "maintainer": "Roger Koenker <rkoenker@illinois.edu>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20647,
    "package_name": "quantregGrowth",
    "title": "Non-Crossing Additive Regression Quantiles and Non-Parametric\nGrowth Charts",
    "description": "Fits non-crossing regression quantiles as a function of linear covariates and multiple smooth terms, including varying coefficients, via B-splines with L1-norm difference penalties.  \n    Random intercepts and variable selection are allowed via the lasso penalties.\n    The smoothing parameters are estimated as part of the model fitting, see Muggeo and others (2021) <doi:10.1177/1471082X20929802>. Monotonicity and concavity \n    constraints on the fitted curves are allowed, see Muggeo and others (2013) <doi:10.1007/s10651-012-0232-1>, \n    and also <doi:10.13140/RG.2.2.12924.85122> or <doi:10.13140/RG.2.2.29306.21445> some code examples.",
    "version": "1.7-2",
    "maintainer": "Vito M. R. Muggeo <vito.muggeo@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20652,
    "package_name": "quantspec",
    "title": "Quantile-Based Spectral Analysis of Time Series",
    "description": "Methods to determine, smooth and plot quantile periodograms for\n    univariate and multivariate time series. See Kley (2016) <doi:10.18637/jss.v070.i03>\n    for a description and tutorial.",
    "version": "1.2-4",
    "maintainer": "Tobias Kley <tobias.kley@uni-goettingen.de>",
    "url": "https://github.com/tobiaskley/quantspec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20660,
    "package_name": "quasar",
    "title": "Valid Inference on Multiple Quantile Regressions",
    "description": "The approach is based on the closed testing procedure to control familywise error rate in a strong sense.\n  The local tests implemented are Wald-type and rank-score. \n  The method is described in De Santis, et al., (2026), <doi:10.48550/arXiv.2511.07999>.",
    "version": "0.2.0",
    "maintainer": "Angela Andreella <angela.andreella@unive.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20665,
    "package_name": "quest",
    "title": "Prepare Questionnaire Data for Analysis",
    "description": "Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an \"s\" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an \"s\" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.",
    "version": "0.2.1",
    "maintainer": "David Disabato <ddisab01@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20676,
    "package_name": "quickcountmx",
    "title": "Estimate Election Results Mexico",
    "description": "Functions to simulate and estimate Mexican election results based on a simple or stratified random sample, the functions were used, among other methodologies, to anticipate the final results of the 2018 Mexican elections.",
    "version": "0.1.0",
    "maintainer": "",
    "url": "https://github.com/tereom/quickcountmx",
    "exports": [],
    "topics": ["bayesian", "bayesian-inference", "electoral-statistics", "r", "rpackage", "stan", "statistics"],
    "score": "NA",
    "stars": 6
  },
  {
    "id": 20682,
    "package_name": "quickregression",
    "title": "Quick Linear Regression",
    "description": "Helps to perform linear regression analysis by reducing manual effort. Reduces the independent variables based on specified p-value and Variance Inflation Factor (VIF) level.",
    "version": "0.2",
    "maintainer": "Darshan Maniyar <maniyar.darshan@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20683,
    "package_name": "quid",
    "title": "Bayesian Mixed Models for Qualitative Individual Differences",
    "description": "Test whether equality and order constraints hold for all \n    individuals simultaneously by comparing Bayesian mixed models through Bayes \n    factors. A tutorial style vignette and a quickstart guide are available, via\n    vignette(\"manual\", \"quid\"), and vignette(\"quickstart\", \"quid\") respectively.\n    See Haaf and Rouder (2017) <doi:10.1037/met0000156>; Haaf, Klaassen and Rouder\n    (2019) <doi:10.31234/osf.io/a4xu9>; and Rouder & Haaf (2021) <doi:10.5334/joc.131>.",
    "version": "0.0.1",
    "maintainer": "Lukas Klima <lukas.klima@live.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20696,
    "package_name": "qwraps2",
    "title": "Quick Wraps 2",
    "description": "A collection of (wrapper) functions the creator found useful\n    for quickly placing data summaries and formatted regression results into\n    '.Rnw' or '.Rmd' files. Functions for generating commonly used graphics,\n    such as receiver operating curves or Bland-Altman plots, are also provided\n    by 'qwraps2'.  'qwraps2' is a updated version of a package 'qwraps'. The\n    original version 'qwraps' was never submitted to CRAN but can be found at\n    <https://github.com/dewittpe/qwraps/>. The implementation and limited scope\n    of the functions within 'qwraps2' <https://github.com/dewittpe/qwraps2/> is\n    fundamentally different from 'qwraps'.",
    "version": "0.6.1",
    "maintainer": "Peter DeWitt <dewittpe@gmail.com>",
    "url": "https://github.com/dewittpe/qwraps2/,\nhttp://www.peteredewitt.com/qwraps2/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20697,
    "package_name": "r.blip",
    "title": "Bayesian Network Learning Improved Project",
    "description": "Allows the user to learn Bayesian networks from datasets containing thousands of variables. It focuses on score-based learning, mainly the 'BIC' and the 'BDeu' score functions. It provides state-of-the-art algorithms for the following tasks: (1) parent set identification - Mauro Scanagatta (2015) <http://papers.nips.cc/paper/5803-learning-bayesian-networks-with-thousands-of-variables>; (2) general structure optimization - Mauro Scanagatta (2018) <doi:10.1007/s10994-018-5701-9>, Mauro Scanagatta (2018) <http://proceedings.mlr.press/v73/scanagatta17a.html>; (3) bounded treewidth structure optimization - Mauro Scanagatta (2016) <http://papers.nips.cc/paper/6232-learning-treewidth-bounded-bayesian-networks-with-thousands-of-variables>; (4) structure learning on incomplete data sets - Mauro Scanagatta (2018) <doi:10.1016/j.ijar.2018.02.004>. Distributed under the LGPL-3 by IDSIA.",
    "version": "1.1",
    "maintainer": "Mauro Scanagatta <mauro@idsia.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20698,
    "package_name": "r.jive",
    "title": "Perform JIVE Decomposition for Multi-Source Data",
    "description": "Performs the Joint and Individual Variation Explained (JIVE) decomposition on a list of data sets when the data share a dimension, returning low-rank matrices that capture the joint and individual structure of the data [O'Connell, MJ and Lock, EF (2016) <doi:10.1093/bioinformatics/btw324>]. It provides two methods of rank selection when the rank is unknown, a permutation test and a Bayesian Information Criterion (BIC) selection algorithm. Also included in the package are three plotting functions for visualizing the variance attributed to each data source: a bar plot that shows the percentages of the variability attributable to joint and individual structure, a heatmap that shows the structure of the variability, and principal component plots. ",
    "version": "2.4",
    "maintainer": "Michael J. O'Connell <oconnemj@miamioh.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20711,
    "package_name": "r2glmm",
    "title": "Computes R Squared for Mixed (Multilevel) Models",
    "description": "The model R squared and semi-partial R squared for the linear and\n    generalized linear mixed model (LMM and GLMM) are computed with confidence\n    limits. The R squared measure from Edwards et.al (2008) <DOI:10.1002/sim.3429>\n    is extended to the GLMM using penalized quasi-likelihood (PQL) estimation\n    (see Jaeger et al. 2016 <DOI:10.1080/02664763.2016.1193725>). Three methods\n    of computation are provided and described as follows. First, The\n    Kenward-Roger approach. Due to some inconsistency between the 'pbkrtest'\n    package and the 'glmmPQL' function, the Kenward-Roger approach in the\n    'r2glmm' package is limited to the LMM. Second, The method introduced\n    by Nakagawa and Schielzeth (2013) <DOI:10.1111/j.2041-210x.2012.00261.x>\n    and later extended by Johnson (2014) <DOI:10.1111/2041-210X.12225>.\n    The 'r2glmm' package only computes marginal R squared for the LMM and does\n    not generalize the statistic to the GLMM; however, confidence limits and\n    semi-partial R squared for fixed effects are useful additions. Lastly, an\n    approach using standardized generalized variance (SGV) can be used for\n    covariance model selection. Package installation instructions can be found\n    in the readme file.",
    "version": "0.1.3",
    "maintainer": "Byron Jaeger <byron.jaeger@gmail.com>",
    "url": "https://github.com/bcjaeger/r2glmm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20722,
    "package_name": "r2spss",
    "title": "Format R Output to Look Like SPSS",
    "description": "Create plots and LaTeX tables that look like SPSS output for use in teaching materials.  Rather than copying-and-pasting SPSS output into documents, R code that mocks up SPSS output can be integrated directly into dynamic LaTeX documents with tools such as knitr.  Functionality includes statistical techniques that are typically covered in introductory statistics classes: descriptive statistics, common hypothesis tests, ANOVA, and linear regression, as well as box plots, histograms, scatter plots, and line plots (including profile plots).",
    "version": "0.3.2",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "https://github.com/aalfons/r2spss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20753,
    "package_name": "rBayesianOptimization",
    "title": "Bayesian Optimization of Hyperparameters",
    "description": "A Pure R implementation of Bayesian Global Optimization with Gaussian Processes.",
    "version": "1.2.2",
    "maintainer": "Yachen Yan <yanyachen21@gmail.com>",
    "url": "https://github.com/yanyachen/rBayesianOptimization",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20774,
    "package_name": "rFSA",
    "title": "Feasible Solution Algorithm for Finding Best Subsets and\nInteractions",
    "description": "Assists in statistical model building to find optimal and semi-optimal higher order interactions\n    and best subsets. Uses the lm(), glm(), and other R functions to fit models generated from a feasible \n    solution algorithm. Discussed in Subset Selection in Regression, A Miller (2002). Applied and explained\n    for least median of squares in Hawkins (1993) <doi:10.1016/0167-9473(93)90246-P>. The feasible solution \n    algorithm comes up with model forms of a specific type that can have fixed variables, higher order \n    interactions and their lower order terms.",
    "version": "0.9.6",
    "maintainer": "Joshua Lambert <joshua.lambert@uc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20852,
    "package_name": "ra4bayesmeta",
    "title": "Reference Analysis for Bayesian Meta-Analysis",
    "description": "Functionality for performing a principled reference analysis in the Bayesian normal-normal hierarchical model used for Bayesian meta-analysis, as described in Ott, Plummer and Roos (2021) <doi:10.1002/sim.9076>. Computes a reference posterior, induced by a minimally informative improper reference prior for the between-study (heterogeneity) standard deviation. Determines additional proper anti-conservative (and conservative) prior benchmarks. Includes functions for reference analyses at both the posterior and the prior level, which, given the data, quantify the informativeness of a heterogeneity prior of interest relative to the minimally informative reference prior and the proper prior benchmarks. The functions operate on data sets which are compatible with the 'bayesmeta' package.",
    "version": "1.0-8",
    "maintainer": "Manuela Ott <manuela.c.ott@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20858,
    "package_name": "radEmu",
    "title": "Using Relative Abundance Data to Estimate of Multiplicative Differences in Mean Absolute Abundance",
    "description": "A differential abundance method for the analysis of microbiome data. radEmu estimates fold-differences in the abundance of taxa across samples relative to \"typical\" fold-differences. Notably, it does not require pseudocounts, nor choosing a denominator taxon.",
    "version": "2.2.1.0",
    "maintainer": "",
    "url": "https://github.com/statdivlab/radEmu",
    "exports": [],
    "topics": ["da", "metagenomics", "microbiome", "statistics"],
    "score": "NA",
    "stars": 61
  },
  {
    "id": 20867,
    "package_name": "radir",
    "title": "Inverse-Regression Estimation of Radioactive Doses",
    "description": "Radioactive doses estimation using individual chromosomal aberrations information. See Higueras M, Puig P, Ainsbury E, Rothkamm K. (2015) <doi:10.1088/0952-4746/35/3/557>.",
    "version": "1.0.4",
    "maintainer": "David Moriña Soler <david.morina@uab.cat>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20879,
    "package_name": "ragtop",
    "title": "Pricing Equity Derivatives with Extensions of Black-Scholes",
    "description": "Algorithms to price American and European\n    equity options, convertible bonds and a\n    variety of other financial derivatives. It uses an\n    extension of the usual Black-Scholes model in which\n    jump to default may occur at a probability specified\n    by a power-law link between stock price and hazard\n    rate as found in the paper by Takahashi, Kobayashi,\n    and Nakagawa (2001) <doi:10.3905/jfi.2001.319302>.  We\n    use ideas and techniques from Andersen and\n    Buffum (2002) <doi:10.2139/ssrn.355308> and\n    Linetsky (2006) <doi:10.1111/j.1467-9965.2006.00271.x>.",
    "version": "1.2.0",
    "maintainer": "Brian K. Boonstra <ragtop@boonstra.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20880,
    "package_name": "rai",
    "title": "Revisiting-Alpha-Investing for Polynomial Regression",
    "description": "A modified implementation of stepwise regression that greedily searches \n    the space of interactions among features in order to build polynomial regression models.\n    Furthermore, the hypothesis tests conducted are valid-post model selection\n    due to the use of a revisiting procedure that implements an alpha-investing\n    rule. As a result, the set of rejected sequential hypotheses is proven to \n    control the marginal false discover rate. When not searching for polynomials,\n    the package provides a statistically valid algorithm\n    to run and terminate stepwise regression. For more information, see \n    Johnson, Stine, and Foster (2019) <arXiv:1510.06322>.",
    "version": "1.0.0",
    "maintainer": "Kory D. Johnson <korydjohnson@gmail.com>",
    "url": "https://github.com/korydjohnson/rai",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20912,
    "package_name": "randomForestSRC",
    "title": "Fast Unified Random Forests for Survival, Regression, and\nClassification (RF-SRC)",
    "description": "Fast OpenMP parallel computing of Breiman's random forests for univariate, multivariate, unsupervised, survival, competing risks, class imbalanced classification and quantile regression. New Mahalanobis splitting for correlated outcomes.  Extreme random forests and randomized splitting.  Suite of imputation methods for missing data.  Fast random forests using subsampling. Confidence regions and standard errors for variable importance. New improved holdout importance. Case-specific importance.  Minimal depth variable importance. Visualize trees on your Safari or Google Chrome browser. Anonymous random forests for data privacy.",
    "version": "3.4.5",
    "maintainer": "Udaya B. Kogalur <ubk@kogalur.com>",
    "url": "https://www.randomforestsrc.org/ https://ishwaran.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20914,
    "package_name": "randomGLM",
    "title": "Random General Linear Model Prediction",
    "description": "A bagging predictor based on generalized linear models (GLMs) is implemented. The method is published in \n   Song, Langfelder and Horvath (2013) <doi:10.1186/1471-2105-14-5>. ",
    "version": "1.10-1",
    "maintainer": "Peter Langfelder <peter.langfelder@gmail.com>",
    "url": "https://horvath.genetics.ucla.edu/rglm/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20934,
    "package_name": "ranger",
    "title": "A Fast Implementation of Random Forests",
    "description": "A fast implementation of Random Forests, particularly suited for high\n          dimensional data. Ensembles of classification, regression, survival and\n          probability prediction trees are supported. Data from genome-wide association\n          studies can be analyzed efficiently. In addition to data frames, datasets of\n          class 'gwaa.data' (R package 'GenABEL') and 'dgCMatrix' (R package 'Matrix') \n          can be directly analyzed.",
    "version": "0.17.0",
    "maintainer": "Marvin N. Wright <cran@wrig.de>",
    "url": "https://imbs-hl.github.io/ranger/,\nhttps://github.com/imbs-hl/ranger",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20942,
    "package_name": "rankhazard",
    "title": "Rank-Hazard Plots",
    "description": "Rank-hazard plots Karvanen and Harrell (2009) <DOI:10.1002/sim.3591> visualize the relative importance of covariates in a proportional hazards model. The key idea is to rank the covariate values and plot the relative hazard as a function of ranks scaled to interval [0,1]. The relative hazard is plotted in respect to the reference hazard, which can bee.g. the hazard related to the median of the covariate.",
    "version": "1.1.1",
    "maintainer": "Nanni Ultima <nanni.ultima@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20964,
    "package_name": "rar",
    "title": "Risk-Adjusted Regression",
    "description": "Perform risk-adjusted regression and sensitivity analysis as\n    developed in \"Mitigating Omitted- and Included-Variable Bias in Estimates of\n    Disparate Impact\" Jung et al. (2024) <arXiv:1809.05651>.",
    "version": "0.0.3",
    "maintainer": "Johann Gaebler <me@jgaeb.com>",
    "url": "https://rar.jgaeb.com, https://github.com/jgaeb/rar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20970,
    "package_name": "rashnu",
    "title": "Balanced Sample Size and Power Calculation Tools",
    "description": "Implements sample size and power calculation methods with a focus on balance and fairness in study design, inspired by the Zoroastrian deity Rashnu, the judge who weighs truth. Supports survival analysis and various hypothesis testing frameworks.",
    "version": "0.1.2",
    "maintainer": "Gyeom Hwangbo <hbgyeom@gmail.com>",
    "url": "https://zarathucorp.github.io/rashnu/,\nhttps://github.com/zarathucorp/rashnu",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20986,
    "package_name": "ratematrix",
    "title": "Bayesian Estimation of the Evolutionary Rate Matrix",
    "description": "The Evolutionary Rate Matrix is a variance-covariance matrix which describes both the rates of trait evolution and the evolutionary correlation among multiple traits. This package has functions to estimate these parameters using Bayesian MCMC. It is possible to test if the pattern of evolutionary correlations among traits has changed between predictive regimes painted along the branches of the phylogenetic tree. Regimes can be created a priori or estimated as part of the MCMC under a joint estimation approach. The package has functions to run MCMC chains, plot results, evaluate convergence, and summarize posterior distributions.",
    "version": "1.2.5",
    "maintainer": "Daniel Caetano <caetanods1@gmail.com>",
    "url": "https://github.com/Caetanods/ratematrix",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 20990,
    "package_name": "ratesci",
    "title": "Confidence Intervals and Tests for Comparisons of Binomial\nProportions or Poisson Rates",
    "description": "Computes confidence intervals for binomial or Poisson rates and\n    their differences or ratios. Including the rate (or risk)\n    difference ('RD') or rate ratio (or relative risk, 'RR') for binomial\n    proportions or Poisson rates, and odds ratio ('OR', binomial only).\n    Also confidence intervals for RD, RR or OR for paired binomial data, \n    and estimation of a proportion from clustered binomial data.  \n    Includes skewness-corrected asymptotic score ('SCAS') methods, \n    which have been developed in Laud (2017) <doi:10.1002/pst.1813> \n    from Miettinen and Nurminen (1985) <doi:10.1002/sim.4780040211> \n    and Gart and Nam (1988) <doi:10.2307/2531848>, and in \n    Laud (2025, under review) for paired proportions. The same score produces \n    hypothesis tests that are improved versions of the non-inferiority test \n    for binomial RD and RR by Farrington and Manning (1990) \n    <doi:10.1002/sim.4780091208>, or a generalisation of the McNemar test for \n    paired data. The package also includes MOVER methods (Method Of Variance\n    Estimates Recovery) for all contrasts, derived from the Newcombe\n    method but with options to use equal-tailed intervals in place of the\n    Wilson score method, and generalised for Bayesian applications\n    incorporating prior information. So-called 'exact' methods for\n    strictly conservative coverage are approximated using continuity\n    adjustments, and the amount of adjustment can be selected to avoid\n    over-conservative coverage.  Also includes methods for stratified\n    calculations (e.g. meta-analysis), either with fixed effect assumption\n    (matching the CMH test) or incorporating stratum heterogeneity.",
    "version": "1.0.0",
    "maintainer": "Pete Laud <p.j.laud@sheffield.ac.uk>",
    "url": "https://github.com/petelaud/ratesci,\nhttps://petelaud.github.io/ratesci/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21015,
    "package_name": "rbc",
    "title": "Regression by Composition",
    "description": "Flexible statistical modelling using a modular framework for\n    regression, in which groups of transformations are composed together\n    and act on probability distributions.",
    "version": "0.1.0",
    "maintainer": "Daniel Farewell <farewelld@cardiff.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21031,
    "package_name": "rblt",
    "title": "Bio-Logging Toolbox",
    "description": "An R-shiny application to visualize bio-loggers time series at a microsecond precision as Acceleration, Temperature, Pressure, Light intensity. It is possible to link behavioral labels extracted\n  from 'BORIS' software <http://www.boris.unito.it> or manually written in a csv file.",
    "version": "0.2.4.7",
    "maintainer": "Sebastien Geiger <sebastien.geiger@iphc.cnrs.fr>",
    "url": "https://github.com/sg4r/rblt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21034,
    "package_name": "rbmn",
    "title": "Handling Linear Gaussian Bayesian Networks",
    "description": "Creation, manipulation, simulation of linear Gaussian Bayesian\n             networks from text files and more...",
    "version": "0.9-6",
    "maintainer": "Marco Scutari <scutari@bnlearn.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21038,
    "package_name": "rbridge",
    "title": "Restricted Bridge Estimation",
    "description": "Bridge Regression estimation with linear restrictions defined in Yuzbasi et al. (2019) <arXiv:1910.03660>. Special cases of this approach fit the restricted LASSO, restricted RIDGE and restricted Elastic Net estimators.",
    "version": "1.0.2",
    "maintainer": "Bahadir Yuzbasi <b.yzb@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21050,
    "package_name": "rcbalance",
    "title": "Large, Sparse Optimal Matching with Refined Covariate Balance",
    "description": "Tools for large, sparse optimal matching of treated units\n\tand control units in observational studies.  Provisions are\n\tmade for refined covariate balance constraints, which include\n\tfine and near-fine balance as special cases.  Matches are \n\toptimal in the sense that they are computed as solutions to\n\tnetwork optimization problems rather than greedy algorithms.\n\tSee Pimentel, et al.(2015) <doi:10.1080/01621459.2014.997879> \n\tand Pimentel (2016), Obs. Studies 2(1):4-23. The rrelaxiv \n\tpackage, which provides an alternative solver for\n\tthe underlying network flow problems, carries an\n\tacademic license and is not available on CRAN, but\n\tmay be downloaded from Github at \n\t<https://github.com/josherrickson/rrelaxiv/>.",
    "version": "1.8.8",
    "maintainer": "Samuel D. Pimentel <spi@berkeley.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21051,
    "package_name": "rcbayes",
    "title": "Estimate Rogers-Castro Migration Age Schedules with Bayesian\nModels",
    "description": "A collection of functions to estimate Rogers-Castro migration age schedules using 'Stan'. This model which describes the fundamental relationship between migration and age in the form of a flexible multi-exponential migration model was most notably proposed in Rogers and Castro (1978) <doi:10.1068/a100475>.",
    "version": "0.3.0",
    "maintainer": "Jessie Yeung <jessieyeung1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21052,
    "package_name": "rcbsubset",
    "title": "Optimal Subset Matching with Refined Covariate Balance",
    "description": "Tools for optimal subset matching of treated units\n\tand control units in observational studies, with support\n\tfor refined covariate balance constraints, (including\n\tfine and near-fine balance as special cases). A close \n\trelative is the 'rcbalance' package.  See Pimentel, \n\tet al.(2015) <doi:10.1080/01621459.2014.997879>\n\tand Pimentel and Kelz (2020) \n\t<doi:10.1080/01621459.2020.1720693>. The rrelaxiv \n\tpackage, which provides an alternative solver for\n\tthe underlying network flow problems, carries an\n\tacademic license and is not available on CRAN, but\n\tmay be downloaded from Github at \n\t<https://github.com/josherrickson/rrelaxiv/>.",
    "version": "1.1.7",
    "maintainer": "Samuel D. Pimentel <spi@berkeley.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21089,
    "package_name": "rcrimeanalysis",
    "title": "An Implementation of Crime Analysis Methods",
    "description": "An implementation of functions for the analysis of crime incident or records\n  management system data. The package implements analysis algorithms scaled for city\n  or regional crime analysis units. The package provides functions for kernel density\n  estimation for crime heat maps, geocoding using the 'Google Maps' API, identification \n  of repeat crime incidents, spatio-temporal map comparison across time intervals, \n  time series analysis (forecasting and decomposition), detection of optimal parameters \n  for the identification of near repeat incidents, and near repeat analysis with crime \n  network linkage.",
    "version": "0.5.0",
    "maintainer": "Jamie Spaulding <jspaulding02@hamline.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21093,
    "package_name": "rct3",
    "title": "Predict Fish Year-Class Strength from Survey Data",
    "description": "Predict fish year-class strength by calibration\n  regression analysis of multiple recruitment index series.",
    "version": "1.0.4",
    "maintainer": "Colin Millar <colin.millar@ices.dk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21097,
    "package_name": "rdacca.hp",
    "title": "Hierarchical and Variation Partitioning for Canonical Analysis",
    "description": "This function conducts variation partitioning and hierarchical partitioning to calculate the unique, shared (referred as to \"common\") and individual contributions of each predictor (or matrix) towards explained variation (R-square and adjusted R-square) on canonical analysis (RDA,CCA and db-RDA), applying the algorithm of Lai J.,Zou Y., Zhang J.,Peres-Neto P.(2022) Generalizing hierarchical and variation partitioning in multiple regression and canonical analyses using the rdacca.hp R package.Methods in Ecology and Evolution,13: 782-788 <DOI:10.1111/2041-210X.13800>. ",
    "version": "1.1-1",
    "maintainer": "Jiangshan Lai <lai@njfu.edu.cn>",
    "url": "https://github.com/laijiangshan/rdacca.hp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21102,
    "package_name": "rdbnomics",
    "title": "Download DBnomics Data",
    "description": "R access to hundreds of millions data series from DBnomics API\n    (<https://db.nomics.world/>).",
    "version": "0.6.4",
    "maintainer": "Sebastien Galais <s915.stem@gmail.com>",
    "url": "https://git.nomics.world/dbnomics/rdbnomics",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21105,
    "package_name": "rddapp",
    "title": "Regression Discontinuity Design Application",
    "description": "Estimation of both single- and multiple-assignment Regression Discontinuity Designs \n    (RDDs). Provides both parametric (global) and non-parametric (local) estimation choices for\n    both sharp and fuzzy designs, along with power analysis and assumption checks. \n    Introductions to the underlying logic and analysis of RDDs are in \n    Thistlethwaite, D. L., Campbell, D. T. (1960) <doi:10.1037/h0044319> and\n    Lee, D. S., Lemieux, T. (2010) <doi:10.1257/jel.48.2.281>.",
    "version": "1.3.3",
    "maintainer": "Felix Thoemmes <fjt36@cornell.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21106,
    "package_name": "rddensity",
    "title": "Manipulation Testing Based on Density Discontinuity",
    "description": "Density discontinuity testing (a.k.a. manipulation testing) is commonly employed in regression discontinuity designs and other program evaluation settings to detect perfect self-selection (manipulation) around a cutoff where treatment/policy assignment changes. This package implements manipulation testing procedures using the local polynomial density estimators: rddensity() to construct test statistics and p-values given a prespecified cutoff, rdbwdensity() to perform data-driven bandwidth selection, and rdplotdensity() to construct density plots.",
    "version": "2.6",
    "maintainer": "Xinwei Ma <x1ma@ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21108,
    "package_name": "rddtools",
    "title": "Toolbox for Regression Discontinuity Design ('RDD')",
    "description": "Set of functions for Regression Discontinuity Design ('RDD'), for\n    data visualisation, estimation and testing.",
    "version": "2.0.2",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "url": "https://github.com/bquast/rddtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21114,
    "package_name": "rdhte",
    "title": "Heterogeneous Treatment Effects in Regression Discontinuity\nDesigns",
    "description": "Understanding heterogeneous causal effects based on pretreatment covariates is a crucial step in modern empirical work in data science. Building on the recent developments in Calonico et al (2025) <https://rdpackages.github.io/references/Calonico-Cattaneo-Farrell-Palomba-Titiunik_2025_HTERD.pdf>, this package provides tools for estimation and inference of heterogeneous treatment effects in Regression Discontinuity (RD) Designs. The package includes two main commands: 'rdhte' to conduct estimation and robust bias-corrected inference for conditional RD treatment effects (given choice of bandwidth parameter); 'rdbwhte', which implements automatic bandwidth selection methods; and 'rdhte_lincom' to test linear combinations of parameters.",
    "version": "0.1.0",
    "maintainer": "Sebastian Calonico <scalonico@ucdavis.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21119,
    "package_name": "rdlearn",
    "title": "Safe Policy Learning under Regression Discontinuity Design with\nMultiple Cutoffs",
    "description": "Implements safe policy learning under regression discontinuity designs\n    with multiple cutoffs, based on Zhang et al. (2022) <doi:10.48550/arXiv.2208.13323>.\n    The learned cutoffs are guaranteed  to perform no worse than the existing\n    cutoffs in terms of overall outcomes. The 'rdlearn' package also includes\n    features for visualizing the learned cutoffs relative to the baseline and\n    conducting sensitivity analyses.",
    "version": "0.1.1",
    "maintainer": "Kentaro Kawato <kentaro1358nohe@gmail.com>",
    "url": "https://github.com/kkawato/rdlearn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21120,
    "package_name": "rdlocrand",
    "title": "Local Randomization Methods for RD Designs",
    "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. Under the local randomization approach, RD designs can be interpreted as randomized experiments inside a window around the cutoff. This package provides tools to perform randomization inference for RD designs under local randomization: rdrandinf() to perform hypothesis testing using randomization inference, rdwinselect() to select a window around the cutoff in which randomization is likely to hold, rdsensitivity() to assess the sensitivity of the results to different window lengths and null hypotheses and rdrbounds() to construct Rosenbaum bounds for sensitivity to unobserved confounders. See Cattaneo, Titiunik and Vazquez-Bare (2016) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2016_Stata.pdf> for further methodological details.",
    "version": "1.1",
    "maintainer": "Gonzalo Vazquez-Bare <gvazquez@econ.ucsb.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21121,
    "package_name": "rdmulti",
    "title": "Analysis of RD Designs with Multiple Cutoffs or Scores",
    "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. The 'rdmulti' package provides tools to analyze RD designs with multiple cutoffs or scores: rdmc() estimates pooled and cutoff specific effects for multi-cutoff designs, rdmcplot() draws RD plots for multi-cutoff designs and rdms() estimates effects in cumulative cutoffs or multi-score designs. See Cattaneo, Titiunik and Vazquez-Bare (2020) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2020_Stata.pdf> for further methodological details. ",
    "version": "1.2",
    "maintainer": "Gonzalo Vazquez-Bare <gvazquez@econ.ucsb.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21126,
    "package_name": "rdpower",
    "title": "Power Calculations for RD Designs",
    "description": "The regression discontinuity (RD) design is a popular quasi-experimental design for causal inference and policy evaluation. The 'rdpower' package provides tools to perform power, sample size and MDE calculations in RD designs: rdpower() calculates the power of an RD design, rdsampsi() calculates the required sample size to achieve a desired power and rdmde() calculates minimum detectable effects. See Cattaneo, Titiunik and Vazquez-Bare (2019) <https://rdpackages.github.io/references/Cattaneo-Titiunik-VazquezBare_2019_Stata.pdf> for further methodological details.",
    "version": "2.3",
    "maintainer": "Gonzalo Vazquez-Bare <gvazquez@econ.ucsb.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21128,
    "package_name": "rdrobust",
    "title": "Robust Data-Driven Statistical Inference in\nRegression-Discontinuity Designs",
    "description": "Regression-discontinuity (RD) designs are quasi-experimental research designs popular in social, behavioral and natural sciences. The RD design is usually employed to study the (local) causal effect of a treatment, intervention or policy. This package provides tools for data-driven graphical and analytical statistical inference in RD\tdesigns: rdrobust() to construct local-polynomial point estimators and robust confidence intervals for average treatment effects at the cutoff in Sharp, Fuzzy and Kink RD settings, rdbwselect() to perform bandwidth selection for the different procedures implemented, and rdplot() to conduct exploratory data analysis (RD plots).",
    "version": "3.0.0",
    "maintainer": "Sebastian Calonico <scalonico@ucdavis.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21136,
    "package_name": "reReg",
    "title": "Recurrent Event Regression",
    "description": "A comprehensive collection of practical and easy-to-use tools for regression analysis of recurrent events, with or without the presence of a (possibly) informative terminal event described in Chiou et al. (2023) <doi:10.18637/jss.v105.i05>. The modeling framework is based on a joint frailty scale-change model, that includes models described in Wang et al. (2001) <doi:10.1198/016214501753209031>, Huang and Wang (2004) <doi:10.1198/016214504000001033>, Xu et al. (2017) <doi:10.1080/01621459.2016.1173557>, and Xu et al. (2019) <doi:10.5705/SS.202018.0224> as special cases. The implemented estimating procedure does not require any parametric assumption on the frailty distribution. The package also allows the users to specify different model forms for both the recurrent event process and the terminal event. ",
    "version": "1.4.7",
    "maintainer": "Sy Han (Steven) Chiou <schiou@smu.edu>",
    "url": "https://github.com/stc04003/reReg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21145,
    "package_name": "reactor",
    "title": "Interactive, Reactive Notebooks for R",
    "description": "Reactor notebooks are web-based documents that combine R code, plots, interactive widgets, and markdown.",
    "version": "0.0.0.9000",
    "maintainer": "",
    "url": "https://github.com/herbps10/reactor",
    "exports": [],
    "topics": ["interactive", "notebook", "notebook-interface", "r", "reactive", "statistics"],
    "score": "NA",
    "stars": 104
  },
  {
    "id": 21172,
    "package_name": "readrba",
    "title": "Download and Tidy Data from the Reserve Bank of Australia",
    "description": "Download up-to-date data from the Reserve Bank of Australia \n    in a tidy data frame. Package includes functions to download current and \n    historical statistical tables \n    (<https://www.rba.gov.au/statistics/tables/>) and forecasts \n    (<https://www.rba.gov.au/publications/smp/forecasts-archive.html>). Data\n    includes a broad range of Australian macroeconomic and financial time\n    series.",
    "version": "0.1.12",
    "maintainer": "Matt Cowgill <mattcowgill@gmail.com>",
    "url": "https://mattcowgill.github.io/readrba/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21183,
    "package_name": "realTimeloads",
    "title": "Analyte Flux and Load from Estimates of Concentration and\nDischarge",
    "description": "Flux (mass per unit time) and Load (mass) are computed from timeseries estimates of analyte concentration and discharge. Concentration timeseries are computed from regression between surrogate and user-provided analyte. Uncertainty in calculations is estimated using bootstrap resampling. Code for the processing of acoustic backscatter from horizontally profiling acoustic Doppler current profilers is provided. All methods detailed in Livsey et al (2020) <doi:10.1007/s12237-020-00734-z>, Livsey et al (2023) <doi:10.1029/2022WR033982>, and references therein.",
    "version": "1.0.0",
    "maintainer": "Daniel Livsey <livsey.daniel@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21204,
    "package_name": "recforest",
    "title": "Random Survival Forest for Recurrent Events",
    "description": "Analyze recurrent events with right-censored data and the potential presence of a terminal event (that prevents further occurrences, like death). 'recofest' extends the random survival forest algorithm, adapting splitting rules and node estimators to handle complexities of recurrent events. The methodology is fully described in Murris, J., Bouaziz, O., Jakubczak, M., Katsahian, S., & Lavenu, A. (2024) (<https://hal.science/hal-04612431v1/document>).",
    "version": "1.0.0",
    "maintainer": "Juliette Murris <murris.juliette@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21263,
    "package_name": "reformulas",
    "title": "Machinery for Processing Random Effect Formulas",
    "description": "Takes formulas including random-effects components (formatted as in 'lme4', 'glmmTMB', etc.) and processes them. Includes various helper functions.",
    "version": "0.4.3.1",
    "maintainer": "Ben Bolker <bolker@mcmaster.ca>",
    "url": "https://github.com/bbolker/reformulas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21271,
    "package_name": "refund",
    "title": "Regression with Functional Data",
    "description": "Methods for regression for functional\n    data, including function-on-scalar, scalar-on-function, and\n    function-on-function regression. Some of the functions are applicable to\n    image data.",
    "version": "0.1-38",
    "maintainer": "Julia Wrobel <julia.wrobel@emory.edu>",
    "url": "https://github.com/refunders/refund",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21275,
    "package_name": "regRSM",
    "title": "Random Subspace Method (RSM) for Linear Regression",
    "description": "Performs Random Subspace Method (RSM) for high-dimensional linear regression to obtain variable importance measures. The final model is chosen based on validation set or Generalized Information Criterion.",
    "version": "0.5",
    "maintainer": "Pawel Teisseyre <teisseyrep@ipipan.waw.pl>",
    "url": "http://www.ipipan.eu/~teisseyrep/SOFTWARE/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21276,
    "package_name": "regclass",
    "title": "Tools for an Introductory Class in Regression and Modeling",
    "description": "Contains basic tools for visualizing, interpreting, and building regression models.  It has been designed for use with the book Introduction to Regression and Modeling with R by Adam Petrie, Cognella Publishers, ISBN: 978-1-63189-250-9.",
    "version": "1.7",
    "maintainer": "Adam Petrie <apetrie@utk.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21281,
    "package_name": "reghelper",
    "title": "Helper Functions for Regression Analysis",
    "description": "A set of functions used to automate commonly used methods in\n    regression analysis. This includes plotting interactions, and calculating\n    simple slopes, standardized coefficients, regions of significance\n    (Johnson & Neyman, 1936; cf. Spiller et al., 2012), etc. See the reghelper\n    documentation for more information, documentation, and examples.",
    "version": "1.1.2",
    "maintainer": "Jeffrey Hughes <jeff.hughes@gmail.com>",
    "url": "https://github.com/jeff-hughes/reghelper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21288,
    "package_name": "regions",
    "title": "Processing Regional Statistics",
    "description": "Validating sub-national statistical typologies, re-coding across \n    standard typologies of sub-national statistics, and making valid aggregate\n    level imputation, re-aggregation, re-weighting and projection down to \n    lower hierarchical levels to create meaningful data panels and time series.",
    "version": "0.1.8",
    "maintainer": "Daniel Antal <daniel.antal@ceemid.eu>",
    "url": "https://regions.dataobservatory.eu/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21294,
    "package_name": "regnet",
    "title": "Network-Based Regularization for Generalized Linear Models",
    "description": "Network-based regularization has achieved success in variable selection for \n    high-dimensional biological data due to its ability to incorporate correlations among \n    genomic features. This package provides procedures of network-based variable selection \n    for generalized linear models (Ren et al. (2017) <doi:10.1186/s12863-017-0495-5> and \n\tRen et al.(2019) <doi:10.1002/gepi.22194>). Continuous, binary, and survival response \n\tare supported. Robust network-based methods are available for continuous and survival \n\tresponses. ",
    "version": "1.0.2",
    "maintainer": "Jie Ren <renjie0910@gmail.com>",
    "url": "https://github.com/jrhub/regnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21295,
    "package_name": "regplot",
    "title": "Enhanced Regression Nomogram Plot",
    "description": "A function to plot a regression nomogram of regression \n    objects. Covariate distributions are superimposed on nomogram scales and the plot\n\tcan be animated to allow on-the-fly changes to distribution representation and to \n\tenable outcome calculation. ",
    "version": "1.1",
    "maintainer": "Roger Marshall <rj.marshall@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21296,
    "package_name": "regport",
    "title": "Regression Model Processing Port",
    "description": "Provides R6 classes, methods and utilities to construct,\n    analyze, summarize, and visualize regression models.",
    "version": "0.3.1",
    "maintainer": "Shixiang Wang <w_shixiang@163.com>",
    "url": "https://github.com/ShixiangWang/regport,\nhttps://shixiangwang.github.io/regport/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21297,
    "package_name": "regr.easy",
    "title": "Easy Linear, Quadratic and Cubic Regression Models",
    "description": "Focused on linear, quadratic and cubic regression models, it has a function for calculating the models, obtaining a list with their parameters, and a function for making the graphs for the respective models.",
    "version": "1.0.2",
    "maintainer": "Wagner Martins dos Santos <wagnnerms97@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21301,
    "package_name": "regress3d",
    "title": "Create 3D Regression Surfaces",
    "description": "Plot regression surfaces and marginal effects in three dimensions. The plots are 'plotly' objects and can be customized using functions and arguments from the 'plotly' package.",
    "version": "1.0.0",
    "maintainer": "Ella Foster-Molina <ella.fostermolina@gmail.com>",
    "url": "https://github.com/ellaFosterMolina/regress3d,\nhttps://ellafostermolina.github.io/regress3d/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21302,
    "package_name": "regressinator",
    "title": "Simulate and Diagnose (Generalized) Linear Models",
    "description": "Simulate samples from populations with known covariate\n    distributions, generate response variables according to common linear and\n    generalized linear model families, draw from sampling distributions of\n    regression estimates, and perform visual inference on diagnostics from model\n    fits.",
    "version": "0.3.0",
    "maintainer": "Alex Reinhart <areinhar@stat.cmu.edu>",
    "url": "https://www.refsmmat.com/regressinator/,\nhttps://github.com/capnrefsmmat/regressinator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21304,
    "package_name": "regrrr",
    "title": "Toolkit for Compiling, (Post-Hoc) Testing, and Plotting\nRegression Results",
    "description": "Compiling regression results into a publishable format, conducting post-hoc hypothesis testing, and plotting moderating effects (the effect of X on Y becomes stronger/weaker as Z increases).",
    "version": "0.1.3",
    "maintainer": "Rui K. Yang <rkzyang@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21306,
    "package_name": "regspec",
    "title": "Non-Parametric Bayesian Spectrum Estimation for Multirate Data",
    "description": "Computes linear Bayesian spectral estimates from multirate\tdata for second-order stationary time series. Provides credible intervals and methods for plotting various spectral estimates. Please see the paper `Should we sample a time series more frequently?' (doi below) for a full description of and motivation for the methodology.",
    "version": "2.7",
    "maintainer": "Ben Powell <ben.powell@york.ac.uk>",
    "url": "https://doi.org/10.1111/rssa.12210",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21309,
    "package_name": "regtomean",
    "title": "Regression Toward the Mean",
    "description": "In repeated measures studies with extreme large or small values it is common \n  that the subjects measurements on average are closer to the mean of the basic population. \n  Interpreting possible changes in the mean in such situations can lead to biased results \n  since the values were not randomly selected, they come from truncated sampling.\n  This method allows to estimate the range of means where treatment effects are likely to occur \n  when regression toward the mean is present. \n  Ostermann, T., Willich, Stefan N. & Luedtke, Rainer. (2008). Regression toward the mean - a detection method for unknown population mean based on Mee and Chua's algorithm. BMC Medical Research Methodology.<doi:10.1186/1471-2288-8-52>.\n  Acknowledgments: We would like to acknowledge \"Lena Roth\" and \"Nico Steckhan\" for the package's initial updates (Q3 2024) and continued supervision and guidance. Both have contributed to discussing and integrating these methods into the package, ensuring they are up-to-date and contextually relevant.",
    "version": "1.2.1",
    "maintainer": "Daniela Recchia <daniela.rodriguesrecchia@uni-wh.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21324,
    "package_name": "relevance",
    "title": "Calculate Relevance and Significance Measures",
    "description": "Calculates relevance and significance values for\n  simple models and for many types of regression models.\n  These are introduced in\n  'Stahel, Werner A.' (2021)\n  \"Measuring Significance and Relevance instead of p-values.\"\n  <https://stat.ethz.ch/~stahel/relevance/stahel-relevance2103.pdf>.\n  These notions are also applied to replication studies,\n  as described in the manuscript\n  'Stahel, Werner A.' (2022)\n  \"'Replicability': Terminology, Measuring Success, and Strategy\"\n  available in the documentation.",
    "version": "2.1",
    "maintainer": "Werner A. Stahel <stahel@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21328,
    "package_name": "reliabilitydiag",
    "title": "Reliability Diagrams Using Isotonic Regression",
    "description": "Checking the reliability of predictions via the CORP approach,\n    which generates provably statistically 'C'onsistent, 'O'ptimally binned, and\n    'R'eproducible reliability diagrams using the 'P'ool-adjacent-violators\n    algorithm. See Dimitriadis, Gneiting, Jordan (2021) <doi:10.1073/pnas.2016191118>.",
    "version": "0.2.1",
    "maintainer": "Alexander I. Jordan <alexander.jordan@h-its.org>",
    "url": "https://github.com/aijordan/reliabilitydiag/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21329,
    "package_name": "relimp",
    "title": "Relative Contribution of Effects in a Regression Model",
    "description": "Functions to facilitate inference on the relative importance of predictors in a linear or generalized linear model, and a couple of useful Tcl/Tk widgets.",
    "version": "1.0-5",
    "maintainer": "David Firth <d.firth@warwick.ac.uk>",
    "url": "http://warwick.ac.uk/relimp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21330,
    "package_name": "relsurv",
    "title": "Relative Survival",
    "description": "Contains functions for analysing relative survival data, including nonparametric estimators of net (marginal relative) survival, relative survival ratio, crude mortality, methods for fitting and checking additive and multiplicative regression models, transformation approach, methods for dealing with population mortality tables. Work has been described in Pohar Perme, Pavlic (2018) <doi:10.18637/jss.v087.i08>.",
    "version": "2.3-3",
    "maintainer": "Damjan Manevski <damjan.manevski@mf.uni-lj.si>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21342,
    "package_name": "remote",
    "title": "Empirical Orthogonal Teleconnections in R",
    "description": "Empirical orthogonal teleconnections in R.\n    'remote' is short for 'R(-based) EMpirical Orthogonal TEleconnections'.\n    It implements a collection of functions to facilitate empirical\n    orthogonal teleconnection analysis. Empirical Orthogonal Teleconnections\n    (EOTs) denote a regression based approach to decompose spatio-temporal\n    fields into a set of independent orthogonal patterns. They are quite\n    similar to Empirical Orthogonal Functions (EOFs) with EOTs producing\n    less abstract results. In contrast to EOFs, which are orthogonal in both\n    space and time, EOT analysis produces patterns that are orthogonal in\n    either space or time.",
    "version": "1.2.3",
    "maintainer": "Tim Appelhans <tim.appelhans@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21348,
    "package_name": "remstimate",
    "title": "Optimization Frameworks for Tie-Oriented and Actor-Oriented\nRelational Event Models",
    "description": "A comprehensive set of tools designed for optimizing likelihood within a tie-oriented (Butts, C., 2008, <doi:10.1111/j.1467-9531.2008.00203.x>) or an actor-oriented modelling framework (Stadtfeld, C., & Block, P., 2017, <doi:10.15195/v4.a14>) in relational event networks. The package accommodates both frequentist and Bayesian approaches. The frequentist approaches that the package incorporates are the Maximum Likelihood Optimization (MLE) and the Gradient-based Optimization  (GDADAMAX). The Bayesian methodologies included in the package are the Bayesian Sampling Importance Resampling (BSIR) and the Hamiltonian Monte Carlo (HMC). The flexibility of choosing between frequentist and Bayesian optimization approaches allows researchers to select the estimation approach which aligns the most with their analytical preferences.",
    "version": "2.3.14",
    "maintainer": "Giuseppe Arena <g.arena@uva.nl>",
    "url": "https://tilburgnetworkgroup.github.io/remstimate/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21354,
    "package_name": "renz",
    "title": "R-Enzymology",
    "description": "Contains utilities for the analysis of Michaelian kinetic data. Beside the classical linearization \n\tmethods (Lineweaver-Burk, Eadie-Hofstee, Hanes-Woolf and Eisenthal-Cornish-Bowden), features include the \n\tability to carry out weighted regression analysis that, in most cases, substantially improves the estimation \n\tof kinetic parameters (Aledo (2021) <doi:10.1002/bmb.21522>). To avoid data transformation and the potential\n\tbiases introduced by them, the package also offers functions to directly fitting data to the Michaelis-Menten \n\tequation, either using ([S], v) or (time, [S]) data. Utilities to simulate substrate progress-curves (making \n\tuse of the Lambert W function) are also provided. The package is accompanied of vignettes that aim to orientate\n\tthe user in the choice of the most suitable method to estimate the kinetic parameter of an Michaelian enzyme.",
    "version": "0.2.1",
    "maintainer": "Juan Carlos Aledo <caledo@uma.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21357,
    "package_name": "repeated",
    "title": "Non-Normal Repeated Measurements Models",
    "description": "Various functions to fit models for non-normal repeated\n    measurements, such as Binary Random Effects Models with Two Levels of Nesting,\n    Bivariate Beta-binomial Regression Models, Marginal Bivariate Binomial Regression Models,\n    Cormack capture-recapture models, Continuous-time Hidden Markov Chain Models, \n    Discrete-time Hidden Markov Chain Models,\n    Changepoint Location Models using a Continuous-time Two-state Hidden Markov Chain,\n    generalized nonlinear autoregression models, multivariate Gaussian copula models,\n    generalized non-linear mixed models with one random effect,  \n    generalized non-linear mixed models using h-likelihood for one random effect, \n    Repeated Measurements Models for Counts with Frailty or Serial Dependence,\n    Repeated Measurements Models for Continuous Variables with Frailty or Serial Dependence,\n    Ordinal Random Effects Models with Dropouts, marginal homogeneity models for square\n    contingency tables, correlated negative binomial models with Kalman update.\n    References include Lindsey's text books, \n    JK Lindsey (2001) <isbn:10-0198508123> and JK Lindsey (1999) <isbn:10-0198505590>.",
    "version": "1.1.10",
    "maintainer": "Bruce Swihart <bruce.swihart@gmail.com>",
    "url": "https://www.commanster.eu/rcode.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21403,
    "package_name": "resde",
    "title": "Estimation in Reducible Stochastic Differential Equations",
    "description": "Maximum likelihood estimation for univariate reducible\n stochastic differential equation models. Discrete, possibly noisy\n observations, not necessarily evenly spaced in time. Can fit\n multiple individuals/units with global and local parameters, by\n fixed-effects or mixed-effects methods. Ref.: Garcia, O. (2019)\n \"Estimating reducible stochastic differential equations by\n conversion to a least-squares problem\", Computational Statistics\n 34(1): 23-46, <doi:10.1007/s00180-018-0837-4>.",
    "version": "1.1",
    "maintainer": "Oscar Garcia <garcia@dasometrics.net>",
    "url": "https://github.com/ogarciav/resde/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21411,
    "package_name": "resilience",
    "title": "Predictors of Resilience to a Stressor in a Single-Arm Study",
    "description": "Studies of resilience in older adults employ a single-arm design where everyone experiences the stressor. The simplistic approach of regressing change versus baseline yields biased estimates due to regression-to-the-mean. This package provides a method to correct the bias. It also allows covariates to be included. The method implemented in the package is described in Varadhan, R., Zhu, J., and Bandeen-Roche, K (2024), Biostatistics 25(4): 1094-1111.",
    "version": "2025.1.1",
    "maintainer": "Ravi Varadhan <ravi.varadhan@jhu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21414,
    "package_name": "reslr",
    "title": "Modelling Relative Sea Level Data",
    "description": "The Bayesian modelling of relative sea-level data\n    using a comprehensive approach that incorporates\n    various statistical models within a unifying framework. \n    Details regarding each statistical models; \n    linear regression (Ashe et al 2019) <doi:10.1016/j.quascirev.2018.10.032>, \n    change point models (Cahill et al 2015) <doi:10.1088/1748-9326/10/8/084002>, \n    integrated Gaussian process models (Cahill et al 2015) <doi:10.1214/15-AOAS824>, \n    temporal splines (Upton et al 2023) <arXiv:2301.09556>, \n    spatio-temporal splines (Upton et al 2023) <arXiv:2301.09556> and\n    generalised additive models (Upton et al 2023) <arXiv:2301.09556>.\n    This package facilitates data loading, \n    model fitting and result summarisation.\n    Notably, it accommodates the inherent measurement errors\n    found in relative sea-level data across multiple dimensions,\n    allowing for their inclusion in the statistical models.",
    "version": "0.1.1",
    "maintainer": "Maeve Upton <uptonmaeve010@gmail.com>",
    "url": "https://github.com/maeveupton/reslr,\nhttps://maeveupton.github.io/reslr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21416,
    "package_name": "resourcecode",
    "title": "Access to the 'RESOURCECODE' Hindcast Database",
    "description": "Utility functions to download data from the 'RESOURCECODE'\n    hindcast database of sea-states, time series of sea-state parameters\n    and time series of 1D and 2D wave spectra.  See\n    <https://resourcecode.ifremer.fr> for more details about the available\n    data.  Also provides facilities to plot and analyse downloaded data,\n    such as computing the sea-state parameters from both the 1D and 2D\n    surface elevation variance spectral density.",
    "version": "0.5.2",
    "maintainer": "Nicolas Raillard <nicolas.raillard@ifremer.fr>",
    "url": "https://github.com/Resourcecode-project/r-resourcecode,\nhttps://resourcecode-project.github.io/r-resourcecode/,\nhttps://resourcecode-project.r-universe.dev/resourcecode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21417,
    "package_name": "resourcecodedata",
    "title": "Resourcecode Database Configuration Data",
    "description": "Includes Resourcecode hindcast database (see\n    <https://resourcecode.ifremer.fr>) configuration data: nodes locations\n    for both the sea-state parameters and the spectra data; examples of\n    time series of 1D and 2D surface elevation variance spectral density.",
    "version": "1.0.0",
    "maintainer": "Nicolas Raillard <nicolas.raillard@ifremer.fr>",
    "url": "https://github.com/Resourcecode-project/r-resourcecodedata/,\nhttps://resourcecode-project.r-universe.dev/resourcecodedata/,\nhttps://resourcecode-project.github.io/r-resourcecodedata/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21455,
    "package_name": "reverseR",
    "title": "Linear Regression Stability to Significance Reversal",
    "description": "Tests linear regressions for significance reversal through leave-one(multiple)-out.",
    "version": "0.2",
    "maintainer": "Andrej-Nikolai Spiess <draspiess@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21489,
    "package_name": "rfriend",
    "title": "Provides Batch Functions and Visualisation for Basic Statistical\nProcedures",
    "description": "Designed to streamline data analysis and statistical testing, reducing the length of R \n    scripts while generating well-formatted outputs in 'pdf', 'Microsoft Word', and 'Microsoft Excel' \n    formats. In essence, the package contains functions which are sophisticated wrappers around \n    existing R functions that are called by using 'f_' (user f_riendly) prefix followed by the normal \n    function name. This first version of the 'rfriend' package focuses  primarily on data exploration, \n    including tools for creating summary tables, f_summary(), performing data transformations, \n    f_boxcox() in part based on 'MASS/boxcox' and 'rcompanion', and f_bestNormalize() \n    which wraps and extends functionality from the 'bestNormalize' package. Furthermore, 'rfriend' \n    can automatically (or on request) generate visualizations such as boxplots, f_boxplot(), \n    QQ-plots, f_qqnorm(), histograms f_hist(), and density plots. Additionally, the package includes \n    four statistical test functions: f_aov(), f_kruskal_test(), f_glm(), f_chisq_test for sequential \n    testing and visualisation of the 'stats' functions: aov(), kruskal.test(), glm() and chisq.test. \n    These functions support testing multiple response variables and predictors, while also handling \n    assumption checks, data transformations, and post hoc tests. Post hoc results are automatically \n    summarized in a table using the compact letter display (cld) format for easy interpretation. The \n    package also provides a function to do model comparison, f_model_comparison(), and several \n    utility functions to simplify common R tasks. For example, f_clear() clears the workspace and \n    restarts R with a single command; f_setwd() sets the working directory to match the directory \n    of the current script; f_theme() quickly changes 'RStudio' themes; and f_factors() converts \n    multiple columns of a data frame to factors, and much more. If you encounter any issues or have \n    feature requests, please feel free to contact me via email.",
    "version": "2.0.0",
    "maintainer": "Sander H. van Delden <plantmind@proton.me>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21536,
    "package_name": "rhnerm",
    "title": "Random Heteroscedastic Nested Error Regression",
    "description": "Performs the random heteroscedastic nested error regression model described in Kubokawa, Sugasawa, Ghosh and Chaudhuri (2016) <doi:10.5705/ss.202014.0070>.",
    "version": "1.1",
    "maintainer": "Shonosuke Sugasawa <shonosuke622@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21539,
    "package_name": "rhosa",
    "title": "Higher-Order Spectral Analysis",
    "description": "Higher-order spectra or polyspectra of time series, such as bispectrum and bicoherence, have been investigated in abundant literature and applied to problems of signal detection in a wide range of fields. This package aims to provide a simple API to estimate and analyze them. The current implementation is based on Brillinger and Irizarry (1998) <doi:10.1016/S0165-1684(97)00217-X> for estimating bispectrum or bicoherence, Lii and Helland (1981) <doi:10.1145/355958.355961> for cross-bispectrum, and Kim and Powers (1979) <doi:10.1109/TPS.1979.4317207> for cross-bicoherence.",
    "version": "0.3.0",
    "maintainer": "Takeshi Abe <tabe@fixedpoint.jp>",
    "url": "https://tabe.github.io/rhosa/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21544,
    "package_name": "riAFTBART",
    "title": "A Flexible Approach for Causal Inference with Multiple\nTreatments and Clustered Survival Outcomes",
    "description": "Random-intercept accelerated failure time (AFT) model utilizing Bayesian additive regression trees (BART) for drawing causal inferences about multiple treatments while accounting for the multilevel survival data structure. It also includes an interpretable sensitivity analysis approach to evaluate how the drawn causal conclusions might be altered in response to the potential magnitude of departure from the no unmeasured confounding assumption.This package implements the methods described by Hu et al. (2022) <doi:10.1002/sim.9548>.",
    "version": "0.3.3",
    "maintainer": "Fengrui Zhang <fz174@sph.rutgers.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21557,
    "package_name": "rid",
    "title": "Multiple Change-Point Detection in Multivariate Time Series",
    "description": "Provides efficient functions for detecting multiple change points in multidimensional time series. The models can be piecewise constant or polynomial. Adaptive threshold selection methods are available, see Fan and Wu (2024)\t<arXiv:2403.00600>.",
    "version": "0.0.1",
    "maintainer": "Xinyuan Fan <fxy22@mails.tsinghua.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21567,
    "package_name": "rifreg",
    "title": "Estimate Recentered Influence Function Regression",
    "description": "Provides functions to compute recentered influence functions\n    (RIF) of a distributional variable at the mean, quantiles, variance,\n    gini or any custom functional of interest. The package allows to\n    regress the RIF on any number of covariates. Generic print, plot and\n    summary functions are also provided. Reference: Firpo, Sergio, Nicole M. Fortin, and Thomas Lemieux. (2009) <doi:10.3982/ECTA6822>. \"Unconditional Quantile Regressions.\".",
    "version": "1.1.0",
    "maintainer": "Samuel Meier <samuel.meier+cran@immerda.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21575,
    "package_name": "rineq",
    "title": "Concentration Index and Decomposition for Health Inequalities",
    "description": "Relative, generalized, and Erreygers corrected concentration index; plot Lorenz curves; and decompose health\n    inequalities into contributing factors. The package currently works with (generalized) linear models, survival models, complex survey models, and marginal effects probit models.\n    originally forked by Brecht Devleesschauwer from the 'decomp' package  (no longer on CRAN), 'rineq' is now maintained by Kaspar Walter Meili. Compared to the earlier 'rineq' version on 'github' by Brecht Devleesschauwer (<https://github.com/brechtdv/rineq>), the regression tree functionality has been removed.\n    Improvements compared to earlier versions include improved plotting of decomposition and concentration, added functionality to calculate the concentration index with different methods, calculation of robust standard errors, and support for the decomposition analysis using marginal effects probit regression models. The development version is available at <https://github.com/kdevkdev/rineq>.",
    "version": "0.3.0",
    "maintainer": "Kaspar Meili <meilikaspar@yahoo.de>",
    "url": "https://github.com/kdevkdev/rineq",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21583,
    "package_name": "rioplot",
    "title": "Turn a Regression Model Inside Out",
    "description": "Turns regression models inside out. Functions decompose variances and coefficients for various regression model types. Functions also visualize regression model objects using techniques developed in Schoon, Melamed, and Breiger (2024) <doi:10.1017/9781108887205>.",
    "version": "1.1.1",
    "maintainer": "David Melamed <dmmelamed@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21592,
    "package_name": "riskRegression",
    "title": "Risk Regression Models and Prediction Scores for Survival\nAnalysis with Competing Risks",
    "description": "Implementation of the following methods for event history analysis.\n    Risk regression models for survival endpoints also in the presence of competing\n    risks are fitted using binomial regression based on a time sequence of binary\n    event status variables. A formula interface for the Fine-Gray regression model\n    and an interface for the combination of cause-specific Cox regression models.\n    A toolbox for assessing and comparing performance of risk predictions (risk\n    markers and risk prediction models). Prediction performance is measured by the\n    Brier score and the area under the ROC curve for binary possibly time-dependent\n    outcome. Inverse probability of censoring weighting and pseudo values are used\n    to deal with right censored data. Lists of risk markers and lists of risk models\n    are assessed simultaneously. Cross-validation repeatedly splits the data, trains\n    the risk prediction models on one part of each split and then summarizes and\n    compares the performance across splits.",
    "version": "2025.09.17",
    "maintainer": "Thomas Alexander Gerds <tag@biostat.ku.dk>",
    "url": "https://github.com/tagteam/riskRegression",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21600,
    "package_name": "risksetROC",
    "title": "Riskset ROC Curve Estimation from Censored Survival Data",
    "description": "Compute time-dependent Incident/dynamic accuracy measures\n        (ROC curve, AUC, integrated AUC )from censored survival data\n        under proportional or non-proportional hazard assumption of\n        Heagerty & Zheng (Biometrics, Vol 61 No 1, 2005, PP 92-105).",
    "version": "1.0.4.1",
    "maintainer": "Paramita Saha-Chaudhuri\n<paramita.sahachaudhuri.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21614,
    "package_name": "rjags",
    "title": "Bayesian Graphical Models using MCMC",
    "description": "Interface to the JAGS MCMC library.",
    "version": "4-17",
    "maintainer": "Martyn Plummer <martyn.plummer@gmail.com>",
    "url": "https://mcmc-jags.sourceforge.io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21630,
    "package_name": "rjuliabugs",
    "title": "Interface to 'JuliaBUGS.jl' from R",
    "description": "Provides an R interface to the 'JuliaBUGS.jl' package (<https://github.com/TuringLang/JuliaBUGS.jl>) for Bayesian inference using the BUGS modeling language. Allows R users to run models in Julia and return results as familiar R objects. Visualization and posterior analysis are supported via the 'bayesplot' and 'posterior' packages.",
    "version": "0.1.0",
    "maintainer": "Mateus Maia <mateus.maiamarques@glasgow.ac.uk>",
    "url": "https://mateusmaiads.github.io/rjuliabugs/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21651,
    "package_name": "rlmDataDriven",
    "title": "Robust Regression with Data Driven Tuning Parameter",
    "description": "Data driven approach for robust regression estimation in homoscedastic and heteroscedastic context. See Wang et al. (2007), <doi:10.1198/106186007X180156> regarding homoscedastic framework.  ",
    "version": "0.4.0",
    "maintainer": "You-Gan Wang <you-gan.wang@qut.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21657,
    "package_name": "rmBayes",
    "title": "Performing Bayesian Inference for Repeated-Measures Designs",
    "description": "A Bayesian credible interval is interpreted with respect to posterior probability, \n    and this interpretation is far more intuitive than that of a frequentist confidence interval. \n    However, standard highest-density intervals can be wide due to between-subjects variability and tends \n    to hide within-subject effects, rendering its relationship with the Bayes factor less clear \n    in within-subject (repeated-measures) designs. \n    This urgent issue can be addressed by using within-subject intervals in within-subject designs, \n    which integrate four methods including the Wei-Nathoo-Masson (2023) <doi:10.3758/s13423-023-02295-1>, \n    the Loftus-Masson (1994) <doi:10.3758/BF03210951>, \n    the Nathoo-Kilshaw-Masson (2018) <doi:10.1016/j.jmp.2018.07.005>, \n    and the Heck (2019) <doi:10.31234/osf.io/whp8t> interval estimates.",
    "version": "0.1.16",
    "maintainer": "Zhengxiao Wei <zhengxiao@uvic.ca>",
    "url": "https://github.com/zhengxiaoUVic/rmBayes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21692,
    "package_name": "rmp",
    "title": "Rounded Mixture Package",
    "description": "Performs univariate probability mass function estimation via Bayesian nonparametric mixtures of rounded kernels as in Canale and Dunson (2011) <doi:10.1198/jasa.2011.tm10552>.",
    "version": "2.2",
    "maintainer": "Antonio Canale <antonio.canale@unipd.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21694,
    "package_name": "rms",
    "title": "Regression Modeling Strategies",
    "description": "Regression modeling, testing, estimation, validation,\n\tgraphics, prediction, and typesetting by storing enhanced model design\n\tattributes in the fit.  'rms' is a collection of functions that\n\tassist with and streamline modeling.  It also contains functions for\n\tbinary and ordinal logistic regression models, ordinal models for\n  continuous Y with a variety of distribution families, and the Buckley-James\n\tmultiple regression model for right-censored responses, and implements\n\tpenalized maximum likelihood estimation for logistic and ordinary\n\tlinear models.  'rms' works with almost any regression model, but it\n\twas especially written to work with binary or ordinal regression\n\tmodels, Cox regression, accelerated failure time models,\n\tordinary linear models,\tthe Buckley-James model, generalized least\n\tsquares for serially or spatially correlated observations, generalized\n\tlinear models, and quantile regression.",
    "version": "8.1-0",
    "maintainer": "Frank E Harrell Jr <fh@fharrell.com>",
    "url": "https://hbiostat.org/R/rms/, https://github.com/harrelfe/rms",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21700,
    "package_name": "rmstBayespara",
    "title": "Bayesian Restricted Mean Survival Time for Cluster Effect",
    "description": "The parametric Bayes analysis for the \n    restricted mean survival time (RMST) with cluster effect,\n    as described in Hanada and Kojima (2024) <doi:10.48550/arXiv.2406.06071>.\n    Bayes estimation with random-effect and frailty-effect can be applied to \n    several parametric models useful in survival time analysis.\n    The RMST under these parametric models can be computed from \n    the obtained posterior samples.",
    "version": "0.1.0",
    "maintainer": "Keisuke Hanada <keisuke.hanada.87@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21701,
    "package_name": "rmstcompsens",
    "title": "Comparing Restricted Mean Survival Time as Sensitivity Analysis",
    "description": "Performs two-sample comparisons using the restricted mean survival time (RMST) when survival curves end at different time points between groups. This package implements a sensitivity approach that allows the threshold  timepoint tau to be specified after the longest survival time in the shorter survival group. Two kinds of between-group contrast estimators (the difference in RMST and the ratio of RMST) are computed: Uno et al(2014)<doi:10.1200/JCO.2014.55.2208>, Uno et al(2022)<https://CRAN.R-project.org/package=survRM2>, Ueno and Morita(2023)<doi:10.1007/s43441-022-00484-z>.",
    "version": "0.1.5",
    "maintainer": "Kentaro Ueno <ueno_kentaro@kuhp.kyoto-u.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21704,
    "package_name": "rmutil",
    "title": "Utilities for Nonlinear Regression and Repeated Measurements\nModels",
    "description": "A toolkit of functions for nonlinear regression and repeated\n    measurements not to be used by itself but called by other Lindsey packages such\n    as 'gnlm', 'stable', 'growth', 'repeated', and 'event' \n    (available at <https://www.commanster.eu/rcode.html>).",
    "version": "1.1.10",
    "maintainer": "Bruce Swihart <bruce.swihart@gmail.com>",
    "url": "https://www.commanster.eu/rcode.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21737,
    "package_name": "robRatio",
    "title": "M-Estimators for Generalized Ratio and Linear Regression Models",
    "description": "Robust estimators for generalized ratio model (Wada, Sakashita and Tsubaki, 2021)<doi:10.17713/ajs.v50i1.994> and linear regression model by the IRLS(iterative reweighted least squares) algorithm are contained.",
    "version": "0.1.0",
    "maintainer": "Kazumi Wada <kazwd2008@gmail.com>",
    "url": "<https://github.com/kazwd2008/robRatio>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21738,
    "package_name": "robStepSplitReg",
    "title": "Robust Stepwise Split Regularized Regression",
    "description": "Functions to perform robust stepwise split regularized regression. The approach first \n             uses a robust stepwise algorithm to split the variables into the models of an ensemble. \n             An adaptive robust regularized estimator is then applied to each subset of predictors\n             in the models of an ensemble. ",
    "version": "2.0.0",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21742,
    "package_name": "robcp",
    "title": "Robust Change-Point Tests",
    "description": "Provides robust methods to detect change-points in uni- or multivariate time series. They can cope with corrupted data and heavy tails. Focus is on the detection of abrupt changes in location, but changes in the scale or dependence structure can be detected as well. This package provides tests for change detection in uni- and multivariate time series based on Huberized versions of CUSUM tests proposed in Duerre and Fried (2019) <DOI:10.48550/arXiv.1905.06201>, and tests for change detection in univariate time series based on 2-sample U-statistics or 2-sample U-quantiles as proposed by Dehling et al. (2015) <DOI:10.1007/978-1-4939-3076-0_12> and Dehling, Fried and Wendler (2020) <DOI:10.1093/biomet/asaa004>. Furthermore, the packages provides tests on changes in the scale or the correlation as proposed in Gerstenberger, Vogel and Wendler (2020) <DOI:10.1080/01621459.2019.1629938>, Dehling et al. (2017) <DOI:10.1017/S026646661600044X>, and Wied et al. (2014) <DOI:10.1016/j.csda.2013.03.005>.",
    "version": "0.3.10",
    "maintainer": "Sheila Goerz <sheila.goerz@tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21744,
    "package_name": "robfilter",
    "title": "Robust Time Series Filters",
    "description": "Implementations for several robust procedures that allow for (online)\n        extraction of the signal of univariate or multivariate time series by\n        applying robust regression techniques to a moving time window are provided.\n        Included are univariate filtering procedures based on repeated-median \n        regression as well as hybrid and trimmed filters derived from it; \n        see Schettlinger et al. (2006) <doi:10.1515/BMT.2006.010>. The adaptive \n        online repeated median by Schettlinger et al. (2010) <doi:10.1002/acs.1105> \n        and the slope comparing adaptive repeated median by Borowski and Fried (2013) \n        <doi:10.1007/s11222-013-9391-7> choose the width of the moving time \n        window adaptively. Multivariate versions are also provided; see  \n        Borowski et al. (2009) <doi:10.1080/03610910802514972> for a multivariate \n        online adaptive repeated median and Borowski (2012) <doi:10.17877/DE290R-14393>  \n        for a multivariate slope comparing adaptive repeated median. Furthermore, \n        a repeated-median based filter with automatic outlier replacement and \n        shift detection is provided; see Fried (2004) <doi:10.1080/10485250410001656444>.",
    "version": "4.1.6",
    "maintainer": "Roland Fried <fried@statistik.tu-dortmund.de>",
    "url": "https://msnat.statistik.tu-dortmund.de/en/team/chair/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21745,
    "package_name": "robflreg",
    "title": "Robust Functional Linear Regression",
    "description": "Functions for implementing robust methods for functional linear regression. In the functional linear regression, we consider scalar-on-function linear regression and function-on-function linear regression.",
    "version": "1.3",
    "maintainer": "Ufuk Beyaztas <ufukbeyaztas@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21751,
    "package_name": "robmixglm",
    "title": "Robust Generalized Linear Models (GLM) using Mixtures",
    "description": "Robust generalized linear models (GLM) using a mixture method, as described in Beath (2018) <doi:10.1080/02664763.2017.1414164>. This assumes that the data are a mixture of standard observations, being a generalised linear model, and outlier observations from an overdispersed generalized linear model. The overdispersed linear model is obtained by including a normally distributed random effect in the linear predictor of the generalized linear model.",
    "version": "1.2-5",
    "maintainer": "Ken Beath <ken@kjbeath.id.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21753,
    "package_name": "roboBayes",
    "title": "Robust Online Bayesian Monitoring",
    "description": "An implementation of Bayesian online changepoint detection \n  (Adams and MacKay (2007) <doi:10.48550/arXiv.0710.3742>) with an option for probability \n  based outlier detection and removal (Wendelberger et. al. (2021) <doi:10.48550/arXiv.2112.12899>). \n  Building on the independent multivariate constant mean model implemented in \n  the 'R' package 'ocp', this package models multivariate data as multivariate \n  normal about a linear trend, defined by user input covariates, with an \n  unstructured error covariance. Changepoints are identified based on a \n  probability threshold for windows of points.",
    "version": "1.3",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21758,
    "package_name": "robreg3S",
    "title": "Three-Step Regression and Inference for Cellwise and Casewise\nContamination",
    "description": "Three-step regression and inference for cellwise and casewise contamination.",
    "version": "0.3-1",
    "maintainer": "Claudio Agostinelli <claudio.agostinelli@unitn.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21759,
    "package_name": "robregcc",
    "title": "Robust Regression with Compositional Covariates",
    "description": "We implement the algorithm estimating the parameters of the robust regression model with compositional covariates. The model simultaneously treats outliers and provides reliable parameter  estimates. Publication reference: Mishra, A., Mueller, C.,(2019) <arXiv:1909.04990>.  ",
    "version": "1.1",
    "maintainer": "Aditya Mishra <amishra@flatironinstitute.org>",
    "url": "https://arxiv.org/abs/1909.04990,\nhttps://github.com/amishra-stats/robregcc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21762,
    "package_name": "robslopes",
    "title": "Fast Algorithms for Robust Slopes",
    "description": "Fast algorithms for the Theil-Sen estimator,\n  Siegel's repeated median slope estimator, and Passing-Bablok regression.\n  The implementation is based on algorithms by\n  Dillencourt et. al (1992) <doi:10.1142/S0218195992000020> and Matousek et. al (1998)  <doi:10.1007/PL00009190>.\n  The implementations are detailed in \n  Raymaekers (2023) <doi:10.32614/RJ-2023-012> and \n  Raymaekers J., Dufey F. (2022) <doi:10.48550/arXiv.2202.08060>.\n  All algorithms run in quasilinear time.",
    "version": "1.1.4",
    "maintainer": "Jakob Raymaekers <jakob.raymaekers@uantwerpen.be>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21763,
    "package_name": "robsurvey",
    "title": "Robust Survey Statistics Estimation",
    "description": "Robust (outlier-resistant) estimators of finite population\n    characteristics like of means, totals, ratios, regression, etc. Available\n    methods are M- and GM-estimators of regression, weight reduction,\n    trimming, and winsorization. The package extends the 'survey'\n    <https://CRAN.R-project.org/package=survey> package.",
    "version": "0.7",
    "maintainer": "Tobias Schoch <tobias.schoch@fhnw.ch>",
    "url": "https://github.com/tobiasschoch/robsurvey",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21764,
    "package_name": "robumeta",
    "title": "Robust Variance Meta-Regression",
    "description": "Functions for conducting robust variance estimation (RVE) meta-regression using both large and small sample RVE estimators under various weighting schemes. These methods are distribution free and provide valid point estimates, standard errors and hypothesis tests even when the degree and structure of dependence between effect sizes is unknown. Also included are functions for conducting sensitivity analyses under correlated effects weighting and producing RVE-based forest plots. ",
    "version": "2.1",
    "maintainer": "Zachary Fisher <fish.zachary@gmail.com>",
    "url": "https://github.com/zackfisher/robumeta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21766,
    "package_name": "robust",
    "title": "Port of the S+ \"Robust Library\"",
    "description": "Methods for robust statistics, a state of the art in the early\n  2000s, notably for robust regression and robust multivariate analysis.",
    "version": "0.7-5",
    "maintainer": "Valentin Todorov <valentin.todorov@chello.at>",
    "url": "https://github.com/valentint/robust",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21768,
    "package_name": "robustBLME",
    "title": "Robust Bayesian Linear Mixed-Effects Models using ABC",
    "description": "Bayesian robust fitting of linear mixed effects models through weighted likelihood equations and approximate Bayesian computation as proposed by Ruli et al. (2017) <arXiv:1706.01752>.",
    "version": "0.1.3",
    "maintainer": "Erlis Ruli <erlisr@yahoo.it>",
    "url": "https://github.com/erlisR/robustBLME",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21771,
    "package_name": "robustGarch",
    "title": "Robust Garch(1,1) Model",
    "description": "A method for modeling robust generalized autoregressive conditional heteroskedasticity (Garch) (1,1) processes, providing robustness toward additive outliers instead of innovation outliers. This work is based on the methodology described by Muler and Yohai (2008) <doi:10.1016/j.jspi.2007.11.003>.",
    "version": "0.4.2",
    "maintainer": "Echo Liu <yuhong.echo.liu@gmail.com>",
    "url": "https://github.com/EchoRLiu/robustGarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21772,
    "package_name": "robustHD",
    "title": "Robust Methods for High-Dimensional Data",
    "description": "Robust methods for high-dimensional data, in particular linear\n    model selection techniques based on least angle regression and sparse\n    regression. Specifically, the package implements robust least angle \n    regression (Khan, Van Aelst & Zamar, 2007; <doi:10.1198/016214507000000950>), \n    (robust) groupwise least angle regression (Alfons, Croux & Gelper, 2016; \n    <doi:10.1016/j.csda.2015.02.007>), and sparse least trimmed squares \n    regression (Alfons, Croux & Gelper, 2013; <doi:10.1214/12-AOAS575>).",
    "version": "0.8.3",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "https://github.com/aalfons/robustHD",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21776,
    "package_name": "robustarima",
    "title": "Robust ARIMA Modeling",
    "description": "Functions for fitting a linear regression model with ARIMA\n  errors using a filtered tau-estimate.\n  The methodology is described in Maronna et al (2017, ISBN:9781119214687).",
    "version": "0.2.7",
    "maintainer": "Stephen Kaluzny <spkaluzny@gmail.com>",
    "url": "https://github.com/spkaluzny/robustarima",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21778,
    "package_name": "robustbetareg",
    "title": "Robust Beta Regression",
    "description": "Robust estimators for the beta regression, useful for modeling \n  bounded continuous data. Currently, four types of robust estimators are supported. \n  They depend on a tuning constant which may be fixed or selected by a \n  data-driven algorithm also implemented in the package. Diagnostic tools \n  associated with the fitted model, such as the residuals and goodness-of-fit \n  statistics, are implemented. Robust Wald-type tests are available. More details\n  about robust beta regression are described in Maluf et al. (2025) \n  <doi:10.1007/s00184-024-00949-1>.",
    "version": "0.3.1",
    "maintainer": "Felipe Queiroz <ffelipeq@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21786,
    "package_name": "robustreg",
    "title": "Robust Regression Functions",
    "description": "Linear regression functions using Huber and bisquare psi functions. Optimal weights are calculated using IRLS algorithm.",
    "version": "0.1-11",
    "maintainer": "Ian M. Johnson <ijca2013@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21787,
    "package_name": "robustsur",
    "title": "Robust Estimation for Seemingly Unrelated Regression Models",
    "description": "Data sets are often corrupted by outliers. When data are multivariate outliers can be classified as case-wise or cell-wise. The latters are particularly challenge to handle. We implement a robust estimation procedure for Seemingly Unrelated Regression Models which is able to cope well with both type of outliers. Giovanni Saraceno, Fatemah Alqallaf, Claudio Agostinelli (2021) <doi:10.48550/arXiv.2107.00975>. ",
    "version": "0.0-8",
    "maintainer": "Claudio Agostinelli <claudio.agostinelli@unitn.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21795,
    "package_name": "rockchalk",
    "title": "Regression Estimation and Presentation",
    "description": "A collection of functions for interpretation and presentation\n    of regression analysis.  These functions are used\n    to produce the statistics lectures in\n    <https://pj.freefaculty.org/guides/>. Includes regression\n    diagnostics, regression tables, and plots of interactions and\n    \"moderator\" variables. The emphasis is on \"mean-centered\" and\n    \"residual-centered\" predictors. The vignette 'rockchalk' offers a\n    fairly comprehensive overview.  The vignette 'Rstyle' has advice\n    about coding in R.  The package title 'rockchalk' refers to our\n    school motto, 'Rock Chalk Jayhawk, Go K.U.'.",
    "version": "1.8.157",
    "maintainer": "Paul E. Johnson <pauljohn@ku.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21806,
    "package_name": "rodd",
    "title": "Optimal Discriminating Designs",
    "description": "A collection of functions for numerical construction of optimal discriminating designs. At the current moment T-optimal designs (which maximize the lower bound for the power of F-test for regression model discrimination), KL-optimal designs (for lognormal errors) and their robust analogues can be calculated with the package.  ",
    "version": "0.2-1",
    "maintainer": "Roman Guchenko <RomanGuchenko@yandex.ru>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21809,
    "package_name": "rogme",
    "title": "Robust Graphical Methods For Group Comparisons",
    "description": "Robust methods to compare and visualise groups of continuous",
    "version": "0.2.1",
    "maintainer": "Guillaume Rousselet <Guillaume.Rousselet@glasgow.ac.uk>",
    "url": "https://github.com/GRousselet/rogme",
    "exports": [],
    "topics": ["data-visualization", "quantile", "r", "robust", "statistics"],
    "score": "NA",
    "stars": 85
  },
  {
    "id": 21811,
    "package_name": "roll",
    "title": "Rolling and Expanding Statistics",
    "description": "Fast and efficient computation of rolling and expanding statistics for time-series data.",
    "version": "1.2.0",
    "maintainer": "Jason Foster <jason.j.foster@gmail.com>",
    "url": "https://github.com/jasonjfoster/roll",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21815,
    "package_name": "rollinglda",
    "title": "Construct Consistent Time Series from Textual Data",
    "description": "A rolling version of the Latent Dirichlet Allocation, see Rieger et al. (2021) <doi:10.18653/v1/2021.findings-emnlp.201>. By a sequential approach, it enables the construction of LDA-based time series of topics that are consistent with previous states of LDA models. After an initial modeling, updates can be computed efficiently, allowing for real-time monitoring and detection of events or structural breaks.",
    "version": "0.1.4",
    "maintainer": "Jonas Rieger <jonas.rieger@tu-dortmund.de>",
    "url": "https://github.com/JonasRieger/rollinglda",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21821,
    "package_name": "rolr",
    "title": "Finding Optimal Three-Group Splits Based on a Survival Outcome",
    "description": "Provides fast procedures for exploring all pairs of\n    cutpoints of a single covariate with respect to survival and determining\n    optimal cutpoints using a hierarchical method and various ordered logrank tests.",
    "version": "1.0.0",
    "maintainer": "Pingping Qu <quping14@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21847,
    "package_name": "rotatogram",
    "title": "A Non-Axis-Dominant Association Plotting Tool",
    "description": "A rotatogram is a method of displaying an association which is axis non-dominant. This is achieved in two ways: First, the method of estimating the slope and intercept uses the least-products method rather than more typical least squared error for the \"dependent\" variable. The least products method has no \"dependent\" variable and is scale independent. Second, the plot is rotated such that the resulting regression line is vertical, reducing the suggestion that the vertical axis is the dominant one. The slope can be read relative to either axis equally.",
    "version": "0.1.3",
    "maintainer": "Noah Haber <noahhaber@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21870,
    "package_name": "rpart.LAD",
    "title": "Least Absolute Deviation Regression Trees",
    "description": "Recursive partitioning for least absolute deviation regression trees. Another algorithm from the 1984 book by \n             Breiman, Friedman, Olshen and Stone in addition to the 'rpart' package (Breiman, Friedman, Olshen, Stone (1984, ISBN:9780412048418).",
    "version": "0.1.3",
    "maintainer": "Stephan Dlugosz <stephan.dlugosz@googlemail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21873,
    "package_name": "rpc",
    "title": "Ridge Partial Correlation",
    "description": "Computes the ridge partial correlation\n    coefficients in a high or ultra-high dimensional linear regression\n    problem. An extended Bayesian information criterion is also\n    implemented for variable selection. Users provide the matrix\n    of covariates as a usual dense matrix or a sparse matrix\n    stored in a compressed sparse column format. Detail of the method\n    is given in the manual.",
    "version": "2.0.3",
    "maintainer": "Somak Dutta <somakd@iastate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21886,
    "package_name": "rplum",
    "title": "Bayesian Age-Depth Modelling of Cores Dated by Pb-210",
    "description": "An approach to age-depth modelling that uses Bayesian statistics to reconstruct accumulation histories for 210Pb-dated deposits using prior information. It can combine 210Pb, radiocarbon, and other dates in the chronologies. See Aquino et al. (2018) <doi:10.1007/s13253-018-0328-7>. Note that parts of the code underlying 'rplum' are derived from the 'rbacon' package by the same authors, and there remains a degree of overlap between the two packages.",
    "version": "1.0.0",
    "maintainer": "Maarten Blaauw <maarten.blaauw@qub.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21890,
    "package_name": "rpnf",
    "title": "Point and Figure Package",
    "description": "A set of functions to analyze and print the development of a\n    commodity using the Point and Figure (P&F) approach. A P&F processor can be used\n    to calculate daily statistics for the time series. These statistics can be used\n    for deeper investigations as well as to create plots. Plots can be generated as\n    well known X/O Plots in plain text format, and additionally in a more graphical\n    format.",
    "version": "1.0.5",
    "maintainer": "Sascha Herrmann <sascha.herrmann.consulting@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21893,
    "package_name": "rpql",
    "title": "Regularized PQL for Joint Selection in GLMMs",
    "description": "Performs joint selection in Generalized Linear Mixed Models (GLMMs) using penalized likelihood methods. Specifically, the Penalized Quasi-Likelihood (PQL) is used as a loss function, and penalties are then augmented to perform simultaneous fixed and random effects selection. Regularized PQL avoids the need for integration (or approximations such as the Laplace's method) during the estimation process, and so the full solution path for model selection can be constructed relatively quickly. ",
    "version": "0.8.3",
    "maintainer": "Francis K.C. Hui <fhui28@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21901,
    "package_name": "rpsftm",
    "title": "Rank Preserving Structural Failure Time Models",
    "description": "Implements methods described by the paper Robins and Tsiatis (1991) <DOI:10.1080/03610929108830654>. These use g-estimation to estimate the causal effect of a treatment in a two-armed randomised control trial where non-compliance exists and is measured, under an assumption of an accelerated failure time model and no unmeasured confounders.",
    "version": "1.2.9",
    "maintainer": "Simon Bond <simon.bond7@nhs.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21907,
    "package_name": "rqPen",
    "title": "Penalized Quantile Regression",
    "description": "Performs penalized quantile regression with LASSO, elastic net, SCAD and MCP penalty functions including group penalties. In addition, offers a group penalty that provides consistent variable selection across quantiles. Provides a function that automatically generates lambdas and evaluates different models with cross validation or BIC, including a large p version of BIC. Below URL provides a link to a work in progress vignette. ",
    "version": "4.1.4",
    "maintainer": "Ben Sherwood <ben.sherwood@ku.edu>",
    "url": "https://github.com/bssherwood/rqpen/blob/master/ignore/rqPenArticle.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21908,
    "package_name": "rqcanon",
    "title": "Canonical Quantile Regression",
    "description": "A quantile regression method for multivariate data to find linear combinations of explanatory and response variables generalizing canonical correlation. The package consists of functions, rqcan() for fitting the coefficients, and summary.rqcan(), which calls a bootstrap function. For details, see the help files for rqcan() and summary.rqcan(), and the reference: Portnoy (2022) <doi:10.1016/j.jmva.2022.105071>.",
    "version": "0.1.0",
    "maintainer": "Stephen Portnoy <sportnoy@illinois.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21922,
    "package_name": "rrat",
    "title": "Robust Regression with Asymmetric Heavy-Tail Noise Distributions",
    "description": "Implementation of Robust Regression tailored to deal with Asymmetric noise Distribution, which was originally proposed by Takeuchi & Bengio & Kanamori (2002) <doi:10.1162/08997660260293300>. In addition, this implementation is extended as introducing potential feature regularization by LASSO etc.",
    "version": "1.0.0",
    "maintainer": "Yi He <yi.he@stats.oxon.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21937,
    "package_name": "rrpack",
    "title": "Reduced-Rank Regression",
    "description": "Multivariate regression methodologies including\n    classical reduced-rank regression (RRR)\n    studied by Anderson (1951) <doi:10.1214/aoms/1177729580> and\n    Reinsel and Velu (1998) <doi:10.1007/978-1-4757-2853-8>,\n    reduced-rank regression via adaptive nuclear norm penalization\n    proposed by Chen et al. (2013) <doi:10.1093/biomet/ast036> and\n    Mukherjee et al. (2015) <doi:10.1093/biomet/asx080>,\n    robust reduced-rank regression (R4) proposed by\n    She and Chen (2017) <doi:10.1093/biomet/asx032>,\n    generalized/mixed-response reduced-rank regression (mRRR) proposed by\n    Luo et al. (2018) <doi:10.1016/j.jmva.2018.04.011>,\n    row-sparse reduced-rank regression (SRRR) proposed by\n    Chen and Huang (2012) <doi:10.1080/01621459.2012.734178>,\n    reduced-rank regression with a sparse singular value decomposition (RSSVD)\n    proposed by Chen et al. (2012) <doi:10.1111/j.1467-9868.2011.01002.x>\n    and sparse and orthogonal factor regression (SOFAR) proposed by\n    Uematsu et al. (2019) <doi:10.1109/TIT.2019.2909889>.",
    "version": "0.1-14",
    "maintainer": "Kun Chen <kun.chen@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21939,
    "package_name": "rrr",
    "title": "Reduced-Rank Regression",
    "description": "Reduced-rank regression, diagnostics and graphics.",
    "version": "1.0.0",
    "maintainer": "Chris Addy <chris.william.addy@gmail.com>",
    "url": "http://github.com/chrisaddy/rrr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21945,
    "package_name": "rrum",
    "title": "Bayesian Estimation of the Reduced Reparameterized Unified Model\nwith Gibbs Sampling",
    "description": "Implementation of Gibbs sampling algorithm for Bayesian Estimation\n    of the Reduced Reparameterized Unified Model ('rrum'), described by \n    Culpepper and Hudson (2017) <doi: 10.1177/0146621617707511>.",
    "version": "0.2.2",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "url": "https://tmsalab.github.io/rrum/, https://github.com/tmsalab/rrum",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 21962,
    "package_name": "rsdmx",
    "title": "Tools for Reading SDMX Data and Metadata",
    "description": "Set of classes and methods to read data and metadata documents",
    "version": "0.6-6",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "url": "https://github.com/eblondel/rsdmx",
    "exports": [],
    "topics": ["api", "datastructures", "dsd", "r", "read", "readsdmx", "sdmx", "sdmx-format", "sdmx-provider", "sdmx-standards", "statistics", "timeseries", "web-services"],
    "score": "NA",
    "stars": 107
  },
  {
    "id": 21971,
    "package_name": "rsimsum",
    "title": "Analysis of Simulation Studies Including Monte Carlo Error",
    "description": "Summarise results from simulation studies and compute Monte Carlo",
    "version": "0.13.0.9000",
    "maintainer": "",
    "url": "https://github.com/ellessenne/rsimsum",
    "exports": [],
    "topics": ["biostatistics", "monte-carlo-error", "r", "rstats", "simulation", "simulation-study", "simulations", "statistics"],
    "score": "NA",
    "stars": 30
  },
  {
    "id": 22001,
    "package_name": "rstanbdp",
    "title": "Bayesian Deming Regression for Method Comparison",
    "description": "Regression methods to quantify the relation \n    between two measurement methods are provided by this package. The\n    focus is on a Bayesian Deming regressions family. With a Bayesian\n    method the Deming regression can be run in a traditional fashion or\n    can be run in a robust way just decreasing the degree of freedom\n    d.f. of the sampling distribution. With d.f. = 1 an extremely robust\n    Cauchy distribution can be sampled. Moreover, models for dealing\n    with heteroscedastic data are also provided. For reference see\n    G. Pioda (2024) <https://piodag.github.io/bd1/>.",
    "version": "0.0.3",
    "maintainer": "Giorgio Pioda <gfwp@ticino.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22006,
    "package_name": "rstiefel",
    "title": "Random Orthonormal Matrix Generation and Optimization on the\nStiefel Manifold",
    "description": "Simulation of random orthonormal matrices from linear and quadratic exponential family distributions on the Stiefel manifold. The most general type of distribution covered is the matrix-variate  Bingham-von Mises-Fisher distribution. Most of the simulation methods are presented in Hoff(2009) \"Simulation of the Matrix Bingham-von Mises-Fisher Distribution, With Applications to Multivariate and Relational Data\" <doi:10.1198/jcgs.2009.07177>. The package also includes functions for optimization on the Stiefel manifold based on algorithms described in Wen and Yin (2013) \"A feasible method for optimization with orthogonality constraints\" <doi:10.1007/s10107-012-0584-1>. ",
    "version": "1.0.1",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22007,
    "package_name": "rstpm2",
    "title": "Smooth Survival Models, Including Generalized Survival Models",
    "description": "R implementation of generalized survival models (GSMs), smooth accelerated failure time (AFT) models and Markov multi-state models. For the GSMs, g(S(t|x))=eta(t,x) for a link function g, survival S at time t with covariates x and a linear predictor eta(t,x). The main assumption is that the time effect(s) are smooth <doi:10.1177/0962280216664760>. For fully parametric models with natural splines, this re-implements Stata's 'stpm2' function, which are flexible parametric survival models developed by Royston and colleagues. We have extended the parametric models to include any smooth parametric smoothers for time. We have also extended the model to include any smooth penalized smoothers from the 'mgcv' package, using penalized likelihood. These models include left truncation, right censoring, interval censoring, gamma frailties and normal random effects <doi:10.1002/sim.7451>, and copulas. For the smooth AFTs, S(t|x) = S_0(t*eta(t,x)), where the baseline survival function S_0(t)=exp(-exp(eta_0(t))) is modelled for natural splines for eta_0, and the time-dependent cumulative acceleration factor eta(t,x)=\\int_0^t exp(eta_1(u,x)) du for log acceleration factor eta_1(u,x). The Markov multi-state models allow for a range of models with smooth transitions to predict transition probabilities, length of stay, utilities and costs, with differences, ratios and standardisation.",
    "version": "1.7.1",
    "maintainer": "Mark Clements <mark.clements@ki.se>",
    "url": "https://github.com/mclements/rstpm2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22013,
    "package_name": "rsurv",
    "title": "Random Generation of Survival Data",
    "description": "Random generation of survival data from a wide range of regression models, including accelerated failure time (AFT), proportional hazards (PH), proportional odds (PO), accelerated hazard (AH), Yang and Prentice (YP), and extended hazard (EH) models. The package 'rsurv' also stands out by its ability to generate survival data from an unlimited number of baseline distributions provided that an implementation of the quantile function of the chosen baseline distribution is available in R. Another nice feature of the package 'rsurv' lies in the fact that linear predictors are specified via a formula-based approach, facilitating the inclusion of categorical variables and interaction terms. The functions implemented in the package 'rsurv' can also be employed to simulate survival data with more complex structures, such as survival data with different types of censoring mechanisms, survival data with cure fraction, survival data with random effects (frailties), multivariate survival data, and competing risks survival data. Details about the R package 'rsurv' can be found in Demarqui (2024) <doi:10.48550/arXiv.2406.01750>.",
    "version": "0.0.2",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/rsurv,\nhttps://fndemarqui.github.io/rsurv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22050,
    "package_name": "rtmpt",
    "title": "Fitting (Exponential/Diffusion) RT-MPT Models",
    "description": "Fit (exponential or diffusion) response-time extended multinomial processing tree (RT-MPT) models \n      by Klauer and\tKellen (2018) <doi:10.1016/j.jmp.2017.12.003> and Klauer, Hartmann, and Meyer-Grant (submitted). \n      The RT-MPT class not only incorporate\tfrequencies like traditional multinomial processing tree (MPT) models, \n      but also latencies. This enables it\tto estimate process completion times and encoding plus motor execution times \n      next to the process probabilities\tof traditional MPTs. 'rtmpt' is a hierarchical Bayesian framework and posterior \n      samples are sampled using a Metropolis-within-Gibbs sampler (for exponential RT-MPTs) or Hamiltonian-within-Gibbs \n      sampler (for diffusion RT-MPTs).",
    "version": "2.0-3",
    "maintainer": "Raphael Hartmann <raphael.hartmann@protonmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22062,
    "package_name": "rts",
    "title": "Raster Time Series Analysis",
    "description": "This framework aims to provide classes and methods for manipulating and processing of raster time series data (e.g. a time series of satellite images).",
    "version": "1.1-14",
    "maintainer": "Babak Naimi <naimi.b@gmail.com>",
    "url": "https://r-gis.net/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22064,
    "package_name": "rtsdata",
    "title": "R Time Series Intelligent Data Storage",
    "description": "A tool that allows to download and save historical time series data \n\tfor future use offline. The intelligent updating functionality will only download \n\tthe new available information; thus, saving you time and Internet bandwidth. \n\tIt will only re-download the full data-set if any inconsistencies are detected. \n\tThis package supports following data provides: 'Yahoo' (finance.yahoo.com), \n\t'FRED' (fred.stlouisfed.org), 'Quandl' (data.nasdaq.com), \n\t'AlphaVantage' (www.alphavantage.co), 'Tiingo' (www.tiingo.com).",
    "version": "0.1.4",
    "maintainer": "Irina Kapler <irkapler@gmail.com>",
    "url": "https://bitbucket.org/rtsvizteam/rtsdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22070,
    "package_name": "rucrdtw",
    "title": "R Bindings for the UCR Suite",
    "description": "R bindings for functions from the UCR Suite by Rakthanmanon et al. (2012) <DOI:10.1145/2339530.2339576>, which enables ultrafast subsequence\n      search for a best match under Dynamic Time Warping and Euclidean Distance.",
    "version": "0.1.6",
    "maintainer": "Philipp Boersch-Supan <pboesu@gmail.com>",
    "url": "https://github.com/pboesu/rucrdtw",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22071,
    "package_name": "rugarch",
    "title": "Univariate GARCH Models",
    "description": "ARFIMA, in-mean, external regressors and various GARCH flavors, with methods for fit, forecast, simulation, inference and plotting.",
    "version": "1.5-4",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "url": "https://github.com/alexiosg/rugarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22075,
    "package_name": "rumidas",
    "title": "Univariate GARCH-MIDAS, Double-Asymmetric GARCH-MIDAS and\nMEM-MIDAS",
    "description": "Adds the MIxing-Data Sampling (MIDAS, Ghysels et al. (2007) <doi:10.1080/07474930600972467>) components to a variety of GARCH and MEM (Engle (2002) <doi:10.1002/jae.683>, Engle and Gallo (2006) <doi:10.1016/j.jeconom.2005.01.018>, and Amendola et al. (2024) <doi:10.1016/j.seps.2023.101764>) models, with the aim of predicting the volatility with additional low-frequency (that is, MIDAS) terms. The estimation takes place through simple functions, which provide in-sample and (if present) and out-of-sample evaluations. 'rumidas' also offers a summary tool, which synthesizes the main information of the estimated model. There is also the possibility of generating one-step-ahead and multi-step-ahead forecasts. ",
    "version": "0.1.3",
    "maintainer": "Vincenzo Candila <vcandila@unisa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22083,
    "package_name": "runjags",
    "title": "Interface Utilities, Model Templates, Parallel Computing Methods\nand Additional Distributions for MCMC Models in JAGS",
    "description": "User-friendly interface utilities for MCMC models via\n    Just Another Gibbs Sampler (JAGS), facilitating the use of parallel\n    (or distributed) processors for multiple chains, automated control\n    of convergence and sample length diagnostics, and evaluation of the\n    performance of a model using drop-k validation or against simulated\n    data. Template model specifications can be generated using a standard\n    lme4-style formula interface to assist users less familiar with the\n    BUGS syntax.  A JAGS extension module provides additional distributions\n    including the Pareto family of distributions, the DuMouchel prior and\n    the half-Cauchy prior.",
    "version": "2.2.2-5",
    "maintainer": "Matthew Denwood <md@sund.ku.dk>",
    "url": "https://github.com/ku-awdc/runjags",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22085,
    "package_name": "runner",
    "title": "Running Operations for Vectors",
    "description": "Lightweight library for rolling windows operations. Package enables\n  full control over the window length, window lag and a time indices. With a runner \n  one can apply any R function on a rolling windows. The package eases work with \n  equally and unequally spaced time series.",
    "version": "0.4.5",
    "maintainer": "Dawid Kałędkowski <dawid.kaledkowski@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22087,
    "package_name": "runstats",
    "title": "Fast Computation of Running Statistics for Time Series",
    "description": "Provides methods for fast computation of running sample \n    statistics for time series. These include: (1) mean, (2) \n    standard deviation, and (3) variance over a fixed-length window \n    of time-series, (4) correlation, (5) covariance, and (6) \n    Euclidean distance (L2 norm) between short-time pattern and \n    time-series. Implemented methods utilize Convolution Theorem to \n    compute convolutions via Fast Fourier Transform (FFT).",
    "version": "1.1.0",
    "maintainer": "Marta Karas <marta.karass@gmail.com>",
    "url": "https://github.com/martakarass/runstats",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22088,
    "package_name": "rupturesRcpp",
    "title": "Object-Oriented Interface for Offline Change-Point Detection",
    "description": "A collection of efficient implementations of popular offline change-point detection algorithms, featuring a consistent, object-oriented interface for practical use.",
    "version": "1.0.2",
    "maintainer": "Minh Long Nguyen <edelweiss611428@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22098,
    "package_name": "rvalues",
    "title": "R-Values for Ranking in High-Dimensional Settings",
    "description": "A collection of functions for computing \"r-values\" from various\n    kinds of user input such as MCMC output or a list of effect size estimates\n    and associated standard errors. Given a large collection of measurement units,\n    the r-value, r, of a particular unit is a reported percentile that may be\n    interpreted as the smallest percentile at which the unit should be placed in the\n    top r-fraction of units.",
    "version": "0.7.1",
    "maintainer": "Nicholas Henderson <nchender@umich.edu>",
    "url": "https://doi.org/10.1111/rssb.12131",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22100,
    "package_name": "rvec",
    "title": "Vectors Representing Random Variables",
    "description": "Random vectors, called rvecs. An rvec holds\n    multiple draws, but tries to behave like a standard\n    R vector, including working well in data frames.\n    Rvecs are useful for analysing\n    output from a simulation or a Bayesian analysis.",
    "version": "1.0.0",
    "maintainer": "John Bryant <john@bayesiandemography.com>",
    "url": "https://bayesiandemography.github.io/rvec/,\nhttps://github.com/bayesiandemography/rvec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22106,
    "package_name": "rvif",
    "title": "Collinearity Detection using Redefined Variance Inflation Factor\nand Graphical Methods",
    "description": "The detection of troubling approximate collinearity in a multiple linear regression model is a classical problem in Econometrics. This package is focused on determining whether or not the degree of approximate multicollinearity in a multiple linear regression model is of concern, meaning that it affects the statistical analysis (i.e. individual significance tests) of the model. This objective is achieved by using the variance inflation factor redefined and the scatterplot between the variance inflation factor and the coefficient of variation. For more details see Salmerón R., García C.B. and García J. (2018) <doi:10.1080/00949655.2018.1463376>, Salmerón, R., Rodríguez, A. and García C. (2020) <doi:10.1007/s00180-019-00922-x>, Salmerón, R., García, C.B, Rodríguez, A. and García, C. (2022) <doi:10.32614/RJ-2023-010>, Salmerón, R., García, C.B. and García, J. (2025) <doi:10.1007/s10614-024-10575-8> and Salmerón, R., García, C.B, García J. (2023, working paper) <doi:10.48550/arXiv.2005.02245>. You can also view the package vignette using 'browseVignettes(\"rvif\")', the package website (<https://www.ugr.es/local/romansg/rvif/index.html>) using 'browseURL(system.file(\"docs/index.html\", package = \"rvif\"))' or version control on GitHub (<https://github.com/rnoremlas/rvif_package>).",
    "version": "3.2",
    "maintainer": "R. Salmerón <romansg@ugr.es>",
    "url": "http://colldetreat.r-forge.r-project.org/,\nhttps://github.com/rnoremlas/rvif_package,\nhttps://www.ugr.es/local/romansg/rvif/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22110,
    "package_name": "rvmethod",
    "title": "Radial Velocity Method for Detecting Exoplanets",
    "description": "Has various functions designed to implement the \n    Hermite-Gaussian Radial Velocity (HGRV) estimation approach of \n    Holzer et al. (2020) <arXiv:2005.14083>, which is a particular application of the radial \n    velocity method for detecting exoplanets. The overall approach consists \n    of four sequential steps, each of which has a function in this package: \n    (1) estimate the template spectrum with the function estimate_template(), \n    (2) find absorption features in the estimated template with the function \n    findabsorptionfeatures(), (3) fit Gaussians to the absorption features \n    with the function Gaussfit(), (4) apply the HGRV with simple linear \n    regression by calling the function hgrv(). This package is meant to be \n    open source. But please cite the paper Holzer et al. (2020) <arXiv:2005.14083> when \n    publishing results that use this package.",
    "version": "0.1.2",
    "maintainer": "Parker Holzer <parker.holzer@yale.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22112,
    "package_name": "rwa",
    "title": "Perform a Relative Weights Analysis",
    "description": "Perform a Relative Weights Analysis (RWA) (a.k.a. Key Drivers Analysis) as per the method described \n    in Tonidandel & LeBreton (2015) <DOI:10.1007/s10869-014-9351-z>, with its original roots in Johnson (2000) <DOI:10.1207/S15327906MBR3501_1>. In essence, RWA decomposes\n    the total variance predicted in a regression model into weights that accurately reflect the proportional \n    contribution of the predictor variables, which addresses the issue of multi-collinearity. In typical scenarios,\n    RWA returns similar results to Shapley regression, but with a significant advantage on computational performance.",
    "version": "0.1.0",
    "maintainer": "Martin Chan <martinchan53@gmail.com>",
    "url": "https://martinctc.github.io/rwa/, https://github.com/martinctc/rwa",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22119,
    "package_name": "rwicc",
    "title": "Regression with Interval-Censored Covariates",
    "description": "Provides functions to simulate and analyze data for a regression model with an interval censored covariate, as described in Morrison et al. (2021) <doi:10.1111/biom.13472>.",
    "version": "0.1.3",
    "maintainer": "Douglas Morrison <dmorrison01@ucla.edu>",
    "url": "https://d-morrison.github.io/rwicc/,\nhttps://github.com/d-morrison/rwicc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22127,
    "package_name": "rwty",
    "title": "R We There Yet? Visualizing MCMC Convergence in Phylogenetics",
    "description": "Implements various tests, visualizations, and metrics\n    for diagnosing convergence of MCMC chains in phylogenetics.  It implements\n    and automates many of the functions of the AWTY package in the R\n    environment, as well as a host of other functions.  Warren, Geneva, and Lanfear (2017), <doi:10.1093/molbev/msw279>.",
    "version": "1.0.3",
    "maintainer": "Dan Warren <dan.l.warren@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22128,
    "package_name": "rwunderground",
    "title": "R Interface to Weather Underground API",
    "description": "Tools for getting historical weather information and forecasts \n    from wunderground.com. Historical weather and forecast data includes, but \n    is not limited to, temperature, humidity, windchill, wind speed, dew point, \n    heat index. Additionally, the weather underground weather API also includes \n    information on sunrise/sunset, tidal conditions, satellite/webcam imagery, \n    weather alerts, hurricane alerts and historical high/low temperatures.",
    "version": "0.1.8",
    "maintainer": "Eric Hare <eric@omnianalytics.io>",
    "url": "https://github.com/ALShum/rwunderground,\nhttp://www.wunderground.com/weather/api",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22140,
    "package_name": "s2dv",
    "title": "Seasonal to Decadal Verification",
    "description": "An advanced version of package 's2dverification'. Intended for \n    seasonal to decadal (s2d) climate forecast verification, but also applicable\n    to other types of forecasts or general climate analysis. This package is \n    specifically designed for comparing experimental and observational datasets. \n    It provides functionality for data retrieval, post-processing, skill score \n    computation against observations, and visualization. Compared to \n    's2dverification', 's2dv' is more compatible with the package 'startR', able \n    to use multiple cores for computation and handle multi-dimensional arrays \n    with a higher flexibility. The Climate Data Operators (CDO) version used in \n    development is 1.9.8. Implements methods described in Wilks (2011) \n    <doi:10.1016/B978-0-12-385022-5.00008-7>, DelSole and Tippett \n    (2016) <doi:10.1175/MWR-D-15-0218.1>, Kharin et al. (2012) \n    <doi:10.1029/2012GL052647>, Doblas-Reyes et al. (2003) \n    <doi:10.1007/s00382-003-0350-4>.",
    "version": "2.2.1",
    "maintainer": "Ariadna Batalla <ariadna.batalla@bsc.es>",
    "url": "https://gitlab.earth.bsc.es/es/s2dv/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22146,
    "package_name": "sBIC",
    "title": "Computing the Singular BIC for Multiple Models",
    "description": "Computes the sBIC for various singular model collections including:\n    binomial mixtures, factor analysis models, Gaussian mixtures,\n    latent forests, latent class analyses, and reduced rank regressions.",
    "version": "0.2.0",
    "maintainer": "Luca Weihs <lucaw@uw.edu>",
    "url": "https://github.com/Lucaweihs/sBIC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22150,
    "package_name": "sGBJ",
    "title": "Survival Extension of the Generalized Berk-Jones Test",
    "description": "Implements an extension of the Generalized Berk-Jones (GBJ) statistic for\n    survival data, sGBJ. It computes the sGBJ statistic and its p-value for testing \n    the association between a gene set and a time-to-event outcome with possible \n    adjustment on additional covariates. Detailed method is available at Villain L, Ferte T, \n    Thiebaut R and Hejblum BP (2021) <doi:10.1101/2021.09.07.459329>.",
    "version": "0.1.1",
    "maintainer": "Laura Villain <sistm.soft.maintain@gmail.com>",
    "url": "https://github.com/lauravillain/sGBJ",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22154,
    "package_name": "sNPLS",
    "title": "NPLS Regression with L1 Penalization",
    "description": "Tools for performing variable selection in three-way data using N-PLS \n    in combination with L1 penalization, Selectivity Ratio and VIP scores. \n    The N-PLS model (Rasmus Bro, 1996 <DOI:10.1002/(SICI)1099-128X(199601)10:1%3C47::AID-CEM400%3E3.0.CO;2-C>) \n    is the natural extension of PLS (Partial Least Squares) to N-way structures, and tries \n    to maximize the covariance between X and Y data arrays. The package also adds\n    variable selection through L1 penalization, Selectivity Ratio and VIP scores.",
    "version": "1.0.27",
    "maintainer": "David Hervas <ddhervas@yahoo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22162,
    "package_name": "sTSD",
    "title": "Simulate Time Series Diagnostics",
    "description": "These are tools that allow users to do time series diagnostics, primarily\n    tests of unit root, by way of simulation. While there is nothing necessarily\n    wrong with the received wisdom of critical values generated decades ago, \n    simulation provides its own perks. Not only is simulation broadly informative\n    as to what these various test statistics do and what are their plausible \n    values, simulation provides more flexibility for assessing unit root by way\n    of different thresholds or different hypothesized distributions.",
    "version": "0.2.0",
    "maintainer": "Steven Miller <steve@svmiller.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22163,
    "package_name": "sValues",
    "title": "Measures of Sturdiness of Regression Coefficients",
    "description": "Implements the s-values proposed by Ed. Leamer.\n    It provides a context-minimal approach for sensitivity analysis using extreme\n    bounds to assess the sturdiness of regression coefficients.",
    "version": "0.1.8",
    "maintainer": "Carlos Cinelli <carloscinelli@hotmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22177,
    "package_name": "saeHB",
    "title": "Small Area Estimation using Hierarchical Bayesian Method",
    "description": "Provides several functions for area level of small area estimation using hierarchical Bayesian (HB) methods with several univariate distributions for variables of interest. The dataset that is used in every function is generated accordingly in the Example. The 'rjags' package is employed to obtain parameter estimates. Model-based estimators involve the HB estimators which include the mean and the variation of mean. For the reference, see Rao and Molina (2015) <doi:10.1002/9781118735855>.",
    "version": "0.2.3",
    "maintainer": "Zaza Yuda Perwira <zazayudaperwira93@gmail.com>",
    "url": "https://github.com/zazaperwira/saeHB",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22178,
    "package_name": "saeHB.ME",
    "title": "Small Area Estimation with Measurement Error using Hierarchical\nBayesian Method",
    "description": "Implementation of small area estimation using Hierarchical Bayesian (HB) Method when auxiliary variable measured with error. The 'rjags' package is employed to obtain parameter estimates. For the references, see Rao and Molina (2015) <doi:10.1002/9781118735855>, Ybarra and Lohr (2008) <doi:10.1093/biomet/asn048>, and Ntzoufras (2009, ISBN-10: 1118210352).  ",
    "version": "1.0.1",
    "maintainer": "Muhammad Rifqi Mubarak <rifqi.mubarak@bps.go.id>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22179,
    "package_name": "saeHB.ME.beta",
    "title": "SAE with Measurement Error using HB under Beta Distribution",
    "description": "Implementation of Small Area Estimation (SAE) using Hierarchical Bayesian (HB) Method when auxiliary variable measured with error under Beta Distribution. The 'rjags' package is employed to obtain parameter estimates. For the references, see J.N.K & Molina (2015) <doi:10.1002/9781118735855>, Ybarra and Sharon (2008) <doi:10.1093/biomet/asn048>, and Ntzoufras (2009, ISBN-10: 1118210352).",
    "version": "1.1.0",
    "maintainer": "Ratih Rodliyah <ratihrodliyah2@gmail.com>",
    "url": "https://github.com/ratihrodliyah/saeHB.ME.beta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22180,
    "package_name": "saeHB.TF.beta",
    "title": "SAE using HB Twofold Subarea Model under Beta Distribution",
    "description": "Estimates area and subarea level proportions using the Small Area Estimation (SAE) Twofold Subarea Model with a hierarchical Bayesian (HB) approach under Beta distribution. A number of simulated datasets generated for illustration purposes are also included. The 'rstan' package is employed to estimate parameters via the Hamiltonian Monte Carlo and No U-Turn Sampler algorithm. The  model-based estimators include the HB mean, the variation of the mean, and quantiles. For references, see Rao and Molina (2015) <doi:10.1002/9781118735855>, Torabi and Rao (2014) <doi:10.1016/j.jmva.2014.02.001>, Leyla Mohadjer et al.(2007) <http://www.asasrms.org/Proceedings/y2007/Files/JSM2007-000559.pdf>, Erciulescu et al.(2019) <doi:10.1111/rssa.12390>, and Yudasena (2024).",
    "version": "0.2.0",
    "maintainer": "Nasya Zahira Putri <nasyazp28@gmail.com>",
    "url": "https://github.com/Nasyazahira/saeHB.TF.beta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22181,
    "package_name": "saeHB.ZIB",
    "title": "Small Area Estimation using Hierarchical Bayesian under Zero\nInflated Binomial Distribution",
    "description": "Provides function for area level of small area estimation using hierarchical Bayesian (HB) method with Zero-Inflated Binomial distribution for variables of interest. Some dataset produced by a data generation are also provided. The 'rjags' package is employed to obtain parameter estimates. Model-based estimators involves the HB estimators which include the mean and the variation of mean.",
    "version": "0.1.1",
    "maintainer": "Rizqina Rahmati <221810583@stis.ac.id>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22182,
    "package_name": "saeHB.panel",
    "title": "Small Area Estimation using Hierarchical Bayesian Method for Rao\nYu Model",
    "description": "We designed this package to provide several functions for area level of small area estimation using hierarchical Bayesian (HB) method. This package provides model using panel data for variable interest.This package also provides a dataset produced by a data generation. The 'rjags' package is employed to obtain parameter estimates. Model-based estimators involves the HB estimators which include the mean and the variation of mean. For the reference, see Rao and Molina (2015).",
    "version": "0.1.1",
    "maintainer": "Velia Tri Marliana <221810642@stis.ac.id>",
    "url": "https://github.com/Veliatrimarliana/saeHB.panel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22183,
    "package_name": "saeHB.panel.beta",
    "title": "Small Area Estimation using HB for Rao Yu Model under Beta\nDistribution",
    "description": "Several functions are provided for small area estimation at\n    the area level using the hierarchical bayesian (HB) method with panel\n    data under beta distribution for variable interest. This package also\n    provides a dataset produced by data generation. The 'rjags' package is\n    employed to obtain parameter estimates. Model-based estimators involve\n    the HB estimators, which include the mean and the variation of the\n    mean. For the reference, see Rao and Molina (2015, ISBN:\n    978-1-118-73578-7).",
    "version": "0.1.5",
    "maintainer": "Dian Rahmawati Salis <dianrahmawatisalis03@gmail.com>",
    "url": "https://github.com/DianRahmawatiSalis/saeHB.panel.beta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22185,
    "package_name": "saeHB.twofold",
    "title": "Hierarchical Bayes Twofold Subarea Level Model SAE",
    "description": "We designed this package to provides several functions for area and subarea level of small area estimation under Twofold Subarea Level Model using hierarchical Bayesian (HB) method with Univariate Normal distribution for variables of interest. Some dataset simulated by a data generation are also provided. The 'rjags' package is employed to obtain parameter estimates using Gibbs Sampling algorithm. Model-based estimators involves the HB estimators which include the mean, the variation of mean, and the quantile. For the reference, see Rao and Molina (2015) <doi:10.1002/9781118735855>, Torabi and Rao (2014) <doi:10.1016/j.jmva.2014.02.001>, Leyla Mohadjer et al.(2007) <http://www.asasrms.org/Proceedings/y2007/Files/JSM2007-000559.pdf>, and Erciulescu et al.(2019) <doi:10.1111/rssa.12390>.",
    "version": "0.1.2",
    "maintainer": "Reyhan Saadi <reyhansaadi335@gmail.com>",
    "url": "https://github.com/reymath99/saeHB.twofold",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22186,
    "package_name": "saeHB.unit",
    "title": "Basic Unit Level Model using Hierarchical Bayesian Approach",
    "description": "Small area estimation unit level models (Battese-Harter-Fuller model) with a Bayesian Hierarchical approach. See also Rao & Molina (2015, ISBN:978-1-118-73578-7) and Battese et al. (1988) <doi:10.1080/01621459.1988.10478561>.",
    "version": "0.1.0",
    "maintainer": "Ridson Al Farizal P <alfrzlp@gmail.com>",
    "url": "https://github.com/Alfrzlp/saeHB.unit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22192,
    "package_name": "saeTrafo",
    "title": "Transformations for Unit-Level Small Area Models",
    "description": "The aim of this package is to offer new methodology for unit-level \n    small area models under transformations and limited population auxiliary \n    information. In addition to this new methodology, the widely used nested \n    error regression model without transformations (see \"An Error-Components \n    Model for Prediction of County Crop Areas Using Survey and Satellite Data\" \n    by Battese, Harter and Fuller (1988) <doi:10.1080/01621459.1988.10478561>) \n    and its well-known uncertainty estimate (see \"The estimation of the mean \n    squared error of small-area estimators\" by Prasad and Rao (1990) \n    <doi:10.1080/01621459.1995.10476570>) are provided. In this package, the \n    log transformation and the data-driven log-shift transformation are \n    provided. If a transformation is selected, an appropriate method is chosen \n    depending on the respective input of the population data: Individual \n    population data (see \"Empirical best prediction under a nested error model \n    with log transformation\" by Molina and Martín (2018) \n    <doi:10.1214/17-aos1608>) but also aggregated population data (see \n    \"Estimating regional income indicators under transformations and access to \n    limited population auxiliary information\" by Würz, Schmid and Tzavidis \n    <unpublished>) can be entered. Especially under limited data access, new \n    methodologies are provided in saeTrafo. Several options are available to \n    assess the used model and to judge, present and export its results. For a \n    detailed description of the package and the methods used see the \n    corresponding vignette.",
    "version": "1.0.6",
    "maintainer": "Nora Würz <nora.wuerz@uni-bamberg.de>",
    "url": "https://github.com/NoraWuerz/saeTrafo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22196,
    "package_name": "saekernel",
    "title": "Small Area Estimation Non-Parametric Based Nadaraya-Watson\nKernel",
    "description": "Propose an area-level, non-parametric regression estimator based on Nadaraya-Watson kernel on small area mean. Adopt a two-stage estimation approach proposed by Prasad and Rao (1990). Mean Squared Error (MSE) estimators are not readily available, so resampling method that called bootstrap is applied. This package are based on the model proposed in Two stage non-parametric approach for small area estimation by Pushpal Mukhopadhyay and Tapabrata Maiti(2004) <http://www.asasrms.org/Proceedings/y2004/files/Jsm2004-000737.pdf>.",
    "version": "0.1.1",
    "maintainer": "Wicak Surya Hasani <221710052@stis.ac.id>",
    "url": "https://github.com/wicaksh/saekernel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22199,
    "package_name": "saery",
    "title": "Small Area Estimation for Rao and Yu Model",
    "description": "Functions to calculate EBLUPs (Empirical Best Linear Unbiased Predictor) estimators and their MSEs (Mean Squared Errors). Estimators are based on an area-level linear mixed model introduced by Rao and Yu (1994) <doi:10.2307/3315407>. The REML (Residual Maximum Likelihood) method is used for fitting the model.",
    "version": "2.0",
    "maintainer": "Perez Agustin <agustin.perez@umh.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22211,
    "package_name": "sahpm",
    "title": "Variable Selection using Simulated Annealing",
    "description": "Highest posterior model is widely accepted as a good model among available models. In terms of variable selection highest posterior model is often the true model. Our stochastic search process SAHPM based on simulated annealing maximization method tries to find the highest posterior model by maximizing the model space with respect to the posterior probabilities of the models. This package currently contains the SAHPM method only for linear models. The codes for GLM will be added in future.",
    "version": "1.0.1",
    "maintainer": "Arnab Maity <arnab.maity@pfizer.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22215,
    "package_name": "salmonMSE",
    "title": "Management Strategy Evaluation for Salmon Species",
    "description": "Simulation tools to evaluate the long-term effects of salmon management strategies, including a combination of habitat, harvest, and\n  habitat actions. The stochastic age-structured operating model accommodates complex life histories, including freshwater survival across \n  early life stages, juvenile survival and fishery exploitation in the marine life stage, partial maturity by age class, and fitness impacts of \n  hatchery programs on natural spawning populations. 'salmonMSE' also provides an age-structured conditioning model to develop operating models \n  fitted to data.",
    "version": "0.1.0",
    "maintainer": "Quang Huynh <quang@bluematterscience.com>",
    "url": "https://docs.salmonmse.com/,\nhttps://github.com/Blue-Matter/salmonMSE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22218,
    "package_name": "samadb",
    "title": "South Africa Macroeconomic Database API",
    "description": "An R API providing access to a relational database with macroeconomic time series data for South Africa,\n obtained from the South African Reserve Bank (SARB) and Statistics South Africa (STATSSA), and updated on a weekly basis\n via the EconData <https://www.econdata.co.za/> platform and automated scraping of the SARB and STATSSA websites.\n The database is maintained at the Department of Economics at Stellenbosch University.",
    "version": "0.3.1",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22225,
    "package_name": "sampleSelection",
    "title": "Sample Selection Models",
    "description": "Two-step\n   and maximum likelihood estimation\n   of Heckman-type sample selection models:\n   standard sample selection models (Tobit-2),\n   endogenous switching regression models (Tobit-5),\n   sample selection models with binary dependent outcome variable,\n   interval regression with sample selection (only ML estimation),\n   and endogenous treatment effects models.\n   These methods are described in the three vignettes\n   that are included in this package \n   and in econometric textbooks such as\n   Greene (2011, Econometric Analysis, 7th edition, Pearson).",
    "version": "1.2-12",
    "maintainer": "Arne Henningsen <arne.henningsen@gmail.com>",
    "url": "http://www.sampleSelection.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22246,
    "package_name": "sams",
    "title": "Merge-Split Samplers for Conjugate Bayesian Nonparametric Models",
    "description": "Markov chain Monte Carlo samplers for posterior simulations of conjugate Bayesian nonparametric\n    mixture models. Functionality is provided for Gibbs sampling as in Algorithm 3 of Neal (2000)\n    <DOI:10.1080/10618600.2000.10474879>, restricted Gibbs merge-split sampling as described in Jain & Neal\n    (2004) <DOI:10.1198/1061860043001>, and sequentially-allocated merge-split sampling <DOI:10.1080/00949655.2021.1998502>, as well as\n    summary and utility functions.",
    "version": "0.4.3",
    "maintainer": "David B. Dahl <dahl@stat.byu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22247,
    "package_name": "sanba",
    "title": "Fitting Shared Atoms Nested Models via MCMC or Variational Bayes",
    "description": "\n    An efficient tool for fitting nested mixture models based on a shared set of \n    atoms via Markov Chain Monte Carlo and variational inference algorithms. \n    Specifically, the package implements the common atoms model (Denti et al., 2023), \n    its finite version (similar to D'Angelo et al., 2023), and a hybrid finite-infinite \n    model (D'Angelo and Denti, 2024). All models implement univariate nested mixtures \n    with Gaussian kernels equipped with a normal-inverse gamma prior distribution \n    on the parameters. Additional functions are provided to help analyze the \n    results of the fitting procedure.   \n    References:       \n    Denti, Camerlenghi, Guindani, Mira (2023) <doi:10.1080/01621459.2021.1933499>,      \n    D’Angelo, Canale, Yu, Guindani (2023) <doi:10.1111/biom.13626>,      \n    D’Angelo, Denti (2024) <doi:10.1214/24-BA1458>.",
    "version": "0.0.3",
    "maintainer": "Francesco Denti <francescodenti.personal@gmail.com>",
    "url": "https://github.com/fradenti/sanba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22275,
    "package_name": "sarp.snowprofile",
    "title": "Snow Profile Analysis for Snowpack and Avalanche Research",
    "description": "Analysis and plotting tools for snow profile data produced from manual snowpack \n  observations and physical snowpack models. The functions in this package support snowpack \n  and avalanche research by reading various formats of data (including CAAML, SMET,\n  generic csv, and outputs from the snow cover model SNOWPACK), manipulate the data, and \n  produce graphics such as stratigraphy and time series profiles. Package developed by \n  the Simon Fraser University Avalanche Research Program <http://www.avalancheresearch.ca>. \n  Graphics apply visualization concepts from Horton, Nowak, and Haegeli (2020, \n  <doi:10.5194/nhess-20-1557-2020>).",
    "version": "1.3.2",
    "maintainer": "Pascal Haegeli <pascal_haegeli@sfu.ca>",
    "url": "http://www.avalancheresearch.ca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22277,
    "package_name": "sarp.snowprofile.pyface",
    "title": "'python' Modules from Snowpack and Avalanche Research",
    "description": "\n    The development of post-processing functionality for simulated snow profiles \n    by the snow and avalanche community is often done in 'python'. This package\n    aims to make some of these tools accessible to 'R' users. Currently integrated \n    modules contain functions to calculate dry snow layer instabilities in support \n    of avalache hazard assessments following the publications of Richter, \n    Schweizer, Rotach, and Van Herwijnen (2019) <doi:10.5194/tc-13-3353-2019>, and \n    Mayer, Van Herwijnen, Techel, and Schweizer (2022) <doi:10.5194/tc-2022-34>.",
    "version": "0.4.3",
    "maintainer": "Florian Herla <fherla@sfu.ca>",
    "url": "http://www.avalancheresearch.ca",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22282,
    "package_name": "sasLM",
    "title": "'SAS' Linear Model",
    "description": "This is a core implementation of 'SAS' procedures for linear models - GLM, REG, ANOVA, TTEST, FREQ, and UNIVARIATE. Some R packages provide type II and type III SS. However, the results of nested and complex designs are often different from those of 'SAS.' Different results does not necessarily mean incorrectness. However, many wants the same results to SAS. This package aims to achieve that. \n             Reference: Littell RC, Stroup WW, Freund RJ (2002, ISBN:0-471-22174-0).",
    "version": "0.10.7",
    "maintainer": "Kyun-Seop Bae <k@acr.kr>",
    "url": "https://cran.r-project.org/package=sasLM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22297,
    "package_name": "saws",
    "title": "Small-Sample Adjustments for Wald Tests Using Sandwich\nEstimators",
    "description": "Tests coefficients with sandwich estimator of variance and with small samples. Regression types supported are gee, linear regression, and conditional logistic regression.",
    "version": "0.9-7.0",
    "maintainer": "Michael P. Fay <mfay@niaid.nih.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22298,
    "package_name": "sazedR",
    "title": "Parameter-Free Domain-Agnostic Season Length Detection in Time\nSeries",
    "description": "Spectral and Average Autocorrelation Zero Distance Density\n    ('sazed') is a method for estimating the season length of a \n    seasonal time series. 'sazed' is aimed at practitioners, as it employs only \n    domain-agnostic preprocessing and does not depend on parameter tuning or \n    empirical constants. The computation of 'sazed' relies on the efficient \n    autocorrelation computation methods suggested by Thibauld Nion (2012, URL: \n    <https://etudes.tibonihoo.net/literate_musing/autocorrelations.html>) and by \n    Bob Carpenter (2012, URL: \n    <https://lingpipe-blog.com/2012/06/08/autocorrelation-fft-kiss-eigen/>).",
    "version": "2.0.2",
    "maintainer": "Tiago Santos <teixeiradossantos@tugraz.at>",
    "url": "https://github.com/mtoller/autocorr_season_length_detection/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22302,
    "package_name": "sbgcop",
    "title": "Semiparametric Bayesian Gaussian Copula Estimation and\nImputation",
    "description": "Estimation and inference for parameters \n        in a Gaussian copula model,\n        treating the univariate marginal distributions as nuisance\n        parameters as described in Hoff (2007) <doi:10.1214/07-AOAS107>. \n        This package also provides a\n        semiparametric imputation procedure for missing multivariate\n        data.",
    "version": "1.0",
    "maintainer": "Peter Hoff <peter.hoff@duke.edu>",
    "url": "https://pdhoff.github.io/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22308,
    "package_name": "sbrl",
    "title": "Scalable Bayesian Rule Lists Model",
    "description": "An efficient implementation of Scalable Bayesian Rule Lists Algorithm, a competitor algorithm for decision tree algorithms; see Hongyu Yang, Cynthia Rudin, Margo Seltzer (2017) <https://proceedings.mlr.press/v70/yang17h.html>. It builds from pre-mined association rules and have a logical structure identical to a decision list or one-sided decision tree. Fully optimized over rule lists, this algorithm strikes practical balance between accuracy, interpretability, and computational speed.",
    "version": "1.4",
    "maintainer": "Hongyu Yang <edwardyhy1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22310,
    "package_name": "sbw",
    "title": "Stable Balancing Weights for Causal Inference and Missing Data",
    "description": "Implements the Stable Balancing Weights by Zubizarreta (2015) <DOI:10.1080/01621459.2015.1023805>. These are the weights of minimum variance that approximately balance the empirical distribution of the observed covariates. For an overview, see Chattopadhyay, Hase and Zubizarreta (2020) <DOI:10.1002/sim.8659>. To solve the optimization problem in 'sbw', the default solver is 'quadprog', which is readily available through CRAN. The solver 'osqp' is also posted on CRAN. To enhance the performance of 'sbw', users are encouraged to install other solvers such as 'gurobi' and 'Rmosek', which require special installation. For the installation of gurobi and pogs, please follow the instructions at <https://docs.gurobi.com/projects/optimizer/en/current/reference/r.html> and <http://foges.github.io/pogs/stp/r>.",
    "version": "1.2",
    "maintainer": "Jose R. Zubizarreta <zubizarreta@hcp.med.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22325,
    "package_name": "scDECO",
    "title": "Estimating Dynamic Correlation",
    "description": "Implementations for two different Bayesian models of differential co-expression. scdeco.cop() fits the bivariate Gaussian copula model from Zichen Ma, Shannon W. Davis, Yen-Yi Ho (2023) <doi:10.1111/biom.13701>, while scdeco.pg() fits the bivariate Poisson-Gamma model from Zhen Yang, Yen-Yi Ho (2022) <doi:10.1111/biom.13457>.",
    "version": "0.1.1",
    "maintainer": "Anderson Bussing <abussing@email.sc.edu>",
    "url": "https://github.com/YenYiHo-Lab/scDECO",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22347,
    "package_name": "scMaSigPro",
    "title": "Application of MaSigPro Bioconductor Package for scRNA Trajectory data",
    "description": "scMaSigPro is a polynomial regression-based approach inspired by",
    "version": "0.0.4",
    "maintainer": "",
    "url": "https://github.com/BioBam/scMaSigPro",
    "exports": [],
    "topics": ["pseudotime", "single-cell-rna-seq", "time-series", "trajectory-analysis"],
    "score": "NA",
    "stars": 18
  },
  {
    "id": 22384,
    "package_name": "scalreg",
    "title": "Scaled Sparse Linear Regression",
    "description": "Algorithms for fitting scaled sparse linear regression and estimating precision matrices.",
    "version": "1.0.1",
    "maintainer": "Tingni Sun <tingni@wharton.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22386,
    "package_name": "scan",
    "title": "Single-Case Data Analyses for Single and Multiple Baseline\nDesigns",
    "description": "A collection of procedures for analysing, visualising, \n  and managing single-case data. These include regression models \n  (multilevel, multivariate, bayesian), between case standardised mean difference, \n  overlap indices ('PND', 'PEM', 'PAND', 'PET', 'tau-u', 'IRD', 'baseline corrected tau', \n  'CDC'), and randomization tests. Data preparation functions support outlier \n  detection, handling missing values, scaling, and custom transformations. \n  An export function helps to generate html, word, and latex tables in a \n  publication friendly style. A shiny app allows to use scan in a graphical \n  user interface.\n  More details can be found in the online book 'Analyzing single-case data with \n  R and scan', Juergen Wilbert (2025)\n  <https://jazznbass.github.io/scan-Book/>.",
    "version": "0.67.0",
    "maintainer": "Juergen Wilbert <juergen.wilbert@uni-muenster.de>",
    "url": "https://github.com/jazznbass/scan/,\nhttps://jazznbass.github.io/scan-Book/,\nhttps://jazznbass.github.io/scan/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22394,
    "package_name": "scar",
    "title": "Shape-Constrained Additive Regression: a Maximum Likelihood\nApproach",
    "description": "Computes the maximum likelihood estimator of the generalised additive and index regression with shape constraints. Each additive component function is assumed to obey one of the nine possible shape restrictions: linear, increasing, decreasing, convex, convex increasing, convex decreasing, concave, concave increasing, or concave decreasing. For details, see Chen and Samworth (2016) <doi:10.1111/rssb.12137>.",
    "version": "0.2-2",
    "maintainer": "Yining Chen <y.chen101@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22423,
    "package_name": "scgwr",
    "title": "Scalable Geographically Weighted Regression",
    "description": "Fast and regularized version of GWR for large dataset, detailed in Murakami, Tsutsumida, Yoshida, Nakaya, and Lu (2019) <arXiv:1905.00266>.",
    "version": "0.1.2-21",
    "maintainer": "Daisuke Murakami <dmuraka@ism.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22461,
    "package_name": "scorepeak",
    "title": "Peak Functions for Peak Detection in Univariate Time Series",
    "description": "Provides peak functions, which enable us to detect peaks in time series. The methods implemented in this package are based on Girish Keshav Palshikar (2009) <https://www.researchgate.net/publication/228853276_Simple_Algorithms_for_Peak_Detection_in_Time-Series>.",
    "version": "0.1.2",
    "maintainer": "Shota Ochi <shotaochi1990@gmail.com>",
    "url": "https://github.com/ShotaOchi/scorepeak",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22463,
    "package_name": "scoringRules",
    "title": "Scoring Rules for Parametric and Simulated Distribution\nForecasts",
    "description": "Dictionary-like reference for computing scoring rules in a wide\n    range of situations. Covers both parametric forecast distributions (such as\n    mixtures of Gaussians) and distributions generated via simulation. Further \n    details can be found in the package vignettes <doi:10.18637/jss.v090.i12>, \n    <doi:10.18637/jss.v110.i08>.",
    "version": "1.1.3",
    "maintainer": "Fabian Krueger <Fabian.Krueger83@gmail.com>",
    "url": "https://github.com/FK83/scoringRules",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22464,
    "package_name": "scoringfunctions",
    "title": "A Collection of Loss Functions for Assessing Point Forecasts",
    "description": "\n    Implements multiple consistent scoring functions\n    (Gneiting T (2011) <doi:10.1198/jasa.2011.r10138>) for assessing point\n    forecasts and point predictions. Detailed documentation of scoring\n    functions' properties is included for facilitating interpretation of\n    results.",
    "version": "1.1",
    "maintainer": "Hristos Tyralis <montchrister@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22472,
    "package_name": "scpi",
    "title": "Prediction Intervals for Synthetic Control Methods with Multiple\nTreated Units and Staggered Adoption",
    "description": "Implementation of prediction and inference procedures for Synthetic Control methods using least square, lasso, ridge, or simplex-type constraints. Uncertainty is quantified with prediction intervals as developed in Cattaneo, Feng, and Titiunik (2021) <doi:10.1080/01621459.2021.1979561> for a single treated unit and in Cattaneo, Feng, Palomba, and Titiunik (2025) <doi:10.1162/rest_a_01588> for multiple treated units and staggered adoption. More details about the software implementation can be found in Cattaneo, Feng, Palomba, and Titiunik (2025) <doi:10.18637/jss.v113.i01>.",
    "version": "3.0.1",
    "maintainer": "Filippo Palomba <fpalomba@princeton.edu>",
    "url": "https://nppackages.github.io/scpi/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22475,
    "package_name": "scpropreg",
    "title": "Simplicially Constrained Regression Models for Proportions",
    "description": "Simplicially constrained regression models for proportions in both sides. The constraint is always that the betas are non-negative and sum to 1. References: Iverson S.J.., Field C., Bowen W.D. and Blanchard W. (2004) \"Quantitative Fatty Acid Signature Analysis: A New Method of Estimating Predator Diets\". Ecological Monographs, 74(2): 211-235. <doi:10.1890/02-4105>.",
    "version": "1.0",
    "maintainer": "Michail Tsagris <mtsagris@uoc.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22503,
    "package_name": "sctransform",
    "title": "Variance Stabilizing Transformations for Single Cell UMI Data",
    "description": "A normalization method for single-cell UMI count data using a \n  variance stabilizing transformation. The transformation is based on a \n  negative binomial regression model with regularized parameters. As part of the\n  same regression framework, this package also provides functions for\n  batch correction, and data correction. See Hafemeister and Satija (2019)\n  <doi:10.1186/s13059-019-1874-1>, and Choudhary and Satija (2022) <doi:10.1186/s13059-021-02584-9>\n  for more details.",
    "version": "0.4.3",
    "maintainer": "Saket Choudhary <saketc@iitb.ac.in>",
    "url": "https://github.com/satijalab/sctransform",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22519,
    "package_name": "sde",
    "title": "Simulation and Inference for Stochastic Differential Equations",
    "description": "Description: Provides functions for simulation and inference for stochastic differential equations (SDEs). It accompanies the book \"Simulation and Inference for Stochastic Differential Equations: With R Examples\" (Iacus, 2008, Springer; ISBN: 978-0-387-75838-1).",
    "version": "2.0.21",
    "maintainer": "Stefano Maria Iacus <smiacus@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22521,
    "package_name": "sdlrm",
    "title": "Modified Skew Discrete Laplace Regression for Integer-Valued and\nPaired Discrete Data",
    "description": "Implementation of the modified skew discrete Laplace (SDL) regression model.\n    The package provides a set of functions for a complete analysis of integer-valued data, \n    where the dependent variable is assumed to follow a modified SDL distribution. This regression\n    model is useful for the analysis of integer-valued data and experimental studies in which\n    paired discrete observations are collected.",
    "version": "0.1.2",
    "maintainer": "Rodrigo Medeiros <rodrigo.matheus@ufrn.br>",
    "url": "https://github.com/rdmatheus/sdlrm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22527,
    "package_name": "sdrt",
    "title": "Estimating the Sufficient Dimension Reduction Subspaces in Time\nSeries",
    "description": "The sdrt() function is designed for estimating subspaces for Sufficient Dimension Reduction (SDR) in time series, with a specific focus on the Time Series Central Mean subspace (TS-CMS). The package employs the Fourier transformation method proposed by Samadi and De Alwis (2023) <doi:10.48550/arXiv.2312.02110> and the Nadaraya-Watson kernel smoother method proposed by Park et al. (2009) <doi:10.1198/jcgs.2009.08076> for estimating the TS-CMS. The package provides tools for estimating distances between subspaces and includes functions for selecting model parameters using the Fourier transformation method. ",
    "version": "1.0.0",
    "maintainer": "Tharindu P. De Alwis <talwis@wpi.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22537,
    "package_name": "sealasso",
    "title": "Standard Error Adjusted Adaptive Lasso",
    "description": "Standard error adjusted adaptive lasso (SEA-lasso) is a version of the adaptive lasso, which incorporates OLS standard error to the L1 penalty weight. This method is intended for variable selection under linear regression settings (n > p). This new weight assignment strategy is especially useful when the collinearity of the design matrix is a concern. ",
    "version": "0.1-3",
    "maintainer": "Wei Qian <weiqian@stat.umn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22540,
    "package_name": "seas",
    "title": "Seasonal Analysis and Graphics, Especially for Climatology",
    "description": "Capable of deriving seasonal statistics, such as \"normals\", and\n  analysis of seasonal data, such as departures. This package also has\n  graphics capabilities for representing seasonal data, including boxplots for\n  seasonal parameters, and bars for summed normals. There are many specific\n  functions related to climatology, including precipitation normals,\n  temperature normals, cumulative precipitation departures and precipitation\n  interarrivals. However, this package is designed to represent any\n  time-varying parameter with a discernible seasonal signal, such as found\n  in hydrology and ecology.",
    "version": "0.7-0",
    "maintainer": "Mike Toews <mwtoews@gmail.com>",
    "url": "https://github.com/mwtoews/seas",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22545,
    "package_name": "seasonalytics",
    "title": "Compute Seasonality Index, Seasonalized and Deseaonalised the\nTime Series Data",
    "description": "The computation of a seasonal index is a fundamental step in time-series forecasting when the data exhibits seasonality. Specifically, a seasonal index quantifies — for each season (e.g. month, quarter, week) — the relative magnitude of the seasonal effect compared to the overall average level of the series. This package has been developed to compute seasonal index for time series data and it also seasonalise and desesaonalise the time series data.",
    "version": "0.1.0",
    "maintainer": "Mr. Ankit Kumar Singh <ankitsinghvns32@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22556,
    "package_name": "sectorgap",
    "title": "Consistent Economic Trend Cycle Decomposition",
    "description": "Determining potential output and the output gap - two inherently unobservable variables - is a major challenge for macroeconomists. 'sectorgap' features a flexible modeling and estimation framework for a multivariate Bayesian state space model identifying economic output fluctuations consistent with subsectors of the economy. The proposed model is able to capture various correlations between output and a set of aggregate as well as subsector indicators. Estimation of the latent states and parameters is achieved using a simple Gibbs sampling procedure and various plotting options facilitate the assessment of the results. For details on the methodology and an illustrative example, see Streicher (2024) <https://www.research-collection.ethz.ch/handle/20.500.11850/653682>.",
    "version": "0.1.0",
    "maintainer": "Sina Streicher <streicher.sina@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22568,
    "package_name": "seer",
    "title": "Feature-Based Forecast Model Selection",
    "description": "A novel meta-learning framework for forecast model selection using time series features. Many applications require a large number of time series to be forecast. Providing better forecasts for these time series is important in decision and policy making. We propose a classification framework which selects forecast models based on features calculated from the time series. We call this framework FFORMS (Feature-based FORecast Model Selection). FFORMS builds a mapping that relates the features of time series to the best forecast model using a random forest. 'seer' package is the implementation of the FFORMS algorithm. For more details see our paper at <https://www.monash.edu/business/econometrics-and-business-statistics/research/publications/ebs/wp06-2018.pdf>.",
    "version": "1.1.8",
    "maintainer": "Thiyanga Talagala <tstalagala@gmail.com>",
    "url": "https://thiyangt.github.io/seer/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22570,
    "package_name": "segMGarch",
    "title": "Multiple Change-Point Detection for High-Dimensional GARCH\nProcesses",
    "description": "Implements a segmentation algorithm for multiple change-point detection in high-dimensional GARCH processes. It simultaneously segments GARCH processes by identifying 'common' change-points, each of which can be shared by a subset or all of the component time series as a change-point in their within-series and/or cross-sectional correlation structure. ",
    "version": "1.3",
    "maintainer": "Karolos Korkas <kkorkas@yahoo.co.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22577,
    "package_name": "segmented",
    "title": "Regression Models with Break-Points / Change-Points Estimation\n(with Possibly Random Effects)",
    "description": "Fitting regression models where, in addition to possible linear terms, one or more covariates have segmented (i.e., broken-line or piece-wise linear) or stepmented (i.e. piece-wise constant) effects. Multiple breakpoints for the same variable are allowed. \n  The estimation method is discussed in Muggeo (2003, <doi:10.1002/sim.1545>) and \n  illustrated in Muggeo (2008, <https://www.r-project.org/doc/Rnews/Rnews_2008-1.pdf>). An approach for hypothesis testing is presented \n  in Muggeo (2016, <doi:10.1080/00949655.2016.1149855>), and interval estimation for the breakpoint is discussed in Muggeo (2017, <doi:10.1111/anzs.12200>). \n  Segmented mixed models, i.e. random effects in the change point, are discussed in Muggeo (2014, <doi:10.1177/1471082X13504721>).\n  Estimation of piecewise-constant relationships and changepoints (mean-shift models) is \n  discussed in Fasola et al. (2018, <doi:10.1007/s00180-017-0740-4>).",
    "version": "2.1-4",
    "maintainer": "Vito M. R. Muggeo <vito.muggeo@unipa.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22581,
    "package_name": "segregation",
    "title": "Entropy-Based Segregation Indices",
    "description": "Computes segregation indices, including the Index of Dissimilarity,",
    "version": "1.1.0.9000",
    "maintainer": "",
    "url": "https://github.com/elbersb/segregation",
    "exports": [],
    "topics": ["entropy", "r", "r-package", "rstats", "segregation", "statistics"],
    "score": "NA",
    "stars": 36
  },
  {
    "id": 22608,
    "package_name": "semTests",
    "title": "Goodness-of-Fit Testing for Structural Equation Models",
    "description": "Supports eigenvalue block-averaging p-values (Foldnes, Grønneberg, 2018) <doi:10.1080/10705511.2017.1373021>,\n    penalized eigenvalue block-averaging p-values (Foldnes, Moss, Grønneberg, 2024) <doi:10.1080/10705511.2024.2372028>, penalized\n    regression p-values (Foldnes, Moss, Grønneberg, 2024) <doi:10.1080/10705511.2024.2372028>, as well as traditional p-values such as Satorra-Bentler. All p-values can\n    be calculated using unbiased or biased gamma estimates (Du, Bentler, 2022) <doi:10.1080/10705511.2022.2063870> \n    and two choices of chi square statistics.",
    "version": "0.7.1",
    "maintainer": "Jonas Moss <jonas.moss.statistics@gmail.com>",
    "url": "https://github.com/JonasMoss/semTests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22620,
    "package_name": "semicmprskcoxmsm",
    "title": "Use Inverse Probability Weighting to Estimate Treatment Effect\nfor Semi Competing Risks Data",
    "description": "Use inverse probability weighting methods to estimate treatment effect under marginal structure model (MSM) for the transition hazard of semi competing risk data, i.e. illness death model. We implement two specific such models, the usual Markov illness death structural model and the general Markov illness death structural model. We also provide the predicted three risks functions from the marginal structure models. Zhang, Y. and Xu, R. (2022) <arXiv:2204.10426>.",
    "version": "0.2.0",
    "maintainer": "Yiran Zhang <yiz038@health.ucsd.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22629,
    "package_name": "semmcmc",
    "title": "Bayesian Structural Equation Modeling in Multiple Omics Data\nIntegration",
    "description": "Provides Markov Chain Monte Carlo (MCMC) routine for the \n             structural equation modelling described in \n             Maity et. al. (2020) <doi:10.1093/bioinformatics/btaa286>. This MCMC sampler is \n             useful when one attempts to perform an integrative survival analysis for multiple \n             platforms of the Omics data where the response is time to event and the \n             predictors are different omics expressions for different platforms. ",
    "version": "0.0.6",
    "maintainer": "Arnab Maity <arnab.maity@pfizer.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22633,
    "package_name": "semsfa",
    "title": "Semiparametric Estimation of Stochastic Frontier Models",
    "description": "Semiparametric Estimation of Stochastic Frontier Models following a two step procedure: in the first step semiparametric or nonparametric regression techniques are used to relax parametric restrictions of the functional form representing technology and in the second step variance parameters are obtained by pseudolikelihood estimators or by method of moments.",
    "version": "1.2",
    "maintainer": "Giancarlo Ferrara <giancarlo.ferrara@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22641,
    "package_name": "sense",
    "title": "Automatic Stacked Ensemble for Regression Tasks",
    "description": "Stacked ensemble for regression tasks based on 'mlr3' framework with a pipeline for preprocessing numeric and factor features and hyper-parameter tuning using grid or random search.",
    "version": "1.1.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://mlr3.mlr-org.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22642,
    "package_name": "sensemakr",
    "title": "Sensitivity Analysis Tools for Regression Models",
    "description": "Implements a suite of sensitivity analysis tools \n  that extends the traditional omitted variable bias framework and makes it easier \n  to understand the impact of omitted variables in regression models, as discussed in Cinelli, C. and Hazlett, C. (2020), \"Making Sense of Sensitivity: Extending Omitted Variable Bias.\" Journal of the Royal Statistical Society, Series B (Statistical Methodology) <doi:10.1111/rssb.12348>.",
    "version": "0.1.6",
    "maintainer": "Carlos Cinelli <carloscinelli@hotmail.com>",
    "url": "https://github.com/carloscinelli/sensemakr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22648,
    "package_name": "sensitivityCalibration",
    "title": "A Calibrated Sensitivity Analysis for Matched Observational\nStudies",
    "description": "Implements the calibrated sensitivity analysis approach for matched observational studies. Our sensitivity analysis framework views matched sets as drawn from a super-population. The unmeasured confounder is modeled as a random variable. We combine matching and model-based covariate-adjustment methods to estimate the treatment effect. The hypothesized unmeasured confounder enters the picture as a missing covariate. We adopt a state-of-art Expectation Maximization (EM) algorithm to handle this missing covariate problem in generalized linear models (GLMs). As our method also estimates the effect of each observed covariate on the outcome and treatment assignment, we are able to calibrate the unmeasured confounder to observed covariates.\n      Zhang, B., Small, D. S. (2018). <arXiv:1812.00215>.",
    "version": "0.0.1",
    "maintainer": "Bo Zhang <bozhan@wharton.upenn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22654,
    "package_name": "sensmediation",
    "title": "Parametric Estimation and Sensitivity Analysis of Direct and\nIndirect Effects",
    "description": "We implement functions to estimate and perform sensitivity analysis to unobserved confounding of direct and indirect effects introduced in Lindmark, de Luna and Eriksson (2018) <doi:10.1002/sim.7620> and Lindmark (2022) <doi:10.1007/s10260-021-00611-4>. The estimation and sensitivity analysis are parametric, based on probit and/or linear regression models. Sensitivity analysis is implemented for unobserved confounding of the exposure-mediator, mediator-outcome and exposure-outcome relationships. ",
    "version": "0.3.1",
    "maintainer": "Anita Lindmark <anita.lindmark@umu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22666,
    "package_name": "sephora",
    "title": "Statistical Estimation of Phenological Parameters",
    "description": "Provides functions and methods for estimating phenological dates (green up, \n  start of a season, maturity, senescence, end of a season and dormancy) from (nearly) \n  periodic Earth Observation time series. These dates are critical points of some \n  derivatives of an idealized curve which, in turn, is obtained through a functional principal \n  component analysis-based regression model. Some of the methods implemented here are \n  based on T. Krivobokova, P. Serra and F. Rosales (2022) <https://www.sciencedirect.com/science/article/pii/S0167947322000998>. \n  Methods for handling and plotting Earth observation time series are also provided.",
    "version": "0.1.31",
    "maintainer": "Inder Tecuapetla-Gómez <itecuapetla@conabio.gob.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22674,
    "package_name": "seqDesign",
    "title": "Simulation and Group Sequential Monitoring of Randomized\nTwo-Stage Treatment Efficacy Trials with Time-to-Event\nEndpoints",
    "description": "A modification of the preventive vaccine efficacy trial design of Gilbert, Grove et al. (2011, Statistical Communications in Infectious Diseases) is implemented, with application generally to individual-randomized clinical trials with multiple active treatment groups and a shared control group, and a study endpoint that is a time-to-event endpoint subject to right-censoring. The design accounts for the issues that the efficacy of the treatment/vaccine groups may take time to accrue while the multiple treatment administrations/vaccinations are given; there is interest in assessing the durability of treatment efficacy over time; and group sequential monitoring of each treatment group for potential harm, non-efficacy/efficacy futility, and high efficacy is warranted. The design divides the trial into two stages of time periods, where each treatment is first evaluated for efficacy in the first stage of follow-up, and, if and only if it shows significant treatment efficacy in stage one, it is evaluated for longer-term durability of efficacy in stage two. The package produces plots and tables describing operating characteristics of a specified design including an unconditional power for intention-to-treat and per-protocol/as-treated analyses; trial duration; probabilities of the different possible trial monitoring outcomes (e.g., stopping early for non-efficacy); unconditional power for comparing treatment efficacies; and distributions of numbers of endpoint events occurring after the treatments/vaccinations are given, useful as input parameters for the design of studies of the association of biomarkers with a clinical outcome (surrogate endpoint problem). The code can be used for a single active treatment versus control design and for a single-stage design.",
    "version": "1.2",
    "maintainer": "Michal Juraska <mjuraska@fredhutch.org>",
    "url": "https://github.com/mjuraska/seqDesign",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22693,
    "package_name": "sequential.pops",
    "title": "Sequential Analysis of Biological Population Sizes",
    "description": "In population management, data come at more or less regular intervals over time in sampling batches (bouts) and decisions should be made with the minimum number of samples and as quickly as possible. This package provides tools to implement, produce charts with stop lines, summarize results and assess sequential analyses that test hypotheses about population sizes. Two approaches are included: the sequential test of Bayesian posterior probabilities (Rincon, D.F. et al. 2025 <doi:10.1111/2041-210X.70053>), and the sequential probability ratio test (Wald, A. 1945 <http://www.jstor.org/stable/2235829>).",
    "version": "0.1.1",
    "maintainer": "Diego F Rincon <diego.rincon@wsu.edu>",
    "url": "https://github.com/rincondf/sequential.pops,\nhttps://rincondf.github.io/sequential.pops/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22717,
    "package_name": "setartree",
    "title": "SETAR-Tree - A Novel and Accurate Tree Algorithm for Global Time\nSeries Forecasting",
    "description": "The implementation of a forecasting-specific tree-based model that is in particular suitable for global time series forecasting, as proposed in Godahewa et al. (2022) <arXiv:2211.08661v1>. The model uses the concept of Self Exciting Threshold Autoregressive (SETAR) models to define the node splits and thus, the model is named SETAR-Tree. The SETAR-Tree uses some time-series-specific splitting and stopping procedures. It trains global pooled regression models in the leaves allowing the models to learn cross-series information. The depth of the tree is controlled by conducting a statistical linearity test as well as measuring the error reduction percentage at each node split. Thus, the SETAR-Tree requires minimal external hyperparameter tuning and provides competitive results under its default configuration. A forest is developed by extending the SETAR-Tree. The SETAR-Forest combines the forecasts provided by a collection of diverse SETAR-Trees during the forecasting process. ",
    "version": "0.2.1",
    "maintainer": "Rakshitha Godahewa <rakshithagw@gmail.com>",
    "url": "https://github.com/rakshitha123/setartree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22757,
    "package_name": "sgPLS",
    "title": "Sparse Group Partial Least Square Methods",
    "description": "Regularized version of partial least square approaches\n providing sparse, group, and sparse group versions of partial\n least square regression models (Liquet, B., Lafaye de Micheaux, P.,\n Hejblum B., Thiebaut, R. (2016) <doi:10.1093/bioinformatics/btv535>).\n Version of PLS Discriminant analysis is also provided.",
    "version": "1.8.1",
    "maintainer": "Benoit Liquet <benoit.liquet@univ-pau.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22765,
    "package_name": "sglg",
    "title": "Fitting Semi-Parametric Generalized log-Gamma Regression Models",
    "description": "Set of tools to fit a linear multiple or semi-parametric regression\n    models with the possibility of non-informative random right or left censoring. \n    Under this setup, the localization parameter of the response variable distribution is modeled by using linear multiple regression\n    or semi-parametric functions, whose non-parametric components may be approximated\n    by natural cubic spline or P-splines. The supported distribution for the model error is a generalized log-gamma distribution which includes\n    the generalized extreme value and standard normal distributions as important special cases. Inference is based on likelihood, penalized likelihood and bootstrap methods.   \n    Lastly, some numerical and graphical devices for diagnostic of the fitted models are offered. ",
    "version": "0.2.5",
    "maintainer": "Carlos Alberto Cardozo Delgado <cardozorpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22777,
    "package_name": "sgstar",
    "title": "Seasonal Generalized Space Time Autoregressive (S-GSTAR) Model",
    "description": "A set of function that implements for seasonal multivariate time series analysis based on Seasonal Generalized Space\n            Time Autoregressive with Seemingly Unrelated Regression (S-GSTAR-SUR) Model by Setiawan(2016)<https://www.researchgate.net/publication/316517889_S-GSTAR-SUR_model_for_seasonal_spatio_temporal_data_forecasting>.",
    "version": "0.1.2",
    "maintainer": "M. Yoga Satria Utama <221709801@stis.ac.id>",
    "url": "https://github.com/yogasatria30/sgstar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22796,
    "package_name": "sharpData",
    "title": "Data Sharpening",
    "description": "Functions and data sets inspired by data sharpening -\n             data perturbation to achieve improved performance in\n             nonparametric estimation, as described in Choi, E., Hall, P.\n             and Rousson, V. (2000).  \n             Capabilities for enhanced local linear regression function\n             and derivative estimation are included, as well as an\n             asymptotically correct iterated data sharpening estimator\n             for any degree of local polynomial regression estimation.\n             A cross-validation-based bandwidth selector is included which,\n             in concert with the iterated sharpener, will often provide\n             superior performance, according to a median integrated squared\n             error criterion.  Sample data sets are provided to illustrate\n             function usage.",
    "version": "1.4",
    "maintainer": "W.J. Braun <john.braun@ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22797,
    "package_name": "sharpPen",
    "title": "Penalized Data Sharpening for Local Polynomial Regression",
    "description": "Functions and data sets for data sharpening.\n    Nonparametric regressions are computed subject to smoothness\n    and other kinds of penalties. ",
    "version": "2.0",
    "maintainer": "D. Wang <wdy@student.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22803,
    "package_name": "shazam",
    "title": "Immunoglobulin Somatic Hypermutation Analysis",
    "description": "Provides a computational framework for analyzing mutations in\n    immunoglobulin (Ig) sequences. Includes methods for Bayesian estimation of\n    antigen-driven selection pressure, mutational load quantification, building of\n    somatic hypermutation (SHM) models, and model-dependent distance calculations.\n    Also includes empirically derived models of SHM for both mice and humans.\n    Citations: \n    Gupta and Vander Heiden, et al (2015) <doi:10.1093/bioinformatics/btv359>, \n    Yaari, et al (2012) <doi:10.1093/nar/gks457>, \n    Yaari, et al (2013) <doi:10.3389/fimmu.2013.00358>, \n    Cui, et al (2016) <doi:10.4049/jimmunol.1502263>.",
    "version": "1.3.1",
    "maintainer": "Susanna Marquez <susanna.marquez@yale.edu>",
    "url": "http://shazam.readthedocs.io",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22839,
    "package_name": "shinyCox",
    "title": "Create 'shiny' Applications for Cox Proportional Hazards Models",
    "description": "Takes one or more fitted Cox proportional hazards models and writes\n    a 'shiny' application to a directory specified by the user. The 'shiny' \n    application displays predicted survival curves based on user input, and \n    contains none of the original data used to create the Cox model or models. \n    The goal is towards visualization and presentation of predicted survival\n    curves.",
    "version": "1.1.3",
    "maintainer": "Harrison Clement <harrisonclement16@gmail.com>",
    "url": "https://github.com/harryc598/shinyCox",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22871,
    "package_name": "shinyPredict",
    "title": "Predictions using Shiny",
    "description": "Creates 'shiny' application ('app.R') for making predictions based on lm(), glm(), or coxph() models.",
    "version": "0.1.1",
    "maintainer": "Jari Haukka <jari.haukka@helsinki.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22881,
    "package_name": "shinyTempSignal",
    "title": "Explore Temporal and Other Phylogenetic Signals",
    "description": "Sequences sampled at different time points can be used to infer molecular phylogenies on natural time scales, but if the sequences records inaccurate sampling times, that are not the actual sampling times, then it will affect the molecular phylogenetic analysis. This shiny application helps exploring temporal characteristics of the evolutionary trees through linear regression analysis and with the ability to identify and remove incorrect labels. The method was extended to support exploring other phylogenetic signals under strict and relaxed models.",
    "version": "0.0.8",
    "maintainer": "Guangchuang Yu <guangchuangyu@gmail.com>",
    "url": "https://github.com/YuLab-SMU/shinyTempSignal,\nhttps://www.sciencedirect.com/science/article/pii/S167385272400033X",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22963,
    "package_name": "shrink",
    "title": "Global, Parameterwise and Joint Shrinkage Factor Estimation",
    "description": "The predictive value of a statistical model can often be improved\n    by applying shrinkage methods. This can be achieved, e.g., by regularized\n    regression or empirical Bayes approaches. Various types of shrinkage factors can\n    also be estimated after a maximum likelihood. While global shrinkage modifies\n    all regression coefficients by the same factor, parameterwise shrinkage factors\n    differ between regression coefficients. With variables which are either highly\n    correlated or associated with regard to contents, such as several columns of a\n    design matrix describing a nonlinear effect, parameterwise shrinkage factors are\n    not interpretable and a compromise between global and parameterwise shrinkage,\n    termed 'joint shrinkage', is a useful extension. A computational shortcut to\n    resampling-based shrinkage factor estimation based on DFBETA residuals can be\n    applied. Global, parameterwise and joint shrinkage for models fitted by lm(),\n    glm(), coxph(), or mfp() is available.",
    "version": "1.2.3",
    "maintainer": "Daniela Dunkler <daniela.dunkler@meduniwien.ac.at>",
    "url": "https://github.com/biometrician/shrink",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22964,
    "package_name": "shrinkDSM",
    "title": "Efficient Bayesian Inference for Dynamic Survival Models with\nShrinkage",
    "description": "Efficient Markov chain Monte Carlo (MCMC) algorithms for fully \n    Bayesian estimation of dynamic survival models with shrinkage priors. \n    Details on the algorithms used are provided in Wagner (2011) <doi:10.1007/s11222-009-9164-5>, \n    Bitto and Frühwirth-Schnatter (2019) <doi:10.1016/j.jeconom.2018.11.006> and\n    Cadonna et al. (2020) <doi:10.3390/econometrics8020020>.",
    "version": "1.0.0",
    "maintainer": "Daniel Winkler <daniel.winkler@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22965,
    "package_name": "shrinkGPR",
    "title": "Scalable Gaussian Process Regression with Hierarchical Shrinkage\nPriors",
    "description": "Efficient variational inference methods for fully Bayesian Gaussian \n  Process Regression (GPR) models with hierarchical shrinkage priors, \n  including the triple gamma prior for effective variable selection and \n  covariance shrinkage in high-dimensional settings. The package leverages normalizing \n  flows to approximate complex posterior distributions. For details on implementation, \n  see Knaus (2025) <doi:10.48550/arXiv.2501.13173>.",
    "version": "1.1.1",
    "maintainer": "Peter Knaus <peter.knaus@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22966,
    "package_name": "shrinkTVP",
    "title": "Efficient Bayesian Inference for Time-Varying Parameter Models\nwith Shrinkage",
    "description": "Efficient Markov chain Monte Carlo (MCMC) algorithms for fully Bayesian estimation of time-varying parameter models with shrinkage priors, both dynamic and static. Details on the algorithms used are provided in Bitto and Frühwirth-Schnatter (2019) <doi:10.1016/j.jeconom.2018.11.006> and  \n  Cadonna et al. (2020) <doi:10.3390/econometrics8020020> and Knaus and Frühwirth-Schnatter (2023)  <doi:10.48550/arXiv.2312.10487>. For details on the package, please see Knaus et al. (2021) <doi:10.18637/jss.v100.i13>. For the multivariate extension, see the 'shrinkTVPVAR' package.",
    "version": "3.1.1",
    "maintainer": "Peter Knaus <peter.knaus@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22967,
    "package_name": "shrinkTVPVAR",
    "title": "Efficient Bayesian Inference for TVP-VAR-SV Models with\nShrinkage",
    "description": "Efficient Markov chain Monte Carlo (MCMC) algorithms for fully Bayesian estimation of time-varying parameter \n  vector autoregressive models with stochastic volatility (TVP-VAR-SV) under shrinkage priors and dynamic shrinkage processes. \n  Details on the TVP-VAR-SV model and the shrinkage priors can be found in Cadonna et al. (2020) <doi:10.3390/econometrics8020020>, \n  details on the software can be found in Knaus et al. (2021) <doi:10.18637/jss.v100.i13>, while details on the dynamic shrinkage process\n  can be found in Knaus and Frühwirth-Schnatter (2023) <doi:10.48550/arXiv.2312.10487>.",
    "version": "1.0.1",
    "maintainer": "Peter Knaus <peter.knaus@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22968,
    "package_name": "shrinkem",
    "title": "Approximate Bayesian Regularization for Parsimonious Estimates",
    "description": "Approximate Bayesian regularization using Gaussian approximations. The input is a vector of estimates\n             and a Gaussian error covariance matrix of the key parameters. Bayesian shrinkage is then applied\n             to obtain parsimonious solutions. The method is described on \n             Karimova, van Erp, Leenders, and Mulder (2024) <DOI:10.31234/osf.io/2g8qm>. Gibbs samplers are used\n             for model fitting. The shrinkage priors that are supported are Gaussian (ridge) priors, Laplace\n             (lasso) priors (Park and Casella, 2008 <DOI:10.1198/016214508000000337>), and horseshoe priors\n             (Carvalho, et al., 2010; <DOI:10.1093/biomet/asq017>). These priors include an option\n             for grouped regularization of different subsets of parameters (Meier et al., 2008; \n             <DOI:10.1111/j.1467-9868.2007.00627.x>). F priors are used for the penalty\n             parameters lambda^2 (Mulder and Pericchi, 2018 <DOI:10.1214/17-BA1092>). This correspond to\n             half-Cauchy priors on lambda (Carvalho, Polson, Scott, 2010 <DOI:10.1093/biomet/asq017>).",
    "version": "0.2.0",
    "maintainer": "Joris Mulder <j.mulder3@tilburguniversity.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22973,
    "package_name": "shutterplot",
    "title": "The R Shutter Plot Package",
    "description": "Shows the scatter plot along with the fitted regression lines. It \n  depicts min, max, the three quartiles, mean, and sd for each variable.\n  It also depicts sd-line, sd-box, r, r-square, prediction boundaries, and regression outliers.  ",
    "version": "0.1.0",
    "maintainer": "Siddhanta Phuyal <siddhantaphuyal7159@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 22979,
    "package_name": "siebanxicor",
    "title": "Query Data Series from Bank of Mexico",
    "description": "Allows to retrieve time series of all indicators available in the Bank of Mexico's Economic Information System (<http://www.banxico.org.mx/SieInternet/>).",
    "version": "1.0.0",
    "maintainer": "Noé Palmerin  <sie@banxico.org.mx>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23012,
    "package_name": "simBKMRdata",
    "title": "Helper Functions for Bayesian Kernel Machine Regression",
    "description": "Provides a suite of helper functions to support Bayesian Kernel \n    Machine Regression (BKMR) analyses in environmental health research. It \n    enables the simulation of realistic multivariate exposure data using \n    Multivariate Skewed Gamma distributions, estimation of distributional \n    parameters by subgroup, and application of adaptive, data-driven thresholds \n    for feature selection via Posterior Inclusion Probabilities (PIPs). It is \n    especially suited for handling skewed exposure data and enhancing the \n    interpretability of BKMR results through principled variable selection. The\n    methodology is shown in Hasan et. al. (2025) <doi:10.1101/2025.04.14.25325822>.",
    "version": "0.2.1",
    "maintainer": "Kazi Tanvir Hasan <khasa006@fiu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23019,
    "package_name": "simFastBOIN",
    "title": "Fast Bayesian Optimal Interval Design for Phase I Dose-Finding\nTrials",
    "description": "\n    Conducting Bayesian Optimal Interval (BOIN) design for phase I \n    dose-finding trials. 'simFastBOIN' provides functions for pre-computing \n    decision tables, conducting trial simulations, and evaluating operating \n    characteristics. The package uses vectorized operations and the \n    Iso::pava() function for isotonic regression to achieve efficient \n    performance while maintaining full compatibility with BOIN methodology. \n    Version 1.3.2 adds p_saf and p_tox parameters for customizable safety and \n    toxicity thresholds. Version 1.3.1 fixes Date field. Version 1.2.1 adds \n    comprehensive 'roxygen2' documentation and enhanced print formatting with \n    flexible table output options. Version 1.2.0 integrated C-based PAVA for \n    isotonic regression. Version 1.1.0 introduced conservative MTD selection \n    (boundMTD) and flexible early stopping rules (n_earlystop_rule). Methods \n    are described in Liu and Yuan (2015) <doi:10.1111/rssc.12089>.",
    "version": "1.3.2",
    "maintainer": "Gosuke Homma <my.name.is.gosuke@gmail.com>",
    "url": "https://github.com/gosukehommaEX/simFastBOIN",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23024,
    "package_name": "simITS",
    "title": "Analysis via Simulation of Interrupted Time Series (ITS) Data",
    "description": "Uses simulation to create prediction intervals for\n    post-policy outcomes in interrupted time series (ITS) designs,\n    following Miratrix (2020) <arXiv:2002.05746>. This package provides\n    methods for fitting ITS models with lagged outcomes and variables to\n    account for temporal dependencies.  It then conducts inference via\n    simulation, simulating a set of plausible counterfactual post-policy\n    series to compare to the observed post-policy series. This package\n    also provides methods to visualize such data, and also to incorporate\n    seasonality models and smoothing and aggregation/summarization.  This\n    work partially funded by Arnold Ventures in collaboration with\n    MDRC.",
    "version": "0.1.1",
    "maintainer": "Luke Miratrix <lmiratrix@g.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23025,
    "package_name": "simMSM",
    "title": "Simulation of Event Histories for Multi-State Models",
    "description": "Simulation of event histories with possibly non-linear baseline hazard rate functions, non-linear (time-varying) covariate effect functions, and dependencies on the past of the history. Random generation of event histories is performed using inversion sampling on the cumulative all-cause hazard rate functions. ",
    "version": "1.1.42",
    "maintainer": "Holger Reulen <hreulen@uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23027,
    "package_name": "simPH",
    "title": "Simulate and Plot Estimates from Cox Proportional Hazards Models",
    "description": "Simulates and plots quantities of interest (relative\n    hazards, first differences, and hazard ratios) for linear coefficients,\n    multiplicative interactions, polynomials, penalised splines, and\n    non-proportional hazards, as well as stratified survival curves from Cox\n    Proportional Hazard models. It also simulates and plots marginal effects\n    for multiplicative interactions. Methods described in Gandrud (2015)\n    <doi:10.18637/jss.v065.i03>.",
    "version": "1.3.14",
    "maintainer": "Christopher Gandrud <christopher.gandrud@gmail.com>",
    "url": "https://CRAN.R-project.org/package=simPH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23041,
    "package_name": "simdd",
    "title": "Simulation of Fisher Bingham and Related Directional\nDistributions",
    "description": "Simulation methods for the Fisher Bingham distribution on the unit sphere, the matrix Bingham distribution on a Grassmann manifold, the matrix Fisher distribution on SO(3), and the bivariate von Mises sine model on the torus.\n The methods use an acceptance/rejection simulation algorithm for the Bingham distribution and are described fully by Kent, Ganeiber and Mardia (2018) <doi:10.1080/10618600.2017.1390468>.\n These methods supersede earlier MCMC simulation methods and are more general than earlier simulation methods.\n The methods can be slower in specific situations where there are existing non-MCMC simulation methods (see Section 8 of Kent, Ganeiber and Mardia (2018) <doi:10.1080/10618600.2017.1390468> for further details).",
    "version": "1.1-2",
    "maintainer": "Kassel Liam Hingee <kassel.hingee@anu.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23049,
    "package_name": "simfit",
    "title": "Test Model Fit with Simulation",
    "description": "Simulates data from model objects (e.g., from lm(), glm()),\n    and plots this along with the original data to compare how well the\n    simulated data matches the original data to determine model fit.",
    "version": "0.1.0",
    "maintainer": "James Green <James.Green@ul.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23059,
    "package_name": "simml",
    "title": "Single-Index Models with Multiple-Links",
    "description": "A major challenge in estimating treatment decision rules from a randomized clinical trial dataset with covariates measured at baseline lies in detecting relatively small treatment effect modification-related variability (i.e., the treatment-by-covariates interaction effects on treatment outcomes) against a relatively large non-treatment-related variability (i.e., the main effects of covariates on treatment outcomes). The class of Single-Index Models with Multiple-Links is a novel single-index model specifically designed to estimate a single-index (a linear combination) of the covariates associated with the treatment effect modification-related variability, while allowing a nonlinear association with the treatment outcomes via flexible link functions. The models provide a flexible regression approach to developing treatment decision rules based on patients' data measured at baseline. We refer to Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1016/j.jspi.2019.05.008> and Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1111/biom.13320> (that allows an unspecified X main effect) for detail of the method. The main function of this package is simml().",
    "version": "0.3.0",
    "maintainer": "Hyung Park <parkh15@nyu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23070,
    "package_name": "simpleMH",
    "title": "Simple Metropolis-Hastings MCMC Algorithm",
    "description": "A very bare-bones interface to use the Metropolis-Hastings Monte \n    Carlo Markov Chain algorithm. It is suitable for teaching and testing \n    purposes. ",
    "version": "0.1.1",
    "maintainer": "Hugo Gruson <hugo.gruson+R@normalesup.org>",
    "url": "https://github.com/Bisaloo/simpleMH",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23087,
    "package_name": "simpr.interaction",
    "title": "Calculating Parameters for Simulation-Based Regression\nInteraction Power Analysis",
    "description": "Provides functionality for analytically calculating parameters (via the \n    'InteractionPoweR' package) useful for simulation of moderated multiple \n    regression, based on the correlations among the predictors and outcome and \n    the reliability of predictors.",
    "version": "0.1.0",
    "maintainer": "Ethan C. Brown <ethancbrown@gmail.com>",
    "url": "https://github.com/statisfactions/simpr.interaction/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23089,
    "package_name": "simqi",
    "title": "Simulate Quantities of Interest from Regression Models",
    "description": "This is an all-encompassing suite to facilitate the simulation of\n    so-called quantities of interest by way of a multivariate normal distribution\n    of the regression model's coefficients and variance-covariance matrix.",
    "version": "0.2.0",
    "maintainer": "Steve Miller <steve@svmiller.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23090,
    "package_name": "simr",
    "title": "Power Analysis for Generalised Linear Mixed Models by Simulation",
    "description": "Calculate power for generalised linear mixed models, using\n    simulation. Designed to work with models fit using the 'lme4' package.\n    Described in Green and MacLeod, 2016 <doi:10.1111/2041-210X.12504>.",
    "version": "1.0.8",
    "maintainer": "Peter Green <simr.peter@gmail.com>",
    "url": "https://github.com/pitakakariki/simr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23091,
    "package_name": "simrec",
    "title": "Simulation of Recurrent Event Data for Non-Constant Baseline\nHazard",
    "description": "Simulation of recurrent event data for non-constant baseline\n    hazard in the total time model with risk-free intervals and possibly a competing event.\n    Possibility to cut the data to an interim data set. Data can be plotted.\n    Details about the method can be found in Jahn-Eimermacher, A. et al. (2015) <doi:10.1186/s12874-015-0005-2>.",
    "version": "1.0.1",
    "maintainer": "Federico Marini <marinif@uni-mainz.de>",
    "url": "https://github.com/federicomarini/simrec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23096,
    "package_name": "simsl",
    "title": "Single-Index Models with a Surface-Link",
    "description": "An implementation of a single-index regression for optimizing individualized dose rules from an observational study. To model interaction effects between baseline covariates and a treatment variable defined on a continuum, we employ two-dimensional penalized spline regression on an index-treatment domain, where the index is defined as a linear combination of the covariates (a single-index). An unspecified main effect for the covariates is allowed, which can also be modeled through a parametric model. A unique contribution of this work is in the parsimonious single-index parametrization specifically defined for the interaction effect term. We refer to Park, Petkova, Tarpey, and Ogden (2020) <doi:10.1111/biom.13320> (for the case of a discrete treatment) and Park, Petkova, Tarpey, and Ogden (2021) \"A single-index model with a surface-link for optimizing individualized dose rules\" <arXiv:2006.00267v2> for detail of the method. The model can take a member of the exponential family as a response variable and can also take an ordinal categorical response. The main function of this package is simsl(). ",
    "version": "0.2.1",
    "maintainer": "Hyung Park <parkh15@nyu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23099,
    "package_name": "simsurv",
    "title": "Simulate Survival Data",
    "description": "Simulate survival times from standard parametric survival\n    distributions (exponential, Weibull, Gompertz), 2-component mixture\n    distributions, or a user-defined hazard, log hazard, cumulative hazard,\n    or log cumulative hazard function. Baseline covariates can be included\n    under a proportional hazards assumption.\n    Time dependent effects (i.e. non-proportional hazards) can be included by\n    interacting covariates with linear time or a user-defined function of time.\n    Clustered event times are also accommodated.\n    The 2-component mixture distributions can allow for a variety of flexible\n    baseline hazard functions reflecting those seen in practice.\n    If the user wishes to provide a user-defined\n    hazard or log hazard function then this is possible, and the resulting\n    cumulative hazard function does not need to have a closed-form solution.\n    For details see the supporting paper <doi:10.18637/jss.v097.i03>.\n    Note that this package is modelled on the 'survsim' package available in\n    the 'Stata' software (see Crowther and Lambert (2012)\n    <https://www.stata-journal.com/sjpdf.html?articlenum=st0275> or\n    Crowther and Lambert (2013) <doi:10.1002/sim.5823>).",
    "version": "1.0.0",
    "maintainer": "Sam Brilleman <sam.brilleman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23118,
    "package_name": "singcar",
    "title": "Comparing Single Cases to Small Samples",
    "description": "When comparing single cases to control populations and no parameters are known researchers and clinicians must estimate these with a control sample. This is often done when testing a case's abnormality on some variable or testing abnormality of the discrepancy between two variables. Appropriate frequentist and Bayesian methods for doing this are here implemented, including tests allowing for the inclusion of covariates. These have been developed first and foremost by John Crawford and Paul Garthwaite, e.g. in Crawford and Howell (1998) <doi:10.1076/clin.12.4.482.7241>, Crawford and Garthwaite (2005) <doi:10.1037/0894-4105.19.3.318>, Crawford and Garthwaite (2007) <doi:10.1080/02643290701290146> and Crawford, Garthwaite and Ryan (2011) <doi:10.1016/j.cortex.2011.02.017>. The package is also equipped with power calculators for each method. ",
    "version": "0.1.5",
    "maintainer": "Jonathan Rittmo <j.rittmo@gmail.com>",
    "url": "https://github.com/jorittmo/singcar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23122,
    "package_name": "singleRcapture",
    "title": "Single-Source Capture-Recapture Models",
    "description": "Implementation of single-source capture-recapture methods for population size estimation using zero-truncated, zero-one truncated and zero-truncated one-inflated Poisson, Geometric and Negative Binomial regression as well as Zelterman's and Chao's regression. Package includes point and interval estimators for the population size with variances estimated using analytical or bootstrap method. Details can be found in: van der Heijden et all. (2003) <doi:10.1191/1471082X03st057oa>, Böhning and van der Heijden (2019) <doi:10.1214/18-AOAS1232>, Böhning et al. (2020) Capture-Recapture Methods for the Social and Medical Sciences or Böhning and Friedl (2021) <doi:10.1007/s10260-021-00556-8>.",
    "version": "0.2.3",
    "maintainer": "Maciej Beręsewicz <maciej.beresewicz@ue.poznan.pl>",
    "url": "https://github.com/ncn-foreigners/singleRcapture",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23126,
    "package_name": "siqr",
    "title": "An R Package for Single-Index Quantile Regression",
    "description": "Single-Index Quantile Regression is effective in some scenarios. We provides functions that allow users to fit Single-Index Quantile Regression model. It also provides functions to do prediction, estimate standard errors of the single-index coefficients via bootstrap, and visualize the estimated univariate function. Please see W., Y., Y. (2010) <doi:10.1016/j.jmva.2010.02.003> for details.",
    "version": "0.8.1",
    "maintainer": "Tianhai Zu <zuti@mail.uc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23129,
    "package_name": "sirt",
    "title": "Supplementary Item Response Theory Models",
    "description": "\n    Supplementary functions for item response models aiming\n    to complement existing R packages. The functionality includes among others\n    multidimensional compensatory and noncompensatory IRT models\n    (Reckase, 2009, <doi:10.1007/978-0-387-89976-3>), \n    MCMC for hierarchical IRT models and testlet models\n    (Fox, 2010, <doi:10.1007/978-1-4419-0742-4>), \n    NOHARM (McDonald, 1982, <doi:10.1177/014662168200600402>), \n    Rasch copula model (Braeken, 2011, <doi:10.1007/s11336-010-9190-4>;\n    Schroeders, Robitzsch & Schipolowski, 2014, <doi:10.1111/jedm.12054>),\n    faceted and hierarchical rater models (DeCarlo, Kim & Johnson, 2011,\n    <doi:10.1111/j.1745-3984.2011.00143.x>),\n    ordinal IRT model (ISOP; Scheiblechner, 1995, <doi:10.1007/BF02301417>), \n    DETECT statistic (Stout, Habing, Douglas & Kim, 1996, \n    <doi:10.1177/014662169602000403>), local structural equation modeling \n    (LSEM; Hildebrandt, Luedtke, Robitzsch, Sommer & Wilhelm, 2016,\n    <doi:10.1080/00273171.2016.1142856>).",
    "version": "4.2-133",
    "maintainer": "Alexander Robitzsch <robitzsch@ipn.uni-kiel.de>",
    "url": "https://github.com/alexanderrobitzsch/sirt,\nhttps://sites.google.com/view/alexander-robitzsch/software",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23132,
    "package_name": "sisal",
    "title": "Sequential Input Selection Algorithm",
    "description": "Implements the SISAL algorithm by Tikka and Hollmén. It is\n        a sequential backward selection algorithm which uses a linear\n        model in a cross-validation setting. Starting from the full\n        model, one variable at a time is removed based on the\n        regression coefficients. From this set of models, a\n        parsimonious (sparse) model is found by choosing the model with\n        the smallest number of variables among those models where the\n        validation error is smaller than a threshold. Also implements\n        extensions which explore larger parts of the search space\n        and/or use ridge regression instead of ordinary least squares.",
    "version": "0.49",
    "maintainer": "Mikko Korpela <mvkorpel@iki.fi>",
    "url": "https://github.com/mvkorpel/sisal",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23133,
    "package_name": "sisireg",
    "title": "Sign-Simplicity-Regression-Solver",
    "description": "Implementation of the SSR-Algorithm. The Sign-Simplicity-Regression model is a nonparametric statistical model which is based on residual signs and simplicity assumptions on the regression function. Goal is to calculate the most parsimonious regression function satisfying the statistical adequacy requirements. Theory and functions are specified in Metzner (2020, ISBN: 979-8-68239-420-3, \"Trendbasierte Prognostik\") and Metzner (2021, ISBN: 979-8-59347-027-0, \"Adäquates Maschinelles Lernen\").",
    "version": "1.2.1",
    "maintainer": "Lars Metzner <lars.metzner@ppi.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23153,
    "package_name": "sjPlot",
    "title": "Data Visualization for Statistics in Social Science",
    "description": "Collection of plotting and table output functions for data",
    "version": "2.9.0",
    "maintainer": "Daniel Lüdecke <d.luedecke@uke.de>",
    "url": "https://github.com/strengejacke/sjPlot",
    "exports": [],
    "topics": ["data-visualization", "plotting", "r", "social-sciences", "statistics"],
    "score": "NA",
    "stars": 636
  },
  {
    "id": 23171,
    "package_name": "skewsamp",
    "title": "Estimate Sample Sizes for Group Comparisons with Skewed\nDistributions",
    "description": "Estimate necessary sample sizes for comparing the location\n    of data from two groups or categories when the distribution of the \n    data is skewed. The package \n    offers a non-parametric method for a Wilcoxon Mann-Whitney test of \n    location shift as well as methods for several generalized linear \n    models, for instance, Gamma regression.",
    "version": "1.0.0",
    "maintainer": "Johannes Brachem <jbrachem@posteo.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23178,
    "package_name": "sklarsomega",
    "title": "Measuring Agreement Using Sklar's Omega Coefficient",
    "description": "Provides tools for applying Sklar's Omega (Hughes, 2022) <doi:10.1007/s11222-022-10105-2>\n    methodology to nominal scores, ordinal scores, percentages, counts, amounts (i.e.,\n    non-negative real numbers), and balances (i.e., any real number). The framework can\n    accommodate any number of units, any number of coders, and missingness; and\n    can be used to measure agreement with a gold standard, intra-coder agreement,\n    and/or inter-coder agreement. Frequentist inference is supported for all levels\n    of measurement. Bayesian inference is supported for continuous scores only.",
    "version": "3.0-3",
    "maintainer": "John Hughes <drjphughesjr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23180,
    "package_name": "sknifedatar",
    "title": "Swiss Knife of Data",
    "description": "Extension of the 'modeltime' ecosystem. In",
    "version": "0.1.2.9000",
    "maintainer": "",
    "url": "https://github.com/rafzamb/sknifedatar",
    "exports": [],
    "topics": ["data", "data-analysis", "data-science", "data-visualization", "forecasting", "r", "statistics", "time-series"],
    "score": "NA",
    "stars": 37
  },
  {
    "id": 23193,
    "package_name": "slcm",
    "title": "Sparse Latent Class Model for Cognitive Diagnosis",
    "description": "Perform a Bayesian estimation of the exploratory \n    Sparse Latent Class Model for Binary Data \n    described by Chen, Y., Culpepper, S. A., and Liang, F. (2020) \n    <doi:10.1007/s11336-019-09693-2>.",
    "version": "0.1.1",
    "maintainer": "James Joseph Balamuta <balamut2@illinois.edu>",
    "url": "https://tmsalab.github.io/slcm/, https://github.com/tmsalab/slcm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23195,
    "package_name": "sleekts",
    "title": "4253H, Twice Smoothing",
    "description": "Compute Time series Resistant Smooth 4253H, twice smoothing method.",
    "version": "1.0.2",
    "maintainer": "Muntashir-Al-Arefin <sheen4783@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23198,
    "package_name": "sleev",
    "title": "Semiparametric Likelihood Estimation with Errors in Variables",
    "description": "Efficient regression analysis under general two-phase sampling, where Phase I includes error-prone data and Phase II contains validated data on a subset.",
    "version": "1.1.6",
    "maintainer": "Ran Tao <r.tao@vanderbilt.edu>",
    "url": "https://github.com/dragontaoran/sleev",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23201,
    "package_name": "slgf",
    "title": "Bayesian Model Selection with Suspected Latent Grouping Factors",
    "description": "Implements the Bayesian model selection method with suspected latent \n    grouping factor methodology of Metzger and Franck (2020), \n    <doi:10.1080/00401706.2020.1739561>. SLGF detects latent \n    heteroscedasticity or group-based regression effects based on the levels of a \n    user-specified categorical predictor. ",
    "version": "2.0.0",
    "maintainer": "Thomas Metzger <metzger.181@osu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23209,
    "package_name": "slm",
    "title": "Stationary Linear Models",
    "description": "Provides statistical procedures for linear regression in the general context where the errors are assumed to be correlated. Different ways to estimate the asymptotic covariance matrix of the least squares estimators are available. Starting from this estimation of the covariance matrix, the confidence intervals and the usual tests on the parameters are modified. The functions of this package are very similar to those of 'lm': it contains methods such as summary(), plot(), confint() and predict(). The 'slm' package is described in the paper by E. Caron, J. Dedecker and B. Michel (2019), \"Linear regression with stationary errors: the R package slm\", arXiv preprint <arXiv:1906.06583>.",
    "version": "1.2.0",
    "maintainer": "Emmanuel Caron <emmanuelcaron3@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23215,
    "package_name": "slp",
    "title": "Discrete Prolate Spheroidal (Slepian) Sequence Regression\nSmoothers",
    "description": "Interface for creation of 'slp' class smoother objects for \n             use in Generalized Additive Models (as implemented by packages \n             'gam' and 'mgcv'). ",
    "version": "1.0-5",
    "maintainer": "Wesley Burr <wesley.burr@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23219,
    "package_name": "sm",
    "title": "Smoothing Methods for Nonparametric Regression and Density\nEstimation",
    "description": "This is software linked to the book\n  'Applied Smoothing Techniques for Data Analysis -\n  The Kernel Approach with S-Plus Illustrations' Oxford University Press.",
    "version": "2.2-6.0",
    "maintainer": "Adrian Bowman <adrian.bowman@glasgow.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23234,
    "package_name": "smartsizer",
    "title": "Power Analysis for a SMART Design",
    "description": "A set of tools for determining the necessary sample size\n    in order to identify the optimal dynamic treatment regime\n    in a sequential, multiple assignment, randomized trial (SMART). \n    Utilizes multiple comparisons with the best methodology \n    to adjust for multiple comparisons.\n    Designed for an arbitrary SMART design. Please see Artman (2018) <doi:10.1093/biostatistics/kxy064> for more details.",
    "version": "1.0.3",
    "maintainer": "William Artman <William_Artman@URMC.Rochester.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23240,
    "package_name": "smcure",
    "title": "Fit Semiparametric Mixture Cure Models",
    "description": "An R-package for Estimating Semiparametric PH and AFT Mixture Cure Models.",
    "version": "2.2",
    "maintainer": "Chao Cai <caic@mailbox.sc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23250,
    "package_name": "smicd",
    "title": "Statistical Methods for Interval-Censored Data",
    "description": "Functions that provide statistical methods for interval-censored (grouped) data. The package supports the estimation of linear and linear mixed regression models with interval-censored dependent variables. Parameter estimates are obtained by a stochastic expectation maximization algorithm. Furthermore, the package enables the direct (without covariates) estimation of statistical indicators from interval-censored data via an iterative kernel density algorithm. Survey and Organisation for Economic Co-operation and Development (OECD) weights can be included into the direct estimation (see, Walter, P. (2019) <doi:10.17169/refubium-1621>).",
    "version": "1.1.5",
    "maintainer": "Paul Walter <paul.w@gmx.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23254,
    "package_name": "smlePH",
    "title": "Sieve Maximum Full Likelihood Estimation for the Right-Censored\nProportional Hazards Model",
    "description": "Fitting the full likelihood proportional hazards model and\n    extracting the residuals.",
    "version": "0.1.0",
    "maintainer": "Taehwa Choi <tchoi@sungshin.ac.kr>",
    "url": "https://github.com/taehwa015/smlePH/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23263,
    "package_name": "smoothSurv",
    "title": "Survival Regression with Smoothed Error Distribution",
    "description": "Contains, as a main contribution, a function to fit\n             a regression model with possibly right, left or interval\n             censored observations and with the error distribution\n             expressed as a mixture of G-splines. Core part\n             of the computation is done in compiled 'C++' written\n             using the 'Scythe' Statistical Library Version 0.3.",
    "version": "2.6",
    "maintainer": "Arnošt Komárek <arnost.komarek@mff.cuni.cz>",
    "url": "https://msekce.karlin.mff.cuni.cz/~komarek/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23266,
    "package_name": "smoothedLasso",
    "title": "A Framework to Smooth L1 Penalized Regression Operators using\nNesterov Smoothing",
    "description": "We provide full functionality to smooth L1 penalized regression operators and to compute regression estimates thereof. For this, the objective function of a user-specified regression operator is first smoothed using Nesterov smoothing (see Y. Nesterov (2005) <doi:10.1007/s10107-004-0552-5>), resulting in a modified objective function with explicit gradients everywhere. The smoothed objective function and its gradient are minimized via BFGS, and the obtained minimizer is returned. Using Nesterov smoothing, the smoothed objective function can be made arbitrarily close to the original (unsmoothed) one. In particular, the Nesterov approach has the advantage that it comes with explicit accuracy bounds, both on the L1/L2 difference of the unsmoothed to the smoothed objective functions as well as on their respective minimizers (see G. Hahn, S.M. Lutz, N. Laha, C. Lange (2020) <doi:10.1101/2020.09.17.301788>). A progressive smoothing approach is provided which iteratively smoothes the objective function, resulting in more stable regression estimates. A function to perform cross validation for selection of the regularization parameter is provided.",
    "version": "1.6",
    "maintainer": "Georg Hahn <ghahn@hsph.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23267,
    "package_name": "smoothemplik",
    "title": "Smoothed Empirical Likelihood",
    "description": "Empirical likelihood methods for asymptotically efficient\n    estimation of models based on conditional or unconditional moment\n    restrictions; see Kitamura, Tripathi & Ahn (2004)\n    <doi:10.1111/j.1468-0262.2004.00550.x> and Owen (2013)\n    <doi:10.1002/cjs.11183>.\n    Kernel-based non-parametric methods for density/regression estimation and\n    numerical routines for empirical likelihood maximisation are implemented in\n    'Rcpp' for speed.",
    "version": "0.0.17",
    "maintainer": "Andreï Victorovitch Kostyrka <andrei.kostyrka@gmail.com>",
    "url": "https://github.com/Fifis/smoothemplik",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23268,
    "package_name": "smoothic",
    "title": "Variable Selection Using a Smooth Information Criterion",
    "description": "Implementation of the SIC epsilon-telescope method, either\n    using single or distributional (multiparameter) regression. Includes classical regression\n    with normally distributed errors and robust regression, where the errors are from\n    the Laplace distribution. The \"smooth generalized normal distribution\" is used,\n    where the estimation of an additional shape parameter allows the user to move\n    smoothly between both types of regression. See O'Neill and Burke (2022)\n    \"Robust Distributional Regression with Automatic Variable Selection\" for more details.\n    <doi:10.48550/arXiv.2212.07317>. This package also contains the data analyses from O'Neill and\n    Burke (2023). \"Variable selection using a smooth information criterion for distributional\n    regression models\". <doi:10.1007/s11222-023-10204-8>.",
    "version": "1.2.1",
    "maintainer": "Meadhbh O'Neill <meadhbhon@gmail.com>",
    "url": "https://meadhbh-oneill.github.io/smoothic/,\nhttps://github.com/meadhbh-oneill/smoothic",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23274,
    "package_name": "smoots",
    "title": "Nonparametric Estimation of the Trend and Its Derivatives in TS",
    "description": "The nonparametric trend and its derivatives in equidistant time \n    series (TS) with short-memory stationary errors can be estimated. The \n    estimation is conducted via local polynomial regression using an \n    automatically selected bandwidth obtained by a built-in iterative plug-in \n    algorithm or a bandwidth fixed by the user. A Nadaraya-Watson kernel \n    smoother is also built-in as a comparison. With version 1.1.0, a linearity \n    test for the trend function, forecasting methods and backtesting \n    approaches are implemented as well.\n    The smoothing methods of the package are described in Feng, Y., Gries, T., \n    and Fritz, M. (2020) <doi:10.1080/10485252.2020.1759598>.",
    "version": "1.1.4",
    "maintainer": "Dominik Schulz <schulzd@mail.uni-paderborn.de>",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/\nhttps://wiwi.uni-paderborn.de/dep4/gries/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23283,
    "package_name": "smurf",
    "title": "Sparse Multi-Type Regularized Feature Modeling",
    "description": "Implementation of the SMuRF algorithm of Devriendt et al. (2021) <doi:10.1016/j.insmatheco.2020.11.010> to fit generalized linear models (GLMs) with multiple types of predictors via regularized maximum likelihood.",
    "version": "1.1.8",
    "maintainer": "Tom Reynkens <tomreynkens.r@gmail.com>",
    "url": "https://gitlab.com/TReynkens/smurf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23288,
    "package_name": "snSMART",
    "title": "Small N Sequential Multiple Assignment Randomized Trial Methods",
    "description": "Consolidated data simulation, sample size calculation and\n    analysis functions for several snSMART (small sample sequential,\n    multiple assignment, randomized trial) designs under one library.  See\n    Wei, B., Braun, T.M., Tamura, R.N. and Kidwell, K.M. \"A Bayesian\n    analysis of small n sequential multiple assignment randomized trials\n    (snSMARTs).\" (2018) Statistics in medicine, 37(26), pp.3723-3732\n    <doi:10.1002/sim.7900>.",
    "version": "0.2.4",
    "maintainer": "Michael Kleinsasser <mkleinsa@umich.edu>",
    "url": "https://github.com/sidiwang/snSMART",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23289,
    "package_name": "sna",
    "title": "Tools for Social Network Analysis",
    "description": "A range of tools for social network analysis, including node and graph-level indices, structural distance and covariance methods, structural equivalence detection, network regression, random graph generation, and 2D/3D network visualization.",
    "version": "2.8",
    "maintainer": "Carter T. Butts <buttsc@uci.edu>",
    "url": "https://statnet.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23298,
    "package_name": "snazzieR",
    "title": "Chic and Sleek Functions for Beautiful Statisticians",
    "description": "Because your linear models deserve better than console output.\n  A sleek color palette and kable styling to make your regression results look sharper than they are.\n  Includes support for Partial Least Squares (PLS) regression via both the SVD and NIPALS algorithms,\n  along with a unified interface for model fitting and fabulous LaTeX and console output formatting.\n  See the package website at \n  <https://finitesample.space/snazzier>.",
    "version": "0.1.2",
    "maintainer": "Aidan J. Wagner <JesusButForGayPeople@proton.me>",
    "url": "https://detectivefierce.github.io/snazzieR/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23308,
    "package_name": "snotelr",
    "title": "Calculate and Visualize 'SNOTEL' Snow Data and Seasonality",
    "description": "Programmatic interface to the 'SNOTEL' snow data\n  (<https://www.nrcs.usda.gov/programs-initiatives/sswsf-snow-survey-and-water-supply-forecasting-program>). Provides easy downloads of snow \n  data into your R work space or a local directory. Additional post-processing \n  routines to extract snow season indexes are provided.",
    "version": "1.5.2",
    "maintainer": "Koen Hufkens <koen.hufkens@gmail.com>",
    "url": "https://github.com/bluegreen-labs/snotelr,\nhttps://bluegreen-labs.github.io/snotelr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23324,
    "package_name": "sobolnp",
    "title": "Nonparametric Sobol Estimator with Bootstrap Bandwidth",
    "description": "Algorithm to estimate the Sobol indices using a non-parametric fit of the regression curve. The bandwidth is estimated using bootstrap to reduce the finite-sample bias. The package is based on the paper Solís, M. (2018) <arXiv:1803.03333>.   ",
    "version": "0.1.0",
    "maintainer": "Maikol Solís <maikol.solis@ucr.ac.cr>",
    "url": "https://github.com/maikol-solis/sobolnp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23346,
    "package_name": "soilhypfit",
    "title": "Modelling of Soil Water Retention and Hydraulic Conductivity\nData",
    "description": "Provides functions for efficiently estimating properties of \n         the Van Genuchten-Mualem model for soil hydraulic parameters from \n         possibly sparse soil water retention and hydraulic conductivity data \n         by multi-response parameter estimation methods \n         (Stewart, W.E., Caracotsios, M. Soerensen, J.P. (1992) \n         \"Parameter estimation from multi-response data\" \n         <doi:10.1002/aic.690380502>). Parameter estimation is simplified \n         by exploiting the fact that residual and saturated water contents \n         and saturated conductivity are conditionally linear parameters \n         (Bates, D. M.  and Watts, D. G. (1988) \n         \"Nonlinear Regression Analysis and Its Applications\" \n         <doi:10.1002/9780470316757>). \n         Estimated parameters are optionally constrained by the evaporation\n         characteristic length (Lehmann, P., Bickel, S., Wei, Z. and Or, D. (2020)\n         \"Physical Constraints for Improved Soil Hydraulic Parameter \n         Estimation by Pedotransfer Functions\" <doi:10.1029/2019WR025963>) \n         to ensure that the estimated parameters are physically valid. \n         Common S3 methods and further utility functions allow to process, \n         explore and visualise estimation results.",
    "version": "0.1-8",
    "maintainer": "Andreas Papritz <papritz@retired.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23348,
    "package_name": "soiltestcorr",
    "title": "Soil Test Correlation and Calibration",
    "description": "A compilation of functions designed to assist users on the correlation analysis of crop yield and soil test values. Functions to estimate crop response patterns to soil nutrient availability and critical soil test values using various approaches such as: 1) the modified arcsine-log calibration curve (Correndo et al. (2017) <doi:10.1071/CP16444>); 2) the graphical Cate-Nelson quadrants analysis (Cate & Nelson (1965)), 3) the statistical Cate-Nelson quadrants analysis (Cate & Nelson (1971) <doi:10.2136/sssaj1971.03615995003500040048x>), 4) the linear-plateau regression (Anderson & Nelson (1975) <doi:10.2307/2529422>), 5) the quadratic-plateau regression (Bullock & Bullock (1994) <doi:10.2134/agronj1994.00021962008600010033x>), and 6) the Mitscherlich-type exponential regression (Melsted & Peck (1977) <doi:10.2134/asaspecpub29.c1>). The package development stemmed from ongoing work with the Fertilizer Recommendation Support Tool (FRST) and Feed the Future Innovation Lab for Collaborative Research on Sustainable Intensification (SIIL) projects. ",
    "version": "2.2.1",
    "maintainer": "Adrian A. Correndo <acorrend@uoguelph.ca>",
    "url": "https://adriancorrendo.github.io/soiltestcorr/,\nhttps://soiltestfrst.org/,\nhttps://www.siildigitalagconsortium.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23374,
    "package_name": "sorocs",
    "title": "A Bayesian Semiparametric Approach to Correlated ROC Surfaces",
    "description": "A Bayesian semiparametric Dirichlet process mixtures to estimate correlated receiver operating characteristic (ROC) surfaces and the associated volume under the surface (VUS) with stochastic order constraints. The reference paper is:Zhen Chen, Beom Seuk Hwang, (2018) \"A Bayesian semiparametric approach to correlated ROC surfaces with stochastic order constraints\". Biometrics, 75, 539-550. <doi:10.1111/biom.12997>. ",
    "version": "0.1.0",
    "maintainer": "Weimin Zhang <zhangwm@hotmail.com>",
    "url": "http://github.com/wzhang17/sorocs.git",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23420,
    "package_name": "spaceNet",
    "title": "Latent Space Models for Multidimensional Networks",
    "description": "Latent space models for multivariate networks (multiplex) estimated via MCMC algorithm. See D Angelo et al. (2018) <arXiv:1803.07166> and D Angelo et al. (2018) <arXiv:1807.03874>.",
    "version": "1.2",
    "maintainer": "Silvia D'Angelo <silvia.dangelo@ucd.ie>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23427,
    "package_name": "spam",
    "title": "SPArse Matrix",
    "description": "Set of functions for sparse matrix algebra.\n    Differences with other sparse matrix packages are:\n    (1) we only support (essentially) one sparse matrix format,\n    (2) based on transparent and simple structure(s),\n    (3) tailored for MCMC calculations within G(M)RF.\n    (4) and it is fast and scalable (with the extension package spam64).\n    Documentation about 'spam' is provided by vignettes included in this package, see also Furrer and Sain (2010) <doi:10.18637/jss.v036.i10>; see 'citation(\"spam\")' for details.",
    "version": "2.11-3",
    "maintainer": "Reinhard Furrer <reinhard.furrer@math.uzh.ch>",
    "url": "https://www.math.uzh.ch/pages/spam/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23434,
    "package_name": "spareg",
    "title": "Sparse Projected Averaged Regression",
    "description": "A flexible framework combining \n  variable screening and random projection techniques for fitting ensembles of \n  predictive generalized linear models to high-dimensional data.\n  Designed for extensibility, the package implements\n  key techniques as S3 classes with user-friendly constructors,\n  enabling easy integration and development of new procedures for \n  high-dimensional applications. For more details see \n  Parzer et al (2024a) <doi:10.48550/arXiv.2312.00130> and\n  Parzer et al (2024b) <doi:10.48550/arXiv.2410.00971>.",
    "version": "1.1.1",
    "maintainer": "Laura Vana-Gür <laura.vana.guer@tuwien.ac.at>",
    "url": "https://github.com/lauravana/spareg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23450,
    "package_name": "sparseDFM",
    "title": "Estimate Dynamic Factor Models with Sparse Loadings",
    "description": "Implementation of various estimation methods for dynamic factor models (DFMs) including principal components analysis (PCA) Stock and Watson (2002) <doi:10.1198/016214502388618960>, 2Stage Giannone et al. (2008) <doi:10.1016/j.jmoneco.2008.05.010>, expectation-maximisation (EM) Banbura and Modugno (2014) <doi:10.1002/jae.2306>, and the novel EM-sparse approach for sparse DFMs Mosley et al. (2023) <arXiv:2303.11892>. Options to use classic multivariate Kalman filter and smoother (KFS) equations from Shumway and Stoffer (1982) <doi:10.1111/j.1467-9892.1982.tb00349.x> or fast univariate KFS equations from Koopman and Durbin (2000) <doi:10.1111/1467-9892.00186>, and options for independent and identically distributed (IID) white noise or auto-regressive (AR(1)) idiosyncratic errors. Algorithms coded in 'C++' and linked to R via 'RcppArmadillo'.   ",
    "version": "1.0",
    "maintainer": "Alex Gibberd <a.gibberd@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23452,
    "package_name": "sparseFLMM",
    "title": "Functional Linear Mixed Models for Irregularly or Sparsely\nSampled Data",
    "description": "Estimation of functional linear mixed models for irregularly or\n    sparsely sampled data based on functional principal component analysis.",
    "version": "0.4.2",
    "maintainer": "Jona Cederbaum <Jona.Cederbaum@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23457,
    "package_name": "sparseLTSEigen",
    "title": "RcppEigen back end for sparse least trimmed squares regression",
    "description": "Use RcppEigen to fit least trimmed squares\n    regression models with an L1 penalty in order to obtain\n    sparse models.",
    "version": "0.2.0.1",
    "maintainer": "Andreas Alfons <alfons@ese.eur.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23464,
    "package_name": "sparsebn",
    "title": "Learning Sparse Bayesian Networks from High-Dimensional Data",
    "description": "Fast methods for learning sparse Bayesian networks from high-dimensional data using sparse regularization, as described in Aragam, Gu, and Zhou (2017) <arXiv:1703.04025>. Designed to handle mixed experimental and observational data with thousands of variables with either continuous or discrete observations.",
    "version": "0.1.0",
    "maintainer": "Bryon Aragam <sparsebn@gmail.com>",
    "url": "https://github.com/itsrainingdata/sparsebn",
    "exports": [],
    "topics": ["bayesian-networks", "covariance-matrices", "experimental-data", "graphical-models", "machine-learning", "r", "regularization", "statistics"],
    "score": "NA",
    "stars": 42
  },
  {
    "id": 23468,
    "package_name": "sparselink",
    "title": "Sparse Regression for Related Problems",
    "description": "Estimates sparse regression models (i.e., with few non-zero coefficients) in high-dimensional multi-task learning and transfer learning settings, as proposed by Rauschenberger et al. (2025) <https://orbilu.uni.lu/handle/10993/63425>.",
    "version": "1.0.0",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@lih.lu>",
    "url": "https://github.com/rauschenberger/sparselink,\nhttps://rauschenberger.github.io/sparselink/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23469,
    "package_name": "sparsenet",
    "title": "Fit Sparse Linear Regression Models via Nonconvex Optimization",
    "description": "Efficient procedure for fitting regularization paths between L1 and L0, using the MC+ penalty of Zhang, C.H. (2010)<doi:10.1214/09-AOS729>. Implements the methodology described in Mazumder, Friedman and Hastie (2011) <DOI: 10.1198/jasa.2011.tm09738>. Sparsenet computes the regularization surface over both the family parameter and the tuning parameter by coordinate descent.",
    "version": "1.7",
    "maintainer": "Trevor Hastie <hastie@stanford.edu>",
    "url": "https://hastie.su.domains/public/Papers/Sparsenet/Mazumder-SparseNetCoordinateDescent-2011.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23473,
    "package_name": "sparsereg",
    "title": "Sparse Bayesian Models for Regression, Subgroup Analysis, and\nPanel Data",
    "description": "Sparse modeling provides a mean selecting a small number of non-zero effects from a large possible number of candidate effects.  This package includes a suite of methods for sparse modeling: estimation via EM or MCMC, approximate confidence intervals with nominal coverage, and diagnostic and summary plots.  The method can implement sparse linear regression and sparse probit regression.  Beyond regression analyses, applications include subgroup analysis, particularly for conjoint experiments, and panel data. Future versions will include extensions to  models with truncated outcomes, propensity score, and instrumental variable analysis.",
    "version": "1.2",
    "maintainer": "Marc Ratkovic <ratkovic@princeton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23474,
    "package_name": "sparsestep",
    "title": "SparseStep Regression",
    "description": "Implements the SparseStep model for solving regression\n    problems with a sparsity constraint on the parameters. The SparseStep\n    regression model was proposed in Van den Burg, Groenen, and Alfons (2017)\n    <arXiv:1701.06967>. In the model, a regularization term is added to the\n    regression problem which approximates the counting norm of the parameters.\n    By iteratively improving the approximation a sparse solution to the\n    regression problem can be obtained.  In this package both the standard\n    SparseStep algorithm is implemented as well as a path algorithm which uses\n    golden section search to determine solutions with different values for the\n    regularization parameter.",
    "version": "1.0.1",
    "maintainer": "Gertjan van den Burg <gertjanvandenburg@gmail.com>",
    "url": "https://github.com/GjjvdBurg/SparseStep,\nhttps://arxiv.org/abs/1701.06967",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23536,
    "package_name": "spcr",
    "title": "Sparse Principal Component Regression",
    "description": "The sparse principal component regression is computed. The regularization parameters are optimized by cross-validation.",
    "version": "2.1.1",
    "maintainer": "Shuichi Kawano <skawano@math.kyushu-u.ac.jp>",
    "url": "https://doi.org/10.1016/j.csda.2015.03.016,\nhttps://doi.org/10.1016/j.csda.2018.03.008,\nhttps://sites.google.com/site/shuichikawanoen/software",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23557,
    "package_name": "specs",
    "title": "Single-Equation Penalized Error-Correction Selector (SPECS)",
    "description": "Implementation of SPECS, your favourite Single-Equation Penalized Error-Correction Selector developed in\n    Smeekes and Wijler (2021) <doi:10.1016/j.jeconom.2020.07.021>. SPECS provides a fully automated estimation procedure for large and potentially\n    (co)integrated datasets. The dataset in levels is converted to a conditional error-correction model, either by the user or\n    by means of the functions included in this package, and various specialised forms of penalized regression can be applied to\n    the model. Automated options for initializing and selecting a sequence of penalties, as well as the construction of penalty\n    weights via an initial estimator, are available. Moreover, the user may choose from a number of pre-specified deterministic\n    configurations to further simplify the model building process.",
    "version": "1.0.1",
    "maintainer": "Etienne Wijler <e.j.j.wijler@vu.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23563,
    "package_name": "spectral",
    "title": "Common Methods of Spectral Data Analysis",
    "description": "On discrete data spectral analysis is performed by Fourier and Hilbert\n    transforms as well as with model based analysis called Lomb-Scargle method.\n    Fragmented and irregularly spaced data can be processed in almost all methods. Both,\n    FFT as well as LOMB methods take multivariate data and return standardized PSD. \n    For didactic reasons an analytical approach for deconvolution of noise spectra and \n    sampling function is provided.\n    A user friendly interface helps to interpret the results.",
    "version": "2.0",
    "maintainer": "Martin Seilmayer <martin.seilmayer@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23564,
    "package_name": "spectralAnomaly",
    "title": "Detect Anomalies Using the Spectral Residual Algorithm",
    "description": "Apply the spectral residual algorithm to data, such as a\n    time series, to detect anomalies. Anomaly scores can be used to determine\n    outliers based upon a threshold or fed into more sophisticated prediction\n    models. Methods are based upon \"Time-Series Anomaly Detection Service at\n    Microsoft\", Ren, H., Xu, B., Wang, Y., et al., (2019)\n    <doi:10.48550/arXiv.1906.03821>.",
    "version": "0.1.1",
    "maintainer": "Allen OBrien <allen.g.obrien@gmail.com>",
    "url": "https://al-obrien.github.io/spectralAnomaly/,\nhttps://github.com/al-obrien/spectralAnomaly",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23565,
    "package_name": "spectralGP",
    "title": "Approximate Gaussian Processes Using the Fourier Basis",
    "description": "Routines for creating, manipulating, and performing \n Bayesian inference about Gaussian processes in \n one and two dimensions using the Fourier basis approximation: \n simulation and plotting of processes, calculation of \n coefficient variances, calculation of process density, \n coefficient proposals (for use in MCMC).  It uses R environments to\n store GP objects as references/pointers.",
    "version": "1.3.3",
    "maintainer": "Chris Paciorek <paciorek@alumni.cmu.edu>",
    "url": "http://www.jstatsoft.org/v19/a2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23575,
    "package_name": "speedyBBT",
    "title": "Efficient Bayesian Inference for the Bradley--Terry Model",
    "description": "A suite of functions that allow a full, fast, and efficient Bayesian treatment of the Bradley--Terry model. \n             Prior assumptions about the model parameters can be encoded through a multivariate normal prior distribution. \n             Inference is performed using a latent variable representation of the model. ",
    "version": "1.0",
    "maintainer": "Rowland Seymour <r.g.seymour@bham.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23578,
    "package_name": "spef",
    "title": "Semiparametric Estimating Functions",
    "description": "Functions for fitting semiparametric regression models for\n        panel count survival data. An overview of the package can be found \n        in Wang and Yan (2011) <doi:10.1016/j.cmpb.2010.10.005> and\n\tChiou et al. (2018) <doi:10.1111/insr.12271>.",
    "version": "1.0.9",
    "maintainer": "Sy Han (Steven) Chiou <schiou@utdallas.edu>",
    "url": "http://github.com/stc04003/spef",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23588,
    "package_name": "spfda",
    "title": "Function-on-Scalar Regression with Group-Bridge Penalty",
    "description": "Implements a group-bridge penalized function-on-scalar regression\n    model proposed by Wang et al. (2023) <doi:10.1111/biom.13684>, to simultaneously\n    estimate functional coefficient and recover the local sparsity.",
    "version": "0.9.2",
    "maintainer": "Zhengjia Wang <dipterix.wang@gmail.com>",
    "url": "https://github.com/dipterix/spfda, https://dipterix.org/spfda/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23591,
    "package_name": "spgwr",
    "title": "Geographically Weighted Regression",
    "description": "Functions for computing geographically weighted\n  regressions are provided, based on work by Chris\n  Brunsdon, Martin Charlton and Stewart Fotheringham. ",
    "version": "0.6-37",
    "maintainer": "Roger Bivand <Roger.Bivand@nhh.no>",
    "url": "https://github.com/rsbivand/spgwr/,\nhttps://rsbivand.github.io/spgwr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23605,
    "package_name": "spikeSlabGAM",
    "title": "Bayesian Variable Selection and Model Choice for Generalized\nAdditive Mixed Models",
    "description": "Bayesian variable selection, model choice, and regularized\n    estimation for (spatial) generalized additive mixed regression models\n    via stochastic search variable selection with spike-and-slab priors.",
    "version": "1.1-20",
    "maintainer": "Fabian Scheipl <fabian.scheipl@stat.uni-muenchen.de>",
    "url": "https://github.com/fabian-s/spikeSlabGAM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23607,
    "package_name": "spikeslab",
    "title": "Prediction and Variable Selection Using Spike and Slab\nRegression",
    "description": "Spike and slab for prediction and variable selection in linear regression models. Uses a generalized elastic net for variable selection.",
    "version": "1.1.6",
    "maintainer": "Udaya B. Kogalur <ubk@kogalur.com>",
    "url": "https://ishwaran.org/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23616,
    "package_name": "spiralize",
    "title": "Visualize Data on Spirals",
    "description": "It visualizes data along an Archimedean spiral <https://en.wikipedia.org/wiki/Archimedean_spiral>, \n    makes so-called spiral graph or spiral chart. \n    It has two major advantages for visualization: 1. It is able to visualize data with very long axis with high \n    resolution. 2. It is efficient for time series data to reveal periodic patterns.",
    "version": "1.1.0",
    "maintainer": "Zuguang Gu <z.gu@dkfz.de>",
    "url": "https://github.com/jokergoo/spiralize,\nhttps://jokergoo.github.io/spiralize/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23625,
    "package_name": "splineCox",
    "title": "A Two-Stage Estimation Approach to Cox Regression Using M-Spline\nFunction",
    "description": "Implements a two-stage estimation approach for Cox \n    regression using five-parameter M-spline functions to model the baseline hazard. It allows\n    for flexible hazard shapes and model selection based on log-likelihood criteria as described in\n    Teranishi et al.(2025).\n    In addition, the package provides functions for constructing and evaluating B-spline copulas \n    based on five M-spline or I-spline basis functions, allowing users to flexibly model and \n    compute bivariate dependence structures. Both the copula function and its density can be evaluated.\n    Furthermore, the package supports computation of dependence measures such as Kendall's tau and \n    Spearman's rho, derived analytically from the copula parameters.",
    "version": "0.0.7",
    "maintainer": "Ren Teranishi <ren.teranishi1227@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23628,
    "package_name": "splines2",
    "title": "Regression Spline Functions and Classes",
    "description": "Constructs basis functions of B-splines, M-splines,\n    I-splines, convex splines (C-splines), periodic splines,\n    natural cubic splines, generalized Bernstein polynomials,\n    their derivatives, and integrals (except C-splines)\n    by closed-form recursive formulas.\n    It also contains a C++ head-only library integrated with Rcpp.\n    See Wang and Yan (2021) <doi:10.6339/21-JDS1020> for details.",
    "version": "0.5.4",
    "maintainer": "Wenjie Wang <wang@wwenjie.org>",
    "url": "https://wwenjie.org/splines2,\nhttps://github.com/wenjie2wang/splines2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23630,
    "package_name": "splinetrials",
    "title": "Facilitate Clinical Trials Analysis Using Natural Cubic Splines",
    "description": "Create mixed models with repeated measures using natural\n    cubic splines applied to an observed continuous time variable, as\n    described by Donohue et al. (2023) <doi:10.1002/pst.2285>. Iterate\n    through multiple covariance structure types until one converges.\n    Categorize observed time according to scheduled visits. Perform\n    subgroup analyses.",
    "version": "0.1.0",
    "maintainer": "Nik Krieger <nikkrieger@gmail.com>",
    "url": "https://github.com/NikKrieger/splinetrials,\nhttps://nikkrieger.github.io/splinetrials/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23656,
    "package_name": "spooky",
    "title": "Time Feature Extrapolation Using Spectral Analysis and\nJack-Knife Resampling",
    "description": "Proposes application of spectral analysis and jack-knife resampling for multivariate sequence forecasting. The application allows for a fast random search in a compact space of hyper-parameters composed by Sequence Length and Jack-Knife Leave-N-Out.",
    "version": "1.4.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/spooky",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23659,
    "package_name": "sport",
    "title": "Sequential Pairwise Online Rating Techniques",
    "description": "Calculates ratings for two-player or \n  multi-player challenges. Methods included in package such as are able to \n  estimate ratings (players strengths) and their evolution in time, also able to \n  predict output of challenge. Algorithms are based on Bayesian Approximation \n  Method, and they don't involve any matrix inversions nor likelihood estimation. \n  Parameters are updated sequentially, and computation doesn't require any \n  additional RAM to make estimation feasible. Additionally, base of the package \n  is written in C++ what makes sport computation even faster. Methods used in the \n  package refer to Mark E. Glickman (1999) \n  <https://www.glicko.net/research/glicko.pdf>; \n  Mark E. Glickman (2001) <doi:10.1080/02664760120059219>; \n  Ruby C. Weng, Chih-Jen Lin (2011) <https://www.jmlr.org/papers/volume12/weng11a/weng11a.pdf>; \n  W. Penny, Stephen J. Roberts (1999) <doi:10.1109/IJCNN.1999.832603>.",
    "version": "0.2.2",
    "maintainer": "Dawid Kałędkowski <dawid.kaledkowski@gmail.com>",
    "url": "https://github.com/gogonzo/sport",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23680,
    "package_name": "spsurv",
    "title": "Bernstein Polynomial Based Semiparametric Survival Analysis",
    "description": "A set of reliable routines to ease semiparametric survival regression modeling based on Bernstein polynomials. 'spsurv' includes proportional hazards, proportional odds and accelerated failure time frameworks for right-censored data. RV Panaro (2020) <arXiv:2003.10548>.",
    "version": "1.0.0",
    "maintainer": "Renato Panaro <renatovp@ime.usp.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23698,
    "package_name": "squant",
    "title": "Subgroup Identification Based on Quantitative Objectives",
    "description": "A subgroup identification method for precision medicine based on \n    quantitative objectives. This method can handle continuous, binary and \n    survival endpoint for both prognostic and predictive case. For the \n    predictive case, the method aims at identifying a subgroup for which \n    treatment is better than control by at least a pre-specified or \n    auto-selected constant. For the prognostic case, the method aims at \n    identifying a subgroup that is at least better than a \n    pre-specified/auto-selected constant. The derived signature is a linear \n    combination of predictors, and the selected subgroup are subjects with \n    the signature > 0. The false discover rate when no true subgroup exists\n    is controlled at a user-specified level.",
    "version": "1.1.7",
    "maintainer": "YAN SUN <sunyanrobin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23704,
    "package_name": "sra",
    "title": "Selection Response Analysis",
    "description": "Artificial selection through selective breeding is an efficient way to induce changes in traits of interest in experimental populations. This package (sra) provides a set of tools to analyse artificial-selection response datasets. The data typically feature for several generations the average value of a trait in a population, the variance of the trait, the population size and the average value of the parents that were chosen to breed. Sra implements two families of models aiming at describing the dynamics of the genetic architecture of the trait during the selection response. The first family relies on purely descriptive (phenomenological) models, based on an autoregressive framework. The second family provides different mechanistic models, accounting e.g. for inbreeding, mutations, genetic and environmental canalization, or epistasis. The parameters underlying the dynamics of the time series are estimated by maximum likelihood. The sra package thus provides (i) a wrapper for the R functions mle() and optim() aiming at fitting in a convenient way a predetermined set of models, and (ii) some functions to plot and analyze the output of the models. ",
    "version": "0.1.4.1",
    "maintainer": "Arnaud Le Rouzic <arnaud.le-rouzic@universite-paris-saclay.fr>",
    "url": "https://github.com/lerouzic/sra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23708,
    "package_name": "sregsurvey",
    "title": "Semiparametric Model-Assisted Estimation in Finite Populations",
    "description": "It is a framework to fit semiparametric regression estimators for the total parameter of a finite population when the interest variable is asymmetric distributed. The main references for this package are Sarndal C.E., Swensson B., and Wretman J. (2003,ISBN: 978-0-387-40620-6, \"Model Assisted Survey Sampling.\" Springer-Verlag) \n  Cardozo C.A, Paula G.A. and Vanegas L.H. (2022) \"Generalized log-gamma additive partial linear mdoels with P-spline smoothing\", Statistical Papers. \n  Cardozo C.A and Alonso-Malaver C.E. (2022). \"Semi-parametric model assisted estimation in finite populations.\" In preparation.    ",
    "version": "0.1.3",
    "maintainer": "Carlos Alberto Cardozo Delgado <cardozorpackages@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23709,
    "package_name": "srlars",
    "title": "Split Robust Least Angle Regression",
    "description": "Functions to perform split robust least angle regression. The approach first uses the\n             least angle regression algorithm to split the variables into the models of an ensemble\n             and robust estimates of the correlation between predictors. An elastic net estimator is \n             then applied to the selected predictors in each model using the imputed data from the\n             detect deviating cell (DDC) method.",
    "version": "1.0.1",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23712,
    "package_name": "srp",
    "title": "Smooth-Rough Partitioning of the Regression Coefficients",
    "description": "Performs the change-point detection in regression coefficients of linear model \n    by partitioning the regression coefficients into two classes of smoothness. The change-point and\n    the regression coefficients are jointly estimated. ",
    "version": "1.2.0",
    "maintainer": "Hyeyoung Maeng <h.maeng@lse.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23719,
    "package_name": "ssMousetrack",
    "title": "Bayesian State-Space Modeling of Mouse-Tracking Experiments via\nStan",
    "description": "Estimates previously compiled state-space modeling for mouse-tracking experiments using the 'rstan' package, which provides the R interface to the Stan C++ library for Bayesian estimation.",
    "version": "1.1.7",
    "maintainer": "Antonio Calcagnì <ant.calcagni@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23730,
    "package_name": "ssd4mosaic",
    "title": "Web Application for the SSD Module of the MOSAIC Platform",
    "description": "Web application using 'shiny' for the SSD (Species\n    Sensitivity Distribution) module of the MOSAIC (MOdeling and\n    StAtistical tools for ecotoxICology) platform. It estimates the\n    Hazardous Concentration for x% of the species (HCx) from toxicity\n    values that can be censored and provides various plotting options for\n    a better understanding of the results. See our companion paper\n    Kon Kam King et al. (2014) <doi:10.48550/arXiv.1311.5772>.",
    "version": "1.0.4-2",
    "maintainer": "Aurélie Siberchicot <aurelie.siberchicot@univ-lyon1.fr>",
    "url": "https://gitlab.in2p3.fr/mosaic-software/mosaic-ssd,\nhttps://mosaic.univ-lyon1.fr/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23738,
    "package_name": "ssgraph",
    "title": "Bayesian Graph Structure Learning using Spike-and-Slab Priors",
    "description": "Bayesian estimation for undirected graphical models using spike-and-slab priors. The package handles continuous, discrete, and mixed data. ",
    "version": "1.16",
    "maintainer": "Reza Mohammadi <a.mohammadi@uva.nl>",
    "url": "https://www.uva.nl/profile/a.mohammadi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23742,
    "package_name": "ssifs",
    "title": "Stochastic Search Inconsistency Factor Selection",
    "description": "Evaluating the consistency assumption of Network Meta-Analysis both globally and locally in the Bayesian framework. Inconsistencies are located by applying Bayesian variable selection to the inconsistency factors. The implementation of the method is described by Seitidis et al. (2023) <doi:10.1002/sim.9891>.",
    "version": "1.0.5",
    "maintainer": "Georgios Seitidis <g.seitidis@uoi.gr>",
    "url": "https://github.com/georgiosseitidis/ssifs,\nhttps://georgiosseitidis.github.io/ssifs/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23747,
    "package_name": "ssmn",
    "title": "Skew Scale Mixtures of Normal Distributions",
    "description": "Performs the EM algorithm for regression models using Skew Scale Mixtures of Normal Distributions.",
    "version": "1.1",
    "maintainer": "Luis Benites Sanchez <lbenitesanchez@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23748,
    "package_name": "ssmodels",
    "title": "Sample Selection Models",
    "description": "In order to facilitate the adjustment of the sample selection models\n  existing in the literature, we created the 'ssmodels' package. Our package\n  allows the adjustment of the classic Heckman model (Heckman (1976),\n  Heckman (1979) <doi:10.2307/1912352>), and the estimation of the parameters of\n  this model via the maximum likelihood method and two-step method, in addition\n  to the adjustment of the Heckman-t models introduced in the literature by\n  Marchenko and Genton (2012) <doi:10.1080/01621459.2012.656011> and the\n  Heckman-Skew model introduced in the literature by Ogundimu and Hutton (2016)\n  <doi:10.1111/sjos.12171>. We also implemented functions to adjust the\n  generalized version of the Heckman model, introduced by Bastos, Barreto-Souza,\n  and Genton (2021) <doi:10.5705/ss.202021.0068>, that allows the inclusion of\n  covariables to the dispersion and correlation parameters, and a function to\n  adjust the Heckman-BS model introduced by Bastos and Barreto-Souza (2020)\n  <doi:10.1080/02664763.2020.1780570> that uses the Birnbaum-Saunders\n  distribution as a joint distribution of the selection and primary regression\n  variables. This package extends and complements existing R packages such as \n  'sampleSelection' (Toomet and Henningsen, 2008) and 'ssmrob' (Zhelonkin et al., 2016), providing additional robust and flexible sample selection models.",
    "version": "2.0.1",
    "maintainer": "Fernando de Souza Bastos <fernando.bastos@ufv.br>",
    "url": "https://fsbmat-ufv.github.io/ssmodels/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23760,
    "package_name": "sstvars",
    "title": "Toolkit for Reduced Form and Structural Smooth Transition Vector\nAutoregressive Models",
    "description": "Penalized and non-penalized maximum likelihood estimation of smooth\n  transition vector autoregressive models with various types of transition weight\n  functions, conditional distributions, and identification methods. Constrained\n  estimation with various types of constraints is available. Residual based\n  model diagnostics, forecasting, simulations, counterfactual analysis, and\n  computation of impulse response functions, generalized impulse response functions,\n  generalized forecast error variance decompositions, as well as historical\n  decompositions. See\n  Heather Anderson, Farshid Vahid (1998) <doi:10.1016/S0304-4076(97)00076-6>,\n  Helmut Lütkepohl, Aleksei Netšunajev (2017) <doi:10.1016/j.jedc.2017.09.001>,\n  Markku Lanne, Savi Virolainen (2025) <doi:10.1016/j.jedc.2025.105162>,\n  Savi Virolainen (2025) <doi:10.48550/arXiv.2404.19707>.",
    "version": "1.2.2",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "url": "https://github.com/saviviro/sstvars",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23764,
    "package_name": "ssym",
    "title": "Fitting Semi-Parametric log-Symmetric Regression Models",
    "description": "Set of tools to fit a semi-parametric regression model suitable for analysis of data sets in which the response variable is continuous, strictly positive, asymmetric and possibly, censored. Under this setup, both the median and the skewness of the response variable distribution are explicitly modeled by using semi-parametric functions, whose non-parametric components may be approximated by natural cubic splines or P-splines. Supported distributions for the model error include log-normal, log-Student-t, log-power-exponential, log-hyperbolic, log-contaminated-normal, log-slash, Birnbaum-Saunders and Birnbaum-Saunders-t distributions.",
    "version": "1.5.8",
    "maintainer": "Luis Hernando Vanegas <hvanegasp@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23768,
    "package_name": "stR",
    "title": "Seasonal Trend Decomposition Using Regression",
    "description": "Methods for decomposing seasonal data: STR (a Seasonal-Trend \n  time series decomposition procedure based on Regression) and Robust STR. In \n  some ways, STR is similar to Ridge Regression and Robust STR can be related to \n  LASSO. They allow for multiple seasonal components, multiple linear covariates \n  with constant, flexible and seasonal influence. Seasonal patterns (for both \n  seasonal components and seasonal covariates) can be fractional and flexible \n  over time; moreover they can be either strictly periodic or have a more \n  complex topology. The methods provide confidence intervals for the estimated \n  components. The methods can also be used for forecasting.",
    "version": "0.7.1",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "url": "https://pkg.robjhyndman.com/stR/,\nhttps://github.com/robjhyndman/stR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23774,
    "package_name": "stability",
    "title": "Stability Analysis of Genotype by Environment Interaction (GEI)",
    "description": "Provides functionalities for performing stability analysis of genotype by environment interaction (GEI) to identify superior and stable genotypes across diverse environments. It implements Eberhart and Russell’s ANOVA method (1966)(<doi:10.2135/cropsci1966.0011183X000600010011x>), Finlay and Wilkinson’s Joint Linear Regression method (1963) (<doi:10.1071/AR9630742>), Wricke’s Ecovalence (1962, 1964), Shukla’s stability variance parameter (1972) (<doi:10.1038/hdy.1972.87>), Kang’s simultaneous selection for high yield and stability (1991) (<doi:10.2134/agronj1991.00021962008300010037x>), Additive Main Effects and Multiplicative Interaction (AMMI) method and Genotype plus Genotypes by Environment (GGE) Interaction methods.",
    "version": "0.6.0",
    "maintainer": "Muhammad Yaseen <myaseen208@gmail.com>",
    "url": "https://myaseen208.com/stability/\nhttps://CRAN.R-project.org/package=stability",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23777,
    "package_name": "stableGR",
    "title": "A Stable Gelman-Rubin Diagnostic for Markov Chain Monte Carlo",
    "description": "Practitioners of Bayesian statistics often use Markov chain Monte Carlo (MCMC) samplers to sample from a posterior distribution. This package determines whether the MCMC sample is large enough   to yield reliable estimates of the target distribution. In particular, this calculates a Gelman-Rubin convergence diagnostic using stable and consistent estimators of Monte Carlo variance. Additionally, this uses the connection between an MCMC sample's effective sample size and the Gelman-Rubin diagnostic to produce a threshold for terminating MCMC simulation. Finally, this informs the user whether enough samples have been collected  and (if necessary) estimates the number of samples needed for a desired level of accuracy. The theory underlying these methods can be found in \"Revisiting the Gelman-Rubin Diagnostic\" by Vats and  Knudson (2018) <arXiv:1812:09384>. ",
    "version": "1.2",
    "maintainer": "Christina Knudson <drchristinaknudson@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23797,
    "package_name": "standardize",
    "title": "Tools for Standardizing Variables for Regression in R",
    "description": "Tools which allow regression variables to be placed on similar\n    scales, offering computational benefits as well as easing interpretation of\n    regression output.",
    "version": "0.2.2",
    "maintainer": "Christopher D. Eager <eager.stats@gmail.com>",
    "url": "https://github.com/CDEager/standardize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23807,
    "package_name": "stargazer",
    "title": "Well-Formatted Regression and Summary Statistics Tables",
    "description": "Produces LaTeX code, HTML/CSS code and ASCII text for well-formatted tables that hold \n    regression analysis results from several models side-by-side, as well as summary\n    statistics.",
    "version": "5.2.3",
    "maintainer": "Marek Hlavac <marek.hlavac@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23810,
    "package_name": "starnet",
    "title": "Stacked Elastic Net",
    "description": "Implements stacked elastic net regression (Rauschenberger 2021 <doi:10.1093/bioinformatics/btaa535>). The elastic net generalises ridge and lasso regularisation (Zou 2005 <doi:10.1111/j.1467-9868.2005.00503.x>). Instead of fixing or tuning the mixing parameter alpha, we combine multiple alpha by stacked generalisation (Wolpert 1992 <doi:10.1016/S0893-6080(05)80023-1>).",
    "version": "1.0.0",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@uni.lu>",
    "url": "https://github.com/rauschenberger/starnet/,\nhttps://rauschenberger.github.io/starnet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23828,
    "package_name": "statcheck",
    "title": "Extract Statistics from Articles and Recompute P-Values",
    "description": "A \"spellchecker\" for statistics. It checks whether your",
    "version": "1.6.1",
    "maintainer": "",
    "url": "https://github.com/MicheleNuijten/statcheck",
    "exports": [],
    "topics": ["cran", "nhst", "p-values", "r", "reproducibility", "statistics"],
    "score": "NA",
    "stars": 184
  },
  {
    "id": 23830,
    "package_name": "statcomp",
    "title": "Statistical Complexity and Information Measures for Time Series\nAnalysis",
    "description": "An implementation of local and global statistical complexity measures (aka Information Theory Quantifiers, ITQ) for time series analysis based on ordinal statistics (Bandt and Pompe (2002) <DOI:10.1103/PhysRevLett.88.174102>). Several distance measures that operate on ordinal pattern distributions, auxiliary functions for ordinal pattern analysis, and generating functions for stochastic and deterministic-chaotic processes for ITQ testing are provided. ",
    "version": "0.1.0",
    "maintainer": "Sebastian Sippel <sebastian.sippel@env.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23846,
    "package_name": "statip",
    "title": "Statistical Functions for Probability Distributions and\nRegression",
    "description": "A collection of miscellaneous statistical functions for \n    probability distributions: 'dbern()', 'pbern()', 'qbern()', 'rbern()' for \n    the Bernoulli distribution, and 'distr2name()', 'name2distr()' for \n    distribution names; \n    probability density estimation: 'densityfun()'; \n    most frequent value estimation: 'mfv()', 'mfv1()'; \n    other statistical measures of location: 'cv()' (coefficient of variation),\n    'midhinge()', 'midrange()', 'trimean()'; \n    construction of histograms: 'histo()', 'find_breaks()'; \n    calculation of the Hellinger distance: 'hellinger()'; \n    use of classical kernels: 'kernelfun()', 'kernel_properties()'; \n    univariate piecewise-constant regression: 'picor()'. ",
    "version": "0.2.3",
    "maintainer": "Paul Poncet <paulponcet@yahoo.fr>",
    "url": "https://github.com/paulponcet/statip",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23850,
    "package_name": "statmod",
    "title": "Statistical Modeling",
    "description": "A collection of algorithms and functions to aid statistical modeling. Includes limiting dilution analysis (aka ELDA), growth curve comparisons, mixed linear models, heteroscedastic regression, inverse-Gaussian probability calculations, Gauss quadrature and a secure convergence algorithm for nonlinear models. Also includes advanced generalized linear model functions including Tweedie and Digamma distributional families, secure convergence and exact distributional calculations for unit deviances.",
    "version": "1.5.1",
    "maintainer": "Gordon Smyth <smyth@wehi.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23859,
    "package_name": "stats4teaching",
    "title": "Simulate Pedagogical Statistical Data",
    "description": "Univariate and multivariate normal data simulation. They also supply a brief summary of the analysis for each experiment/design:\n    - Independent samples.\n    - One-way and two-way Anova.\n    - Paired samples (T-Test & Regression).\n    - Repeated measures (Anova & Multiple Regression).\n    - Clinical Assay. ",
    "version": "0.1.0",
    "maintainer": "Cabello Esteban <estebancabellogarcia@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23860,
    "package_name": "statsExpressions",
    "title": "Tidy Dataframes and Expressions with Statistical Details",
    "description": "Utilities for producing dataframes with rich details for the\n    most common types of statistical approaches and tests: parametric,\n    nonparametric, robust, and Bayesian t-test, one-way ANOVA, correlation\n    analyses, contingency table analyses, and meta-analyses. The functions\n    are pipe-friendly and provide a consistent syntax to work with tidy\n    data. These dataframes additionally contain expressions with\n    statistical details, and can be used in graphing packages. This\n    package also forms the statistical processing backend for\n    'ggstatsplot'. References: Patil (2021) <doi:10.21105/joss.03236>.",
    "version": "1.7.2",
    "maintainer": "Indrajeet Patil <patilindrajeet.science@gmail.com>",
    "url": "https://indrajeetpatil.github.io/statsExpressions/,\nhttps://github.com/IndrajeetPatil/statsExpressions",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23862,
    "package_name": "statsr",
    "title": "Companion Software for the Coursera Statistics with R Specialization",
    "description": "Data and functions to support Bayesian and frequentist inference and decision making",
    "version": "0.3.0",
    "maintainer": "Merlise Clyde <clyde@duke.edu>",
    "url": "https://github.com/StatsWithR/statsr",
    "exports": [],
    "topics": ["bayesian-inference", "coursera", "statistics"],
    "score": "NA",
    "stars": 73
  },
  {
    "id": 23868,
    "package_name": "stdReg",
    "title": "Regression Standardization",
    "description": "Contains functionality for regression standardization. Four general classes of models are allowed; generalized linear models, conditional generalized estimating equation models, Cox proportional hazards models and shared frailty gamma-Weibull models. Sjolander, A. (2016) <doi:10.1007/s10654-016-0157-3>.",
    "version": "3.4.2",
    "maintainer": "Arvid Sjolander <arvid.sjolander@ki.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23869,
    "package_name": "stdReg2",
    "title": "Regression Standardization for Causal Inference",
    "description": "Contains more modern tools for causal inference using regression \n      standardization. Four general classes of models are implemented; generalized \n      linear models, conditional generalized estimating equation models, \n      Cox proportional hazards models, and shared frailty gamma-Weibull models. \n      Methodological details are described in Sjölander, A. (2016) <doi:10.1007/s10654-016-0157-3>. \n      Also includes functionality for doubly robust estimation for generalized linear models \n      in some special cases, and the ability to implement custom models. ",
    "version": "1.0.3",
    "maintainer": "Michael C Sachs <sachsmc@gmail.com>",
    "url": "https://sachsmc.github.io/stdReg2/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23882,
    "package_name": "stepPenal",
    "title": "Stepwise Forward Variable Selection in Penalized Regression",
    "description": "Model Selection Based on Combined Penalties. This package implements a stepwise forward variable selection algorithm based on a penalized likelihood criterion that combines the L0 with L2 or L1 norms.",
    "version": "0.2",
    "maintainer": "Eleni Vradi <vradi.eleni@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23884,
    "package_name": "stepR",
    "title": "Multiscale Change-Point Inference",
    "description": "Allows fitting of step-functions to univariate serial data where neither the number of jumps nor their positions is known by implementing the multiscale regression estimators SMUCE, simulataneous multiscale changepoint estimator, (K. Frick, A. Munk and H. Sieling, 2014) <doi:10.1111/rssb.12047> and HSMUCE, heterogeneous SMUCE, (F. Pein, H. Sieling and A. Munk, 2017) <doi:10.1111/rssb.12202>. In addition, confidence intervals for the change-point locations and bands for the unknown signal can be obtained.",
    "version": "2.1-10",
    "maintainer": "Pein Florian <f.pein@lancaster.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23885,
    "package_name": "stepSplitReg",
    "title": "Stepwise Split Regularized Regression",
    "description": "Functions to perform stepwise split regularized regression. The approach first \n             uses a stepwise algorithm to split the variables into the models with a goodness\n             of fit criterion, and then regularization is applied to each model. The weights \n             of the models in the ensemble are determined based on a criterion selected by \n             the user.",
    "version": "1.0.5",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23887,
    "package_name": "stepdownfdp",
    "title": "A Step-Down Procedure to Control the False Discovery Proportion",
    "description": "Provides a step-down procedure for controlling the False\n  Discovery Proportion (FDP) in a competition-based setup, implementing \n  Dong et al. (2020) <arXiv:2011.11939>. Such setups include target-decoy \n  competition (TDC) in computational mass spectrometry and the knockoff \n  construction in linear regression.",
    "version": "1.0.0",
    "maintainer": "Arya Ebadi <aeba3842@uni.sydney.edu.au>",
    "url": "https://github.com/uni-Arya/stepdownfdp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23888,
    "package_name": "stepgbm",
    "title": "Stepwise Variable Selection for Generalized Boosted Regression\nModeling",
    "description": "An introduction to a couple of novel predictive variable selection methods for generalised boosted regression modeling (gbm). They are based on various variable influence methods (i.e., relative variable influence (RVI) and knowledge informed RVI (i.e., KIRVI, and KIRVI2)) that adopted similar ideas as AVI, KIAVI and KIAVI2 in the 'steprf' package, and also based on predictive accuracy in stepwise algorithms. For details of the variable selection methods, please see: Li, J., Siwabessy, J., Huang, Z. and Nichol, S. (2019) <doi:10.3390/geosciences9040180>. Li, J., Alvarez, B., Siwabessy, J., Tran, M., Huang, Z., Przeslawski, R., Radke, L., Howard, F., Nichol, S. (2017). <DOI: 10.13140/RG.2.2.27686.22085>.",
    "version": "1.0.1",
    "maintainer": "Jin Li <jinli68@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23892,
    "package_name": "stepp",
    "title": "Subpopulation Treatment Effect Pattern Plot (STEPP)",
    "description": "A method to explore the treatment-covariate interactions in survival or generalized \n\tlinear model (GLM) for continuous, binomial and count data arising from two or more treatment \n\tarms of a clinical trial. A permutation distribution approach to inference is implemented, \n\tbased on permuting the covariate values within each treatment group.",
    "version": "3.2.7",
    "maintainer": "Wai-ki Yip <yuser86@yahoo.com>",
    "url": "https://www.r-project.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23907,
    "package_name": "stilt",
    "title": "Separable Gaussian Process Interpolation (Emulation)",
    "description": "Functions for simplified emulation of time series computer model output in model parameter space using Gaussian processes. Stilt can be used more generally for Kriging of spatio-temporal fields. There are functions to predict at new parameter settings, to test the emulator using cross-validation (which includes information on 95% confidence interval empirical coverage), and to produce contour plots over 2D slices in model parameter space.",
    "version": "1.3.1",
    "maintainer": "Kelsey Ruckert <datamgmt@scrim.psu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23909,
    "package_name": "stima",
    "title": "Simultaneous Threshold Interaction Modeling Algorithm",
    "description": "Regression trunk model estimation proposed by Dusseldorp and Meulman (2004) <doi:10.1007/bf02295641> and Dusseldorp, Conversano, Van Os (2010) <doi:10.1198/jcgs.2010.06089>, integrating a regression tree and a multiple regression model.   ",
    "version": "1.2.4",
    "maintainer": "Elise Dusseldorp <elise.dusseldorp@fsw.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23911,
    "package_name": "stlARIMA",
    "title": "STL Decomposition and ARIMA Hybrid Forecasting Model",
    "description": "Univariate time series forecasting with STL decomposition based auto regressive integrated moving average (ARIMA) hybrid  model. For method details see Xiong T, Li C, Bao Y (2018). <doi:10.1016/j.neucom.2017.11.053>. ",
    "version": "0.1.0",
    "maintainer": "Ronit Jaiswal <ronitjaiswal2912@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23912,
    "package_name": "stlELM",
    "title": "Hybrid Forecasting Model Based on STL Decomposition and ELM",
    "description": "Univariate time series forecasting with STL decomposition based Extreme Learning Machine hybrid  model. For method details see Xiong T, Li C, Bao Y (2018). <doi:10.1016/j.neucom.2017.11.053>. ",
    "version": "0.1.1",
    "maintainer": "Girish Kumar Jha <girish.stat@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23916,
    "package_name": "stlplus",
    "title": "Enhanced Seasonal Decomposition of Time Series by Loess",
    "description": "Decompose a time series into seasonal, trend, and remainder\n    components using an implementation of Seasonal Decomposition of Time\n    Series by Loess (STL) that provides several enhancements over the STL\n    method in the stats package.  These enhancements include handling missing\n    values, providing higher order (quadratic) loess smoothing with automated\n    parameter choices, frequency component smoothing beyond the seasonal and\n    trend components, and some basic plot methods for diagnostics.",
    "version": "0.5.1",
    "maintainer": "Ryan Hafen <rhafen@gmail.com>",
    "url": "https://github.com/hafen/stlplus",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23917,
    "package_name": "stm",
    "title": "Estimation of the Structural Topic Model",
    "description": "The Structural Topic Model (STM) allows researchers \n  to estimate topic models with document-level covariates. \n  The package also includes tools for model selection, visualization,\n  and estimation of topic-covariate regressions. Methods developed in\n  Roberts et. al. (2014) <doi:10.1111/ajps.12103> and \n  Roberts et. al. (2016) <doi:10.1080/01621459.2016.1141684>. Vignette\n  is Roberts et. al. (2019) <doi:10.18637/jss.v091.i02>.",
    "version": "1.3.8",
    "maintainer": "Brandon Stewart <bms4@princeton.edu>",
    "url": "http://www.structuraltopicmodel.com/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23926,
    "package_name": "stochvol",
    "title": "Efficient Bayesian Inference for Stochastic Volatility (SV)\nModels",
    "description": "Efficient algorithms for fully Bayesian estimation of stochastic volatility (SV) models with and without asymmetry (leverage) via Markov chain Monte Carlo (MCMC) methods. Methodological details are given in Kastner and Frühwirth-Schnatter (2014) <doi:10.1016/j.csda.2013.01.002> and Hosszejni and Kastner (2019) <doi:10.1007/978-3-030-30611-3_8>; the most common use cases are described in Hosszejni and Kastner (2021) <doi:10.18637/jss.v100.i12> and Kastner (2016) <doi:10.18637/jss.v069.i05> and the package examples.",
    "version": "3.2.8",
    "maintainer": "Darjus Hosszejni <darjus.hosszejni@icloud.com>",
    "url": "https://gregorkastner.github.io/stochvol/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23945,
    "package_name": "stpm",
    "title": "Stochastic Process Model for Analysis of Longitudinal and\nTime-to-Event Outcomes",
    "description": "Utilities to estimate parameters of the models with survival functions \n             induced by stochastic covariates. Miscellaneous functions for data preparation \n             and simulation are also provided. For more information, see: \n             (i)\"Stochastic model for analysis of longitudinal data on aging and mortality\" \n             by Yashin A. et al. (2007), \n             Mathematical Biosciences, 208(2), 538-551, <DOI:10.1016/j.mbs.2006.11.006>;\n             (ii) \"Health decline, aging and mortality: how are they related?\" \n             by Yashin A. et al. (2007), \n             Biogerontology 8(3), 291(302), <DOI:10.1007/s10522-006-9073-3>.",
    "version": "1.7.12",
    "maintainer": "Ilya Y. Zhbannikov <ilya.zhbannikov@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23957,
    "package_name": "strata.MaxCombo",
    "title": "Stratified Max-Combo Test",
    "description": "Non-proportional hazard (NPH) is commonly observed in immuno-oncology studies, where the survival curves of the treatment and control groups show delayed separation. To properly account for NPH, several statistical methods have been developed. One such method is Max-Combo test, which is a straightforward and flexible hypothesis testing method that can simultaneously test for constant, early, middle, and late treatment effects. However, the majority of the Max-Combo test performed in clinical studies are unstratified, ignoring the important prognostic stratification factors. To fill this gap, we have developed an R package for stratified Max-Combo testing that accounts for stratified baseline factors. Our package explores various methods for calculating combined test statistics, estimating joint distributions, and determining the p-values.",
    "version": "0.0.1",
    "maintainer": "Yuwen Liu <yuwenliu9@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23959,
    "package_name": "stratamatch",
    "title": "Stratification and Matching for Large Observational Data Sets",
    "description": "A pilot matching design to automatically \n    stratify and match large datasets.  The manual_stratify() function allows\n    users to manually stratify a dataset based on categorical variables of \n    interest, while the auto_stratify() function does automatically by\n    allocating a held-aside (pilot) data set, fitting a prognostic score  \n    (see Hansen (2008) <doi:10.1093/biomet/asn004>) on the pilot set, and stratifying the data set based\n    on prognostic score quantiles.  The strata_match() function then does optimal\n    matching of the data set in parallel within strata.",
    "version": "0.1.9",
    "maintainer": "Rachael C. Aikens <rockyaikens@gmail.com>",
    "url": "https://github.com/raikens1/stratamatch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23967,
    "package_name": "straweib",
    "title": "Stratified Weibull Regression Model",
    "description": "The main function is icweib(), which fits a stratified Weibull\n    proportional hazards model for left censored, right censored, interval censored,\n    and non-censored survival data. We parameterize the Weibull regression model\n    so that it allows a stratum-specific baseline hazard function, but where the\n    effects of other covariates are assumed to be constant across strata. \n    Please refer to Xiangdong Gu, David Shapiro, Michael D. Hughes and\n    Raji Balasubramanian (2014) <doi:10.32614/RJ-2014-003> for more details.",
    "version": "1.1",
    "maintainer": "Xiangdong Gu <ustcgxd@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23981,
    "package_name": "stressr",
    "title": "Fetch and plot financial stress index and component data",
    "description": "Forms queries to submit to the Cleveland Federal Reserve Bank web\n    site's financial stress index data site.  Provides query functions for both\n    the composite stress index and the components data. By default the download\n    includes daily time series data starting September 25, 1991.  The functions\n    return a class of either type easing or cfsi which contain a list of items\n    related to the query and its graphical presentation.  The list includes the\n    time series data as an xts object.  The package provides four lattice time\n    series plots to render the time series data in a manner similar to the\n    bank's own presentation.",
    "version": "1.0.0",
    "maintainer": "Matt Barry <mrb@softisms.com>",
    "url": "https://github.com/mrbcuda/stressr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23996,
    "package_name": "strucchange",
    "title": "Testing, Monitoring, and Dating Structural Changes",
    "description": "Testing, monitoring and dating structural changes in (linear)\n             regression models. strucchange features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n\t     fit, plot and test fluctuation processes (e.g., CUSUM, MOSUM,\n\t     recursive/moving estimates) and F statistics, respectively.\n             It is possible to monitor incoming data online using\n             fluctuation processes.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals.\n             Emphasis is always given to methods for visualizing the data.",
    "version": "1.5-4",
    "maintainer": "Achim Zeileis <Achim.Zeileis@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 23997,
    "package_name": "strucchangeRcpp",
    "title": "Testing, Monitoring, and Dating Structural Changes: C++ Version",
    "description": "A fast implementation with additional experimental features for\n             testing, monitoring and dating structural changes in (linear)\n             regression models. 'strucchangeRcpp' features tests/methods from\n\t     the generalized fluctuation test framework as well as from\n\t     the F test (Chow test) framework. This includes methods to\n             fit, plot and test fluctuation processes (e.g. cumulative/moving\n             sum, recursive/moving estimates) and F statistics, respectively.\n             These methods are described in Zeileis et al. (2002)\n             <doi:10.18637/jss.v007.i02>.\n             Finally, the breakpoints in regression models with structural\n             changes can be estimated together with confidence intervals,\n             and their magnitude as well as the model fit can be evaluated\n             using a variety of statistical measures.",
    "version": "1.5-4-1.0.1",
    "maintainer": "Dainius Masiliunas <pastas4@gmail.com>",
    "url": "https://github.com/bfast2/strucchangeRcpp/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24010,
    "package_name": "stylest2",
    "title": "Estimating Speakers of Texts",
    "description": "Estimates the authors or speakers of texts. Methods developed in Huang, Perry, and Spirling (2020) <doi:10.1017/pan.2019.49>. The model is built on a Bayesian framework in which the distinctiveness of each speaker is defined by how different, on average, the speaker's terms are to everyone else in the corpus of texts. An optional cross-validation method is implemented to select the subset of terms that generate the most accurate speaker predictions. Once a set of terms is selected, the model can be estimated. Speaker distinctiveness and term influence can be recovered from parameters in the model using package functions. Once fitted, the model can be used to predict authorship of new texts.",
    "version": "0.1",
    "maintainer": "Christian Baehr <cbaehr@princeton.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24014,
    "package_name": "subdetect",
    "title": "Detect Subgroup with an Enhanced Treatment Effect",
    "description": "A test for the existence of a subgroup with enhanced treatment \n    effect. And, a sample size calculation procedure for the subgroup \n    detection test.",
    "version": "1.3",
    "maintainer": "Shannon T. Holloway <shannon.t.holloway@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24028,
    "package_name": "success",
    "title": "Survival Control Charts Estimation Software",
    "description": "Quality control charts for survival outcomes.\n    Allows users to construct the Continuous Time Generalized\n    Rapid Response CUSUM (CGR-CUSUM) <doi:10.1093/biostatistics/kxac041>, \n    the Biswas & Kalbfleisch (2008)  <doi:10.1002/sim.3216> CUSUM, \n    the Bernoulli CUSUM and the risk-adjusted funnel plot for survival data \n    <doi:10.1002/sim.1970>. \n    These procedures can be used to monitor survival processes for a change \n    in the failure rate.",
    "version": "1.1.1",
    "maintainer": "Daniel Gomon <dgstatsoft@gmail.com>",
    "url": "https://github.com/d-gomon/success",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24033,
    "package_name": "sufficientForecasting",
    "title": "Sufficient Forecasting using Factor Models",
    "description": "The sufficient forecasting (SF) method is implemented by this package for a single time series forecasting using many predictors and a possibly nonlinear forecasting function. Assuming that the predictors are driven by some latent factors, the SF first conducts factor analysis and then performs sufficient dimension reduction on the estimated factors to derive predictive indices for forecasting. The package implements several dimension reduction approaches, including principal components (PC), sliced inverse regression (SIR), and directional regression (DR). Methods for dimension reduction are as described in: Fan, J., Xue, L. and Yao, J. (2017) <doi:10.1016/j.jeconom.2017.08.009>, Luo, W., Xue, L., Yao, J. and Yu, X. (2022) <doi:10.1093/biomet/asab037> and Yu, X., Yao, J. and Xue, L. (2022) <doi:10.1080/07350015.2020.1813589>.",
    "version": "0.1.0",
    "maintainer": "Jing Fu <jingfu991224@outlook.com>",
    "url": "https://github.com/JingFu1224/sufficientForecasting",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24056,
    "package_name": "superdiag",
    "title": "A Comprehensive Test Suite for Testing Markov Chain\nNonconvergence",
    "description": "The 'superdiag' package provides a comprehensive test suite for testing\n  Markov Chain nonconvergence. It integrates five standard empirical MCMC convergence\n  diagnostics (Gelman-Rubin, Geweke, Heidelberger-Welch, Raftery-Lewis, and Hellinger\n  distance) and plotting functions for trace plots and density histograms. The functions\n  of the package can be used to present all diagnostic statistics and graphs at once\n  for conveniently checking MCMC nonconvergence.",
    "version": "2.0",
    "maintainer": "Le Bao <lbaole17@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24059,
    "package_name": "superpc",
    "title": "Supervised Principal Components",
    "description": "Does prediction in the case of a censored survival outcome, or a regression outcome, using the \"supervised principal component\" approach. 'Superpc' is especially useful for high-dimensional data when the number of features p dominates the number of samples n (p >> n paradigm), as generated, for instance, by high-throughput technologies.",
    "version": "1.12",
    "maintainer": "Jean-Eudes Dazard <jean-eudes.dazard@case.edu>",
    "url": "http://www-stat.stanford.edu/~tibs/superpc,\nhttps://github.com/jedazard/superpc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24072,
    "package_name": "sure",
    "title": "Surrogate Residuals for Ordinal and General Regression Models",
    "description": "An implementation of the surrogate approach to residuals and \n  diagnostics for ordinal and general regression models; for details, see Liu \n  and Zhang (2017) <doi:10.1080/01621459.2017.1292915>. These residuals can be \n  used to construct standard residual plots for model diagnostics (e.g., \n  residual-vs-fitted value plots, residual-vs-covariate plots, Q-Q plots, etc.). \n  The package also provides an 'autoplot' function for producing standard \n  diagnostic plots using 'ggplot2' graphics. The package currently supports \n  cumulative link models from packages 'MASS', 'ordinal', 'rms', and 'VGAM'. \n  Support for binary regression models using the standard 'glm' function is also \n  available.",
    "version": "0.2.0",
    "maintainer": "Brandon Greenwell <greenwell.brandon@gmail.com>",
    "url": "https://github.com/AFIT-R/sure",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24077,
    "package_name": "surrosurv",
    "title": "Evaluation of Failure Time Surrogate Endpoints in Individual\nPatient Data Meta-Analyses",
    "description": "Provides functions for the evaluation of\n    surrogate endpoints when both the surrogate and the true endpoint are failure\n    time variables. The approaches implemented are:\n    (1) the two-step approach (Burzykowski et al, 2001) <DOI:10.1111/1467-9876.00244> with a copula model (Clayton, Plackett, Hougaard) at\n    the first step and either a linear regression of log-hazard ratios at the second\n    step (either adjusted or not for measurement error);\n    (2) mixed proportional hazard models estimated via mixed Poisson GLM\n    (Rotolo et al, 2017 <DOI:10.1177/0962280217718582>).",
    "version": "1.1.27",
    "maintainer": "Dan Chaltiel <dan.chaltiel@gustaveroussy.fr>",
    "url": "https://github.com/Oncostat/surrosurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24078,
    "package_name": "surrosurvROC",
    "title": "Surrogate Survival ROC",
    "description": "Nonparametric and semiparametric estimations of the time-dependent ROC curve for an incomplete failure time data with surrogate failure time endpoints.",
    "version": "0.1.0",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24079,
    "package_name": "surtvep",
    "title": "Cox Non-Proportional Hazards Model with Time-Varying\nCoefficients",
    "description": "Fit Cox non-proportional hazards models with time-varying coefficients. \n    Both unpenalized procedures (Newton and proximal Newton) and penalized procedures \n    (P-splines and smoothing splines) are included using B-spline basis functions for \n    estimating time-varying coefficients. For penalized procedures, cross validations, \n    mAIC, TIC or GIC are implemented to select tuning parameters. Utilities for \n    carrying out post-estimation visualization, summarization, point-wise confidence \n    interval and hypothesis testing are also provided.\n    For more information, see Wu et al. (2022) <doi: 10.1007/s10985-021-09544-2> and \n    Luo et al. (2023) <doi:10.1177/09622802231181471>.",
    "version": "1.0.0",
    "maintainer": "Lingfeng Luo <lfluo@umich.edu>",
    "url": "https://github.com/UM-KevinHe/surtvep,\nhttps://um-kevinhe.github.io/surtvep/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24080,
    "package_name": "surv2sampleComp",
    "title": "Inference for Model-Free Between-Group Parameters for Censored\nSurvival Data",
    "description": "Performs inference of several model-free group contrast measures, which include difference/ratio of cumulative incidence rates at given time points, quantiles, and restricted mean survival times (RMST). Two kinds of covariate adjustment procedures (i.e., regression and augmentation) for inference of the metrics based on RMST are also included.",
    "version": "1.0-5",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24082,
    "package_name": "survAUC",
    "title": "Estimators of Prediction Accuracy for Time-to-Event Data",
    "description": "Provides a variety of functions to estimate time-dependent true/false positive rates and AUC curves from a set of censored survival data.",
    "version": "1.4-0",
    "maintainer": "Frederic Bertrand <frederic.bertrand@lecnam.net>",
    "url": "https://fbertran.github.io/survAUC/,\nhttps://github.com/fbertran/survAUC/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24083,
    "package_name": "survAWKMT2",
    "title": "Two-Sample Tests Based on Differences of Kaplan-Meier Curves",
    "description": "Tests for equality of two survival functions based on integrated weighted differences of two Kaplan-Meier curves.",
    "version": "1.0.1",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24084,
    "package_name": "survBootOutliers",
    "title": "Concordance Based Bootstrap Methods for Outlier Detection in\nSurvival Analysis",
    "description": "Three new methods to perform outlier detection in a survival context. In total there are six methods provided, the first three methods are traditional residual-based outlier detection methods, the second three are the concordance-based. Package developed during the work on the two following publications: Pinto J., Carvalho A. and Vinga S. (2015) <doi:10.5220/0005225300750082>; Pinto J.D., Carvalho A.M., Vinga S. (2015) <doi:10.1007/978-3-319-27926-8_22>.",
    "version": "1.0",
    "maintainer": "Joao Pinto <joao.pinto@tecnico.ulisboa.pt>",
    "url": "https://github.com/jonydog/survBootOutliers",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24085,
    "package_name": "survC",
    "title": "Survival Model Validation Utilities",
    "description": "Provides helper functions to compute linear predictors, time-dependent ROC curves,\n    and Harrell's concordance index for Cox proportional hazards models as described in Therneau (2024) <https://CRAN.R-project.org/package=survival>,\n    Therneau and Grambsch (2000, ISBN:0-387-98784-3), Hung and Chiang (2010) <doi:10.1002/cjs.10046>, Uno et al. (2007) <doi:10.1198/016214507000000149>,\n    Blanche, Dartigues, and Jacqmin-Gadda (2013) <doi:10.1002/sim.5958>, Blanche, Latouche, and Viallon (2013) <doi:10.1007/978-1-4614-8981-8_11>,\n    Harrell et al. (1982) <doi:10.1001/jama.1982.03320430047030>, Peto and Peto (1972) <doi:10.2307/2344317>, Schemper (1992) <doi:10.2307/2349009>,\n    and Uno et al. (2011) <doi:10.1002/sim.4154>.",
    "version": "0.1.0",
    "maintainer": "Minhyuk Kim <mhkim@zarathu.com>",
    "url": "https://newjoseph.github.io/survC/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24086,
    "package_name": "survC1",
    "title": "C-Statistics for Risk Prediction Models with Censored Survival\nData",
    "description": "Performs inference for C of risk prediction models with censored survival data, using the method proposed by Uno et al. (2011) <doi:10.1002/sim.4154>. Inference for the difference in C between two competing prediction models is also implemented.",
    "version": "1.0-3",
    "maintainer": "Hajime Uno <huno@jimmy.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24088,
    "package_name": "survCurve",
    "title": "Plots Survival Curves Element by Element",
    "description": "Plots survival models from the 'survival' package. Additionally, it\n    plots curves of multistate models from the 'mstate' package. Typically, a\n    plot is drawn by the sequence survplot(), confIntArea(), survCurve() and\n    nrAtRisk(). The separation of the plot in this 4 functions allows for great\n    flexibility to make a custom plot for publication.",
    "version": "1.0",
    "maintainer": "Melchior Burri <melchiorburri@msn.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24093,
    "package_name": "survIDINRI",
    "title": "IDI and NRI for Comparing Competing Risk Prediction Models with\nCensored Survival Data",
    "description": "Performs inference for a class of measures to compare competing risk prediction models with censored survival data. The class includes the integrated discrimination improvement index (IDI) and category-less net reclassification index (NRI).",
    "version": "1.1-2",
    "maintainer": "Hajime Uno <huno@jimmy.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24095,
    "package_name": "survMisc",
    "title": "Miscellaneous Functions for Survival Data",
    "description": "A collection of functions to help in the analysis of\n    right-censored survival data. These extend the methods available in\n    package:survival.",
    "version": "0.5.6",
    "maintainer": "Chris Dardis <christopherdardis@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24097,
    "package_name": "survParamSim",
    "title": "Parametric Survival Simulation with Parameter Uncertainty",
    "description": "Perform survival simulation with parametric survival model generated from 'survreg' function in 'survival' package.\n    In each simulation coefficients are resampled from variance-covariance matrix of parameter estimates to \n    capture uncertainty in model parameters.\n    Prediction intervals of Kaplan-Meier estimates and hazard ratio of treatment effect can be further calculated using simulated survival data.",
    "version": "0.1.7",
    "maintainer": "Kenta Yoshida <yoshida.kenta.6@gmail.com>",
    "url": "https://github.com/yoshidk6/survParamSim,\nhttps://yoshidk6.github.io/survParamSim/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24099,
    "package_name": "survPresmooth",
    "title": "Presmoothed Estimation in Survival Analysis",
    "description": "Presmoothed estimators of survival, density, cumulative and non-cumulative hazard functions with right-censored survival data. For details, see Lopez-de-Ullibarri and Jacome (2013) <doi:10.18637/jss.v054.i11>.",
    "version": "1.1-12",
    "maintainer": "Ignacio Lopez de Ullibarri <ilu@udc.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24100,
    "package_name": "survRM2",
    "title": "Comparing Restricted Mean Survival Time",
    "description": "Performs two-sample comparisons using the restricted mean survival time (RMST) as a summary measure of the survival time distribution. Three kinds of between-group contrast metrics (i.e., the difference in RMST, the ratio of RMST and the ratio of the restricted mean time lost (RMTL)) are computed. It performs an ANCOVA-type covariate adjustment as well as unadjusted analyses for those measures. ",
    "version": "1.0-4",
    "maintainer": "Hajime Uno <huno@jimmy.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24101,
    "package_name": "survRM2adapt",
    "title": "Flexible and Coherent Test/Estimation Procedure Based on\nRestricted Mean Survival Times",
    "description": "Estimates the restricted mean survival time (RMST) with the time window [0, tau], where tau is adaptively selected from the procedure, proposed by Horiguchi et al. (2018) <doi:10.1002/sim.7661>. It also estimates the RMST with the time window [tau1, tau2], where tau1 is adaptively selected from the procedure, proposed by Horiguchi et al. (2023) <doi:10.1002/sim.9662>.",
    "version": "1.1.0",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24102,
    "package_name": "survRM2perm",
    "title": "Permutation Test for Comparing Restricted Mean Survival Time",
    "description": "Performs the permutation test using difference in the restricted mean survival time (RMST) between groups as a summary measure of the survival time distribution. When the sample size is less than 50 per group, it has been shown that there is non-negligible inflation of the type I error rate in the commonly used asymptotic test for the RMST comparison. Generally, permutation tests can be useful in such a situation. However, when we apply the permutation test for the RMST comparison, particularly in small sample situations, there are some cases where the survival function in either group cannot be defined due to censoring in the permutation process. Horiguchi and Uno (2020) <doi:10.1002/sim.8565> have examined six workable solutions to handle this numerical issue. It performs permutation tests with implementation of the six methods outlined in the paper when the numerical issue arises during the permutation process. The result of the asymptotic test is also provided for a reference.",
    "version": "0.1.0",
    "maintainer": "Miki Horiguchi <horiguchimiki@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24104,
    "package_name": "survSNP",
    "title": "Power Calculations for SNP Studies with Censored Outcomes",
    "description": "Conduct asymptotic and empirical power and sample size calculations for Single-Nucleotide Polymorphism (SNP) association studies with right censored time to event outcomes.",
    "version": "0.26",
    "maintainer": "Alexander Sibley <dcibioinformatics@duke.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24105,
    "package_name": "survSens",
    "title": "Sensitivity Analysis with Time-to-Event Outcomes",
    "description": "Performs a dual-parameter sensitivity analysis of treatment effect to unmeasured confounding in observational studies with either survival or competing risks outcomes. Huang, R., Xu, R. and Dulai, P.S.(2020) <doi:10.1002/sim.8672>.",
    "version": "1.1.0",
    "maintainer": "Rong Huang <roh019@ucsd.edu>",
    "url": "https://github.com/Rong0707/survSens",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24106,
    "package_name": "survSpearman",
    "title": "Nonparametric Spearman's Correlation for Survival Data",
    "description": "Nonparametric estimation of Spearman's rank correlation with bivariate survival (right-censored) data as described in Eden, S.K., Li, C., Shepherd B.E. (2021), Nonparametric Estimation of Spearman's Rank Correlation with Bivariate Survival Data, Biometrics (under revision). The package also provides functions that visualize bivariate survival data and bivariate probability mass function.",
    "version": "1.0.1",
    "maintainer": "Svetlana Eden <svetlana.k.eden.1@vumc.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24113,
    "package_name": "survexp.fr",
    "title": "Relative Survival, AER and SMR Based on French Death Rates",
    "description": "It computes Relative survival, AER and SMR based on French death rates.",
    "version": "1.2",
    "maintainer": "Hugo Varet <varethugo@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24114,
    "package_name": "survextrap",
    "title": "Bayesian Flexible Parametric Survival Modelling and\nExtrapolation",
    "description": "Survival analysis using a flexible Bayesian model for individual-level right-censored data, optionally combined with aggregate data on counts of survivors in different periods of time. An M-spline is used to describe the hazard function, with a prior on the coefficients that controls over-fitting. Proportional hazards or flexible non-proportional hazards models can be used to relate survival to predictors. Additive hazards (relative survival) models, waning treatment effects, and mixture cure models are also supported. Priors can be customised and calibrated to substantive beliefs. Posterior distributions are estimated using 'Stan', and outputs are arranged in a tidy format. See Jackson (2023) <doi:10.1186/s12874-023-02094-1>.",
    "version": "1.0",
    "maintainer": "Christopher Jackson <chris.jackson@mrc-bsu.cam.ac.uk>",
    "url": "https://github.com/chjackson/survextrap,\nhttps://chjackson.github.io/survextrap/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24118,
    "package_name": "surveySimR",
    "title": "Estimation of Population Total under Complex Sampling Design",
    "description": "Sample surveys use scientific methods to draw inferences about population parameters by observing a representative part of the population, called sample. The SRSWOR (Simple Random Sampling Without Replacement) is one of the most widely used probability sampling designs, wherein every unit has an equal chance of being selected and units are not repeated.This function draws multiple SRSWOR samples from a finite population and estimates the population parameter i.e. total of HT, Ratio, and Regression estimators. Repeated simulations (e.g., 500 times) are used to assess and compare estimators using metrics such as percent relative bias (%RB), percent relative root means square error (%RRMSE).For details on sampling methodology,\n             see, Cochran (1977) \"Sampling Techniques\" <https://archive.org/details/samplingtechniqu0000coch_t4x6>.",
    "version": "0.1.0",
    "maintainer": "Nobin Chandra Paul <ncp375@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24128,
    "package_name": "surveyvoi",
    "title": "Survey Value of Information",
    "description": "Decision support tool for prioritizing sites for ecological\n    surveys based on their potential to improve plans for conserving\n    biodiversity (e.g. plans for establishing protected areas). Given a set of\n    sites that could potentially be acquired for conservation management,\n    it can be used to generate and evaluate plans for surveying additional\n    sites. Specifically, plans for ecological surveys can be\n    generated using various conventional approaches (e.g. maximizing expected\n    species richness, geographic coverage, diversity of sampled environmental\n    algorithms. After generating such survey plans, they can be evaluated using\n    conditions) and maximizing value of information. Please note that several\n    functions depend on the 'Gurobi' optimization software (available from\n    <https://www.gurobi.com>). Additionally, the 'JAGS' software (available from\n    <https://mcmc-jags.sourceforge.io/>) is required to fit hierarchical\n    generalized linear models. For further details, see Hanson et al. (2023) <doi:10.1111/1365-2664.14309>.",
    "version": "1.1.1",
    "maintainer": "Jeffrey O Hanson <jeffrey.hanson@uqconnect.edu.au>",
    "url": "https://prioritizr.github.io/surveyvoi/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24130,
    "package_name": "survival",
    "title": "Survival Analysis",
    "description": "Contains the core survival analysis routines, including\n\t     definition of Surv objects, \n\t     Kaplan-Meier and Aalen-Johansen (multi-state) curves, Cox models,\n\t     and parametric accelerated failure time models.",
    "version": "3.8-3",
    "maintainer": "Terry M Therneau <therneau.terry@mayo.edu>",
    "url": "https://github.com/therneau/survival",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24132,
    "package_name": "survivalMPL",
    "title": "Penalised Maximum Likelihood for Survival Analysis Models",
    "description": "Estimate the regression coefficients and the baseline hazard \n      of proportional hazard Cox models with left, right or interval censored survival data \n      using maximum penalised likelihood. A 'non-parametric' smooth estimate of the baseline hazard\n      function is provided.",
    "version": "0.2-4",
    "maintainer": "Dominique-Laurent Couturier <dominique.couturier@mrc-bsu.cam.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24133,
    "package_name": "survivalMPLdc",
    "title": "Penalised Likelihood for Survival Analysis with Dependent\nCensoring",
    "description": "Fitting Cox proportional hazard model under dependent right censoring using copula and maximum penalised likelihood methods.",
    "version": "0.1.1",
    "maintainer": "Jing Xu <kenny.xu@duke-nus.edu.sg>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24135,
    "package_name": "survivalREC",
    "title": "Nonparametric Estimation of the Distribution of Gap Times for\nRecurrent Events",
    "description": "Provides estimates for the bivariate and trivariate distribution \n             functions and bivariate and trivariate survival functions for \n             censored gap times. Two approaches, using existing methodologies, \n             are considered: (i) the Lin's estimator, which is based on the \n             extension the Kaplan-Meier estimator of the distribution function \n             for the first event time and the Inverse Probability of Censoring \n             Weights for the second time (Lin DY, Sun W, Ying Z (1999) \n             <doi:10.1093/biomet/86.1.59> and (ii) another estimator\n             based on Kaplan-Meier weights (Una-Alvarez J, Meira-Machado L (2008)\n             <https://w3.math.uminho.pt/~lmachado/Biometria_conference.pdf>). \n             The proposed methods are the landmark estimators based on \n             subsampling approach, and the estimator based on weighted cumulative\n             hazard estimator. The package also provides nonparametric estimator\n             conditional to a given continuous covariate. All these methods have\n             been submitted to be published.  ",
    "version": "1.1",
    "maintainer": "Gustavo Soutinho <gustavosoutinho@sapo.pt>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24136,
    "package_name": "survivalROC",
    "title": "Time-Dependent ROC Curve Estimation from Censored Survival Data",
    "description": "Compute time-dependent ROC curve from censored survival\n        data using Kaplan-Meier (KM) or Nearest Neighbor Estimation\n        (NNE) method of Heagerty, Lumley & Pepe (Biometrics, Vol 56 No\n        2, 2000, PP 337-344).",
    "version": "1.0.3.1",
    "maintainer": "Paramita Saha-Chaudhuri\n<paramita.sahachaudhuri.work@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24137,
    "package_name": "survivalSL",
    "title": "Super Learner for Survival Prediction from Censored Data",
    "description": "Several functions and S3 methods to construct a super learner in the presence of censored times-to-event and to evaluate its prognostic capacities.",
    "version": "1.0",
    "maintainer": "Yohann Foucher <yohann.foucher@univ-poitiers.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24138,
    "package_name": "survivalVignettes",
    "title": "Survival Analysis Vignettes and Optional Datasets",
    "description": "Vignettes for the 'survival' package. Split from the 'survival' \n    package since the vignettes were getting large. Also, since 'survival' is a \n    recommended package it cannot make use of other packages outside of \n    base+recommended (e.g. 'rmarkdown').",
    "version": "0.1.6",
    "maintainer": "Elizabeth Atkinson <atkinson@mayo.edu>",
    "url": "https://github.com/bethatkinson/survivalVignettes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24141,
    "package_name": "survivalsvm",
    "title": "Survival Support Vector Analysis",
    "description": "Performs support vectors analysis for data sets with survival\n    outcome. Three approaches are available in the package: The regression approach\n    takes censoring into account when formulating the inequality constraints of\n    the support vector problem. In the ranking approach, the inequality constraints\n    set the objective to maximize the concordance index for comparable pairs\n    of observations. The hybrid approach combines the regression and ranking\n    constraints in the same model.",
    "version": "0.0.6",
    "maintainer": "Cesaire J. K. Fouodo <cesaire.kuetefouodo@uni-luebeck.de>",
    "url": "https://github.com/imbs-hl/survivalsvm",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24143,
    "package_name": "survlab",
    "title": "Survival Model-Based Imputation for Laboratory Non-Detect Data",
    "description": "Implements survival-model-based imputation for censored\n    laboratory measurements, including Tobit-type models with several\n    distribution options. Suitable for data with values below detection\n    or quantification limits, the package identifies the best-fitting\n    distribution and produces realistic imputations that respect the\n    censoring thresholds.",
    "version": "0.1.0",
    "maintainer": "Luís Pereira <d57177@alunos.uevora.pt>",
    "url": "https://lpereira-ue.github.io/survlab/,\nhttps://github.com/lpereira-ue/survlab",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24144,
    "package_name": "survminer",
    "title": "Drawing Survival Curves using 'ggplot2'",
    "description": "Contains the function 'ggsurvplot()' for drawing easily beautiful\n    and 'ready-to-publish' survival curves with the 'number at risk' table\n    and 'censoring count plot'. Other functions are also available to plot \n    adjusted curves for `Cox` model and to visually examine 'Cox' model assumptions.",
    "version": "0.5.1",
    "maintainer": "Alboukadel Kassambara <alboukadel.kassambara@gmail.com>",
    "url": "https://rpkgs.datanovia.com/survminer/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24145,
    "package_name": "survmixer",
    "title": "Design of Clinical Trials with Survival Endpoints Based on\nBinary Responses",
    "description": "Sample size and effect size calculations for survival endpoints based on mixture survival-by-response model. The methods implemented can be found in  Bofill, Shen & Gómez (2021) <arXiv:2008.12887>. ",
    "version": "1.3",
    "maintainer": "Marta Bofill Roig <marta.bofillroig@meduniwien.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24146,
    "package_name": "survobj",
    "title": "Objects to Simulate Survival Times",
    "description": "Generate objects that simulate survival times. Random values for the distributions are generated using the method described by Bender (2003) <https://epub.ub.uni-muenchen.de/id/eprint/1716> and Leemis (1987) in Operations Research, 35(6), 892–894.",
    "version": "3.1.1",
    "maintainer": "Aponte John <john.j.aponte@gmail.com>",
    "url": "https://johnaponte.github.io/survobj/,\nhttps://github.com/johnaponte/survobj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24147,
    "package_name": "survout",
    "title": "Excel Conversion of R Surival Analysis Output",
    "description": "Simple and quick method of exporting the most often used\n    survival analysis results to an Excel sheet.",
    "version": "0.1.0",
    "maintainer": "Xuefei Jia <xuefeij.ai@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24149,
    "package_name": "survsim",
    "title": "Simulation of Simple and Complex Survival Data",
    "description": "Simulation of simple and complex survival data including recurrent and multiple events and competing risks. See Moriña D, Navarro A. (2014) <doi:10.18637/jss.v059.i02> and Moriña D, Navarro A. (2017) <doi:10.1080/03610918.2016.1175621>.",
    "version": "1.1.8",
    "maintainer": "David Moriña Soler <dmorina@ub.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24150,
    "package_name": "survstan",
    "title": "Fitting Survival Regression Models via 'Stan'",
    "description": "Parametric survival regression models under the maximum likelihood approach via 'Stan'. Implemented regression models include accelerated failure time models, proportional hazards models, proportional odds models, accelerated hazard models, Yang and Prentice models, and extended hazard models. Available baseline survival distributions include exponential, Weibull, log-normal, log-logistic, gamma, generalized gamma, rayleigh, Gompertz and fatigue (Birnbaum-Saunders) distributions. References: Lawless (2002) <ISBN:9780471372158>; Bennett (1982) <doi:10.1002/sim.4780020223>; Chen and Wang(2000) <doi:10.1080/01621459.2000.10474236>; Demarqui and Mayrink (2021) <doi:10.1214/20-BJPS471>.",
    "version": "0.0.7.1",
    "maintainer": "Fabio Demarqui <fndemarqui@est.ufmg.br>",
    "url": "https://github.com/fndemarqui/survstan,\nhttps://fndemarqui.github.io/survstan/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24169,
    "package_name": "svars",
    "title": "Data-Driven Identification of SVAR Models",
    "description": "Implements data-driven identification methods for structural vector autoregressive (SVAR) models as described in Lange et al. (2021) <doi:10.18637/jss.v097.i05>. \n             Based on an existing VAR model object (provided by e.g. VAR() from the 'vars' package), the structural \n             impact matrix is obtained via data-driven identification techniques (i.e. changes in volatility (Rigobon, R. (2003) <doi:10.1162/003465303772815727>),  patterns of GARCH (Normadin, M., Phaneuf, L. (2004) <doi:10.1016/j.jmoneco.2003.11.002>),\n             independent component analysis (Matteson, D. S, Tsay, R. S., (2013) <doi:10.1080/01621459.2016.1150851>), least dependent innovations (Herwartz, H., Ploedt, M., (2016) <doi:10.1016/j.jimonfin.2015.11.001>), \n             smooth transition in variances (Luetkepohl, H., Netsunajev, A. (2017) <doi:10.1016/j.jedc.2017.09.001>) or non-Gaussian maximum likelihood (Lanne, M., Meitz, M., Saikkonen, P. (2017) <doi:10.1016/j.jeconom.2016.06.002>)).",
    "version": "1.3.12",
    "maintainer": "Alexander Lange <alexander.lange@uni-goettingen.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24177,
    "package_name": "svines",
    "title": "Stationary Vine Copula Models",
    "description": "Provides functionality to fit and simulate from stationary vine \n  copula models for time series, see Nagler et al. (2022) \n  <doi:10.1016/j.jeconom.2021.11.015>.",
    "version": "0.2.7",
    "maintainer": "Thomas Nagler <mail@tnagler.com>",
    "url": "https://github.com/tnagler/svines",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24181,
    "package_name": "svrpath",
    "title": "The SVR Path Algorithm",
    "description": "Computes the entire solution paths for Support Vector Regression(SVR) with respect to the regularization parameter, lambda and epsilon in epsilon-intensive loss function, efficiently. We call each path algorithm svrpath and epspath. See Wang, G. et al (2008) <doi:10.1109/TNN.2008.2002077> for details regarding the method.",
    "version": "0.1.2",
    "maintainer": "Do Hyun Kim <09dohkim@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24187,
    "package_name": "svycoxme",
    "title": "Mixed-Effects Cox Models for Complex Samples",
    "description": "Mixed-effect proportional hazards models for multistage stratified, \n   cluster-sampled, unequally weighted survey samples. Provides variance \n   estimation by Taylor series linearisation or replicate weights. ",
    "version": "1.0.0",
    "maintainer": "Bradley Drayton <brad.drayton@auckland.ac.nz>",
    "url": "https://github.com/bdrayton/svycoxme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24188,
    "package_name": "svydiags",
    "title": "Regression Model Diagnostics for Survey Data",
    "description": "Diagnostics for fixed effects linear and general linear regression models fitted with survey data. Extensions of standard diagnostics to complex survey data are included: standardized residuals, leverages, Cook's D, dfbetas, dffits, condition indexes, and variance inflation factors as found in Li and Valliant (Surv. Meth., 2009, 35(1), pp. 15-24; Jnl. of Off. Stat., 2011, 27(1), pp. 99-119; Jnl. of Off. Stat., 2015, 31(1), pp. 61-75); Liao and Valliant (Surv. Meth., 2012, 38(1), pp. 53-62; Surv. Meth., 2012, 38(2), pp. 189-202).  Variance inflation factors and condition indexes are also computed for some general linear models as described in Liao (U. Maryland thesis, 2010).",
    "version": "0.7",
    "maintainer": "Richard Valliant <valliant@umich.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24189,
    "package_name": "svylme",
    "title": "Linear Mixed Models for Complex Survey Data",
    "description": "Linear mixed models for complex survey data, by pairwise composite likelihood, as described in Lumley & Huang (2023) <arXiv:2311.13048>. Supports nested and crossed random effects, and correlated random effects as in genetic models.  Allows for multistage sampling and for other designs where pairwise sampling probabilities are specified or can be calculated. ",
    "version": "1.5-1",
    "maintainer": "Thomas Lumley <t.lumley@auckland.ac.nz>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24190,
    "package_name": "svytest",
    "title": "Survey Weight Diagnostic Tests",
    "description": "Provides diagnostic tests for assessing the informativeness of survey weights in regression models. Implements difference-in-coefficients tests (Hausman 1978 <doi:10.2307/1913827>; Pfeffermann 1993 <doi:10.2307/1403631>), weight-association tests (DuMouchel and Duncan 1983 <doi:10.2307/2288185>; Pfeffermann and Sverchkov 1999 <https://www.jstor.org/stable/25051118>; Pfeffermann and Sverchkov 2003 <ISBN:9780470845672>; Wu and Fuller 2005 <https://www.jstor.org/stable/27590461>), estimating equations tests (Pfeffermann and Sverchkov 2003 <ISBN:9780470845672>), and non-parametric permutation tests. Includes simulation utilities replicating Wang et al. (2023 <doi:10.1111/insr.12509>) and extensions.",
    "version": "1.1.0",
    "maintainer": "Corbin Lubianski <cnlubianski@yahoo.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24200,
    "package_name": "swaprinc",
    "title": "Swap Principal Components into Regression Models",
    "description": "Obtaining accurate and stable estimates of regression coefficients\n    can be challenging when the suggested statistical model has issues related to\n    multicollinearity, convergence, or overfitting. One solution is to use\n    principal component analysis (PCA) results in the regression, as discussed in\n    Chan and Park (2005) <doi:10.1080/01446190500039812>. The swaprinc() package\n    streamlines comparisons between a raw regression model with the full set of\n    raw independent variables and a principal component regression model where\n    principal components are estimated on a subset of the independent variables,\n    then swapped into the regression model in place of those variables. The\n    swaprinc() function compares one raw regression model to one principal\n    component regression model, while the compswap() function compares one raw\n    regression model to many principal component regression models. Package\n    functions include parameters to center, scale, and undo centering and scaling,\n    as described by Harvey and Hansen (2022)\n    <https://cran.r-project.org/package=LearnPCA/vignettes/Vig_03_Step_By_Step_PCA.pdf>.\n    Additionally, the package supports using Gifi methods to extract principal\n    components from categorical variables, as outlined by Rossiter (2021)\n    <https://www.css.cornell.edu/faculty/dgr2/_static/files/R_html/NonlinearPCA.html#2_Package>.",
    "version": "1.0.1",
    "maintainer": "Mackson Ncube <macksonncube.stats@gmail.com>",
    "url": "https://github.com/mncube/swaprinc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24219,
    "package_name": "switchSelection",
    "title": "Endogenous Switching and Sample Selection Regression Models",
    "description": "Estimate the parameters of multivariate endogenous switching and sample selection models using methods described in Newey (2009) <doi:10.1111/j.1368-423X.2008.00263.x>, E. Kossova, B. Potanin (2018) <https://ideas.repec.org/a/ris/apltrx/0346.html>, E. Kossova, L. Kupriianova, B. Potanin (2020) <https://ideas.repec.org/a/ris/apltrx/0391.html> and E. Kossova, B. Potanin (2022) <https://ideas.repec.org/a/ris/apltrx/0455.html>. ",
    "version": "2.0.0",
    "maintainer": "Bogdan Potanin <bogdanpotanin@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24229,
    "package_name": "sym.arma",
    "title": "Autoregressive and Moving Average Symmetric Models",
    "description": "Functions for fitting the Autoregressive and Moving Average Symmetric Model for univariate time series introduced by Maior and Cysneiros (2018), <doi:10.1007/s00362-016-0753-z>. Fitting method: conditional maximum likelihood estimation. For details see: Wei (2006), Time Series Analysis: Univariate and Multivariate Methods, Section 7.2.",
    "version": "1.0",
    "maintainer": "Vinicius Quintas Souto Maior <vinicius@de.ufpe.br>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24241,
    "package_name": "synMicrodata",
    "title": "Synthetic Microdata Generator",
    "description": "This tool fits a non-parametric Bayesian model called a \"hierarchically coupled mixture model with local dependence (HCMM-LD)\" to the original microdata in order to generate synthetic microdata for privacy protection. The non-parametric feature of the adopted model is useful for capturing the joint distribution of the original input data in a highly flexible manner, leading to the generation of synthetic data whose distributional features are similar to that of the input data. The package allows the original input data to have missing values and impute them with the posterior predictive distribution, so no missing values exist in the synthetic data output. The method builds on the work of Murray and Reiter (2016) <doi:10.1080/01621459.2016.1174132>. ",
    "version": "2.1.3",
    "maintainer": "Juhee Lee <ljh988488@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24254,
    "package_name": "synthesis",
    "title": "Generate Synthetic Data from Statistical Models",
    "description": "Generate synthetic time series from commonly used statistical models, including linear, nonlinear and chaotic systems. Applications to testing methods can be found in Jiang, Z., Sharma, A., & Johnson, F. (2019) <doi:10.1016/j.advwatres.2019.103430> and Jiang, Z., Sharma, A., & Johnson, F. (2020) <doi:10.1029/2019WR026962> associated with an open-source tool by Jiang, Z., Rashid, M. M., Johnson, F., & Sharma, A. (2020) <doi:10.1016/j.envsoft.2020.104907>.",
    "version": "1.2.5",
    "maintainer": "Ze Jiang <ze.jiang@unsw.edu.au>",
    "url": "https://github.com/zejiang-unsw/synthesis#readme",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24256,
    "package_name": "synthesizer",
    "title": "Fast, Robust, and High-Quality Synthetic Data Generation with a\nTuneable Privacy-Utility Trade-Off",
    "description": "Synthesize numeric, categorical, mixed and time series data. Data \n    circumstances including mixed (or zero-inflated) distributions and missing\n    data patterns are reproduced in the synthetic data. A single parameter allows\n    balancing between high-quality synthetic data that represents correlations of\n    the original data and lower quality but more privacy safe synthetic data\n    without correlations. Tuning can be done per variable or for the whole\n    dataset.",
    "version": "0.6.0",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "url": "https://github.com/markvanderloo/synthesizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24285,
    "package_name": "tab",
    "title": "Create Summary Tables for Statistical Reports",
    "description": "Contains functions for creating various types of summary tables, e.g. comparing characteristics across levels of a categorical variable and summarizing fitted generalized linear models, generalized estimating equations, and Cox proportional hazards models. Functions are available to handle data from simple random samples as well as complex surveys.",
    "version": "5.1.1",
    "maintainer": "Dane R. Van Domelen <vandomed@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24299,
    "package_name": "tableone",
    "title": "Create 'Table 1' to Describe Baseline Characteristics with or without Propensity Score Weights",
    "description": "Creates 'Table 1', i.e., description of baseline patient",
    "version": "0.13.2",
    "maintainer": "Kazuki Yoshida <kazukiyoshida@mail.harvard.edu>",
    "url": "https://github.com/kaz-yos/tableone",
    "exports": [],
    "topics": ["baseline-characteristics", "cran", "descriptive-statistics", "r", "statistics"],
    "score": "NA",
    "stars": 230
  },
  {
    "id": 24381,
    "package_name": "tboot",
    "title": "Tilted Bootstrap",
    "description": "Creates simulated clinical trial data with realistic correlation structures and assumed efficacy levels by using a tilted bootstrap resampling approach. Samples are drawn from observed data with some samples appearing more frequently than others. May also be used for simulating from a joint Bayesian distribution along with clinical trials based on the Bayesian distribution.",
    "version": "0.2.1",
    "maintainer": "Nathan Morris <morris_nathan@lilly.com>",
    "url": "https://github.com/njm18/tboot",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24382,
    "package_name": "tbrf",
    "title": "Time-Based Rolling Functions",
    "description": "Provides rolling statistical functions based\n    on date and time windows instead of n-lagged observations.",
    "version": "0.1.7",
    "maintainer": "Michael Schramm <mpschramm@gmail.com>",
    "url": "https://mps9506.github.io/tbrf/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24390,
    "package_name": "tcpl",
    "title": "ToxCast Data Analysis Pipeline",
    "description": "The ToxCast Data Analysis Pipeline ('tcpl') is an R package that manages, curve-fits, plots, and stores ToxCast data to populate its linked MySQL database, 'invitrodb'. The package was developed for the chemical screening data curated by the US EPA's Toxicity Forecaster (ToxCast) program, but 'tcpl' can be used to support diverse chemical screening efforts.",
    "version": "3.3.1",
    "maintainer": "Madison Feshuk <feshuk.madison@epa.gov>",
    "url": "https://github.com/USEPA/CompTox-ToxCast-tcpl,\nhttps://www.epa.gov/comptox-tools/toxicity-forecasting-toxcast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24395,
    "package_name": "tdPRC",
    "title": "Time-Dependent Precision-Recall Curve Estimation for\nRight-Censored Data",
    "description": "This contains functions that can be used to estimate the time-dependent \n              precision-recall curve (PRC) and the corresponding area under the PRC \n              for right-censored survival data. It also compute time-dependent \n              ROC curve and its corresponding area under the ROC curve (AUC). See\n              Beyene, Chen and Kifle (2024) <doi:10.1002/bimj.202300135>.",
    "version": "1.0.0",
    "maintainer": "Kassu Mehari Beyene <m2kassu@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24396,
    "package_name": "tdROC",
    "title": "Nonparametric Estimation of Time-Dependent ROC, Brier Score, and\nSurvival Difference from Right Censored Time-to-Event Data with\nor without Competing Risks",
    "description": "The tdROC package facilitates the estimation of time-dependent ROC \n    (Receiver Operating Characteristic) curves and the Area Under the time-dependent \n    ROC Curve (AUC) in the context of survival data, accommodating scenarios with \n    right censored data and the option to account for competing risks. In addition \n    to the ROC/AUC estimation, the package also estimates time-dependent Brier score and \n    survival difference. Confidence intervals of various estimated quantities can be \n    obtained from bootstrap. The package also offers plotting functions for visualizing \n    time-dependent ROC curves.",
    "version": "2.0",
    "maintainer": "Xiaoyang Li <xli35@mdanderson.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24429,
    "package_name": "telescope",
    "title": "Bayesian Mixtures with an Unknown Number of Components",
    "description": "Fits Bayesian finite mixtures with an unknown number of components using the telescoping sampler and different component distributions. For more details see Frühwirth-Schnatter et al. (2021) <doi:10.1214/21-BA1294>.",
    "version": "0.2-1",
    "maintainer": "Gertraud Malsiner-Walli <Gertraud.Malsiner-Walli@wu.ac.at>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24444,
    "package_name": "tensorBF",
    "title": "Bayesian Tensor Factorization",
    "description": "Bayesian Tensor Factorization for decomposition of tensor data sets using the trilinear CANDECOMP/PARAFAC (CP) factorization, with automatic component selection. The complete data analysis pipeline is provided, including functions and recommendations for data normalization and model definition, as well as missing value prediction and model visualization. The method performs factorization for three-way tensor datasets and the inference is implemented with Gibbs sampling.",
    "version": "1.0.2",
    "maintainer": "Suleiman A Khan <khan.suleiman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24448,
    "package_name": "tensorTS",
    "title": "Factor and Autoregressive Models for Tensor Time Series",
    "description": "Factor and autoregressive models for matrix and tensor valued time series. We provide functions for estimation, simulation and prediction. The models are discussed in \n    Li et al (2021) <doi:10.48550/arXiv.2110.00928>, Chen et al (2020) <DOI:10.1080/01621459.2021.1912757>, \n    Chen et al (2020) <DOI:10.1016/j.jeconom.2020.07.015>, and Xiao et al (2020) <doi:10.48550/arXiv.2006.02611>.",
    "version": "1.0.2",
    "maintainer": "Zebang Li <zl326@stat.rutgers.edu>",
    "url": "https://github.com/zebang/tensorTS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24457,
    "package_name": "term",
    "title": "Create, Manipulate and Query Parameter Terms",
    "description": "Creates, manipulates, queries and repairs vectors of\n    parameter terms.  Parameter terms are the labels used to reference\n    values in vectors, matrices and arrays. They represent the names in\n    coefficient tables and the column names in 'mcmc' and 'mcmc.list'\n    objects.",
    "version": "0.3.6",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://poissonconsulting.github.io/term/,\nhttps://github.com/poissonconsulting/term",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24477,
    "package_name": "testcorr",
    "title": "Testing Zero Correlation",
    "description": "Computes the test statistics for examining the significance of autocorrelation in univariate time series, cross-correlation in bivariate time series, Pearson correlations in multivariate series and test statistics for i.i.d. property of univariate series given in Dalla, Giraitis and Phillips (2022), <https://www.cambridge.org/core/journals/econometric-theory/article/abs/robust-tests-for-white-noise-and-crosscorrelation/4D77C12C52433F4C6735E584C779403A>, <https://elischolar.library.yale.edu/cowles-discussion-paper-series/57/>.",
    "version": "0.3.0",
    "maintainer": "Violetta Dalla <vidalla@econ.uoa.gr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24517,
    "package_name": "textir",
    "title": "Inverse Regression for Text Analysis",
    "description": "Multinomial (inverse) regression inference for text documents and associated attributes.  For details see: Taddy (2013 JASA) Multinomial Inverse Regression for Text Analysis <arXiv:1012.2098> and Taddy (2015, AoAS), Distributed Multinomial Regression, <arXiv:1311.6139>. A minimalist partial least squares routine is also included.  Note that the topic modeling capability of earlier 'textir' is now a separate package, 'maptpx'.",
    "version": "2.0-5",
    "maintainer": "Matt Taddy <mataddy@gmail.com>",
    "url": "http://taddylab.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24525,
    "package_name": "textreg",
    "title": "n-Gram Text Regression, aka Concise Comparative Summarization",
    "description": "Function for sparse regression on raw text, regressing a labeling\n    vector onto a feature space consisting of all possible phrases.",
    "version": "0.1.5",
    "maintainer": "Luke Miratrix <lmiratrix@stat.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24535,
    "package_name": "tfarima",
    "title": "Transfer Function and ARIMA Models",
    "description": "Build customized transfer function and ARIMA models with multiple operators \n    and parameter restrictions. Provides tools for model identification, estimation \n    using exact or conditional maximum likelihood, diagnostic checking, automatic outlier \n    detection, calendar effects, forecasting, and seasonal adjustment. The new version \n    also supports unobserved component ARIMA model specification and estimation \n    for structural time series analysis.",
    "version": "0.4.1",
    "maintainer": "Jose L. Gallego <jose.gallego@unican.es>",
    "url": "https://github.com/gallegoj/tfarima",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24558,
    "package_name": "thames",
    "title": "Truncated Harmonic Mean Estimator of the Marginal Likelihood",
    "description": "Implements the truncated harmonic mean estimator (THAMES) \n    of the reciprocal marginal likelihood using posterior samples and\n    unnormalized log posterior values via reciprocal importance sampling.\n    Metodiev, Perrot-Dockès, Ouadah, Irons, Latouche, & Raftery (2024). \n    Bayesian Analysis. <doi:10.1214/24-BA1422>.",
    "version": "0.1.2",
    "maintainer": "Nicholas J. Irons <nicholasjonirons@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24559,
    "package_name": "thamesmix",
    "title": "Truncated Harmonic Mean Estimator of the Marginal Likelihood for\nMixtures",
    "description": "Implements the truncated harmonic mean estimator (THAMES) \n    of the reciprocal marginal likelihood for uni- and multivariate mixture \n    models using posterior samples and unnormalized log posterior values via \n    reciprocal importance sampling.\n    Metodiev, Irons, Perrot-Dockès, Latouche & Raftery (2025)\n    <doi:10.48550/arXiv.2504.21812>.",
    "version": "0.1.3",
    "maintainer": "Martin Metodiev <m.metodiev@tutanota.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24570,
    "package_name": "thief",
    "title": "Temporal Hierarchical Forecasting",
    "description": "Methods and tools for generating forecasts at different temporal\n    frequencies using a hierarchical time series approach.",
    "version": "0.3",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "url": "http://pkg.robjhyndman.com/thief,\nhttps://github.com/robjhyndman/thief",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24583,
    "package_name": "threg",
    "title": "Threshold Regression",
    "description": "Fit a threshold regression model based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data.",
    "version": "1.0.3",
    "maintainer": "Tao Xiao <taoxiao1@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24584,
    "package_name": "thregI",
    "title": "Threshold Regression for Interval-Censored Data with a Cure Rate\nOption",
    "description": "Fit a threshold regression model for Interval Censored Data based on the first-hitting-time of a boundary by the sample path of a Wiener diffusion process. The threshold regression methodology is well suited to applications involving survival and time-to-event data.",
    "version": "1.0.4",
    "maintainer": "Man-Hua Chen <mchen@mail.tku.edu.tw>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24623,
    "package_name": "tidychangepoint",
    "title": "A Tidy Framework for Changepoint Detection Analysis",
    "description": "Changepoint detection algorithms for R are widespread but have\n    different interfaces and reporting conventions. \n    This makes the comparative analysis of results difficult. \n    We solve this problem by providing a tidy, unified interface for several \n    different changepoint detection algorithms. \n    We also provide consistent numerical and graphical reporting leveraging \n    the 'broom' and 'ggplot2' packages. ",
    "version": "1.0.2",
    "maintainer": "Benjamin S. Baumer <ben.baumer@gmail.com>",
    "url": "https://beanumber.github.io/tidychangepoint/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24700,
    "package_name": "tidysynth",
    "title": "A Tidy Implementation of the Synthetic Control Method",
    "description": "A synthetic control offers a way of evaluating the effect of an intervention in comparative case studies. The package makes a number of improvements when implementing the method in R. These improvements allow users to inspect, visualize, and tune the synthetic control more easily. A key benefit of a tidy implementation is that the entire preparation process for building the synthetic control can be accomplished in a single pipe.",
    "version": "0.2.1",
    "maintainer": "Eric Dunford <ed769@georgetown.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24705,
    "package_name": "tidytitanic",
    "title": "Dataframes Based on Titanic Passengers and Crew",
    "description": "A version of the Titanic survival data tailored for people analytics demonstrations and practice. While another package, 'titanic', reproduces the Kaggle competition files with minimal preprocessing, 'tidytitanic' combines the train and test datasets into the single dataset, 'passengers', for exploration and summary across all passengers. It also extracts personal identifiers—such as first names, last names, and titles from the raw 'name' field, enabling demographic analysis. The 'passengers' data does not cover the crew, but this package also provides the more bare-bones, crew-containing datasets 'tidy_titanic' and 'flat_titanic' based on the 'Titanic' data set from 'datasets' for further exploration. This human-centered data package is designed to support exploratory data analysis, feature engineering, and pedagogical use cases.",
    "version": "0.0.1",
    "maintainer": "Evangeline Reynolds <evangeline.mae@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24715,
    "package_name": "tidyversity",
    "title": "Tidy Tools for Academics",
    "description": "Tidy tools designed for use in academic research. Use functions",
    "version": "0.0.1",
    "maintainer": "",
    "url": "https://github.com/mkearney/tidyversity",
    "exports": [],
    "topics": ["academic", "analysis", "general-linear-model", "latent-variables", "linear-models", "logistic-regression", "mkearney-r-package", "negative-binomial-regression", "poisson-regression", "r", "regression", "research", "robust-regression", "rstats", "science", "statistics", "structural-equation-modeling", "tidy", "tidyverse", "tidyversity"],
    "score": "NA",
    "stars": 171
  },
  {
    "id": 24716,
    "package_name": "tidyvpc",
    "title": "VPC Percentiles and Prediction Intervals",
    "description": "Perform a Visual Predictive Check (VPC), while accounting for \n    stratification, censoring, and prediction correction. Using piping from \n    'magrittr', the intuitive syntax gives users a flexible and powerful method \n    to generate VPCs using both traditional binning and a new binless approach \n    Jamsen et al. (2018) <doi:10.1002/psp4.12319> with Additive Quantile \n    Regression (AQR) and Locally Estimated Scatterplot Smoothing (LOESS) \n    prediction correction. ",
    "version": "1.5.2",
    "maintainer": "James Craig <james.craig@certara.com>",
    "url": "https://github.com/certara/tidyvpc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24722,
    "package_name": "tigerhitteR",
    "title": "Pre-Process of Time Series Data Set in R",
    "description": "Pre-process for discrete time series data set which is not continuous at the column\n    of 'date'. Refilling records of missing 'date' and other columns to the hollow data set so that\n    final data set is able to be dealt with time series analysis.",
    "version": "1.1.0",
    "maintainer": "Will Kuan <aiien61will@gmail.com>",
    "url": "https://github.com/Willdata/tigerhitteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24738,
    "package_name": "tilting",
    "title": "Variable Selection via Tilted Correlation Screening Algorithm",
    "description": "Implements an algorithm for variable selection in high-dimensional linear regression using the \"tilted correlation\", a new way of measuring the contribution of each variable to the response which takes into account high correlations among the variables in a data-driven way.",
    "version": "1.1.1",
    "maintainer": "Haeran Cho <haeran.cho@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24742,
    "package_name": "timeDate",
    "title": "Rmetrics - Chronological and Calendar Objects",
    "description": "The 'timeDate' class fulfils the conventions of the ISO 8601 \n\tstandard as well as of the ANSI C and POSIX standards. Beyond\n\tthese standards it provides the \"Financial Center\" concept\n\twhich allows to handle data records collected in different time \n\tzones and mix them up to have always the proper time stamps with \n\trespect to your personal financial center, or alternatively to the GMT\n\treference time. It can thus also handle time stamps from historical \n\tdata records from the same time zone, even if the financial \n\tcenters changed day light saving times at different calendar\n\tdates.",
    "version": "4051.111",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/timeDateDoc/ (doc),\nhttps://r-forge.r-project.org/scm/viewvc.php/pkg/timeDate/?root=rmetrics\n(devel), https://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24743,
    "package_name": "timeEL",
    "title": "Time to Event Analysis via Empirical Likelihood Inference",
    "description": "Computation of t-year survival probabilities and t-year\n    risks with right censored survival data. The Kaplan-Meier estimator\n    is used to provide estimates for data without competing risks and\n    the Aalen-Johansen estimator is used when there are competing risks.\n    Confidence intervals and p-values are obtained using either usual\n    Wald-type inference or empirical likelihood inference, as described\n    in Thomas and Grunkemeier (1975) <doi:10.1080/01621459.1975.10480315>\n    and Blanche (2020) <doi:10.1007/s10985-018-09458-6>. Functions for\n    both one-sample and two-sample inference are provided. Unlike Wald-type\n    inference, empirical likelihood inference always leads to consistent\n    conclusions, in terms of statistical significance, when comparing\n    two risks (or survival probabilities) via either a ratio or a difference.",
    "version": "0.9.1",
    "maintainer": "Paul Blanche <paulfblanche@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24747,
    "package_name": "timeROC",
    "title": "Time-Dependent ROC Curve and AUC for Censored Survival Data",
    "description": "Estimation of time-dependent ROC curve and area under time dependent ROC curve (AUC) in the presence of censored data, with or without competing risks. Confidence intervals of AUCs and tests for comparing AUCs of two rival markers measured on the same subjects can be computed, using the iid-representation of the AUC estimator. Plot functions for time-dependent ROC curves and AUC curves are provided. Time-dependent Positive Predictive Values (PPV) and Negative Predictive Values (NPV) can also be computed. See Blanche et al. (2013) <doi:10.1002/sim.5958> and references therein for the details of the methods implemented in the package.",
    "version": "0.4",
    "maintainer": "Paul Blanche <paulfblanche@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24748,
    "package_name": "timeSeries",
    "title": "Financial Time Series Objects (Rmetrics)",
    "description": "'S4' classes and various tools for financial time series:\n  Basic functions such as scaling and sorting, subsetting,\n  mathematical operations and statistical functions.",
    "version": "4052.112",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/timeSeriesDoc/ (doc),\nhttps://CRAN.R-project.org/package=timeSeries,\nhttps://www.rmetrics.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24749,
    "package_name": "timeSeriesDataSets",
    "title": "Time Series Data Sets",
    "description": "Provides a diverse collection of time series datasets\n    spanning various fields such as economics, finance, energy, healthcare, and more.\n    Designed to support time series analysis in R by offering datasets from\n    multiple disciplines, making it a valuable resource for researchers and analysts.",
    "version": "0.1.0",
    "maintainer": "Renzo Caceres Rossi <arenzocaceresrossi@gmail.com>",
    "url": "https://github.com/lightbluetitan/timeseriesdatasets_R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24750,
    "package_name": "timechange",
    "title": "Efficient Manipulation of Date-Times",
    "description": "Efficient routines for manipulation of date-time objects while\n   accounting for time-zones and daylight saving times. The package includes\n   utilities for updating of date-time components (year, month, day etc.),\n   modification of time-zones, rounding of date-times, period addition and\n   subtraction etc. Parts of the 'CCTZ' source code, released under the Apache\n   2.0 License, are included in this package. See\n   <https://github.com/google/cctz> for more details.",
    "version": "0.3.0",
    "maintainer": "Vitalie Spinu <spinuvit@gmail.com>",
    "url": "https://github.com/vspinu/timechange/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24753,
    "package_name": "timedelay",
    "title": "Time Delay Estimation for Stochastic Time Series of\nGravitationally Lensed Quasars",
    "description": "We provide a toolbox to estimate the time delay between the brightness time series of gravitationally lensed quasar images via Bayesian and profile likelihood approaches. The model is based on a state-space representation for  irregularly observed time series data generated from a latent continuous-time Ornstein-Uhlenbeck process. Our Bayesian method adopts scientifically motivated hyper-prior distributions and a Metropolis-Hastings within Gibbs sampler, producing posterior samples of the model parameters that include the time delay. A profile likelihood of the time delay is a simple approximation to the marginal posterior distribution of the time delay. Both Bayesian and profile likelihood approaches complement each other, producing almost identical results; the Bayesian way is more principled but the profile likelihood is easier to implement. A new functionality is added in version 1.0.9 for estimating the time delay between doubly-lensed light curves observed in two bands. See also Tak et al. (2017) <doi:10.1214/17-AOAS1027>, Tak et al. (2018) <doi:10.1080/10618600.2017.1415911>, Hu and Tak (2020) <arXiv:2005.08049>.",
    "version": "1.0.11",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24760,
    "package_name": "timereg",
    "title": "Flexible Regression Models for Survival Data",
    "description": "Programs for Martinussen and Scheike (2006), `Dynamic Regression\n    Models for Survival Data', Springer Verlag.  Plus more recent developments.\n    Additive survival model, semiparametric proportional odds model, fast\n    cumulative residuals, excess risk models and more. Flexible competing risks\n    regression including GOF-tests. Two-stage frailty modelling. PLS for the\n    additive risk model. Lasso in the 'ahaz' package.",
    "version": "2.0.7",
    "maintainer": "Thomas Scheike <ts@biostat.ku.dk>",
    "url": "https://github.com/scheike/timereg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24762,
    "package_name": "timeseriesdb",
    "title": "A Time Series Database for Official Statistics with R and\nPostgreSQL",
    "description": "Archive and manage times series data from official statistics. The 'timeseriesdb' package was designed to manage a large catalog of time series from official statistics which are typically published on a monthly, quarterly or yearly basis. Thus timeseriesdb is optimized to handle updates caused by data revision as well as elaborate, multi-lingual meta information. ",
    "version": "1.0.0-1.1.2",
    "maintainer": "Matthias Bannert <bannert@kof.ethz.ch>",
    "url": "https://github.com/mbannert/timeseriesdb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24764,
    "package_name": "timetools",
    "title": "Seasonal/Sequential (Instants/Durations, Even or not) Time\nSeries",
    "description": "Objects to manipulate sequential and seasonal time series. Sequential time series based on time instants and time duration are handled. Both can be regularly or unevenly spaced (overlapping duration are allowed). Only POSIX* format are used for dates and times. The following classes are provided : 'POSIXcti', 'POSIXctp', 'TimeIntervalDataFrame', 'TimeInstantDataFrame', 'SubtimeDataFrame' ; methods to switch from a class to another and to modify the time support of series (hourly time series to daily time series for instance) are also defined. Tools provided can be used for instance to handle environmental monitoring data (not always produced on a regular time base).",
    "version": "1.15.5",
    "maintainer": "Vladislav Navel <vnavel@yahoo.fr>",
    "url": "https://sourceforge.net/projects/timetools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24767,
    "package_name": "timsac",
    "title": "Time Series Analysis and Control Package",
    "description": "Functions for statistical analysis, prediction and control of time\n series based mainly on Akaike and Nakagawa (1988) <ISBN 978-90-277-2786-2>.",
    "version": "1.3.8-4",
    "maintainer": "Masami Saga <msaga@mtb.biglobe.ne.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24792,
    "package_name": "tipmap",
    "title": "Tipping Point Analysis for Bayesian Dynamic Borrowing",
    "description": "Tipping point analysis for clinical trials that employ Bayesian dynamic borrowing via robust meta-analytic predictive (MAP) priors. Further functions facilitate expert elicitation of a primary weight of the informative component of the robust MAP prior and computation of operating characteristics. Intended use is the planning, analysis and interpretation of extrapolation studies in pediatric drug development, but applicability is generally wider.",
    "version": "0.5.2",
    "maintainer": "Christian Stock <christian.stock@boehringer-ingelheim.com>",
    "url": "https://github.com/Boehringer-Ingelheim/tipmap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24796,
    "package_name": "tipse",
    "title": "Tipping Point Analysis for Survival Endpoints",
    "description": "Implements tipping point sensitivity analysis for time-to-event endpoints under different missing data scenarios, as described in Oodally et al. (2025) <doi:10.48550/arXiv.2506.19988>. Supports both model-based and model-free imputation, multiple imputation workflows, plausibility assessment and visualizations. Enables robust assessment for regulatory and exploratory analyses.",
    "version": "1.1",
    "maintainer": "Ajmal Oodally <ajmal.oodally@novartis.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24797,
    "package_name": "tis",
    "title": "Time Indexes and Time Indexed Series",
    "description": "Functions and S3 classes for time indexes and time indexed\n        series, which are compatible with FAME frequencies.",
    "version": "1.39",
    "maintainer": "Brian Salzer <brian.m.salzer@frb.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24799,
    "package_name": "titeIR",
    "title": "Isotonic Designs for Phase 1 Trials with Late-Onset Toxicities",
    "description": "Functions to design phase 1 trials using an isotonic regression based design incorporating time-to-event information. Simulation and design functions are available, which incorporate information about followup and DLTs, and apply isotonic regression to devise estimates of DLT probability.",
    "version": "0.1.0",
    "maintainer": "Lee McDaniel <lmcda4@lsuhsc.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24807,
    "package_name": "tldr",
    "title": "T Loux Doing R: Functions to Simplify Data Analysis and\nReporting",
    "description": "Gives a number of functions to aid common data \n    analysis processes and reporting statistical results in an 'RMarkdown' file. \n    Data analysis functions combine multiple base R functions used to describe \n    simple bivariate relationships into a single, easy to use function. \n    Reporting functions will return character strings to report p-values, \n    confidence intervals, and hypothesis test and regression results. Strings \n    will be LaTeX-formatted as necessary and will knit pretty in an 'RMarkdown' \n    document. The package also provides wrappers function in the 'tableone' \n    package to make the results knit-able.",
    "version": "0.4.0",
    "maintainer": "Travis Loux <travis.loux@slu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24832,
    "package_name": "tmle",
    "title": "Targeted Maximum Likelihood Estimation",
    "description": "Targeted maximum likelihood estimation of point treatment effects (Targeted Maximum Likelihood Learning, The International Journal of Biostatistics, 2(1), 2006.  This version automatically estimates the additive treatment effect among the treated (ATT) and among the controls (ATC).  The tmle() function calculates the adjusted marginal difference in mean outcome associated with a binary point treatment, for continuous or binary outcomes.  Relative risk and odds ratio estimates are also reported for binary outcomes. Missingness in the outcome is allowed, but not in treatment assignment or baseline covariate values.  The population mean is calculated when there is missingness, and no variation in the treatment assignment. The tmleMSM() function estimates the parameters of a marginal structural model for a binary point treatment effect. Effect estimation stratified by a binary mediating variable is also available. An ID argument can be used to identify repeated measures. Default settings call 'SuperLearner' to estimate the Q and g portions of the likelihood, unless values or a user-supplied regression function are passed in as arguments. ",
    "version": "2.1.1",
    "maintainer": "Susan Gruber <sgruber@cal.berkeley.edu>",
    "url": "https://CRAN.R-project.org/package=tmle",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24848,
    "package_name": "tnet",
    "title": "Weighted, Two-Mode, and Longitudinal Networks Analysis",
    "description": "Binary ties limit the richness of network analyses as relations are unique. The two-mode structure contains a number of features lost when projection it to a one-mode network. Longitudinal datasets allow for an understanding of the causal relationship among ties, which is not the case in cross-sectional datasets as ties are dependent upon each other.",
    "version": "3.0.16",
    "maintainer": "Tore Opsahl <tore@opsahl.co.uk>",
    "url": "http://toreopsahl.com/tnet/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24869,
    "package_name": "tools4uplift",
    "title": "Tools for Uplift Modeling",
    "description": "Uplift modeling aims at predicting the causal effect of an action such as a marketing campaign on a particular individual. In order to simplify the task for practitioners in uplift modeling, we propose a combination of tools that can be separated into the following ingredients: i) quantization, ii) visualization, iii) variable selection, iv) parameters estimation and, v) model validation. For more details, see <https://dms.umontreal.ca/~murua/research/UpliftRegression.pdf>.",
    "version": "1.0.0",
    "maintainer": "Mouloud Belbahri <mouloud.belbahri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24910,
    "package_name": "toweranNA",
    "title": "A Method for Handling Missing Values in Prediction Applications",
    "description": "Non-imputational method for handling missing values in \n   a prediction context, meaning that not only are there missing\n   values in the training dataset, but also some values may be missing\n   in future cases to be predicted. Based on the notion of regression\n   averaging (Matloff (2017, ISBN: 9781498710916)).",
    "version": "0.1.0",
    "maintainer": "Norm Matloff <nsmatloff@ucdavis.edu>",
    "url": "https://github.com/matloff/toweranNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24920,
    "package_name": "tpr",
    "title": "Temporal Process Regression",
    "description": "Regression models for temporal process responses with\n        time-varying coefficient.",
    "version": "0.3-3",
    "maintainer": "Jun Yan <jun.yan@uconn.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24943,
    "package_name": "tradepolicy",
    "title": "Replication of 'An Advanced Guide To Trade Policy Analysis'",
    "description": "Datasets from Yotov, et al. (2016, ISBN:978-92-870-4367-2) \"An\n    Advanced Guide to Trade Policy Analysis\" and functions to report regression\n    summaries with clustered robust standard errors.",
    "version": "0.7.0",
    "maintainer": "Mauricio Vargas Sepulveda <m.sepulveda@mail.utoronto.ca>",
    "url": "https://github.com/pachadotdev/tradepolicy/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24966,
    "package_name": "tranSurv",
    "title": "Transformation-Based Regression under Dependent Truncation",
    "description": "A latent, quasi-independent truncation time is assumed to be linked with the observed dependent truncation time, the event time, and an unknown transformation parameter via a structural transformation model. The transformation parameter is chosen to minimize the conditional Kendall's tau (Martin and Betensky, 2005) <doi:10.1198/016214504000001538> or the regression coefficient estimates (Jones and Crowley, 1992) <doi:10.2307/2336782>. The marginal distribution for the truncation time and the event time are completely left unspecified. The methodology is applied to survival curve estimation and regression analysis.",
    "version": "1.2.4",
    "maintainer": "Sy Han (Steven) Chiou <schiou@smu.edu>",
    "url": "https://github.com/stc04003/tranSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24981,
    "package_name": "translate.logit",
    "title": "Translation of Logit Regression Coefficients into Percentages",
    "description": "Translation of logit models coefficients into percentages, following Deauvieau (2010) <doi:10.1177/0759106309352586>.",
    "version": "1.0.2",
    "maintainer": "Nicolas Robette <nicolas.robette@uvsq.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24984,
    "package_name": "transmdl",
    "title": "Semiparametric Transformation Models",
    "description": "To make the semiparametric transformation models easier to apply in real studies, \n    we introduce this R package, in which the MLE in transformation models via \n    an EM algorithm proposed by Zeng D, Lin DY(2007) <doi:10.1111/j.1369-7412.2007.00606.x> \n    and adaptive lasso method in transformation models proposed by Liu XX, Zeng \n    D(2013) <doi:10.1093/biomet/ast029> are implemented.  \n    C++ functions are used to compute complex loops. The coefficient vector and \n    cumulative baseline hazard function can be estimated, along with the \n    corresponding standard errors and P values.",
    "version": "0.1.0",
    "maintainer": "Fengyu Wen <Wenfy1207@qq.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24985,
    "package_name": "transmem",
    "title": "Treatment of Membrane-Transport Data",
    "description": "Treatment and visualization of membrane (selective) transport \n    data. Transport profiles involving up to three species are produced as\n    publication-ready plots and several membrane performance parameters \n    (e.g. separation factors as defined in Koros et al. (1996) \n    <doi:10.1351/pac199668071479> and non-linear regression parameters\n    for the equations described in Rodriguez de San Miguel et al. (2014)\n    <doi:10.1016/j.jhazmat.2014.03.052>) can be obtained. Many widely used \n    experimental setups (e.g. membrane physical aging) can be easily studied\n    through the package's graphical representations. ",
    "version": "0.1.1",
    "maintainer": "Cristhian Paredes <craparedesca@unal.edu.co>",
    "url": "https://CRAN.R-project.org/package=transmem",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24991,
    "package_name": "transreg",
    "title": "Penalised Regression with Multiple Sets of Prior Effects\n('Transfer Learning')",
    "description": "Improves the predictive performance of ridge and lasso regression exploiting one or more sources of prior information on the importance and direction of effects (Rauschenberger and others 2023, <doi:10.1093/bioinformatics/btad680>). For running the vignette (optional), install 'fwelnet' and 'ecpc' from <https://github.com/kjytay/fwelnet> and <https://github.com/Mirrelijn/ecpc>, respectively.",
    "version": "1.0.5",
    "maintainer": "Armin Rauschenberger <armin.rauschenberger@uni.lu>",
    "url": "https://github.com/rauschenberger/transreg,\nhttps://rauschenberger.github.io/transreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24996,
    "package_name": "traumar",
    "title": "Calculate Metrics for Trauma System Performance",
    "description": "Hospitals, hospital systems, and even trauma systems that\n    provide care to injured patients may not be aware of robust metrics\n    that can help gauge the efficacy of their programs in saving the lives\n    of injured patients.  'traumar' provides robust functions driven by\n    the academic literature to automate the calculation of relevant\n    metrics to individuals desiring to measure the performance of their\n    trauma center or even a trauma system.  'traumar' also provides some\n    helper functions for the data analysis journey. Users can refer to the\n    following publications for descriptions of the methods used in\n    'traumar'.  TRISS methodology, including probability of survival, and\n    the W, M, and Z Scores - Flora (1978)\n    <doi:10.1097/00005373-197810000-00003>, Boyd et al. (1987,\n    PMID:3106646), Llullaku et al. (2009) <doi:10.1186/1749-7922-4-2>,\n    Singh et al. (2011) <doi:10.4103/0974-2700.86626>, Baker et al. (1974,\n    PMID:4814394), and Champion et al. (1989)\n    <doi:10.1097/00005373-198905000-00017>. For the Relative Mortality\n    Metric, see Napoli et al. (2017) <doi:10.1080/24725579.2017.1325948>,\n    Schroeder et al. (2019) <doi:10.1080/10903127.2018.1489021>, and\n    Kassar et al. (2016) <doi:10.1177/00031348221093563>. For more\n    information about methods to calculate over- and under-triage in\n    trauma hospital populations and samples, please see the following\n    publications - Peng & Xiang (2016) <doi:10.1016/j.ajem.2016.08.061>,\n    Beam et al. (2022) <doi:10.23937/2474-3674/1510136>, Roden-Foreman et\n    al. (2017) <doi:10.1097/JTN.0000000000000283>.",
    "version": "1.2.3",
    "maintainer": "Nicolas Foss <nicolas.foss@hhs.iowa.gov>",
    "url": "https://bemts-hhs.github.io/traumar/,\nhttps://github.com/bemts-hhs/traumar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 24997,
    "package_name": "traveltimeHMM",
    "title": "Estimate and predict travel time on road networks",
    "description": "Implements a Hidden Markov Model (HMM) with a random trip effect to estimate the distribution of travel time. The HMM is used to capture dependency on hidden congestion states. The trip effect is used to capture dependency on driver behaviour. Variations of those two types of dependencies leads to four models to estimate the distribution of travel time. Prediction methods for each model is provided.",
    "version": "0.0.0.9",
    "maintainer": "Mohamad Elmasri <elmasri.m@gmail.com>",
    "url": "https://github.com/melmasri/traveltimeHMM",
    "exports": [],
    "topics": ["arrival-times", "bayesian", "eta", "gps", "hidden-markov-models", "hmm", "routing", "transportation", "transportation-network", "travel-time", "uncertainty-quantification"],
    "score": "NA",
    "stars": 11
  },
  {
    "id": 25038,
    "package_name": "trend",
    "title": "Non-Parametric Trend Tests and Change-Point Detection",
    "description": "The analysis of environmental data often requires\n\t     the detection of trends and change-points. \n\t     This package includes tests for trend detection\n\t     (Cox-Stuart Trend Test, Mann-Kendall Trend Test, \n\t     (correlated) Hirsch-Slack Test,\n             partial Mann-Kendall Trend Test, multivariate (multisite)\n\t     Mann-Kendall Trend Test, (Seasonal) Sen's slope, \n\t     partial Pearson and Spearman correlation trend test),\n             change-point detection (Lanzante's test procedures, \n\t     Pettitt's test, Buishand Range Test,\n\t     Buishand U Test, Standard Normal Homogeinity Test),\n\t     detection of non-randomness (Wallis-Moore Phase Frequency Test,\n\t     Bartels rank von Neumann's ratio test, Wald-Wolfowitz Test)\n\t     and the two sample Robust Rank-Order Distributional Test.",
    "version": "1.1.6",
    "maintainer": "Thorsten Pohlert <thorsten.pohlert@gmx.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25039,
    "package_name": "trendchange",
    "title": "Innovative Trend Analysis and Time-Series Change Point Analysis",
    "description": "Innovative Trend Analysis is a graphical method to examine the trends in time series data. Sequential Mann-Kendall test uses the intersection of prograde and retrograde series to indicate the possible change point in time series data. Distribution free cumulative sum charts indicate location and significance of the change point in time series.\n  Zekai, S. (2011). <doi:10.1061/(ASCE)HE.1943-5584.0000556>.\n  Grayson, R. B. et al. (1996). Hydrological Recipes: Estimation Techniques in Australian Hydrology. Cooperative Research Centre for Catchment Hydrology, Australia, p. 125. \n  Sneyers, S. (1990). On the statistical analysis of series of observations. Technical note no 5 143, WMO No 725 415. Secretariat of the World Meteorological Organization, Geneva, 192 pp.",
    "version": "1.2",
    "maintainer": "Sandeep Kumar Patakamuri <sandeep.patakamuri@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25043,
    "package_name": "trendsegmentR",
    "title": "Linear Trend Segmentation",
    "description": "Performs the detection of linear trend changes for univariate time series \n    by implementing the bottom-up unbalanced wavelet transformation proposed by \n    H. Maeng and P. Fryzlewicz (2023). The estimated number and locations of the \n    change-points are returned with the piecewise-linear estimator for signal.",
    "version": "1.3.1",
    "maintainer": "Hyeyoung Maeng <hyeyoung.maeng@durham.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25044,
    "package_name": "trendseries",
    "title": "Extract Trends from Time Series",
    "description": "Extract trends from monthly and quarterly economic time series.\n    Provides two main functions: augment_trends() for pipe-friendly 'tibble' workflows\n    and extract_trends() for direct time series analysis. Includes key econometric\n    filters and modern parameter experimentation tools.",
    "version": "1.1.0",
    "maintainer": "Vinicius Oike <viniciusoike@gmail.com>",
    "url": "https://github.com/viniciusoike/trendseries,\nhttps://viniciusoike.github.io/trendseries/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25045,
    "package_name": "trendtestR",
    "title": "Exploratory Trend Analysis and Visualization for Time-Series and\nGrouped Data",
    "description": "Provides a set of exploratory data analysis (EDA) tools for \n    visualizing trends, diagnosing data types for beginner-friendly workflows, \n    and automatically routing to suitable statistical tests or trend exploration \n    models. Includes unified plotting functions for trend lines, grouped boxplots, \n    and comparative scatterplots; automated statistical testing (e.g., t-test, \n    Wilcoxon, ANOVA, Kruskal-Wallis, Tukey, Dunn) with optional effect size \n    calculation; and model-based trend analysis using generalized additive \n    models (GAM) for count data, generalized linear models (GLM) for continuous \n    data, and zero-inflated models (ZIP/ZINB) for count data with potential \n    zero-inflation. \n    Also supports time-window continuity checks, cross-year \n    handling in compare_monthly_cases(), and ARIMA-ready preparation with \n    stationarity diagnostics, ensuring consistent parameter styles for \n    reproducible research and user-friendly workflows.Methods are \n    based on R Core Team (2024) <https://www.R-project.org/>, \n    Wood, S.N.(2017, ISBN:978-1498728331),\n    Hyndman RJ, Khandakar Y (2008) <doi:10.18637/jss.v027.i03>, \n    Simon Jackman (2024) <https://github.com/atahk/pscl/>,    \n    Achim Zeileis, Christian Kleiber, Simon Jackman (2008) <doi:10.18637/jss.v027.i08>.",
    "version": "1.0.1",
    "maintainer": "Gelan Huang <huanggelan97@icloud.com>",
    "url": "https://github.com/GrahnH/trendtestR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25066,
    "package_name": "triplediff",
    "title": "Triple-Difference Estimators",
    "description": "Implements triple-difference (DDD) estimators for both average\n              treatment effects and event-study parameters. Methods include regression\n              adjustment, inverse-probability weighting, and doubly-robust estimators,\n              all of which rely on a conditional DDD parallel-trends assumption and\n              allow covariate adjustment across multiple pre- and post-treatment\n              periods. The methodology is detailed in Ortiz-Villavicencio and\n              Sant'Anna (2025) <doi:10.48550/arXiv.2505.09942>.",
    "version": "0.1.0",
    "maintainer": "Marcelo Ortiz-Villavicencio <marcelo.ortiz@emory.edu>",
    "url": "http://marcelortiz.com/triplediff/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25071,
    "package_name": "triptych",
    "title": "Diagnostic Graphics to Evaluate Forecast Performance",
    "description": "Overall predictive performance is measured by a mean score\n    (or loss), which decomposes into miscalibration, discrimination, and\n    uncertainty components. The main focus is visualization of these distinct\n    and complementary aspects in joint displays.\n    See Dimitriadis, Gneiting, Jordan, Vogel (2024) <doi:10.1016/j.ijforecast.2023.09.007>.",
    "version": "0.1.3",
    "maintainer": "Alexander I. Jordan <alexander.jordan@h-its.org>",
    "url": "https://github.com/aijordan/triptych/,\nhttps://aijordan.github.io/triptych/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25072,
    "package_name": "tristan",
    "title": "Tristan's helper functions for RStanARM models and MCMC samples",
    "description": "This package provides from helper functions for preparing posterior samples from Stan models.",
    "version": "0.0.0.9000",
    "maintainer": "",
    "url": "https://github.com/tjmahr/tristan",
    "exports": [],
    "topics": ["bayesian", "rstanarm", "rstats", "stan"],
    "score": "NA",
    "stars": 5
  },
  {
    "id": 25077,
    "package_name": "trouBBlme4SolveR",
    "title": "Troubles Solver for 'lme4'",
    "description": "The main function of the package aims to update 'lmer()'/'glmer()' models depending on their warnings, so trying to avoid convergence and singularity problems.",
    "version": "0.1.4",
    "maintainer": "Iago Giné-Vázquez <iago.gin-vaz@protonmail.com>",
    "url": "https://gitlab.com/iagogv/troubblme4solver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25078,
    "package_name": "trps",
    "title": "Bayesian Trophic Position Models using 'stan'",
    "description": "Bayesian trophic position models using 'stan' by leveraging 'brms' for \n             stable isotope data. Trophic position models are derived by using \n             equations from \n             Post (2002) <doi:10.1890/0012-9658(2002)083[0703:USITET]2.0.CO;2>,\n             Vander Zanden and Vadeboncoeur (2002) <doi:10.1890/0012-9658(2002)083[2152:FAIOBA]2.0.CO;2>, \n             and \n             Heuvel et al. (2024) <doi:10.1139/cjfas-2024-0028>. ",
    "version": "0.1.0",
    "maintainer": "Benjamin L. Hlina <benjamin.hlina@gmail.com>",
    "url": "https://benjaminhlina.github.io/trps/,\nhttps://github.com/benjaminhlina/trps",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25082,
    "package_name": "truelies",
    "title": "Bayesian Methods to Estimate the Proportion of Liars in Coin\nFlip Experiments",
    "description": "Implements Bayesian methods, described in\n    Hugh-Jones (2019) <doi:10.1007/s40881-019-00069-x>, for estimating the\n    proportion of liars in coin flip-style experiments, where subjects\n    report a random outcome and are paid for reporting a \"good\" outcome.",
    "version": "0.2.0",
    "maintainer": "David Hugh-Jones <davidhughjones@gmail.com>",
    "url": "https://github.com/hughjonesd/truelies",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25084,
    "package_name": "truncAIPW",
    "title": "Doubly Robust Estimation under Covariate-Induced Dependent Left\nTruncation",
    "description": "Doubly robust estimation for the mean of an arbitrarily transformed survival time under covariate-induced dependent left truncation and noninformative right censoring. The functions truncAIPW(), truncAIPW_cen1(), and truncAIPW_cen2() compute the doubly robust estimators under the scenario without censoring and the two censoring scenarios, respectively. The package also contains three simulated data sets 'simu', 'simu_c1', and 'simu_c2', which are used to illustrate the usage of the functions in this package.\n    Reference: Wang, Y., Ying, A., Xu, R. (2022) \"Doubly robust estimation under covariate-induced dependent left truncation\" <arXiv:2208.06836>.",
    "version": "1.0.1",
    "maintainer": "Yuyao Wang <yuw079@ucsd.edu>",
    "url": "https://arxiv.org/pdf/2208.06836.pdf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25085,
    "package_name": "truncSP",
    "title": "Semi-Parametric Estimators of Truncated Regression Models",
    "description": "Estimators for semi-parametric linear regression models with truncated response variables (fixed truncation point). The estimators implemented are the Symmetrically Trimmed Least Squares (STLS) estimator introduced by Powell (1986) <doi:10.2307/1914308>, the Quadratic Mode (QME) estimator introduced by Lee (1993) <doi:10.1016/0304-4076(93)90056-B>, and the Left Truncated (LT) estimator introduced by Karlsson (2006) <doi:10.1007/s00184-005-0023-x>.",
    "version": "1.2.4",
    "maintainer": "Anita Lindmark <anita.lindmark@umu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25088,
    "package_name": "truncnormbayes",
    "title": "Estimates Moments for a Truncated Normal Distribution using\n'Stan'",
    "description": "Finds the posterior modes for the mean and standard deviation for a\n  truncated normal distribution with one or two known truncation points.\n  The method used extends Bayesian methods for parameter estimation for a singly\n  truncated normal distribution under the Jeffreys prior (see Zhou X,\n  Giacometti R, Fabozzi FJ, Tucker AH (2014). \"Bayesian estimation of truncated\n  data with applications to operational risk measurement\".\n  <doi:10.1080/14697688.2012.752103>). This package additionally allows for a\n  doubly truncated normal distribution.",
    "version": "0.0.3",
    "maintainer": "Peter Solymos <peter@analythium.io>",
    "url": "https://github.com/mathurlabstanford/truncnormbayes,\nhttps://mathurlabstanford.github.io/truncnormbayes/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25097,
    "package_name": "tsBSS",
    "title": "Blind Source Separation and Supervised Dimension Reduction for\nTime Series",
    "description": "Different estimators are provided to solve the blind source separation problem for multivariate time series with stochastic volatility and supervised dimension reduction problem for multivariate time series. Different functions based on AMUSE and SOBI are also provided for estimating the dimension of the white noise subspace. The package is fully described in Nordhausen, Matilainen, Miettinen, Virta and Taskinen (2021) <doi:10.18637/jss.v098.i15>.",
    "version": "1.0.0",
    "maintainer": "Markus Matilainen <markus.matilainen@outlook.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25098,
    "package_name": "tsDyn",
    "title": "Nonlinear Time Series Models with Regime Switching",
    "description": "Implements nonlinear autoregressive (AR) time series models. For univariate series, a non-parametric approach is available through additive nonlinear AR. Parametric modeling and testing for regime switching dynamics is available when the transition is either direct (TAR: threshold AR) or smooth (STAR: smooth transition AR, LSTAR). For multivariate series, one can estimate a range of TVAR or threshold cointegration TVECM models with two or three regimes. Tests can be conducted for TVAR as well as for TVECM (Hansen and Seo 2002 and Seo 2006). ",
    "version": "11.0.5.2",
    "maintainer": "Matthieu Stigler <Matthieu.Stigler@gmail.com>",
    "url": "https://github.com/MatthieuStigler/tsDyn/wiki",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25099,
    "package_name": "tsLSTMx",
    "title": "Predict Time Series Using LSTM Model Including Exogenous\nVariable to Denote Zero Values",
    "description": "It is a versatile tool for predicting time series data using Long Short-Term Memory (LSTM) models. It is specifically designed to handle time series with an exogenous variable, allowing users to denote whether data was available for a particular period or not. The package encompasses various functionalities, including hyperparameter tuning, custom loss function support, model evaluation, and one-step-ahead forecasting. With an emphasis on ease of use and flexibility, it empowers users to explore, evaluate, and deploy LSTM models for accurate time series predictions and forecasting in diverse applications. More details can be found in Garai and Paul (2023) <doi:10.1016/j.iswa.2023.200202>.",
    "version": "0.1.0",
    "maintainer": "Sandip Garai <sandipnicksandy@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25100,
    "package_name": "tsModel",
    "title": "Time Series Modeling for Air Pollution and Health",
    "description": "Tools for specifying time series regression models.",
    "version": "0.6-2",
    "maintainer": "Roger D. Peng <roger.peng@austin.utexas.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25101,
    "package_name": "tsPI",
    "title": "Improved Prediction Intervals for ARIMA Processes and Structural\nTime Series",
    "description": "Prediction intervals for ARIMA and structural time series\n    models using importance sampling approach with uninformative priors for model\n    parameters, leading to more accurate coverage probabilities in frequentist\n    sense. Instead of sampling the future observations and hidden states of the\n    state space representation of the model, only model parameters are sampled,\n    and the method is based solving the equations corresponding to the conditional\n    coverage probability of the prediction intervals. This makes method relatively\n    fast compared to for example MCMC methods, and standard errors of prediction\n    limits can also be computed straightforwardly.",
    "version": "1.0.4",
    "maintainer": "Jouni Helske <jouni.helske@iki.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25102,
    "package_name": "tsSelect",
    "title": "Execution of Time Series Models",
    "description": "Execution of various time series models and choosing the best one\n    either by a specific error metric or by picking the best one by majority vote.\n    The models are based on the \"forecast\" package, written by Prof. Rob Hyndman.",
    "version": "0.1.8",
    "maintainer": "Avi Blinder <aviblinder@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25104,
    "package_name": "tsapp",
    "title": "Time Series, Analysis and Application",
    "description": "Accompanies the book Rainer Schlittgen and Cristina Sattarhoff (2020) <https://www.degruyter.com/view/title/575978>  \"Angewandte Zeitreihenanalyse mit R, 4. Auflage\" . The package contains the time series and functions used therein. It was developed over many years teaching courses about time series analysis.  ",
    "version": "1.0.4",
    "maintainer": "Rainer Schlittgen <R.Schlittgen@t-online.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25105,
    "package_name": "tsaux",
    "title": "Time Series Forecasting Auxiliary Functions",
    "description": "A suite of auxiliary functions that enhance time series estimation and forecasting, including a robust anomaly detection routine based on Chen and Liu (1993) <doi:10.2307/2290724> (imported and wrapped from the 'tsoutliers' package), utilities for managing calendar and time conversions, performance metrics to assess both point forecasts and distributional predictions, advanced simulation by allowing the generation of time series components—such as trend, seasonal, ARMA, irregular, and anomalies—in a modular fashion based on the innovations form of the state space model and a number of transformation methods including Box-Cox, Logit, 'Softplus-Logit' and Sigmoid.",
    "version": "1.0.0",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "url": "https://github.com/tsmodels/tsaux",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25108,
    "package_name": "tscopula",
    "title": "Time Series Copula Models",
    "description": "Functions for the analysis of time series using copula models.  \n    The package is based on methodology described in the following references.\n    McNeil, A.J. (2021) <doi:10.3390/risks9010014>,\n    Bladt, M., & McNeil, A.J. (2021) <doi:10.1016/j.ecosta.2021.07.004>,\n    Bladt, M., & McNeil, A.J. (2022) <doi:10.1515/demo-2022-0105>.",
    "version": "0.3.9",
    "maintainer": "Alexander McNeil <alexanderjmcneil@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25110,
    "package_name": "tsdataleaks",
    "title": "Exploit Data Leakages in Time Series Forecasting Competitions",
    "description": "Forecasting competitions are of increasing importance as a mean to learn best practices and gain knowledge. Data leakage is one of the most common issues that can often be found in competitions. Data leaks can happen when the training data contains information about the test data. For example: randomly chosen blocks of time series are concatenated to form a new time series, scale-shifts, repeating patterns in time series,  white noise is added in the original time series to form a new time series, etc.  'tsdataleaks' package can be used to detect data leakages in a collection of  time series.",
    "version": "2.1.1",
    "maintainer": "Thiyanga S. Talagala <ttalagala@sjp.ac.lk>",
    "url": "https://github.com/thiyangt/tsdataleaks",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25112,
    "package_name": "tsdecomp",
    "title": "Decomposition of Time Series Data",
    "description": "ARIMA-model-based decomposition of quarterly and \n monthly time series data.\n The methodology is developed and described, among others, in \n Burman (1980) <DOI:10.2307/2982132> and \n Hillmer and Tiao (1982) <DOI:10.2307/2287770>.",
    "version": "0.2",
    "maintainer": "Javier López-de-Lacalle <javlacalle@yahoo.es>",
    "url": "https://jalobe.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25114,
    "package_name": "tsdisagg2",
    "title": "Time Series Disaggregation",
    "description": "Disaggregates low frequency time series data to higher frequency series. Implements the following methods for temporal disaggregation: Boot, Feibes and Lisman (1967) <DOI:10.2307/2985238>, Chow and Lin (1971) <DOI:10.2307/1928739>, Fernandez (1981) <DOI:10.2307/1924371> and Litterman (1983) <DOI:10.2307/1391858>.",
    "version": "0.1.0",
    "maintainer": "Jorge Vieira <jorgealexandrevieira@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25116,
    "package_name": "tseffects",
    "title": "Dynamic (Causal) Inferences from Time Series (with Interactions)",
    "description": "Autoregressive distributed lag (A[R]DL) models (and their reparameterized equivalent, the Generalized Error-Correction Model [GECM]) (see De Boef and Keele 2008 <doi:10.1111/j.1540-5907.2007.00307.x>) are the workhorse models in uncovering dynamic inferences. ADL models are simple to estimate; this is what makes them attractive. Once these models are estimated, what is less clear is how to uncover a rich set of dynamic inferences from these models. We provide tools for recovering those inferences in three forms: causal inferences from ADL models, traditional time series quantities of interest (short- and long-run effects), and dynamic conditional relationships.",
    "version": "0.1.4",
    "maintainer": "Soren Jordan <sorenjordanpols@gmail.com>",
    "url": "https://sorenjordan.github.io/tseffects/,\nhttps://github.com/sorenjordan/tseffects",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25119,
    "package_name": "tseries",
    "title": "Time Series Analysis and Computational Finance",
    "description": "Time series analysis and computational finance.",
    "version": "0.10-59",
    "maintainer": "Kurt Hornik <Kurt.Hornik@R-project.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25120,
    "package_name": "tseriesChaos",
    "title": "Analysis of Nonlinear Time Series",
    "description": "Routines for the analysis of nonlinear time series. This\n        work is largely inspired by the TISEAN project, by Rainer\n        Hegger, Holger Kantz and Thomas Schreiber:\n        <http://www.mpipks-dresden.mpg.de/~tisean/>.",
    "version": "0.1-13.1",
    "maintainer": "Antonio Fabio Di Narzo <antonio.fabio@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25121,
    "package_name": "tseriesEntropy",
    "title": "Entropy Based Analysis and Tests for Time Series",
    "description": "Implements an Entropy measure of dependence based on the Bhattacharya-Hellinger-Matusita distance. Can be used as a (nonlinear) autocorrelation/crosscorrelation function for continuous and categorical time series. The package includes tests for serial and cross dependence and nonlinearity based on it. Some routines have a parallel version that can be used in a multicore/cluster environment. The package makes use of S4 classes.",
    "version": "0.7-2",
    "maintainer": "Simone Giannerini <simone.giannerini@unibo.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25122,
    "package_name": "tseriesTARMA",
    "title": "Analysis of Nonlinear Time Series Through Threshold\nAutoregressive Moving Average Models (TARMA) Models",
    "description": "Routines for nonlinear time series analysis based on Threshold Autoregressive Moving Average (TARMA) models. It provides functions and methods for: TARMA model fitting and forecasting, including robust estimators, see Goracci et al. JBES (2025) <doi:10.1080/07350015.2024.2412011>; tests for threshold effects, see Giannerini et al. JoE (2024) <doi:10.1016/j.jeconom.2023.01.004>, Goracci et al. Statistica Sinica (2023) <doi:10.5705/ss.202021.0120>, Angelini et al. (2024) <doi:10.48550/arXiv.2308.00444>;  unit-root tests based on TARMA models, see Chan et al. Statistica Sinica (2024) <doi:10.5705/ss.202022.0125>.",
    "version": "0.5-1",
    "maintainer": "Simone Giannerini <simone.giannerini@uniud.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25125,
    "package_name": "tsfknn",
    "title": "Time Series Forecasting Using Nearest Neighbors",
    "description": "Allows forecasting time series using nearest neighbors regression\n    Francisco Martinez, Maria P. Frias, Maria D. Perez-Godoy and Antonio J.\n    Rivera (2019) <doi:10.1007/s10462-017-9593-z>. When the forecasting horizon\n    is higher than 1, two multi-step ahead forecasting strategies can be used.\n    The model built is autoregressive, that is, it is only based on the \n    observations of the time series. The nearest neighbors used in a prediction\n    can be consulted and plotted.",
    "version": "0.6.0",
    "maintainer": "Francisco Martinez <fmartin@ujaen.es>",
    "url": "https://github.com/franciscomartinezdelrio/tsfknn",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25126,
    "package_name": "tsfngm",
    "title": "Time Series Forecasting using Nonlinear Growth Models",
    "description": "Nonlinear growth models are extremely useful in gaining insight into the underlying mechanism. These models are generally 'mechanistic,' with parameters that have biological meaning. This package allows you to fit and forecast time series data using nonlinear growth models. ",
    "version": "0.1.0",
    "maintainer": "Mrinmoy Ray <mrinmoy4848@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25127,
    "package_name": "tsforecast",
    "title": "Time Series Forecasting Functions",
    "description": "Fundamental time series forecasting models such as autoregressive integrated moving average (ARIMA), exponential smoothing, and simple moving average are included. For ARIMA models, the output follows the traditional parameterisation by Box and Jenkins (1970, ISBN: 0816210942, 9780816210947). Furthermore, there are functions for detailed time series exploration and decomposition, respectively. All data and result visualisations are generated by 'ggplot2' instead of conventional R graphical output. For more details regarding the theoretical background of the models see Hyndman, R.J. and Athanasopoulos, G. (2021) <https://otexts.com/fpp3/>.",
    "version": "1.2.1",
    "maintainer": "Ka Yui Karl Wu <karlwuky@suss.edu.sg>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25129,
    "package_name": "tsgarch",
    "title": "Univariate GARCH Models",
    "description": "Multiple flavors of the Generalized Autoregressive Conditional Heteroskedasticity (GARCH) model with a large choice of conditional distributions. Methods for specification, estimation, prediction, filtering, simulation, statistical testing and more. Represents a partial re-write and re-think of 'rugarch', making use of automatic differentiation for estimation.",
    "version": "1.0.3",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "url": "https://github.com/tsmodels/tsgarch",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25130,
    "package_name": "tsgc",
    "title": "Time Series Methods Based on Growth Curves",
    "description": "The 'tsgc' package provides comprehensive tools for the analysis and forecasting of epidemic trajectories.\n    It is designed to model the progression of an epidemic over time while accounting for the various uncertainties\n    inherent in real-time data. Underpinned by a dynamic Gompertz model, the package adopts a state space approach,\n    using the Kalman filter for flexible and robust estimation of the non-linear growth pattern commonly observed in\n    epidemic data. The reinitialization feature enhances the model’s ability to adapt to the emergence of new waves.\n    The forecasts generated by the package are of value to public health officials and researchers who need to\n    understand and predict the course of an epidemic to inform decision-making. Beyond its application in public\n    health, the package is also a useful resource for researchers and practitioners in fields where the trajectories\n    of interest resemble those of epidemics, such as innovation diffusion. The package includes functionalities for\n    data preprocessing, model fitting, and forecast visualization, as well as tools for evaluating forecast accuracy.\n    The core methodologies implemented in 'tsgc' are based on well-established statistical techniques as described in\n    Harvey and Kattuman (2020) <doi:10.1162/99608f92.828f40de>, Harvey and Kattuman (2021)\n    <doi:10.1098/rsif.2021.0179>, and Ashby, Harvey, Kattuman, and Thamotheram (2024)\n    <https://www.jbs.cam.ac.uk/wp-content/uploads/2024/03/cchle-tsgc-paper-2024.pdf>.",
    "version": "0.0",
    "maintainer": "Craig Thamotheram <cpt@tacindex.com>",
    "url": "https://github.com/Craig-PT/tsgc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25132,
    "package_name": "tsibble",
    "title": "Tidy Temporal Data Frames and Tools",
    "description": "Provides a 'tbl_ts' class (the 'tsibble') for\n    temporal data in an data- and model-oriented format. The 'tsibble'\n    provides tools to easily manipulate and analyse temporal data, such as\n    filling in time gaps and aggregating over calendar periods.",
    "version": "1.1.6",
    "maintainer": "Earo Wang <earo.wang@gmail.com>",
    "url": "https://tsibble.tidyverts.org",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25133,
    "package_name": "tsibbledata",
    "title": "Diverse Datasets for 'tsibble'",
    "description": "Provides diverse datasets in the 'tsibble' data structure. These datasets are useful for learning and demonstrating how tidy temporal data can tidied, visualised, and forecasted.",
    "version": "0.4.1",
    "maintainer": "Mitchell O'Hara-Wild <mail@mitchelloharawild.com>",
    "url": "https://tsibbledata.tidyverts.org/,\nhttps://github.com/tidyverts/tsibbledata/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25134,
    "package_name": "tsibbletalk",
    "title": "Interactive Graphics for Tsibble Objects",
    "description": "A shared tsibble data easily communicates between\n    htmlwidgets on both client and server sides, powered by 'crosstalk'. A\n    shiny module is provided to visually explore periodic/aperiodic\n    temporal patterns.",
    "version": "0.1.0",
    "maintainer": "Earo Wang <earo.wang@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25135,
    "package_name": "tsintermittent",
    "title": "Intermittent Time Series Forecasting",
    "description": "Time series methods for intermittent demand forecasting. Includes Croston's method and its variants (Moving Average, SBA), and the TSB method. Users can obtain optimal parameters on a variety of loss functions, or use fixed ones (Kourenztes (2014) <doi:10.1016/j.ijpe.2014.06.007>). Intermittent time series classification methods and iMAPA that uses multiple temporal aggregation levels are also provided (Petropoulos & Kourenztes (2015) <doi:10.1057/jors.2014.62>).",
    "version": "1.10",
    "maintainer": "Nikolaos Kourentzes <nikolaos@kourentzes.com>",
    "url": "https://kourentzes.com/forecasting/2014/06/23/intermittent-demand-forecasting-package-for-r/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25138,
    "package_name": "tsmethods",
    "title": "Time Series Methods",
    "description": "Generic methods for use in a time series probabilistic framework, allowing for a common calling convention across packages. Additional methods for time series prediction ensembles and probabilistic plotting of predictions is included. A more detailed description is available at <https://www.nopredict.com/packages/tsmethods> which shows the currently implemented methods in the 'tsmodels' framework. ",
    "version": "1.0.2",
    "maintainer": "Alexios Galanos <alexios@4dscape.com>",
    "url": "https://www.nopredict.com/packages/tsmethods,\nhttps://github.com/tsmodels/tsmethods",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25142,
    "package_name": "tsnet",
    "title": "Fitting, Comparing, and Visualizing Networks Based on Time\nSeries Data",
    "description": "Fit, compare, and visualize Bayesian graphical vector autoregressive (GVAR) network models using 'Stan'. These models are commonly used in psychology to represent temporal and contemporaneous relationships between multiple variables in intensive longitudinal data. Fitted models can be compared with a test based on matrix norm differences of posterior point estimates to quantify the differences between two estimated networks. See also Siepe, Kloft & Heck (2024) <doi:10.31234/osf.io/uwfjc>.",
    "version": "0.2.0",
    "maintainer": "Björn S. Siepe <bjoernsiepe@gmail.com>",
    "url": "https://github.com/bsiepe/tsnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25143,
    "package_name": "tsoutliers",
    "title": "Detection of Outliers in Time Series",
    "description": "Detection of outliers in time series following the \n    Chen and Liu (1993) <DOI:10.2307/2290724> procedure. \n    Innovational outliers, additive outliers, level shifts, \n    temporary changes and seasonal level shifts are considered.",
    "version": "0.6-10",
    "maintainer": "Javier López-de-Lacalle <javlacalle@yahoo.es>",
    "url": "https://jalobe.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25146,
    "package_name": "tsqn",
    "title": "Applications of the Qn Estimator to Time Series (Univariate and\nMultivariate)",
    "description": "Time Series Qn is a package with applications of the Qn estimator of Rousseeuw and Croux (1993) <doi:10.1080/01621459.1993.10476408> to univariate and multivariate Time Series in time and frequency domains. More specifically, the robust estimation of autocorrelation or autocovariance matrix functions from Ma and Genton (2000, 2001) <doi:10.1111/1467-9892.00203>, <doi:10.1006/jmva.2000.1942> and Cotta (2017) <doi:10.13140/RG.2.2.14092.10883> are provided. The robust pseudo-periodogram of Molinares et. al. (2009) <doi:10.1016/j.jspi.2008.12.014> is also given. This packages also provides the M-estimator of the long-memory parameter d based on the robustification of the GPH estimator proposed by Reisen et al. (2017) <doi:10.1016/j.jspi.2017.02.008>. ",
    "version": "1.0.0",
    "maintainer": "Higor Cotta <cotta.higor@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25147,
    "package_name": "tsriadditive",
    "title": "Two Stage Residual Inclusion Additive Hazards Estimator",
    "description": "Additive hazards models with two stage residual inclusion method are fitted under either survival data or competing risks data. The estimator incorporates an instrumental variable and therefore can recover causal estimand in the presence of unmeasured confounding under some assumptions. A.Ying, R. Xu and J. Murphy. (2019) <doi:10.1002/sim.8071>.",
    "version": "1.0.0",
    "maintainer": "Andrew Ying <aying9339@gmail.com>",
    "url": "https://onlinelibrary.wiley.com/doi/abs/10.1002/sim.8071",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25148,
    "package_name": "tsrobprep",
    "title": "Robust Preprocessing of Time Series Data",
    "description": "Methods for handling the missing values outliers are introduced in\n    this package. The recognized missing values and outliers are replaced \n    using a model-based approach. The model may consist of both autoregressive\n    components and external regressors. The methods work robust and efficient,\n    and they are fully tunable. The primary motivation for writing the package\n    was preprocessing of the energy systems data, e.g. power plant production\n    time series, but the package could be used with any time series data. For \n    details, see Narajewski et al. (2021) <doi:10.1016/j.softx.2021.100809>.",
    "version": "0.3.2",
    "maintainer": "Michał Narajewski <michal.narajewski@uni-due.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25149,
    "package_name": "tssim",
    "title": "Simulation of Daily and Monthly Time Series",
    "description": "Flexible simulation of time series using time series\n    components, including seasonal, calendar and outlier effects. Main\n    algorithm described in Ollech, D. (2021) <doi:10.1515/jtse-2020-0028>.",
    "version": "0.2.7",
    "maintainer": "Daniel Ollech <daniel.ollech@bundesbank.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25151,
    "package_name": "tstools",
    "title": "A Time Series Toolbox for Official Statistics",
    "description": "Plot official statistics' time series conveniently: automatic\n    legends, highlight windows, stacked bar chars with positive and\n    negative contributions, sum-as-line option, two y-axes with automatic\n    horizontal grids that fit both axes and other popular chart types.\n    'tstools' comes with a plethora of defaults to let you plot without\n    setting an abundance of parameters first, but gives you the\n    flexibility to tweak the defaults. In addition to charts, 'tstools'\n    provides a super fast, 'data.table' backed time series I/O that allows\n    the user to export / import long format, wide format and transposed\n    wide format data to various file types.",
    "version": "0.4.4",
    "maintainer": "Stéphane Bisinger <bisinger@kof.ethz.ch>",
    "url": "https://kof-ch.github.io/tstools/,\nhttps://github.com/KOF-ch/tstools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25152,
    "package_name": "tsutils",
    "title": "Time Series Exploration, Modelling and Forecasting",
    "description": "Includes: (i) tests and visualisations that can help the modeller explore time series components and perform decomposition; (ii) modelling shortcuts, such as functions to construct lagmatrices and seasonal dummy variables of various forms; (iii) an implementation of the Theta method; (iv) tools to facilitate the design of the forecasting process, such as ABC-XYZ analyses; and (v) \"quality of life\" functions, such as treating time series for trailing and leading values.",
    "version": "0.9.4",
    "maintainer": "Nikolaos Kourentzes <nikolaos@kourentzes.com>",
    "url": "https://github.com/trnnick/tsutils/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25156,
    "package_name": "tswge",
    "title": "Time Series for Data Science",
    "description": "Accompanies the texts Time Series for Data Science with R by Woodward, Sadler and Robertson & Applied Time Series Analysis with R, 2nd edition by Woodward, Gray, and Elliott.  It is helpful for data analysis and for time series instruction.",
    "version": "2.2.0",
    "maintainer": "Bivin Sadler <bsadler@smu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25193,
    "package_name": "tvcure",
    "title": "Additive Cure Survival Model with Time-Varying Covariates",
    "description": "Fit of a double additive cure survival model with time-varying covariates. The additive terms in the long- and short-term survival submodels, modelling the cure probability and the event timing for susceptible units, are estimated using Laplace P-splines. For more details, see Lambert and Kreyenfeld (2025) <doi:10.1093/jrsssa/qnaf035>.",
    "version": "0.6.6",
    "maintainer": "Philippe Lambert <p.lambert@uliege.be>",
    "url": "<https://github.com/plambertULiege/tvcure>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25203,
    "package_name": "twang",
    "title": "Toolkit for Weighting and Analysis of Nonequivalent Groups",
    "description": "Provides functions for propensity score\n        estimating and weighting, nonresponse weighting, and diagnosis\n        of the weights.",
    "version": "2.6.2",
    "maintainer": "Lane Burgette <burgette@rand.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25204,
    "package_name": "twangContinuous",
    "title": "Toolkit for Weighting and Analysis of Nonequivalent Groups -\nContinuous Exposures",
    "description": "Provides functions for propensity score\n        estimation and weighting for continuous exposures as described in Zhu, Y., \n        Coffman, D. L., & Ghosh, D. (2015). A boosting algorithm for\n        estimating generalized propensity scores with continuous treatments.\n        Journal of Causal Inference, 3(1), 25-40. <doi:10.1515/jci-2014-0022>.",
    "version": "1.0.0",
    "maintainer": "Donna Coffman <donna.coffman@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25205,
    "package_name": "twangMediation",
    "title": "Twang Causal Mediation Modeling via Weighting",
    "description": "Provides functions for estimating natural direct and indirect effects for mediation analysis. It uses weighting where the weights are functions of estimates of the probability of exposure or treatment assignment (Hong, G (2010). <https://cepa.stanford.edu/sites/default/files/workshops/GH_JSM%20Proceedings%202010.pdf> Huber, M. (2014). <doi:10.1002/jae.2341>). Estimation of probabilities can use generalized boosting or logistic regression. Additional functions provide diagnostics of the model fit and weights. The vignette provides details and examples.",
    "version": "1.2.1",
    "maintainer": "Dan McCaffrey <dmccaffrey@ets.org>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25208,
    "package_name": "twdtw",
    "title": "Time-Weighted Dynamic Time Warping",
    "description": "Implements Time-Weighted Dynamic Time Warping (TWDTW), \n    a measure for quantifying time series similarity. The TWDTW algorithm, \n    described in Maus et al. (2016) <doi:10.1109/JSTARS.2016.2517118> and \n    Maus et al. (2019) <doi:10.18637/jss.v088.i05>, is applicable to multi-dimensional \n    time series of various resolutions. It is particularly suitable for comparing \n    time series with seasonality for environmental and ecological data analysis, \n    covering domains such as remote sensing imagery, climate data, hydrology, \n    and animal movement. The 'twdtw' package offers a user-friendly 'R' interface, \n    efficient 'Fortran' routines for TWDTW calculations, flexible time weighting \n    definitions, as well as utilities for time series preprocessing and visualization.",
    "version": "1.0-1",
    "maintainer": "Victor Maus <vwmaus1@gmail.com>",
    "url": "https://github.com/vwmaus/twdtw/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25231,
    "package_name": "twopartm",
    "title": "Two-Part Model with Marginal Effects",
    "description": "Fit two-part regression models for zero-inflated data. The models and their components are represented using S4 classes and methods. Average Marginal effects and predictive margins with standard errors and\n  confidence intervals can be calculated from two-part model objects. Belotti, F., Deb, P., Manning, W. G., & Norton, E. C. (2015) <doi:10.1177/1536867X1501500102>.",
    "version": "0.1.0",
    "maintainer": "Yajie Duan <yajieritaduan@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25254,
    "package_name": "uGMAR",
    "title": "Estimate Univariate Gaussian and Student's t Mixture\nAutoregressive Models",
    "description": "Maximum likelihood estimation of univariate Gaussian Mixture Autoregressive (GMAR),\n    Student's t Mixture Autoregressive (StMAR), and Gaussian and Student's t Mixture Autoregressive (G-StMAR) models, \n    quantile residual tests, graphical diagnostics, forecast and simulate from GMAR, StMAR and G-StMAR processes. \n    Leena Kalliovirta, Mika Meitz, Pentti Saikkonen (2015) <doi:10.1111/jtsa.12108>, \n    Mika Meitz, Daniel Preve, Pentti Saikkonen (2023) <doi:10.1080/03610926.2021.1916531>,\n    Savi Virolainen (2022) <doi:10.1515/snde-2020-0060>.",
    "version": "3.6.0",
    "maintainer": "Savi Virolainen <savi.virolainen@helsinki.fi>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25261,
    "package_name": "ubms",
    "title": "Bayesian Models for Data from Unmarked Animals using 'Stan'",
    "description": "Fit Bayesian hierarchical models of animal abundance and occurrence\n    via the 'rstan' package, the R interface to the 'Stan' C++ library.\n    Supported models include single-season occupancy, dynamic occupancy, and\n    N-mixture abundance models. Covariates on model parameters are specified\n    using a formula-based interface similar to package 'unmarked', while also\n    allowing for estimation of random slope and intercept terms. References:\n    Carpenter et al. (2017) <doi:10.18637/jss.v076.i01>;\n    Fiske and Chandler (2011) <doi:10.18637/jss.v043.i10>.",
    "version": "1.2.8",
    "maintainer": "Ken Kellner <contact@kenkellner.com>",
    "url": "https://ecoverseR.github.io/ubms/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25269,
    "package_name": "ufRisk",
    "title": "Risk Measure Calculation in Financial TS",
    "description": "Enables the user to calculate Value at Risk (VaR) and Expected \n    Shortfall (ES) by means of various parametric and semiparametric \n    GARCH-type models. For the latter the estimation of the nonparametric scale\n    function is carried out by means of a data-driven smoothing approach. Model\n    quality, in terms of forecasting VaR and ES, can be assessed by means of \n    various backtesting methods such as the traffic light test for VaR and a \n    newly developed traffic light test for ES. The approaches implemented in \n    this package are described in e.g. Feng Y., Beran J., Letmathe S. and \n    Ghosh S. (2020) <https://ideas.repec.org/p/pdn/ciepap/137.html> as well as \n    Letmathe S., Feng Y. and Uhde A. (2021) \n    <https://ideas.repec.org/p/pdn/ciepap/141.html>. ",
    "version": "1.0.7",
    "maintainer": "Sebastian Letmathe <sebastian.letmathe@uni-paderborn.de>",
    "url": "https://wiwi.uni-paderborn.de/en/dep4/feng/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25271,
    "package_name": "ugatsdb",
    "title": "Uganda Time Series Database API",
    "description": "An R API providing easy access to a relational database with macroeconomic, \n             financial and development related time series data for Uganda. \n             Overall more than 5000 series at varying frequency (daily, monthly, \n             quarterly, annual in fiscal or calendar years) can be accessed through \n             the API. The data is provided by the Bank of Uganda, \n             the Ugandan Ministry of Finance, Planning and Economic Development,\n             the IMF and the World Bank. The database is being updated once a month. ",
    "version": "0.2.3",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "url": "https://mepd.finance.go.ug/apps.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25272,
    "package_name": "ugomquantreg",
    "title": "Quantile Regression Modeling for Unit-Gompertz Responses",
    "description": "Unit-Gompertz density, cumulative distribution, quantile functions and random deviate\n   \tgeneration of the unit-Gompertz distribution. In addition, there are a function for fitting the \n   \tGeneralized Additive Models for Location, Scale and Shape.",
    "version": "1.0.0",
    "maintainer": "Josmar Mazucheli <jmazucheli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25273,
    "package_name": "ui",
    "title": "Uncertainty Intervals and Sensitivity Analysis for Missing Data",
    "description": "Implements functions to derive uncertainty intervals for (i) regression (linear and probit) parameters when outcome is missing not at random (non-ignorable missingness) introduced in Genbaeck, M., Stanghellini, E., de Luna, X. (2015) <doi:10.1007/s00362-014-0610-x> and Genbaeck, M., Ng, N., Stanghellini, E., de Luna, X. (2018) <doi:10.1007/s10433-017-0448-x>; and (ii) double robust and outcome regression estimators of average causal effects (on the treated) with possibly unobserved confounding introduced in Genbaeck, M., de Luna, X. (2018) <doi:10.1111/biom.13001>.",
    "version": "0.1.1",
    "maintainer": "Minna Genbaeck <minna.genback@umu.se>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25293,
    "package_name": "umx",
    "title": "Structural Equation Modeling and Twin Modeling in R",
    "description": "Quickly create, run, and report structural equation models, and twin models.",
    "version": "4.60.0",
    "maintainer": "Timothy C. Bates <timothy.c.bates@gmail.com>",
    "url": "https://github.com/tbates/umx",
    "exports": [],
    "topics": ["behavior-genetics", "cran", "genetics", "openmx", "psychology", "r", "sem", "statistics", "structural-equation-modeling", "tutorials", "twin-models", "umx"],
    "score": "NA",
    "stars": 48
  },
  {
    "id": 25311,
    "package_name": "uniReg",
    "title": "Unimodal Penalized Spline Regression using B-Splines",
    "description": "Univariate spline regression. It is possible to add the shape constraint of unimodality and predefined or\n\tself-defined penalties on the B-spline coefficients.",
    "version": "1.1",
    "maintainer": "Claudia Koellmann <koellmann@statistik.tu-dortmund.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25312,
    "package_name": "uniah",
    "title": "Unimodal Additive Hazards Model",
    "description": "Nonparametric estimation of a unimodal or U-shape covariate effect under additive hazards model.",
    "version": "1.2",
    "maintainer": "Yunro Chung <yunro.chung@asu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25315,
    "package_name": "unifed",
    "title": "The Unifed Distribution",
    "description": "Probability functions, family for glm() and Stan code for working with the unifed distribution (Quijano Xacur, 2019; <doi:10.1186/s40488-019-0102-6>).",
    "version": "1.1.6",
    "maintainer": "Oscar Alberto Quijano Xacur <oscar.quijano@use.startmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25326,
    "package_name": "unitizer",
    "title": "Interactive R Unit Tests",
    "description": "Simplifies regression tests by comparing objects produced by test\n    code with earlier versions of those same objects.  If objects are unchanged\n    the tests pass, otherwise execution stops with error details.  If in\n    interactive mode, tests can be reviewed through the provided interactive\n    environment.",
    "version": "1.4.23",
    "maintainer": "Brodie Gaslam <brodie.gaslam@yahoo.com>",
    "url": "https://github.com/brodieG/unitizer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25327,
    "package_name": "unitquantreg",
    "title": "Parametric Quantile Regression Models for Bounded Data",
    "description": "A collection of parametric quantile regression models for bounded data. At present, the package provides 13 parametric quantile regression models. It can specify regression structure for any quantile and shape parameters. It also provides several S3 methods to extract information from fitted model, such as residual analysis, prediction, plotting, and model comparison. For more computation efficient the [dpqr]'s, likelihood, score and hessian functions are written in C++. For further details see Mazucheli et. al (2022) <doi:10.1016/j.cmpb.2022.106816>.",
    "version": "0.0.6",
    "maintainer": "André F. B. Menezes <andrefelipemaringa@gmail.com>",
    "url": "https://andrmenezes.github.io/unitquantreg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25334,
    "package_name": "universals",
    "title": "S3 Generics for Bayesian Analyses",
    "description": "Provides S3 generic methods and some default implementations\n    for Bayesian analyses that generate Markov Chain Monte Carlo (MCMC)\n    samples.  The purpose of 'universals' is to reduce package\n    dependencies and conflicts.  The 'nlist' package implements many of\n    the methods for its 'nlist' class.",
    "version": "0.0.5",
    "maintainer": "Joe Thorley <joe@poissonconsulting.ca>",
    "url": "https://poissonconsulting.github.io/universals/,\nhttps://github.com/poissonconsulting/universals",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25340,
    "package_name": "unmconf",
    "title": "Modeling with Unmeasured Confounding",
    "description": "Tools for fitting and assessing Bayesian multilevel regression \n  models that account for unmeasured confounders.",
    "version": "1.0.0",
    "maintainer": "David Kahle <david@kahle.io>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25348,
    "package_name": "unsystation",
    "title": "Stationarity Test Based on Unsystematic Sub-Sampling",
    "description": "Performs a test for second-order stationarity of time series based\n    on unsystematic sub-samples.",
    "version": "0.2.1",
    "maintainer": "Haeran Cho <haeran.cho@bristol.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25353,
    "package_name": "uotm",
    "title": "Uncertainty of Time Series Model Selection Methods",
    "description": "We propose a new procedure, called model uncertainty variance, which can quantify the uncertainty of model selection on Autoregressive Moving Average models. The model uncertainty variance not pay attention to the accuracy of prediction, but focus on model selection uncertainty and providing more information of the model selection results. And to estimate the model measures, we propose an simplify and faster algorithm based on bootstrap method, which is proven to be effective and feasible by Monte-Carlo simulation. At the same time, we also made some optimizations and adjustments to the Model Confidence Bounds algorithm, so that it can be applied to the time series model selection method. The consistency of the algorithm result is also verified by Monte-Carlo simulation. We propose a new procedure, called model uncertainty variance, which can quantify the uncertainty of model selection on Autoregressive Moving Average models. The model uncertainty variance focuses on model selection uncertainty and providing more information of the model selection results. To estimate the model uncertainty variance, we propose an simplified and faster algorithm based on bootstrap method, which is proven to be effective and feasible by Monte-Carlo simulation. At the same time, we also made some optimizations and adjustments to the Model Confidence Bounds algorithm, so that it can be applied to the time series model selection method. The consistency of the algorithm result is also verified by Monte-Carlo simulation. Please see Li,Y., Luo,Y., Ferrari,D., Hu,X. and Qin,Y. (2019) Model Confidence Bounds for Variable Selection. Biometrics, 75:392-403.<DOI:10.1111/biom.13024> for more information.",
    "version": "0.1.6",
    "maintainer": "Heming Deng Developer <dheming@ruc.edu.cn>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25359,
    "package_name": "upset.hp",
    "title": "Generate UpSet Plots of VP and HP Based on the ASV Concept",
    "description": "Using matrix layout to visualize the unique, common, or individual contribution of each predictor (or matrix of predictors) towards explained variation on different models. These contributions were derived from variation partitioning (VP) and hierarchical partitioning (HP), applying the algorithm of \"Lai et al. (2022) Generalizing hierarchical and variation partitioning in multiple regression and canonical analyses using the rdacca.hp R package.Methods in Ecology and Evolution, 13: 782-788 <doi:10.1111/2041-210X.13800>\".",
    "version": "0.0.5",
    "maintainer": "Jiangshan Lai <lai@njfu.edu.cn>",
    "url": "https://github.com/laijiangshan/upset.hp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25368,
    "package_name": "urca",
    "title": "Unit Root and Cointegration Tests for Time Series Data",
    "description": "Unit root and cointegration tests encountered in applied \n econometric analysis are implemented.",
    "version": "1.3-4",
    "maintainer": "Bernhard Pfaff <bernhard@pfaffikus.de>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25374,
    "package_name": "uroot",
    "title": "Unit Root Tests for Seasonal Time Series",
    "description": "Seasonal unit roots and seasonal stability tests.\n    P-values based on response surface regressions are available for both tests.\n    P-values based on bootstrap are available for seasonal unit root tests.",
    "version": "2.1-3",
    "maintainer": "Georgi N. Boshnakov <georgi.boshnakov@manchester.ac.uk>",
    "url": "https://geobosh.github.io/uroot/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25396,
    "package_name": "usl",
    "title": "Analyze System Scalability with the Universal Scalability Law",
    "description": "The Universal Scalability Law (Gunther 2007)\n    <doi:10.1007/978-3-540-31010-5> is a model to predict hardware and\n    software scalability. It uses system capacity as a function of load to\n    forecast the scalability for the system.",
    "version": "3.0.4",
    "maintainer": "Stefan Moeding <stm@moeding.net>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25410,
    "package_name": "utilityFunctionTools",
    "title": "P-Spline Regression for Utility Functions and Derived Measures",
    "description": "Predicts a smooth and continuous (individual) utility function from utility points, and computes measures of intensity for risk and higher-order risk measures (or any other measure computed with user-written function) based on this utility function and its derivatives according to the method introduced in Schneider (2017) <http://hdl.handle.net/21.11130/00-1735-0000-002E-E306-0>.",
    "version": "1.0",
    "maintainer": "Sebastian O. Schneider <sschneider@coll.mpg.de>",
    "url": "https://www.sebastianoschneider.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25412,
    "package_name": "utsf",
    "title": "Univariate Time Series Forecasting",
    "description": "An engine for univariate time series forecasting using\n    different regression models in an autoregressive way. The engine\n    provides an uniform interface for applying the different models. \n    Furthermore, it is extensible so that users can easily apply their\n    own regression models to univariate time series forecasting and \n    benefit from all the features of the engine, such as preprocessings\n    or estimation of forecast accuracy.",
    "version": "1.3.1",
    "maintainer": "Francisco Martinez <fmartin@ujaen.es>",
    "url": "https://github.com/franciscomartinezdelrio/utsf",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25429,
    "package_name": "vagam",
    "title": "Variational Approximations for Generalized Additive Models",
    "description": "Fits generalized additive models (GAMs) using a variational approximations (VA) framework. In brief, the VA framework provides a fully or at least closed to fully tractable lower bound approximation to the marginal likelihood of a GAM when it is parameterized as a mixed model (using penalized splines, say). In doing so, the VA framework aims offers both the stability and natural inference tools available in the mixed model approach to GAMs, while achieving computation times comparable to that of using the penalized likelihood approach to GAMs. See Hui et al. (2018) <doi:10.1080/01621459.2018.1518235>.",
    "version": "1.1",
    "maintainer": "Han Lin Shang <hanlin.shang@anu.edu.au>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25445,
    "package_name": "valorate",
    "title": "Velocity and Accuracy of the LOg-RAnk TEst",
    "description": "The algorithm implemented in this package was\n    designed to quickly estimates the distribution of the \n    log-rank especially for heavy unbalanced groups. VALORATE \n    estimates the null distribution and the p-value of the \n    log-rank test based on a recent formulation. For a given \n    number of alterations that define the size of survival \n    groups, the estimation involves a weighted sum of \n    distributions that are conditional on a co-occurrence term \n    where mutations and events are both present. The estimation \n    of conditional distributions is quite fast allowing the \n    analysis of large datasets in few minutes \n    <https://bioinformatics.mx/index.php/bioinfo-tools/>.",
    "version": "1.0-5",
    "maintainer": "Victor Trevino <vtrevino@itesm.mx>",
    "url": "https://bioinformatics.mx/index.php/bioinfo-tools/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25451,
    "package_name": "valueprhr",
    "title": "Value-Price Analysis with Bayesian and Panel Data Methods",
    "description": "Provides tools for analyzing the relationship between direct\n    prices (based on labor values) and prices of production using Bayesian\n    generalized linear models, panel data methods, partial least squares\n    regression, canonical correlation analysis, and panel vector\n    autoregression. Includes functions for model comparison, out-of-sample\n    validation, and structural break detection. Here, methods use raw accounting data with explicit temporal structure, following Gomez Julian (2023) <doi:10.17605/OSF.IO/7J8KF>\n    and standard econometric techniques for panel data analysis.",
    "version": "0.1.0",
    "maintainer": "Jose Mauricio Gomez Julian <isadore.nabi@pm.me>",
    "url": "https://github.com/isadorenabi/valueprhr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25470,
    "package_name": "vardpoor",
    "title": "Variance Estimation for Sample Surveys by the Ultimate Cluster\nMethod",
    "description": "Generation of domain variables, linearization of several non-linear population statistics (the ratio of two totals, weighted income percentile, relative median income ratio, at-risk-of-poverty rate, at-risk-of-poverty threshold, Gini coefficient, gender pay gap, the aggregate replacement ratio, the relative median income ratio, median income below at-risk-of-poverty gap, income quintile share ratio, relative median at-risk-of-poverty gap), computation of regression residuals in case of weight calibration, variance estimation of sample surveys by the ultimate cluster method (Hansen, Hurwitz and Madow, Sample Survey Methods And Theory, vol. I: Methods and Applications; vol. II: Theory. 1953, New York: John Wiley and Sons), variance estimation for longitudinal, cross-sectional measures and measures of change for single and multistage stage cluster sampling designs (Berger, Y. G., 2015, <doi:10.1111/rssa.12116>). Several other precision measures are derived - standard error, the coefficient of variation, the margin of error, confidence interval, design effect.",
    "version": "0.20.1",
    "maintainer": "Martins Liberts <martins.liberts@csb.gov.lv>",
    "url": "https://csblatvia.github.io/vardpoor/,\nhttps://github.com/CSBLatvia/vardpoor/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25474,
    "package_name": "varian",
    "title": "Variability Analysis in R",
    "description": "Uses a Bayesian model to\n    estimate the variability in a repeated\n    measure outcome and use that as an outcome or a predictor\n    in a second stage model.",
    "version": "0.2.2",
    "maintainer": "Joshua F. Wiley <josh@elkhartgroup.com>",
    "url": "https://github.com/ElkhartGroup/varian",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25483,
    "package_name": "vars",
    "title": "VAR Modelling",
    "description": "Estimation, lag selection, diagnostic testing, forecasting, causality analysis, forecast error variance decomposition and impulse response functions of VAR models and estimation of SVAR and SVEC models.",
    "version": "1.6-1",
    "maintainer": "Bernhard Pfaff <bernhard@pfaffikus.de>",
    "url": "https://www.pfaffikus.de",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25487,
    "package_name": "vasicekreg",
    "title": "Regression Modeling Using Vasicek Distribution",
    "description": "Vasicek density, cumulative distribution, quantile functions and random deviate\n   \tgeneration of Vasicek distribution. In addition, there are two functions for fitting the \n   \tGeneralized Additive Models for Location Scale and Shape introduced by Rigby and \n   \tStasinopoulos (2005, <doi:10.1111/j.1467-9876.2005.00510.x>). Some functions \n   \tare written in C++ using 'Rcpp', developed by Eddelbuettel and Francois (2011, <doi:10.18637/jss.v040.i08>).",
    "version": "1.0.1",
    "maintainer": "Josmar Mazucheli <jmazucheli@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25497,
    "package_name": "vccp",
    "title": "Vine Copula Change Point Detection in Multivariate Time Series",
    "description": "Implements the Vine Copula Change Point (VCCP) methodology for the estimation of the number and location of multiple change points in the vine copula structure of multivariate time series. The method uses vine copulas, various state-of-the-art segmentation methods to identify multiple change points, and a likelihood ratio test or the stationary bootstrap for inference. The vine copulas allow for various forms of dependence between time series including tail, symmetric and asymmetric dependence. The functions have been extensively tested on simulated multivariate time series data and fMRI data. For details on the VCCP methodology, please see Xiong & Cribben (2021).",
    "version": "0.1.1",
    "maintainer": "Xin Xiong <xinxiong@hsph.harvard.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25500,
    "package_name": "vcdExtra",
    "title": "'vcd' Extensions and Additions",
    "description": "Provides additional data sets, methods and documentation to complement the 'vcd' package for Visualizing Categorical Data\n    and the 'gnm' package for Generalized Nonlinear Models.\n\tIn particular, 'vcdExtra' extends mosaic, assoc and sieve plots from 'vcd' to handle 'glm()' and 'gnm()' models and\n\tadds a 3D version in 'mosaic3d'.  Additionally, methods are provided for comparing and visualizing lists of\n\t'glm' and 'loglm' objects. This package is now a support package for the book, \"Discrete Data Analysis with R\" by\n  Michael Friendly and David Meyer.",
    "version": "0.8.7",
    "maintainer": "Michael Friendly <friendly@yorku.ca>",
    "url": "https://friendly.github.io/vcdExtra/,\nhttps://github.com/friendly/vcdExtra",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25508,
    "package_name": "vcrpart",
    "title": "Tree-Based Varying Coefficient Regression for Generalized Linear\nand Ordinal Mixed Models",
    "description": "Recursive partitioning for varying coefficient generalized linear models and ordinal linear mixed models. Special features are coefficient-wise partitioning, non-varying coefficients and partitioning of time-varying variables in longitudinal regression. A description of a part of this package was published by Burgin and Ritschard (2017) <doi:10.18637/jss.v080.i06>.",
    "version": "1.0-7",
    "maintainer": "Reto Burgin <rbuergin@gmx.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25510,
    "package_name": "vctsfr",
    "title": "Visualizing Collections of Time Series Forecasts",
    "description": "A way of visualizing collections of time series and, optionally\n    their future values, forecasts for their future values and prediction\n    intervals for the forecasts. A web-based GUI can be used to display the\n    information in a collection of time series.",
    "version": "0.1.1",
    "maintainer": "Francisco Martinez <fmartin@ujaen.es>",
    "url": "https://github.com/franciscomartinezdelrio/vctsfr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25520,
    "package_name": "vectorwavelet",
    "title": "Vector Wavelet Coherence for Multiple Time Series",
    "description": "New wavelet methodology (vector wavelet coherence) (Oygur, T., Unal, G, 2020 <doi:10.1007/s40435-020-00706-y>) \n  to handle dynamic co-movements of multivariate time series via extending multiple and quadruple wavelet coherence methodologies. \n  This package can be used to perform multiple wavelet coherence, quadruple wavelet coherence, and n-dimensional vector wavelet coherence analyses.",
    "version": "0.1.0",
    "maintainer": "Tunc Oygur <info@tuncoygur.com.tr>",
    "url": "https://github.com/toygur/vectorwavelet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25546,
    "package_name": "verification",
    "title": "Weather Forecast Verification",
    "description": "Utilities for verifying discrete, continuous and probabilistic forecasts, and forecasts expressed as parametric distributions are included.",
    "version": "1.45",
    "maintainer": "Eric Gilleland <eric.gilleland@colostate.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25553,
    "package_name": "vest",
    "title": "automatic feature eng for forecasting",
    "description": "More about what it does (maybe more than one line)",
    "version": "0.1.0",
    "maintainer": "vitor cerqueira <cerqueira.vitormanuel@gmail.com>",
    "url": "https://github.com/vcerqueira/deprecated-vest",
    "exports": [],
    "topics": ["feature-extraction", "time-series"],
    "score": "NA",
    "stars": 18
  },
  {
    "id": 25559,
    "package_name": "vglmer",
    "title": "Variational Inference for Hierarchical Generalized Linear Models",
    "description": "Estimates hierarchical models using variational inference. \n    At present, it can estimate logistic, linear, and negative binomial models. \n    It can accommodate models with an arbitrary number of random effects and \n    requires no integration to estimate. It also provides the ability to improve \n    the quality of the approximation using marginal augmentation. \n    Goplerud (2022) <doi:10.1214/21-BA1266> and Goplerud (2024) <doi:10.1017/S0003055423000035> \n    provide details on the variational algorithms.",
    "version": "1.0.6",
    "maintainer": "Max Goplerud <mgoplerud@austin.utexas.edu>",
    "url": "https://github.com/mgoplerud/vglmer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25576,
    "package_name": "viking",
    "title": "State-Space Models Inference by Kalman or Viking",
    "description": "Inference methods for state-space models, relying on the Kalman Filter or on Viking (Variational Bayesian VarIance tracKING). See J. de Vilmarest (2022) <https://theses.hal.science/tel-03716104/>.",
    "version": "1.0.2",
    "maintainer": "Joseph de Vilmarest <joseph.de-vilmarest@vikingconseil.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25582,
    "package_name": "vinereg",
    "title": "D-Vine Quantile Regression",
    "description": "\n  Implements D-vine quantile regression models with\n  parametric or nonparametric pair-copulas. See \n  Kraus and Czado (2017) <doi:10.1016/j.csda.2016.12.009> and\n  Schallhorn et al. (2017) <doi:10.48550/arXiv.1705.08310>.",
    "version": "0.12.1",
    "maintainer": "Thomas Nagler <mail@tnagler.com>",
    "url": "https://tnagler.github.io/vinereg/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25583,
    "package_name": "violinplotter",
    "title": "Plotting and Comparing Means with Violin Plots",
    "description": "Produces violin plots with optional nonparametric (Mann-Whitney test) and parametric (Tukey's honest significant difference) mean comparison and linear regression. This package aims to be a simple and quick visualization tool for comparing means and assessing trends of categorical factors.",
    "version": "3.0.1",
    "maintainer": "Jefferson Paril <jeffersonparil@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25590,
    "package_name": "viralx",
    "title": "Explainers for Regression Models in HIV Research",
    "description": "A dedicated viral-explainer model tool designed to empower researchers in the field of HIV research, particularly in viral load and CD4 (Cluster of Differentiation 4) lymphocytes regression modeling. Drawing inspiration from the 'tidymodels' framework for rigorous model building of Max Kuhn and Hadley Wickham (2020) <https://www.tidymodels.org>, and the 'DALEXtra' tool for explainability by Przemyslaw Biecek (2020) <doi:10.48550/arXiv.2009.13248>. It aims to facilitate interpretable and reproducible research in biostatistics and computational biology for the benefit of understanding HIV dynamics.",
    "version": "1.3.1",
    "maintainer": "Juan Pablo Acuña González <22253567@uagro.mx>",
    "url": "https://github.com/juanv66x/viralx",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25600,
    "package_name": "visStatistics",
    "title": "Automated Selection and Visualisation of Statistical Hypothesis\nTests",
    "description": "Automatically selects\n    and visualises statistical hypothesis tests between two vectors,\n    based on their class, distribution, sample size, and a user-defined\n    confidence level (conf.level). Visual outputs - including box plots, bar charts,\n    regression lines with confidence bands, mosaic plots,\n    residual plots, and Q-Q plots - are annotated with relevant test statistics,\n    assumption checks, and post-hoc analyses where applicable. \n    The algorithmic workflow helps the user focus on the interpretation of test\n    results rather than test selection. It is particularly suited for quick data\n    analysis, e.g., in statistical consulting projects or educational settings. \n    The test selection algorithm proceeds as follows:   \n    Input vectors of class numeric or integer are\n    considered numerical; those of class factor are considered categorical.\n    Assumptions of residual normality and homogeneity of variances are \n    considered met if the corresponding test yields a p-value greater than the \n    significance level alpha = 1 - conf.level. \n    (1) When the response vector is numerical and the\n    predictor vector is categorical, a test of central tendencies is selected.\n    If the categorical predictor has exactly two levels, \n    t.test() is applied when group sizes exceed 30\n    (Lumley et al. (2002) <doi:10.1146/annurev.publhealth.23.100901.140546>). \n    For smaller samples, normality of residuals is tested using shapiro.test(); \n    if met, t.test() is used; otherwise, wilcox.test(). \n    If the predictor is categorical with more than two levels, an\n    aov() is initially fitted. Residual normality is evaluated using both\n    shapiro.test() and ad.test(); residuals are considered approximately normal\n    if at least one test yields a p-value above alpha. If this assumption is met, \n    bartlett.test() assesses variance homogeneity. \n    If variances are homogeneous, aov() is used; otherwise oneway.test(). \n    Both tests are followed by TukeyHSD().\n    If residual normality cannot be assumed, kruskal.test() is followed by \n    pairwise.wilcox.test().\n    (2) When both the response and predictor vectors are numerical, \n    a simple linear regression model is fitted using lm(). \n    (3) When both vectors are categorical, Cochran's rule \n    (Cochran (1954) <doi:10.2307/3001666>)\n    is applied to test independence either by chisq.test() or fisher.test(). ",
    "version": "0.1.7",
    "maintainer": "Sabine Schilling <sabineschilling@gmx.ch>",
    "url": "https://github.com/shhschilling/visStatistics,\nhttps://shhschilling.github.io/visStatistics/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25611,
    "package_name": "visreg",
    "title": "Visualization of Regression Models",
    "description": "Provides a convenient interface for constructing plots to\n    visualize the fit of regression models arising from a wide variety\n    of models in R ('lm', 'glm', 'coxph', 'rlm', 'gam', 'locfit', 'lmer',\n    'randomForest', etc.)",
    "version": "2.8.0",
    "maintainer": "Patrick Breheny <patrick-breheny@uiowa.edu>",
    "url": "https://pbreheny.github.io/visreg/,\nhttps://github.com/pbreheny/visreg",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25625,
    "package_name": "vital",
    "title": "Tidy Analysis Tools for Mortality, Fertility, Migration and\nPopulation Data",
    "description": "Analysing vital statistics based on tools\n    consistent with the tidyverse. Tools are provided for data visualization,\n    life table calculations, computing net migration numbers, Lee-Carter\n    modelling; functional data modelling and forecasting.",
    "version": "2.0.1",
    "maintainer": "Rob Hyndman <Rob.Hyndman@monash.edu>",
    "url": "https://pkg.robjhyndman.com/vital/,\nhttps://github.com/robjhyndman/vital",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25626,
    "package_name": "vitality",
    "title": "Fitting Routines for the Vitality Family of Mortality Models",
    "description": "Provides fitting routines for four versions of the\n        Vitality family of mortality models.",
    "version": "1.3",
    "maintainer": "David J. Sharrow <dsharrow@uw.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25646,
    "package_name": "voigt",
    "title": "The Voigt Distribution",
    "description": "Random generation, density function and parameter estimation for the Voigt distribution. The main objective of this package is to provide R users with efficient estimation of Voigt parameters using classic iid data in a Bayesian framework. The estimating function allows flexible prior specification, specification of fixed parameters and several options for Markov Chain Monte Carlo posterior simulation. A basic version of the algorithm is described in: Cannas M. and Piras, N. (2025) <doi:10.1007/978-3-031-96303-2_53>.",
    "version": "2.0",
    "maintainer": "Massimo Cannas <massimo.cannas@unica.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25675,
    "package_name": "vse4ts",
    "title": "Identify Memory Patterns in Time Series Using Variance Scale\nExponent",
    "description": "Methods for calculating the variance scale exponent to\n    identify memory patterns in time series data. Includes tests for white\n    noise, short memory, and long memory. See Fu, H. et al. (2018)\n    <doi:10.1016/j.physa.2018.06.092>.",
    "version": "1.0.0",
    "maintainer": "Mengyang Zheng <mengyang.zheng@outlook.com>",
    "url": "https://z-my-cn.github.io/vse4ts/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25733,
    "package_name": "washeR",
    "title": "Time Series Outlier Detection",
    "description": "Time series outlier detection with non parametric test. This is a new outlier detection methodology (washer): efficient for time saving elaboration and implementation procedures, adaptable for general assumptions and for needing very short time series, reliable and effective as involving robust non parametric test. You can find two approaches: single time series (a vector) and grouped time series (a data frame). For other informations: Andrea Venturini (2011) Statistica - Universita di Bologna, Vol.71, pp.329-344. For an informal explanation look at R-bloggers on web.",
    "version": "0.1.3",
    "maintainer": "Andrea Venturini <andrea.venturini@bancaditalia.it>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25738,
    "package_name": "waspr",
    "title": "Wasserstein Barycenters of Subset Posteriors",
    "description": "Functions to compute Wasserstein barycenters\n    of subset posteriors using the swapping algorithm developed by Puccetti, \n    Rüschendorf and Vanduffel (2020) <doi:10.1016/j.jmaa.2017.02.003>. The \n    Wasserstein barycenter is a geometric approach for combining subset \n    posteriors. It allows for parallel and distributed computation of the \n    posterior in case of complex models and/or big datasets, thereby increasing\n    computational speed tremendously.",
    "version": "1.0.1",
    "maintainer": "Jolien Cremers <joliencremers@gmail.com>",
    "url": "https://github.com/joliencremers/waspr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25748,
    "package_name": "wavScalogram",
    "title": "Wavelet Scalogram Tools for Time Series Analysis",
    "description": "Provides scalogram based wavelet tools for time series analysis: wavelet power spectrum, scalogram, windowed scalogram, windowed scalogram difference (see Bolos et al. (2017) <doi:10.1016/j.amc.2017.05.046>), scale index and windowed scale index (Benitez et al. (2010) <doi:10.1016/j.camwa.2010.05.010>).",
    "version": "1.1.3",
    "maintainer": "Vicente J. Bolos <vicente.bolos@uv.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25749,
    "package_name": "waveband",
    "title": "Computes Credible Intervals for Bayesian Wavelet Shrinkage",
    "description": "Computes Bayesian wavelet shrinkage credible intervals for\n\tnonparametric regression.\n\tThe method uses cumulants to derive Bayesian credible intervals for\n\twavelet regression estimates.\n\tThe first four cumulants of the posterior distribution of the\n\testimates are expressed in terms of the observed data and integer\n\tpowers of the mother wavelet functions.\n\tThese powers are closely approximated by linear combinations of\n\twavelet scaling functions at an appropriate finer scale.\n\tHence, a suitable modification of the discrete wavelet transform allows\n\tthe posterior cumulants to be found efficiently for any data set.\n\tJohnson transformations then yield the credible intervals themselves.\n\tBarber, S., Nason, G.P. and Silverman, B.W. (2002)\n\t<doi:10.1111/1467-9868.00332>.",
    "version": "4.7.4",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25751,
    "package_name": "wavelets",
    "title": "Functions for Computing Wavelet Filters, Wavelet Transforms and\nMultiresolution Analyses",
    "description": "Contains functions for computing and plotting\n        discrete wavelet transforms (DWT) and maximal overlap discrete\n        wavelet transforms (MODWT), as well as their inverses.\n        Additionally, it contains functionality for computing and\n        plotting wavelet transform filters that are used in the above\n        decompositions as well as multiresolution analyses.",
    "version": "0.3-0.2",
    "maintainer": "Eric Aldrich <ealdrich@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25752,
    "package_name": "wavemulcor",
    "title": "Wavelet Routines for Global and Local Multiple Regression and\nCorrelation",
    "description": "Wavelet routines that calculate single sets of wavelet\n    multiple regressions and correlations, and cross-regressions and\n    cross-correlations from a multivariate time series.  Dynamic versions\n    of the routines allow the wavelet local multiple (cross-)regressions\n    and (cross-)correlations to evolve over time.",
    "version": "3.1.2",
    "maintainer": "Javier Fernandez-Macho <javier.fernandezmacho@ehu.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25755,
    "package_name": "waveslim",
    "title": "Basic Wavelet Routines for One-, Two-, and Three-Dimensional\nSignal Processing",
    "description": "Basic wavelet routines for time series (1D), image (2D) and array \n  (3D) analysis.  The code provided here is based on wavelet methodology \n  developed in Percival and Walden (2000); Gencay, Selcuk and Whitcher (2001); \n  the dual-tree complex wavelet transform (DTCWT) from Kingsbury (1999, 2001) as\n  implemented by Selesnick; and Hilbert wavelet pairs (Selesnick 2001, 2002).  \n  All figures in chapters 4-7 of GSW (2001) are reproducible using this package \n  and R code available at the book website(s) below.",
    "version": "1.8.5",
    "maintainer": "Brandon Whitcher <bwhitcher@gmail.com>",
    "url": "https://waveslim.blogspot.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25756,
    "package_name": "wavethresh",
    "title": "Wavelets Statistics and Transforms",
    "description": "Performs 1, 2 and 3D real and complex-valued wavelet transforms,\n\tnondecimated transforms, wavelet packet transforms, nondecimated\n\twavelet packet transforms, multiple wavelet transforms,\n\tcomplex-valued wavelet transforms, wavelet shrinkage for\n\tvarious kinds of data, locally stationary wavelet time series,\n\tnonstationary multiscale transfer function modeling, density\n\testimation.",
    "version": "4.7.3",
    "maintainer": "Guy Nason <g.nason@imperial.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25760,
    "package_name": "wbacon",
    "title": "Weighted BACON Algorithms",
    "description": "The BACON algorithms are methods for multivariate outlier\n    nomination (detection) and robust linear regression by Billor, Hadi,\n    and Velleman (2000) <doi:10.1016/S0167-9473(99)00101-2>. The extension\n    to weighted problems is due to Beguin and Hulliger (2008)\n    <https://www150.statcan.gc.ca/n1/en/catalogue/12-001-X200800110616>; see\n    also <doi:10.21105/joss.03238>.",
    "version": "0.6-3",
    "maintainer": "Tobias Schoch <tobias.schoch@gmail.com>",
    "url": "https://github.com/tobiasschoch/wbacon",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25765,
    "package_name": "wbsts",
    "title": "Multiple Change-Point Detection for Nonstationary Time Series",
    "description": "Implements detection for the number and locations of\n    the change-points in a time series using the Wild Binary Segmentation and\n    the Locally Stationary Wavelet model of Korkas and Fryzlewicz (2017) <doi:10.5705/ss.202015.0262>.",
    "version": "2.1",
    "maintainer": "Karolos Korkas <kkorkas@yahoo.co.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25768,
    "package_name": "wcep",
    "title": "Survival Analysis for Weighted Composite Endpoints",
    "description": "Analyze given data frame with multiple endpoints and return Kaplan-Meier survival probabilities together with the specified confidence interval. See Nabipoor M, Westerhout CM, Rathwell S, and Bakal JA (2023) <doi:10.1186/s12874-023-01857-0>.",
    "version": "1.0.3",
    "maintainer": "Sarah Rathwell <srathwel@ualberta.ca>",
    "url": "https://github.com/sarah-0k/wcep",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25770,
    "package_name": "wcox",
    "title": "Weights to Correct for Outcome Dependent Sampling in Time to\nEvent Data",
    "description": "A new inverse probability of selection weighted Cox model to deal with outcome-dependent sampling in survival analysis.",
    "version": "1.0",
    "maintainer": "Vera Arntzen <v.h.arntzen@math.leidenuniv.nl>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25779,
    "package_name": "weakARMA",
    "title": "Tools for the Analysis of Weak ARMA Models",
    "description": "Numerous time series admit autoregressive moving average (ARMA)\n  representations, in which the errors are uncorrelated but not necessarily\n  independent.\n  These models are called weak ARMA by opposition to the standard ARMA models, \n  also called strong ARMA models, in which the error terms are supposed to be \n  independent and identically distributed (iid).\n  This package allows the study of nonlinear time series models through weak \n  ARMA representations.\n  It determines identification, estimation and validation for ARMA models and \n  for AR and MA models in particular. \n  Functions can also be used in the strong case.\n  This package also works on white noises by omitting arguments 'p', 'q', 'ar'\n  and 'ma'.\n  See Francq, C. and Zakoïan, J. (1998) <doi:10.1016/S0378-3758(97)00139-0> and \n  Boubacar Maïnassara, Y. and Saussereau, B. (2018)\n  <doi:10.1080/01621459.2017.1380030> for more details.",
    "version": "1.0.3",
    "maintainer": "Julien Yves Rolland <julien.rolland@univ-fcomte.fr>",
    "url": "https://plmlab.math.cnrs.fr/jrolland/weakARMA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25781,
    "package_name": "weathR",
    "title": "Interact with the U.S. National Weather Service API",
    "description": "Enables interaction with the National Weather Service application programming web-interface for fetching of real-time and forecast meteorological data. Users can provide latitude and longitude, Automated Surface Observing System identifier, or Automated Weather Observing System identifier to fetch recent weather observations and recent forecasts for the given location or station. Additionally, auxiliary functions exist to identify stations nearest to a point, convert wind direction from character to degrees, and fetch active warnings. Results are returned as simple feature objects whenever possible.",
    "version": "0.1.0",
    "maintainer": "Jeffrey Fowler <JeffreyF6120@gmail.com>",
    "url": "https://github.com/JeffreyFowler/weathR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25787,
    "package_name": "webSDM",
    "title": "Including Known Interactions in Species Distribution Models",
    "description": "A collection of tools to fit and work with trophic Species Distribution Models. Trophic Species Distribution Models combine knowledge of trophic interactions with Bayesian structural equation models that model each species as a function of its prey (or predators) and environmental conditions. It exploits the topological ordering of the known trophic interaction network to predict species distribution in space and/or time, where the prey (or predator) distribution is unavailable. The method implemented by the package is described in Poggiato, Andréoletti, Pollock and Thuiller (2022) <doi:10.22541/au.166853394.45823739/v1>.",
    "version": "1.1-5",
    "maintainer": "Giovanni Poggiato <giov.poggiato@gmail.com>",
    "url": "https://github.com/giopogg/webSDM,\nhttps://giopogg.github.io/webSDM/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25808,
    "package_name": "wec",
    "title": "Weighted Effect Coding",
    "description": "Provides functions to create factor variables with contrasts based on weighted effect coding, and their interactions. In weighted effect coding the estimates from a first order regression model show the deviations per group from the sample mean. This is especially useful when a researcher has no directional hypotheses and uses a sample from a population in which the number of observation per group is different.",
    "version": "0.4-1",
    "maintainer": "Rense Nieuwenhuis <rense.nieuwenhuis@sofi.su.se>",
    "url": "http://www.ru.nl/sociology/mt/wec/downloads/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25812,
    "package_name": "weibulltools",
    "title": "Statistical Methods for Life Data Analysis",
    "description": "Provides statistical methods and visualizations that are often \n             used in reliability engineering. Comprises a compact and easily \n             accessible set of methods and visualization tools that make the \n             examination and adjustment as well as the analysis and interpretation \n             of field data (and bench tests) as simple as possible.\n             Non-parametric estimators like Median Ranks, \n             Kaplan-Meier (Abernethy, 2006, <ISBN:978-0-9653062-3-2>), \n             Johnson (Johnson, 1964, <ISBN:978-0444403223>), and Nelson-Aalen \n             for failure probability estimation within samples that contain \n             failures as well as censored data are included.   \n             The package supports methods like Maximum Likelihood and Rank Regression, \n             (Genschel and Meeker, 2010, <DOI:10.1080/08982112.2010.503447>) \n             for the estimation of multiple parametric lifetime distributions,  \n             as well as the computation of confidence intervals of quantiles and \n             probabilities using the delta method related to Fisher's confidence \n             intervals (Meeker and Escobar, 1998, <ISBN:9780471673279>) and the \n             beta-binomial confidence bounds. \n             If desired, mixture model analysis can be done with segmented regression\n             and the EM algorithm.\n             Besides the well-known Weibull analysis, the package also contains \n             Monte Carlo methods for the correction and completion of imprecisely \n             recorded or unknown lifetime characteristics.\n             (Verband der Automobilindustrie e.V. (VDA), 2016, <ISSN:0943-9412>). \n             Plots are created statically ('ggplot2') or interactively ('plotly') and \n             can be customized with functions of the respective visualization package.\n             The graphical technique of probability plotting as well as the addition \n             of regression lines and confidence bounds to existing plots are \n             supported. ",
    "version": "2.1.0",
    "maintainer": "Tim-Gunnar Hensel <tim-gunnar.hensel@tu-berlin.de>",
    "url": "https://tim-tu.github.io/weibulltools/,\nhttps://github.com/Tim-TU/weibulltools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25813,
    "package_name": "weightQuant",
    "title": "Weights for Incomplete Longitudinal Data and Quantile Regression",
    "description": "Estimation of observation-specific weights for incomplete longitudinal data and bootstrap procedure for weighted quantile regressions. See Jacqmin-Gadda, Rouanet, Mba, Philipps, Dartigues (2020) for details <doi:10.1177/0962280220909986>.",
    "version": "1.0.1",
    "maintainer": "Viviane Philipps <Viviane.Philipps@u-bordeaux.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25814,
    "package_name": "weightedCL",
    "title": "Efficient and Feasible Inference for High-Dimensional Normal\nCopula Regression Models",
    "description": "Estimates high-dimensional multivariate normal copula regression models with the weighted composite likelihood estimating equations in Nikoloulopoulos (2023) <doi:10.1016/j.csda.2022.107654>. It provides autoregressive moving average correlation structures and binary, ordinal, Poisson, and negative binomial regressions.",
    "version": "0.7",
    "maintainer": "Aristidis K. Nikoloulopoulos <a.nikoloulopoulos@uea.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25815,
    "package_name": "weightedGCM",
    "title": "Weighted Generalised Covariance Measure Conditional Independence\nTest",
    "description": "A conditional independence test that can be applied both to\n    univariate and multivariate random variables. The test is based on a\n    weighted form of the sample covariance of the residuals after a\n    nonlinear regression on the conditioning variables. Details are\n    described in Scheidegger, Hoerrmann and Buehlmann (2022) \"The Weighted\n    Generalised Covariance Measure\" <http://jmlr.org/papers/v23/21-1328.html>.\n    The test is a generalisation of the Generalised Covariance Measure (GCM)\n    implemented in the R package 'GeneralisedCovarianceMeasure' by Jonas Peters and\n    Rajen D. Shah based on Shah and Peters (2020) \"The Hardness of\n    Conditional Independence Testing and the Generalised Covariance\n    Measure\" <doi:10.1214/19-AOS1857>.",
    "version": "0.1.1",
    "maintainer": "Cyrill Scheidegger <cyrill.scheidegger@stat.math.ethz.ch>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25817,
    "package_name": "weightedScores",
    "title": "Weighted Scores Method for Regression Models with Dependent Data",
    "description": "The weighted scores method and composite likelihood information criteria as an intermediate step for variable/correlation selection for longitudinal ordinal and count data in Nikoloulopoulos, Joe and Chaganty (2011)  <doi:10.1093/biostatistics/kxr005>, Nikoloulopoulos (2016) <doi:10.1002/sim.6871> and Nikoloulopoulos (2017) <arXiv:1510.07376>.",
    "version": "0.9.5.3",
    "maintainer": "Aristidis K. Nikoloulopoulos <a.nikoloulopoulos@uea.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25818,
    "package_name": "weightedsurv",
    "title": "Survival Analysis with Subject-Specific (Case Weights) and\nTime-Dependent Weighting",
    "description": "Provides survival analysis functions with support for time-dependent \n    and subject-specific (e.g., propensity score) weighting. Implements weighted \n    estimation for Cox models, Kaplan-Meier survival curves, and treatment \n    differences with point-wise and simultaneous confidence bands. Includes \n    restricted mean survival time (RMST) comparisons evaluated across all \n    potential truncation times with both point-wise and simultaneous confidence \n    bands. See Cole, S. R. & Hernán, M. A. (2004) \n    <doi:10.1016/j.cmpb.2003.10.004> for methodological background.",
    "version": "0.1.0",
    "maintainer": "Larry Leon <larry.leon.05@post.harvard.edu>",
    "url": "https://github.com/larry-leon/weightedsurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25821,
    "package_name": "weights",
    "title": "Weighting and Weighted Statistics",
    "description": "Provides a variety of functions for producing simple weighted statistics, such as weighted Pearson's correlations, partial correlations, Chi-Squared statistics, histograms, and t-tests as well as simple weighting graphics including weighted histograms, box plots, bar plots, and violin plots.  Also includes software for quickly recoding survey data and plotting estimates from interaction terms in regressions (and multiply imputed regressions) both with and without weights and summarizing various types of regressions. Some portions of this package were assisted by AI-generated suggestions using OpenAI's GPT model, with human review and integration.",
    "version": "1.1.2",
    "maintainer": "Josh Pasek <josh@joshpasek.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25825,
    "package_name": "welchADF",
    "title": "Welch-James Statistic for Robust Hypothesis Testing under\nHeterocedasticity and Non-Normality",
    "description": "Implementation of Johansen's general formulation of Welch-James's statistic with Approximate Degrees of Freedom, which makes it suitable for testing \n    any linear hypothesis concerning cell means in univariate and multivariate mixed model designs when the data pose non-normality and non-homogeneous variance. Some \n    improvements, namely trimmed means and Winsorized variances, and bootstrapping for calculating an empirical critical value, have been added to the classical formulation. \n    The code departs from a previous SAS implementation by L.M. Lix and H.J. Keselman, available at <http://supp.apa.org/psycarticles/supplemental/met_13_2_110/SAS_Program.pdf> and\n    published in Keselman, H.J., Wilcox, R.R., and Lix, L.M. (2003) <DOI:10.1111/1469-8986.00060>.",
    "version": "0.3.2",
    "maintainer": "Pablo J. Villacorta <pjvi@decsai.ugr.es>",
    "url": "<http://decsai.ugr.es/~pjvi/r-packages.html>",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25829,
    "package_name": "wevid",
    "title": "Quantifying Performance of a Binary Classifier Through Weight of\nEvidence",
    "description": "The distributions of the weight of evidence (log Bayes factor) favouring case over noncase status in a test dataset (or test folds generated by cross-validation) can be used to quantify the performance of a diagnostic test (McKeigue (2019), <doi:10.1177/0962280218776989>). The package can be used with any test dataset on which you have observed case-control status and have computed prior and posterior probabilities of case status using a model learned on a training dataset. To quantify how the predictor will behave as a risk stratifier, the quantiles of the distributions of weight of evidence in cases and controls can be calculated and plotted.",
    "version": "0.6.2",
    "maintainer": "Marco Colombo <mar.colombo13@gmail.com>",
    "url": "http://www.homepages.ed.ac.uk/pmckeigu/preprints/classify/wevidtutorial.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25830,
    "package_name": "wex",
    "title": "Compute the Exact Observation Weights for the Kalman Filter and\nSmoother",
    "description": "Computes the exact observation weights for the Kalman filter and smoother, based on the method described in Koopman and Harvey (2003) <www.sciencedirect.com/science/article/pii/S0165188902000611>.\n    The package supports in-depth exploration of state-space models, enabling researchers and practitioners to extract meaningful insights from time series data.\n    This functionality is especially valuable in dynamic factor models, where the computed weights can be used to decompose the contributions of individual variables to the latent factors.\n    See the README file for examples.",
    "version": "0.1.0",
    "maintainer": "Tim Ginker <timginker@gmail.com>",
    "url": "https://github.com/timginker/wex",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25832,
    "package_name": "wfe",
    "title": "Weighted Linear Fixed Effects Regression Models for Causal\nInference",
    "description": "Provides a computationally efficient way of fitting\n\t     weighted linear fixed effects estimators for causal\n\t     inference with various weighting schemes. Weighted linear\n\t     fixed effects estimators can be used to estimate the\n\t     average treatment effects under different identification\n\t     strategies. This includes stratified randomized\n\t     experiments, matching and stratification for\n\t     observational studies, first differencing, and\n\t     difference-in-differences. The package implements methods\n\t     described in Imai and Kim (2017) \"When should We Use\n\t     Linear Fixed Effects Regression Models for Causal\n\t     Inference with Longitudinal Data?\", available at\n\t     <https://imai.fas.harvard.edu/research/FEmatch.html>.",
    "version": "1.9.1",
    "maintainer": "In Song Kim <insong@mit.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25856,
    "package_name": "whitestrap",
    "title": "White Test and Bootstrapped White Test for Heteroskedasticity",
    "description": "Formal implementation of White test of heteroskedasticity and a bootstrapped version of it, developed under the methodology of Jeong, J., Lee, K. (1999) <https://yonsei.pure.elsevier.com/en/publications/bootstrapped-whites-test-for-heteroskedasticity-in-regression-mod>.",
    "version": "0.0.1",
    "maintainer": "Jorge Lopez Perez <jorge@loperez.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25861,
    "package_name": "widals",
    "title": "Weighting by Inverse Distance with Adaptive Least Squares",
    "description": "Computationally easy modeling, interpolation, forecasting of massive temporal-spacial data.",
    "version": "0.6.2",
    "maintainer": "Dave Zes <zesdave@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25872,
    "package_name": "wikipediatrend",
    "title": "Public Subject Attention via Wikipedia Page View Statistics",
    "description": "A convenience wrapper for the Wikipedia page access statistics API",
    "version": "2.1.6",
    "maintainer": "Peter Meissner <retep.meissner@gmail.com>",
    "url": "https://github.com/petermeissner/wikipediatrend",
    "exports": [],
    "topics": ["access-statistics", "data-frame", "r", "statistics", "wikipedia-page"],
    "score": "NA",
    "stars": 77
  },
  {
    "id": 25879,
    "package_name": "wildmeta",
    "title": "Cluster Wild Bootstrapping for Meta-Analysis",
    "description": "Conducts single coefficient tests and multiple-contrast hypothesis tests of meta-regression models using cluster wild bootstrapping, based on methods examined in Joshi, Pustejovsky, and Beretvas (2022) <DOI:10.1002/jrsm.1554>. ",
    "version": "0.3.2",
    "maintainer": "Megha Joshi <megha.j456@gmail.com>",
    "url": "https://meghapsimatrix.github.io/wildmeta/index.html",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25899,
    "package_name": "wktmo",
    "title": "Converting Weekly Data to Monthly Data",
    "description": "Converts weekly data to monthly data.\n     Users can use three types of week formats: ISO week, epidemiology week (epi week) and calendar date. ",
    "version": "1.0.5",
    "maintainer": "You Li <You.Li2@ed.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25933,
    "package_name": "worldbank",
    "title": "Client for World Banks's 'Indicators' and 'Poverty and\nInequality Platform (PIP)' APIs",
    "description": "Download and search data from the 'World Bank Indicators\n    API', which provides access to nearly 16,000 time series indicators.\n    See\n    <https://datahelpdesk.worldbank.org/knowledgebase/articles/889392-about-the-indicators-api-documentation>\n    for further details about the API.",
    "version": "0.7.1",
    "maintainer": "Maximilian Mücke <muecke.maximilian@gmail.com>",
    "url": "https://m-muecke.github.io/worldbank/,\nhttps://github.com/m-muecke/worldbank",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25952,
    "package_name": "wqc",
    "title": "Wavelet Quantile Correlation Analysis",
    "description": "Estimate and plot wavelet quantile correlations(Kumar and Padakandla,2022) between two time series. Wavelet quantile correlation is used to capture the dependency between two time series across quantiles and different frequencies. This method is useful in identifying potential hedges and safe-haven instruments for investment purposes. See Kumar and Padakandla(2022) <doi:10.1016/j.frl.2022.102707> for further details.",
    "version": "0.1.2",
    "maintainer": "Anoop S Kumar <akumar.sasikumar@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25953,
    "package_name": "wql",
    "title": "Exploring Water Quality Monitoring Data",
    "description": "Functions to assist in the processing and\n    exploration of data from environmental monitoring programs.\n    The package name stands for \"water quality\" and reflects the\n    original focus on time series data for physical and chemical\n    properties of water, as well as the biota. Intended for\n    programs that sample approximately monthly, quarterly or\n    annually at discrete stations, a feature of many legacy data\n    sets. Most of the functions should be useful for analysis of\n    similar-frequency time series regardless of the subject\n    matter.",
    "version": "1.0.3",
    "maintainer": "Jemma Stachelek <jemma.stachelek@gmail.com>",
    "url": "https://github.com/jsta/wql",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25971,
    "package_name": "wsbackfit",
    "title": "Weighted Smooth Backfitting for Structured Models",
    "description": "Non- and semiparametric regression for generalized additive, partial linear, and varying coefficient models as well as their combinations via smoothed backfitting. Based on Roca-Pardinas J and Sperlich S (2010) <doi:10.1007/s11222-009-9130-2>; Mammen E, Linton O and Nielsen J (1999) <doi:10.1214/aos/1017939138>; Lee YK, Mammen E, Park BU (2012) <doi:10.1214/12-AOS1026>.",
    "version": "1.0-5",
    "maintainer": "Javier Roca-Pardinas <roca@uvigo.es>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25980,
    "package_name": "wwntests",
    "title": "Hypothesis Tests for Functional Time Series",
    "description": "Provides a collection of white noise hypothesis tests for functional time series and related visualizations. \n  These include tests based on the norms of autocovariance operators that are built under both strong and weak \n  white noise assumptions. Additionally, tests based on the spectral density operator and on principal component\n  dimensional reduction are included, which are built under strong white noise assumptions. \n  Also, this package provides goodness-of-fit tests for functional autoregressive of order 1 models.\n  These methods are described in Kokoszka et al. (2017) <doi:10.1016/j.jmva.2017.08.004>, Characiejus and Rice (2019) \n  <doi:10.1016/j.ecosta.2019.01.003>, Gabrys and Kokoszka (2007) <doi:10.1198/016214507000001111>, \n  and Kim et al. (2023) <doi: 10.1214/23-SS143>\n  respectively.",
    "version": "1.1.0",
    "maintainer": "Mihyun Kim <mihyun.kim@mail.wvu.edu>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25981,
    "package_name": "wxgenR",
    "title": "A Stochastic Weather Generator with Seasonality",
    "description": "A weather generator to simulate precipitation and temperature for regions with seasonality. Users input training data containing precipitation, temperature, and seasonality (up to 26 seasons). Including weather season as a training variable allows users to explore the effects of potential changes in season duration as well as average start- and end-time dates due to phenomena like climate change. Data for training should be a single time series but can originate from station data, basin averages, grid cells, etc.\n    Bearup, L., Gangopadhyay, S., & Mikkelson, K. (2021). \"Hydroclimate Analysis Lower Santa Cruz River Basin Study (Technical Memorandum No ENV-2020-056).\" Bureau of Reclamation.\n    Gangopadhyay, S., Bearup, L. A., Verdin, A., Pruitt, T., Halper, E., & Shamir, E. (2019, December 1). \"A collaborative stochastic weather generator for climate impacts assessment in the Lower Santa Cruz River Basin, Arizona.\" Fall Meeting 2019, American Geophysical Union. <https://ui.adsabs.harvard.edu/abs/2019AGUFMGC41G1267G>.",
    "version": "1.4.4",
    "maintainer": "David Woodson <dwoodson@usbr.gov>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25982,
    "package_name": "wyz.code.metaTesting",
    "title": "Wizardry Code Meta Testing",
    "description": "Meta testing is the ability to test a function without having to \n    provide its parameter values.\n    Those values will be generated, based on semantic naming of parameters, as \n    introduced by package 'wyz.code.offensiveProgramming'.\n    Value generation logic can be completed with your own data types \n    and generation schemes. This to meet your most specific requirements and to \n    answer to a wide variety of usages, from general use case to very specific\n    ones.\n    While using meta testing, it becomes easier to generate stress test \n    campaigns, non-regression test campaigns and robustness test campaigns, as \n    generated tests can be saved and reused from session to session. \n    Main benefits of using 'wyz.code.metaTesting' is ability to discover valid \n    and invalid function parameter combinations, ability to infer valid \n    parameter values, and to provide smart summaries that allows you to focus\n    on dysfunctional cases. ",
    "version": "1.1.22",
    "maintainer": "Fabien Gelineau <neonira@gmail.com>",
    "url": "https://neonira.github.io/offensiveProgrammingBook_v1.2.2/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25986,
    "package_name": "x12",
    "title": "Interface to 'X12-ARIMA'/'X13-ARIMA-SEATS' and Structure for\nBatch Processing of Seasonal Adjustment",
    "description": "The 'X13-ARIMA-SEATS' <https://www.census.gov/data/software/x13as.html> methodology and software is a widely used software and developed by the US Census Bureau. It can be accessed from 'R' with this package and 'X13-ARIMA-SEATS' binaries are provided by the 'R' package 'x13binary'.",
    "version": "1.11.0",
    "maintainer": "Alexander Kowarik <alexander.kowarik@statistik.gv.at>",
    "url": "https://github.com/statistikat/x12",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 25987,
    "package_name": "x13binary",
    "title": "Provide the 'x13ashtml' Seasonal Adjustment Binary",
    "description": "The US Census Bureau provides a seasonal adjustment program now\n called 'X-13ARIMA-SEATS' building on both earlier programs called X-11 and\n X-12 as well as the SEATS program by the Bank of Spain. The US Census Bureau\n offers both source and binary versions -- which this package integrates for\n use by other R packages.",
    "version": "1.1.61.1",
    "maintainer": "Dirk Eddelbuettel <edd@debian.org>",
    "url": "https://github.com/x13org/x13binary",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26022,
    "package_name": "xhaz",
    "title": "Excess Hazard Modelling Considering Inappropriate Mortality\nRates",
    "description": "Fits relative survival regression models with or without proportional excess hazards and with the additional possibility to correct for background mortality by one or more parameter(s). These models are relevant when the observed mortality in the studied group is not comparable to that of the general population or in population-based studies where the available life tables used for net survival estimation are insufficiently stratified. In the latter case, the proposed model by Touraine et al. (2020) <doi:10.1177/0962280218823234> can be used. The user can also fit a model that relaxes the proportional expected hazards assumption considered in the Touraine et al. excess hazard model. This extension was proposed by Mba et al. (2020) <doi:10.1186/s12874-020-01139-z> to allow non-proportional effects of the additional variable on the general population mortality. In non-population-based studies, researchers can identify non-comparability source of bias in terms of expected mortality of selected individuals. An excess hazard model correcting this selection bias is presented in Goungounga et al. (2019) <doi:10.1186/s12874-019-0747-3>. This class of model with a random effect at the cluster level on excess hazard is presented in Goungounga et al. (2023) <doi:10.1002/bimj.202100210>.   ",
    "version": "2.0.2",
    "maintainer": "Juste Goungounga <juste.goungounga@ehesp.fr>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26044,
    "package_name": "xnet",
    "title": "Two-Step Kernel Ridge Regression for Network Predictions",
    "description": "Fit a two-step kernel ridge regression model for\n        predicting edges in networks, and carry out cross-validation\n        using shortcuts for swift and accurate performance assessment\n        (Stock et al, 2018 <doi:10.1093/bib/bby095> ).",
    "version": "0.1.11",
    "maintainer": "Joris Meys <Joris.Meys@UGent.be>",
    "url": "https://github.com/CenterForStatistics-UGent/xnet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26047,
    "package_name": "xpect",
    "title": "Probabilistic Time Series Forecasting with XGBoost and Conformal\nInference",
    "description": "Implements a probabilistic approach to time series forecasting combining XGBoost regression with conformal inference methods. The package provides functionality for generating predictive distributions, evaluating uncertainty, and optimizing hyperparameters using Bayesian, coarse-to-fine, or random search strategies.",
    "version": "1.0",
    "maintainer": "Giancarlo Vercellino <giancarlo.vercellino@gmail.com>",
    "url": "https://rpubs.com/giancarlo_vercellino/xpect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26050,
    "package_name": "xplorerr",
    "title": "Tools for Interactive Data Exploration",
    "description": "Tools for interactive data exploration built using 'shiny'. Includes apps for descriptive",
    "version": "0.2.0",
    "maintainer": "",
    "url": "https://github.com/rsquaredacademy/xplorerr",
    "exports": [],
    "topics": ["data", "exploration", "r", "rstats", "shiny-apps", "statistics", "visualization"],
    "score": "NA",
    "stars": 37
  },
  {
    "id": 26061,
    "package_name": "xsp",
    "title": "The Chi-Square Periodogram",
    "description": "The circadian period of a time series data is predicted and the statistical significance of the periodicity are calculated using the chi-square periodogram.",
    "version": "0.1.2",
    "maintainer": "Hitoshi Iuchi <hiuchi@sfc.keio.ac.jp>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26094,
    "package_name": "yhat",
    "title": "Interpreting Regression Effects",
    "description": "The purpose of this package is to provide methods to interpret multiple\n  linear regression and canonical correlation results including beta weights,structure coefficients, \n  validity coefficients, product measures, relative weights, all-possible-subsets regression,\n  dominance analysis, commonality analysis, and adjusted effect sizes.",
    "version": "2.0-5",
    "maintainer": "Kim Nimon <kim.nimon@gmail.com>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26100,
    "package_name": "yodel",
    "title": "A General Bayesian Model Averaging Helper",
    "description": "Provides helper functions to perform Bayesian model averaging\n    using Markov chain Monte Carlo samples from separate models. Calculates\n    weights and obtains draws from the model-averaged posterior for quantities\n    of interest specified by the user. Weight calculations can be done using\n    marginal likelihoods or log-predictive likelihoods as in Ando, T., & Tsay,\n    R. (2010) <doi:10.1016/j.ijforecast.2009.08.001>.",
    "version": "1.0.0",
    "maintainer": "Richard Payne <paynestatistics@gmail.com>",
    "url": "https://github.com/rich-payne/yodel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26107,
    "package_name": "yuima",
    "title": "The YUIMA Project Package for SDEs",
    "description": "Simulation and Inference for SDEs and Other Stochastic Processes.",
    "version": "1.15.34",
    "maintainer": "Yuta Koike <kyuta@ms.u-tokyo.ac.jp>",
    "url": "https://yuimaproject.com",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26136,
    "package_name": "zic",
    "title": "Bayesian Inference for Zero-Inflated Count Models",
    "description": "Provides MCMC algorithms for the analysis of\n        zero-inflated count models. The case of stochastic search\n        variable selection (SVS) is also considered.  All MCMC samplers\n        are coded in C++ for improved efficiency. A data set\n        considering the demand for health care is provided.",
    "version": "0.9.1",
    "maintainer": "Markus Jochmann <markus.jochmann@ncl.ac.uk>",
    "url": "",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26152,
    "package_name": "zoib",
    "title": "Bayesian Inference for Beta Regression and Zero-or-One Inflated\nBeta Regression",
    "description": "Fits beta regression and zero-or-one inflated beta regression and obtains Bayesian Inference of the model via the Markov Chain Monte Carlo approach implemented in JAGS.",
    "version": "1.6.1",
    "maintainer": "Fang Liu <fang.liu.131@nd.edu>",
    "url": "https://www.r-project.org, https://www3.nd.edu/~fliu2/#tools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  },
  {
    "id": 26153,
    "package_name": "zoid",
    "title": "Bayesian Zero-and-One Inflated Dirichlet Regression Modelling",
    "description": "Fits Dirichlet regression and zero-and-one inflated Dirichlet regression with Bayesian methods implemented in Stan. These models are sometimes referred to as trinomial mixture models; covariates and overdispersion can optionally be included.",
    "version": "1.3.1",
    "maintainer": "Eric J. Ward <eric.ward@noaa.gov>",
    "url": "https://noaa-nwfsc.github.io/zoid/",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0
  }
]
