[
  {
    "id": 492,
    "package_name": "dplyr",
    "title": "A Grammar of Data Manipulation",
    "description": "A fast, consistent tool for working with data frame like\nobjects, both in memory and out of memory.",
    "version": "1.1.4.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-4757-117X>),\nRomain Fran\u00e7ois [aut] (ORCID: <https://orcid.org/0000-0002-2444-4226>),\nLionel Henry [aut],\nKirill M\u00fcller [aut] (ORCID: <https://orcid.org/0000-0002-1416-3412>),\nDavis Vaughan [aut] (ORCID: <https://orcid.org/0000-0003-4777-038X>),\nPosit Software, PBC [cph, fnd]",
    "url": "https://dplyr.tidyverse.org, https://github.com/tidyverse/dplyr",
    "bug_reports": "https://github.com/tidyverse/dplyr/issues",
    "repository": "",
    "exports": [
      [
        ".data"
      ],
      [
        "%>%"
      ],
      [
        "across"
      ],
      [
        "add_count"
      ],
      [
        "add_count_"
      ],
      [
        "add_row"
      ],
      [
        "add_rownames"
      ],
      [
        "add_tally"
      ],
      [
        "add_tally_"
      ],
      [
        "all_equal"
      ],
      [
        "all_of"
      ],
      [
        "all_vars"
      ],
      [
        "anti_join"
      ],
      [
        "any_of"
      ],
      [
        "any_vars"
      ],
      [
        "arrange"
      ],
      [
        "arrange_"
      ],
      [
        "arrange_all"
      ],
      [
        "arrange_at"
      ],
      [
        "arrange_if"
      ],
      [
        "as_data_frame"
      ],
      [
        "as_label"
      ],
      [
        "as_tibble"
      ],
      [
        "as.tbl"
      ],
      [
        "auto_copy"
      ],
      [
        "between"
      ],
      [
        "bind_cols"
      ],
      [
        "bind_rows"
      ],
      [
        "c_across"
      ],
      [
        "case_match"
      ],
      [
        "case_when"
      ],
      [
        "check_dbplyr"
      ],
      [
        "coalesce"
      ],
      [
        "collapse"
      ],
      [
        "collect"
      ],
      [
        "combine"
      ],
      [
        "common_by"
      ],
      [
        "compute"
      ],
      [
        "consecutive_id"
      ],
      [
        "contains"
      ],
      [
        "copy_to"
      ],
      [
        "count"
      ],
      [
        "count_"
      ],
      [
        "cross_join"
      ],
      [
        "cumall"
      ],
      [
        "cumany"
      ],
      [
        "cume_dist"
      ],
      [
        "cummean"
      ],
      [
        "cur_column"
      ],
      [
        "cur_data"
      ],
      [
        "cur_data_all"
      ],
      [
        "cur_group"
      ],
      [
        "cur_group_id"
      ],
      [
        "cur_group_rows"
      ],
      [
        "data_frame"
      ],
      [
        "db_analyze"
      ],
      [
        "db_begin"
      ],
      [
        "db_commit"
      ],
      [
        "db_create_index"
      ],
      [
        "db_create_indexes"
      ],
      [
        "db_create_table"
      ],
      [
        "db_data_type"
      ],
      [
        "db_desc"
      ],
      [
        "db_drop_table"
      ],
      [
        "db_explain"
      ],
      [
        "db_has_table"
      ],
      [
        "db_insert_into"
      ],
      [
        "db_list_tables"
      ],
      [
        "db_query_fields"
      ],
      [
        "db_query_rows"
      ],
      [
        "db_rollback"
      ],
      [
        "db_save_query"
      ],
      [
        "db_write_table"
      ],
      [
        "dense_rank"
      ],
      [
        "desc"
      ],
      [
        "dim_desc"
      ],
      [
        "distinct"
      ],
      [
        "distinct_"
      ],
      [
        "distinct_all"
      ],
      [
        "distinct_at"
      ],
      [
        "distinct_if"
      ],
      [
        "distinct_prepare"
      ],
      [
        "do"
      ],
      [
        "do_"
      ],
      [
        "dplyr_col_modify"
      ],
      [
        "dplyr_reconstruct"
      ],
      [
        "dplyr_row_slice"
      ],
      [
        "ends_with"
      ],
      [
        "enexpr"
      ],
      [
        "enexprs"
      ],
      [
        "enquo"
      ],
      [
        "enquos"
      ],
      [
        "ensym"
      ],
      [
        "ensyms"
      ],
      [
        "everything"
      ],
      [
        "explain"
      ],
      [
        "expr"
      ],
      [
        "filter"
      ],
      [
        "filter_"
      ],
      [
        "filter_all"
      ],
      [
        "filter_at"
      ],
      [
        "filter_if"
      ],
      [
        "filter_out"
      ],
      [
        "first"
      ],
      [
        "full_join"
      ],
      [
        "funs"
      ],
      [
        "funs_"
      ],
      [
        "glimpse"
      ],
      [
        "group_by"
      ],
      [
        "group_by_"
      ],
      [
        "group_by_all"
      ],
      [
        "group_by_at"
      ],
      [
        "group_by_drop_default"
      ],
      [
        "group_by_if"
      ],
      [
        "group_by_prepare"
      ],
      [
        "group_cols"
      ],
      [
        "group_data"
      ],
      [
        "group_indices"
      ],
      [
        "group_indices_"
      ],
      [
        "group_keys"
      ],
      [
        "group_map"
      ],
      [
        "group_modify"
      ],
      [
        "group_nest"
      ],
      [
        "group_rows"
      ],
      [
        "group_size"
      ],
      [
        "group_split"
      ],
      [
        "group_trim"
      ],
      [
        "group_vars"
      ],
      [
        "group_walk"
      ],
      [
        "grouped_df"
      ],
      [
        "groups"
      ],
      [
        "ident"
      ],
      [
        "if_all"
      ],
      [
        "if_any"
      ],
      [
        "if_else"
      ],
      [
        "inner_join"
      ],
      [
        "intersect"
      ],
      [
        "is_grouped_df"
      ],
      [
        "is.grouped_df"
      ],
      [
        "is.src"
      ],
      [
        "is.tbl"
      ],
      [
        "join_by"
      ],
      [
        "lag"
      ],
      [
        "last"
      ],
      [
        "last_col"
      ],
      [
        "last_dplyr_warnings"
      ],
      [
        "lead"
      ],
      [
        "left_join"
      ],
      [
        "lst"
      ],
      [
        "make_tbl"
      ],
      [
        "matches"
      ],
      [
        "min_rank"
      ],
      [
        "mutate"
      ],
      [
        "mutate_"
      ],
      [
        "mutate_all"
      ],
      [
        "mutate_at"
      ],
      [
        "mutate_each"
      ],
      [
        "mutate_each_"
      ],
      [
        "mutate_if"
      ],
      [
        "n"
      ],
      [
        "n_distinct"
      ],
      [
        "n_groups"
      ],
      [
        "na_if"
      ],
      [
        "near"
      ],
      [
        "nest_by"
      ],
      [
        "nest_join"
      ],
      [
        "new_grouped_df"
      ],
      [
        "new_rowwise_df"
      ],
      [
        "nth"
      ],
      [
        "ntile"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "order_by"
      ],
      [
        "percent_rank"
      ],
      [
        "pick"
      ],
      [
        "progress_estimated"
      ],
      [
        "pull"
      ],
      [
        "quo"
      ],
      [
        "quo_name"
      ],
      [
        "quos"
      ],
      [
        "recode"
      ],
      [
        "recode_factor"
      ],
      [
        "recode_values"
      ],
      [
        "reframe"
      ],
      [
        "relocate"
      ],
      [
        "rename"
      ],
      [
        "rename_"
      ],
      [
        "rename_all"
      ],
      [
        "rename_at"
      ],
      [
        "rename_if"
      ],
      [
        "rename_vars_"
      ],
      [
        "rename_with"
      ],
      [
        "replace_values"
      ],
      [
        "replace_when"
      ],
      [
        "right_join"
      ],
      [
        "row_number"
      ],
      [
        "rows_append"
      ],
      [
        "rows_delete"
      ],
      [
        "rows_insert"
      ],
      [
        "rows_patch"
      ],
      [
        "rows_update"
      ],
      [
        "rows_upsert"
      ],
      [
        "rowwise"
      ],
      [
        "same_src"
      ],
      [
        "sample_frac"
      ],
      [
        "sample_n"
      ],
      [
        "select"
      ],
      [
        "select_"
      ],
      [
        "select_all"
      ],
      [
        "select_at"
      ],
      [
        "select_if"
      ],
      [
        "select_vars_"
      ],
      [
        "semi_join"
      ],
      [
        "setdiff"
      ],
      [
        "setequal"
      ],
      [
        "show_query"
      ],
      [
        "slice"
      ],
      [
        "slice_"
      ],
      [
        "slice_head"
      ],
      [
        "slice_max"
      ],
      [
        "slice_min"
      ],
      [
        "slice_sample"
      ],
      [
        "slice_tail"
      ],
      [
        "sql"
      ],
      [
        "sql_escape_ident"
      ],
      [
        "sql_escape_string"
      ],
      [
        "sql_join"
      ],
      [
        "sql_select"
      ],
      [
        "sql_semi_join"
      ],
      [
        "sql_set_op"
      ],
      [
        "sql_subquery"
      ],
      [
        "sql_translate_env"
      ],
      [
        "src"
      ],
      [
        "src_df"
      ],
      [
        "src_local"
      ],
      [
        "src_mysql"
      ],
      [
        "src_postgres"
      ],
      [
        "src_sqlite"
      ],
      [
        "src_tbls"
      ],
      [
        "starts_with"
      ],
      [
        "summarise"
      ],
      [
        "summarise_"
      ],
      [
        "summarise_all"
      ],
      [
        "summarise_at"
      ],
      [
        "summarise_each"
      ],
      [
        "summarise_each_"
      ],
      [
        "summarise_if"
      ],
      [
        "summarize"
      ],
      [
        "summarize_"
      ],
      [
        "summarize_all"
      ],
      [
        "summarize_at"
      ],
      [
        "summarize_each"
      ],
      [
        "summarize_each_"
      ],
      [
        "summarize_if"
      ],
      [
        "sym"
      ],
      [
        "symdiff"
      ],
      [
        "syms"
      ],
      [
        "tally"
      ],
      [
        "tally_"
      ],
      [
        "tbl"
      ],
      [
        "tbl_df"
      ],
      [
        "tbl_nongroup_vars"
      ],
      [
        "tbl_ptype"
      ],
      [
        "tbl_vars"
      ],
      [
        "tibble"
      ],
      [
        "top_frac"
      ],
      [
        "top_n"
      ],
      [
        "transmute"
      ],
      [
        "transmute_"
      ],
      [
        "transmute_all"
      ],
      [
        "transmute_at"
      ],
      [
        "transmute_if"
      ],
      [
        "tribble"
      ],
      [
        "type_sum"
      ],
      [
        "ungroup"
      ],
      [
        "union"
      ],
      [
        "union_all"
      ],
      [
        "validate_grouped_df"
      ],
      [
        "validate_rowwise_df"
      ],
      [
        "vars"
      ],
      [
        "when_all"
      ],
      [
        "when_any"
      ],
      [
        "where"
      ],
      [
        "with_groups"
      ],
      [
        "with_order"
      ],
      [
        "wrap_dbplyr_obj"
      ]
    ],
    "topics": [
      [
        "data-manipulation"
      ],
      [
        "grammar"
      ],
      [
        "cpp"
      ]
    ],
    "score": 25.1164,
    "stars": 4975,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "dplyr A Grammar of Data Manipulation A fast, consistent tool for working with data frame like\nobjects, both in memory and out of memory. .data %>% across add_count add_count_ add_row add_rownames add_tally add_tally_ all_equal all_of all_vars anti_join any_of any_vars arrange arrange_ arrange_all arrange_at arrange_if as_data_frame as_label as_tibble as.tbl auto_copy between bind_cols bind_rows c_across case_match case_when check_dbplyr coalesce collapse collect combine common_by compute consecutive_id contains copy_to count count_ cross_join cumall cumany cume_dist cummean cur_column cur_data cur_data_all cur_group cur_group_id cur_group_rows data_frame db_analyze db_begin db_commit db_create_index db_create_indexes db_create_table db_data_type db_desc db_drop_table db_explain db_has_table db_insert_into db_list_tables db_query_fields db_query_rows db_rollback db_save_query db_write_table dense_rank desc dim_desc distinct distinct_ distinct_all distinct_at distinct_if distinct_prepare do do_ dplyr_col_modify dplyr_reconstruct dplyr_row_slice ends_with enexpr enexprs enquo enquos ensym ensyms everything explain expr filter filter_ filter_all filter_at filter_if filter_out first full_join funs funs_ glimpse group_by group_by_ group_by_all group_by_at group_by_drop_default group_by_if group_by_prepare group_cols group_data group_indices group_indices_ group_keys group_map group_modify group_nest group_rows group_size group_split group_trim group_vars group_walk grouped_df groups ident if_all if_any if_else inner_join intersect is_grouped_df is.grouped_df is.src is.tbl join_by lag last last_col last_dplyr_warnings lead left_join lst make_tbl matches min_rank mutate mutate_ mutate_all mutate_at mutate_each mutate_each_ mutate_if n n_distinct n_groups na_if near nest_by nest_join new_grouped_df new_rowwise_df nth ntile num_range one_of order_by percent_rank pick progress_estimated pull quo quo_name quos recode recode_factor recode_values reframe relocate rename rename_ rename_all rename_at rename_if rename_vars_ rename_with replace_values replace_when right_join row_number rows_append rows_delete rows_insert rows_patch rows_update rows_upsert rowwise same_src sample_frac sample_n select select_ select_all select_at select_if select_vars_ semi_join setdiff setequal show_query slice slice_ slice_head slice_max slice_min slice_sample slice_tail sql sql_escape_ident sql_escape_string sql_join sql_select sql_semi_join sql_set_op sql_subquery sql_translate_env src src_df src_local src_mysql src_postgres src_sqlite src_tbls starts_with summarise summarise_ summarise_all summarise_at summarise_each summarise_each_ summarise_if summarize summarize_ summarize_all summarize_at summarize_each summarize_each_ summarize_if sym symdiff syms tally tally_ tbl tbl_df tbl_nongroup_vars tbl_ptype tbl_vars tibble top_frac top_n transmute transmute_ transmute_all transmute_at transmute_if tribble type_sum ungroup union union_all validate_grouped_df validate_rowwise_df vars when_all when_any where with_groups with_order wrap_dbplyr_obj data-manipulation grammar cpp"
  },
  {
    "id": 1349,
    "package_name": "tidyr",
    "title": "Tidy Messy Data",
    "description": "Tools to help to create tidy data, where each column is a\nvariable, each row is an observation, and each cell contains a\nsingle value. 'tidyr' contains tools for changing the shape\n(pivoting) and hierarchy (nesting and 'unnesting') of a\ndataset, turning deeply nested lists into rectangular data\nframes ('rectangling'), and extracting values out of string\ncolumns. It also includes tools for working with missing values\n(both implicit and explicit).",
    "version": "1.3.1.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nDavis Vaughan [aut],\nMaximilian Girlich [aut],\nKevin Ushey [ctb],\nPosit Software, PBC [cph, fnd]",
    "url": "https://tidyr.tidyverse.org, https://github.com/tidyverse/tidyr",
    "bug_reports": "https://github.com/tidyverse/tidyr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "all_of"
      ],
      [
        "any_of"
      ],
      [
        "as_tibble"
      ],
      [
        "build_longer_spec"
      ],
      [
        "build_wider_spec"
      ],
      [
        "check_pivot_spec"
      ],
      [
        "chop"
      ],
      [
        "complete"
      ],
      [
        "complete_"
      ],
      [
        "contains"
      ],
      [
        "crossing"
      ],
      [
        "crossing_"
      ],
      [
        "drop_na"
      ],
      [
        "drop_na_"
      ],
      [
        "ends_with"
      ],
      [
        "everything"
      ],
      [
        "expand"
      ],
      [
        "expand_"
      ],
      [
        "expand_grid"
      ],
      [
        "extract"
      ],
      [
        "extract_"
      ],
      [
        "extract_numeric"
      ],
      [
        "fill"
      ],
      [
        "fill_"
      ],
      [
        "full_seq"
      ],
      [
        "gather"
      ],
      [
        "gather_"
      ],
      [
        "hoist"
      ],
      [
        "last_col"
      ],
      [
        "matches"
      ],
      [
        "nest"
      ],
      [
        "nest_"
      ],
      [
        "nest_legacy"
      ],
      [
        "nesting"
      ],
      [
        "nesting_"
      ],
      [
        "num_range"
      ],
      [
        "one_of"
      ],
      [
        "pack"
      ],
      [
        "pivot_longer"
      ],
      [
        "pivot_longer_spec"
      ],
      [
        "pivot_wider"
      ],
      [
        "pivot_wider_spec"
      ],
      [
        "replace_na"
      ],
      [
        "separate"
      ],
      [
        "separate_"
      ],
      [
        "separate_longer_delim"
      ],
      [
        "separate_longer_position"
      ],
      [
        "separate_rows"
      ],
      [
        "separate_rows_"
      ],
      [
        "separate_wider_delim"
      ],
      [
        "separate_wider_position"
      ],
      [
        "separate_wider_regex"
      ],
      [
        "spread"
      ],
      [
        "spread_"
      ],
      [
        "starts_with"
      ],
      [
        "tibble"
      ],
      [
        "tidyr_legacy"
      ],
      [
        "tribble"
      ],
      [
        "unchop"
      ],
      [
        "uncount"
      ],
      [
        "unite"
      ],
      [
        "unite_"
      ],
      [
        "unnest"
      ],
      [
        "unnest_"
      ],
      [
        "unnest_auto"
      ],
      [
        "unnest_legacy"
      ],
      [
        "unnest_longer"
      ],
      [
        "unnest_wider"
      ],
      [
        "unpack"
      ]
    ],
    "topics": [
      [
        "tidy-data"
      ],
      [
        "cpp"
      ]
    ],
    "score": 23.328,
    "stars": 1419,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "tidyr Tidy Messy Data Tools to help to create tidy data, where each column is a\nvariable, each row is an observation, and each cell contains a\nsingle value. 'tidyr' contains tools for changing the shape\n(pivoting) and hierarchy (nesting and 'unnesting') of a\ndataset, turning deeply nested lists into rectangular data\nframes ('rectangling'), and extracting values out of string\ncolumns. It also includes tools for working with missing values\n(both implicit and explicit). %>% all_of any_of as_tibble build_longer_spec build_wider_spec check_pivot_spec chop complete complete_ contains crossing crossing_ drop_na drop_na_ ends_with everything expand expand_ expand_grid extract extract_ extract_numeric fill fill_ full_seq gather gather_ hoist last_col matches nest nest_ nest_legacy nesting nesting_ num_range one_of pack pivot_longer pivot_longer_spec pivot_wider pivot_wider_spec replace_na separate separate_ separate_longer_delim separate_longer_position separate_rows separate_rows_ separate_wider_delim separate_wider_position separate_wider_regex spread spread_ starts_with tibble tidyr_legacy tribble unchop uncount unite unite_ unnest unnest_ unnest_auto unnest_legacy unnest_longer unnest_wider unpack tidy-data cpp"
  },
  {
    "id": 833,
    "package_name": "modelr",
    "title": "Modelling Functions that Work with the Pipe",
    "description": "Functions for modelling that help you seamlessly integrate\nmodelling into a pipeline of data manipulation and\nvisualisation.",
    "version": "0.1.11.9000",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre],\nPosit Software, PBC [cph, fnd]",
    "url": "https://modelr.tidyverse.org, https://github.com/tidyverse/modelr",
    "bug_reports": "https://github.com/tidyverse/modelr/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "add_predictions"
      ],
      [
        "add_predictors"
      ],
      [
        "add_residuals"
      ],
      [
        "bootstrap"
      ],
      [
        "crossv_kfold"
      ],
      [
        "crossv_loo"
      ],
      [
        "crossv_mc"
      ],
      [
        "data_grid"
      ],
      [
        "fit_with"
      ],
      [
        "formulae"
      ],
      [
        "formulas"
      ],
      [
        "gather_predictions"
      ],
      [
        "gather_residuals"
      ],
      [
        "geom_ref_line"
      ],
      [
        "mae"
      ],
      [
        "mape"
      ],
      [
        "model_matrix"
      ],
      [
        "mse"
      ],
      [
        "na.warn"
      ],
      [
        "permute"
      ],
      [
        "permute_"
      ],
      [
        "qae"
      ],
      [
        "resample"
      ],
      [
        "resample_bootstrap"
      ],
      [
        "resample_partition"
      ],
      [
        "resample_permutation"
      ],
      [
        "rmse"
      ],
      [
        "rsae"
      ],
      [
        "rsquare"
      ],
      [
        "seq_range"
      ],
      [
        "spread_predictions"
      ],
      [
        "spread_residuals"
      ],
      [
        "typical"
      ]
    ],
    "topics": [
      [
        "modelling"
      ]
    ],
    "score": 16.6011,
    "stars": 399,
    "primary_category": "tidyverse",
    "source_universe": "tidyverse",
    "search_text": "modelr Modelling Functions that Work with the Pipe Functions for modelling that help you seamlessly integrate\nmodelling into a pipeline of data manipulation and\nvisualisation. %>% add_predictions add_predictors add_residuals bootstrap crossv_kfold crossv_loo crossv_mc data_grid fit_with formulae formulas gather_predictions gather_residuals geom_ref_line mae mape model_matrix mse na.warn permute permute_ qae resample resample_bootstrap resample_partition resample_permutation rmse rsae rsquare seq_range spread_predictions spread_residuals typical modelling"
  },
  {
    "id": 443,
    "package_name": "datawizard",
    "title": "Easy Data Wrangling and Statistical Transformations",
    "description": "A lightweight package to assist in key steps involved in\nany data analysis workflow: (1) wrangling the raw data to get\nit in the needed form, (2) applying preprocessing steps and\nstatistical transformations, and (3) compute statistical\nsummaries of data properties and distributions. It is also the\ndata wrangling backend for packages in 'easystats' ecosystem.\nReferences: Patil et al. (2022) <doi:10.21105/joss.04684>.",
    "version": "1.3.0",
    "maintainer": "Etienne Bacher <etienne.bacher@protonmail.com>",
    "author": "Indrajeet Patil [aut] (ORCID: <https://orcid.org/0000-0003-1995-6531>),\nEtienne Bacher [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-9271-5075>),\nDominique Makowski [aut] (ORCID:\n<https://orcid.org/0000-0001-5375-9967>),\nDaniel L\u00fcdecke [aut] (ORCID: <https://orcid.org/0000-0002-8895-3206>),\nMattan S. Ben-Shachar [aut] (ORCID:\n<https://orcid.org/0000-0002-4287-4801>),\nBrenton M. Wiernik [aut] (ORCID:\n<https://orcid.org/0000-0001-9560-6336>),\nR\u00e9mi Th\u00e9riault [ctb] (ORCID: <https://orcid.org/0000-0003-4315-6788>),\nThomas J. Faulkenberry [rev],\nRobert Garrett [rev]",
    "url": "https://easystats.github.io/datawizard/",
    "bug_reports": "https://github.com/easystats/datawizard/issues",
    "repository": "",
    "exports": [
      [
        "adjust"
      ],
      [
        "as.prop.table"
      ],
      [
        "assign_labels"
      ],
      [
        "categorize"
      ],
      [
        "center"
      ],
      [
        "centre"
      ],
      [
        "change_scale"
      ],
      [
        "coef_var"
      ],
      [
        "coerce_to_numeric"
      ],
      [
        "colnames_to_row"
      ],
      [
        "column_as_rownames"
      ],
      [
        "contr.deviation"
      ],
      [
        "convert_na_to"
      ],
      [
        "convert_to_na"
      ],
      [
        "data_addprefix"
      ],
      [
        "data_addsuffix"
      ],
      [
        "data_adjust"
      ],
      [
        "data_arrange"
      ],
      [
        "data_codebook"
      ],
      [
        "data_duplicated"
      ],
      [
        "data_extract"
      ],
      [
        "data_filter"
      ],
      [
        "data_group"
      ],
      [
        "data_join"
      ],
      [
        "data_match"
      ],
      [
        "data_merge"
      ],
      [
        "data_modify"
      ],
      [
        "data_partition"
      ],
      [
        "data_peek"
      ],
      [
        "data_read"
      ],
      [
        "data_relocate"
      ],
      [
        "data_remove"
      ],
      [
        "data_rename"
      ],
      [
        "data_rename_rows"
      ],
      [
        "data_reorder"
      ],
      [
        "data_replicate"
      ],
      [
        "data_restoretype"
      ],
      [
        "data_rotate"
      ],
      [
        "data_seek"
      ],
      [
        "data_select"
      ],
      [
        "data_separate"
      ],
      [
        "data_summary"
      ],
      [
        "data_tabulate"
      ],
      [
        "data_to_long"
      ],
      [
        "data_to_wide"
      ],
      [
        "data_transpose"
      ],
      [
        "data_ungroup"
      ],
      [
        "data_unique"
      ],
      [
        "data_unite"
      ],
      [
        "data_write"
      ],
      [
        "degroup"
      ],
      [
        "demean"
      ],
      [
        "describe_distribution"
      ],
      [
        "detrend"
      ],
      [
        "display"
      ],
      [
        "distribution_coef_var"
      ],
      [
        "distribution_mode"
      ],
      [
        "empty_columns"
      ],
      [
        "empty_rows"
      ],
      [
        "extract_column_names"
      ],
      [
        "find_columns"
      ],
      [
        "kurtosis"
      ],
      [
        "labels_to_levels"
      ],
      [
        "mean_sd"
      ],
      [
        "means_by_group"
      ],
      [
        "median_mad"
      ],
      [
        "normalize"
      ],
      [
        "print_html"
      ],
      [
        "print_md"
      ],
      [
        "ranktransform"
      ],
      [
        "recode_into"
      ],
      [
        "recode_values"
      ],
      [
        "remove_empty"
      ],
      [
        "remove_empty_columns"
      ],
      [
        "remove_empty_rows"
      ],
      [
        "replace_nan_inf"
      ],
      [
        "rescale"
      ],
      [
        "rescale_weights"
      ],
      [
        "reshape_ci"
      ],
      [
        "reshape_longer"
      ],
      [
        "reshape_wider"
      ],
      [
        "reverse"
      ],
      [
        "reverse_scale"
      ],
      [
        "row_count"
      ],
      [
        "row_means"
      ],
      [
        "row_sums"
      ],
      [
        "row_to_colnames"
      ],
      [
        "rowid_as_column"
      ],
      [
        "rownames_as_column"
      ],
      [
        "skewness"
      ],
      [
        "slide"
      ],
      [
        "smoothness"
      ],
      [
        "standardise"
      ],
      [
        "standardize"
      ],
      [
        "text_concatenate"
      ],
      [
        "text_format"
      ],
      [
        "text_fullstop"
      ],
      [
        "text_lastchar"
      ],
      [
        "text_paste"
      ],
      [
        "text_remove"
      ],
      [
        "text_wrap"
      ],
      [
        "to_factor"
      ],
      [
        "to_numeric"
      ],
      [
        "unnormalize"
      ],
      [
        "unstandardise"
      ],
      [
        "unstandardize"
      ],
      [
        "visualisation_recipe"
      ],
      [
        "weighted_mad"
      ],
      [
        "weighted_mean"
      ],
      [
        "weighted_median"
      ],
      [
        "weighted_sd"
      ],
      [
        "winsorize"
      ]
    ],
    "topics": [
      [
        "data"
      ],
      [
        "dplyr"
      ],
      [
        "hacktoberfest"
      ],
      [
        "janitor"
      ],
      [
        "manipulation"
      ],
      [
        "reshape"
      ],
      [
        "tidyr"
      ],
      [
        "wrangling"
      ]
    ],
    "score": 14.9487,
    "stars": 232,
    "primary_category": "statistics",
    "source_universe": "easystats",
    "search_text": "datawizard Easy Data Wrangling and Statistical Transformations A lightweight package to assist in key steps involved in\nany data analysis workflow: (1) wrangling the raw data to get\nit in the needed form, (2) applying preprocessing steps and\nstatistical transformations, and (3) compute statistical\nsummaries of data properties and distributions. It is also the\ndata wrangling backend for packages in 'easystats' ecosystem.\nReferences: Patil et al. (2022) <doi:10.21105/joss.04684>. adjust as.prop.table assign_labels categorize center centre change_scale coef_var coerce_to_numeric colnames_to_row column_as_rownames contr.deviation convert_na_to convert_to_na data_addprefix data_addsuffix data_adjust data_arrange data_codebook data_duplicated data_extract data_filter data_group data_join data_match data_merge data_modify data_partition data_peek data_read data_relocate data_remove data_rename data_rename_rows data_reorder data_replicate data_restoretype data_rotate data_seek data_select data_separate data_summary data_tabulate data_to_long data_to_wide data_transpose data_ungroup data_unique data_unite data_write degroup demean describe_distribution detrend display distribution_coef_var distribution_mode empty_columns empty_rows extract_column_names find_columns kurtosis labels_to_levels mean_sd means_by_group median_mad normalize print_html print_md ranktransform recode_into recode_values remove_empty remove_empty_columns remove_empty_rows replace_nan_inf rescale rescale_weights reshape_ci reshape_longer reshape_wider reverse reverse_scale row_count row_means row_sums row_to_colnames rowid_as_column rownames_as_column skewness slide smoothness standardise standardize text_concatenate text_format text_fullstop text_lastchar text_paste text_remove text_wrap to_factor to_numeric unnormalize unstandardise unstandardize visualisation_recipe weighted_mad weighted_mean weighted_median weighted_sd winsorize data dplyr hacktoberfest janitor manipulation reshape tidyr wrangling"
  },
  {
    "id": 372,
    "package_name": "cleanepi",
    "title": "Clean and Standardize Epidemiological Data",
    "description": "Cleaning and standardizing tabular data package, tailored\nspecifically for curating epidemiological data. It streamlines\nvarious data cleaning tasks that are typically expected when\nworking with datasets in epidemiology. It returns the processed\ndata in the same format, and generates a comprehensive report\ndetailing the outcomes of each cleaning task.",
    "version": "1.1.2.9000",
    "maintainer": "Bubacarr Bah <Bubacarr.Bah1@lshtm.ac.uk>",
    "author": "Karim Man\u00e9 [aut] (ORCID: <https://orcid.org/0000-0002-9892-2999>),\nThibaut Jombart [ctb] (Thibaut contributed in development of\ndate_guess().),\nAbdoelnaser Degoot [aut] (ORCID:\n<https://orcid.org/0000-0001-8788-2496>),\nBankol\u00e9 Ahadzie [aut],\nNuredin Mohammed [aut],\nBubacarr Bah [aut, cre] (ORCID:\n<https://orcid.org/0000-0003-3318-6668>),\nHugo Gruson [ctb, rev] (ORCID: <https://orcid.org/0000-0002-4094-1476>),\nPratik R. Gupte [rev] (ORCID: <https://orcid.org/0000-0001-5294-7819>),\nJames M. Azam [rev] (ORCID: <https://orcid.org/0000-0001-5782-7330>),\nJoshua W. Lambert [rev, ctb] (ORCID:\n<https://orcid.org/0000-0001-5218-3046>),\nChris Hartgerink [rev] (ORCID: <https://orcid.org/0000-0003-1050-6809>),\nAndree Valle-Campos [rev, ctb] (ORCID:\n<https://orcid.org/0000-0002-7779-481X>),\nLondon School of Hygiene and Tropical Medicine, LSHTM [cph] (ROR:\n<https://ror.org/00a0jsq62>),\ndata.org [fnd]",
    "url": "https://epiverse-trace.github.io/cleanepi/,\nhttps://github.com/epiverse-trace/cleanepi",
    "bug_reports": "https://github.com/epiverse-trace/cleanepi/issues",
    "repository": "",
    "exports": [
      [
        "%>%"
      ],
      [
        "add_to_dictionary"
      ],
      [
        "add_to_report"
      ],
      [
        "check_date_sequence"
      ],
      [
        "check_subject_ids"
      ],
      [
        "clean_data"
      ],
      [
        "clean_using_dictionary"
      ],
      [
        "convert_numeric_to_date"
      ],
      [
        "convert_to_numeric"
      ],
      [
        "correct_misspelled_values"
      ],
      [
        "correct_subject_ids"
      ],
      [
        "find_duplicates"
      ],
      [
        "get_default_params"
      ],
      [
        "print_report"
      ],
      [
        "remove_constants"
      ],
      [
        "remove_duplicates"
      ],
      [
        "replace_missing_values"
      ],
      [
        "scan_data"
      ],
      [
        "standardize_column_names"
      ],
      [
        "standardize_dates"
      ],
      [
        "timespan"
      ]
    ],
    "topics": [
      [
        "data-cleaning"
      ],
      [
        "epidemiology"
      ],
      [
        "epiverse"
      ]
    ],
    "score": 7.6232,
    "stars": 10,
    "primary_category": "epidemiology",
    "source_universe": "epiverse-trace",
    "search_text": "cleanepi Clean and Standardize Epidemiological Data Cleaning and standardizing tabular data package, tailored\nspecifically for curating epidemiological data. It streamlines\nvarious data cleaning tasks that are typically expected when\nworking with datasets in epidemiology. It returns the processed\ndata in the same format, and generates a comprehensive report\ndetailing the outcomes of each cleaning task. %>% add_to_dictionary add_to_report check_date_sequence check_subject_ids clean_data clean_using_dictionary convert_numeric_to_date convert_to_numeric correct_misspelled_values correct_subject_ids find_duplicates get_default_params print_report remove_constants remove_duplicates replace_missing_values scan_data standardize_column_names standardize_dates timespan data-cleaning epidemiology epiverse"
  },
  {
    "id": 411,
    "package_name": "cotram",
    "title": "Count Transformation Models",
    "description": "Count transformation models featuring parameters\ninterpretable as discrete hazard ratios, odds ratios,\nreverse-time discrete hazard ratios, or transformed\nexpectations. An appropriate data transformation for a count\noutcome and regression coefficients are simultaneously\nestimated by maximising the exact discrete log-likelihood using\nthe computational framework provided in package 'mlt',\ntechnical details are given in Siegfried & Hothorn (2020)\n<DOI:10.1111/2041-210X.13383>. The package also contains an\nexperimental implementation of multivariate count\ntransformation models with an application to multi-species\ndistribution models <DOI:10.48550/arXiv.2201.13095>.",
    "version": "0.6-0",
    "maintainer": "Sandra Siegfried <sandra.siegfried@alumni.uzh.ch>",
    "author": "Sandra Siegfried [aut, cre] (ORCID:\n<https://orcid.org/0000-0002-7312-1001>),\nLuisa Barbanti [aut] (ORCID: <https://orcid.org/0000-0001-5352-5802>),\nTorsten Hothorn [aut] (ORCID: <https://orcid.org/0000-0001-8301-0471>)",
    "url": "http://ctm.R-forge.R-project.org",
    "bug_reports": "",
    "repository": "",
    "exports": [
      [
        "cotram"
      ],
      [
        "mcotram"
      ]
    ],
    "topics": [],
    "score": 5.8756,
    "stars": 0,
    "primary_category": "infrastructure",
    "source_universe": "r-forge",
    "search_text": "cotram Count Transformation Models Count transformation models featuring parameters\ninterpretable as discrete hazard ratios, odds ratios,\nreverse-time discrete hazard ratios, or transformed\nexpectations. An appropriate data transformation for a count\noutcome and regression coefficients are simultaneously\nestimated by maximising the exact discrete log-likelihood using\nthe computational framework provided in package 'mlt',\ntechnical details are given in Siegfried & Hothorn (2020)\n<DOI:10.1111/2041-210X.13383>. The package also contains an\nexperimental implementation of multivariate count\ntransformation models with an application to multi-species\ndistribution models <DOI:10.48550/arXiv.2201.13095>. cotram mcotram "
  },
  {
    "id": 40,
    "package_name": "DoOR.functions",
    "title": "Integrating Heterogeneous Odorant Response Data into a Common\nResponse Model: A DoOR to the Complete Olfactome",
    "description": "This is a function package providing functions to perform\ndata manipulations and visualizations for DoOR.data. See the\nURLs for the original and the DoOR 2.0 publication.",
    "version": "2.0.2",
    "maintainer": "Daniel M\u00fcnch <daniel.muench@uni-konstanz.de>",
    "author": "Daniel M\u00fcnch [aut, cre],\nC. Giovanni Galizia [aut],\nShouwen Ma [aut],\nMartin Strauch [aut],\nAnja Nissler [aut],\nWolf Huetteroth [ctb]",
    "url": "https://docs.ropensci.org/DoOR.functions,\nhttp://neuro.uni.kn/DoOR, http://dx.doi.org/10.1038/srep21841,\nhttp://dx.doi.org/10.1093/chemse/bjq042,\nhttps://github.com/ropensci/DoOR.functions",
    "bug_reports": "https://github.com/ropensci/DoOR.functions/issues",
    "repository": "",
    "exports": [
      [
        "back_project"
      ],
      [
        "calculate_model"
      ],
      [
        "count_studies"
      ],
      [
        "countStudies"
      ],
      [
        "create_door_database"
      ],
      [
        "door_default_values"
      ],
      [
        "door_norm"
      ],
      [
        "dplot_across_osns"
      ],
      [
        "dplot_across_ru"
      ],
      [
        "dplot_al_map"
      ],
      [
        "dplot_compare_profiles"
      ],
      [
        "dplot_response_matrix"
      ],
      [
        "dplot_response_profile"
      ],
      [
        "dplot_tuningCurve"
      ],
      [
        "estimate_missing_value"
      ],
      [
        "export_door_data"
      ],
      [
        "get_dataset"
      ],
      [
        "get_normalized_responses"
      ],
      [
        "get_responses"
      ],
      [
        "identify_sensillum"
      ],
      [
        "import_new_data"
      ],
      [
        "load2list"
      ],
      [
        "map_receptor"
      ],
      [
        "model_response"
      ],
      [
        "model_response_seq"
      ],
      [
        "private_odorant"
      ],
      [
        "project_points"
      ],
      [
        "remove_study"
      ],
      [
        "reset_sfr"
      ],
      [
        "select_model"
      ],
      [
        "sparse"
      ],
      [
        "trans_id"
      ],
      [
        "update_door_database"
      ],
      [
        "update_door_odorinfo"
      ]
    ],
    "topics": [
      [
        "peer-reviewed"
      ]
    ],
    "score": 5.5977,
    "stars": 11,
    "primary_category": "ropensci",
    "source_universe": "ropensci",
    "search_text": "DoOR.functions Integrating Heterogeneous Odorant Response Data into a Common\nResponse Model: A DoOR to the Complete Olfactome This is a function package providing functions to perform\ndata manipulations and visualizations for DoOR.data. See the\nURLs for the original and the DoOR 2.0 publication. back_project calculate_model count_studies countStudies create_door_database door_default_values door_norm dplot_across_osns dplot_across_ru dplot_al_map dplot_compare_profiles dplot_response_matrix dplot_response_profile dplot_tuningCurve estimate_missing_value export_door_data get_dataset get_normalized_responses get_responses identify_sensillum import_new_data load2list map_receptor model_response model_response_seq private_odorant project_points remove_study reset_sfr select_model sparse trans_id update_door_database update_door_odorinfo peer-reviewed"
  },
  {
    "id": 203,
    "package_name": "TFORGE",
    "title": "Tests for Geophysical Eigenvalues",
    "description": "The eigenvalues of observed symmetric matrices are often of intense scientific interest. This package offers single sample tests for the eigenvalues of the population mean or the eigenvalue-multiplicity of the population mean. For k-samples, this package offers tests for equal eigenvalues between samples. Included is support for matrices with constraints common to geophysical tensors (constant trace, sum of squared eigenvalues, or both) and eigenvectors are usually considered nuisance parameters. Pivotal bootstrap methods enable these tests to have good performance for small samples (n=15 for 3x3 matrices). These methods were developed and studied by Hingee, Scealy and Wood (2026, \"Nonparametric bootstrap inference for the eigenvalues of geophysical tensors\", accepted by the Journal of American Statistical Association). Also available is a 2-sample test using a Gaussian orthogonal ensemble approximation and an eigenvalue-multiplicity test that assumes orthogonally-invariant covariance. ",
    "version": "0.1.16",
    "maintainer": "Kassel Liam Hingee <kassel.hingee@anu.edu.au>",
    "author": "Kassel Liam Hingee [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9894-2407>),\n  Art B. Owen [cph] (./R/scel.R only),\n  Board of Trustees Leland Stanford Junior University [cph] (./R/scel.R\n    only)",
    "url": "https://github.com/kasselhingee/TFORGE",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=TFORGE",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "TFORGE Tests for Geophysical Eigenvalues The eigenvalues of observed symmetric matrices are often of intense scientific interest. This package offers single sample tests for the eigenvalues of the population mean or the eigenvalue-multiplicity of the population mean. For k-samples, this package offers tests for equal eigenvalues between samples. Included is support for matrices with constraints common to geophysical tensors (constant trace, sum of squared eigenvalues, or both) and eigenvectors are usually considered nuisance parameters. Pivotal bootstrap methods enable these tests to have good performance for small samples (n=15 for 3x3 matrices). These methods were developed and studied by Hingee, Scealy and Wood (2026, \"Nonparametric bootstrap inference for the eigenvalues of geophysical tensors\", accepted by the Journal of American Statistical Association). Also available is a 2-sample test using a Gaussian orthogonal ensemble approximation and an eigenvalue-multiplicity test that assumes orthogonally-invariant covariance.   "
  },
  {
    "id": 496,
    "package_name": "dsTidyverse",
    "title": "'DataSHIELD' 'Tidyverse' Serverside Package",
    "description": "Implementation of selected 'Tidyverse' functions within 'DataSHIELD', an open-source federated analysis solution in R. Currently, DataSHIELD contains very limited tools for data manipulation, so the aim of this package is to improve the researcher experience by implementing essential functions for data manipulation, including subsetting, filtering, grouping, and renaming variables. This is the serverside package which should be installed on the server holding the data, and is used in conjuncture with the clientside package 'dsTidyverseClient' which is installed in the local R environment of the analyst. For more information, see <https://tidyverse.org/> and <https://datashield.org/>.",
    "version": "1.1.1",
    "maintainer": "Tim Cadman <t.j.cadman@umcg.nl>",
    "author": "Tim Cadman [aut, cre] (ORCID: <https://orcid.org/0000-0002-7682-5645>),\n  Mariska Slofstra [aut] (ORCID: <https://orcid.org/0000-0002-0400-0468>),\n  Stuart Wheater [aut],\n  Demetris Avraam [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dsTidyverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dsTidyverse 'DataSHIELD' 'Tidyverse' Serverside Package Implementation of selected 'Tidyverse' functions within 'DataSHIELD', an open-source federated analysis solution in R. Currently, DataSHIELD contains very limited tools for data manipulation, so the aim of this package is to improve the researcher experience by implementing essential functions for data manipulation, including subsetting, filtering, grouping, and renaming variables. This is the serverside package which should be installed on the server holding the data, and is used in conjuncture with the clientside package 'dsTidyverseClient' which is installed in the local R environment of the analyst. For more information, see <https://tidyverse.org/> and <https://datashield.org/>.  "
  },
  {
    "id": 497,
    "package_name": "dsTidyverseClient",
    "title": "'DataSHIELD' 'Tidyverse' Clientside Package",
    "description": "Implementation of selected 'Tidyverse' functions within 'DataSHIELD', an open-source federated analysis solution in R. Currently, 'DataSHIELD' contains very limited tools for data manipulation, so the aim of this package is to improve the researcher experience by implementing essential functions for data manipulation, including subsetting, filtering, grouping, and renaming variables. This is the clientside package which should be installed locally, and is used in conjuncture with the serverside package 'dsTidyverse' which is installed on the remote server holding the data. For more information, see <https://tidyverse.org/> and <https://datashield.org/>.",
    "version": "1.0.3",
    "maintainer": "Tim Cadman <t.j.cadman@umcg.nl>",
    "author": "Tim Cadman [aut, cre] (ORCID: <https://orcid.org/0000-0002-7682-5645>),\n  Mariska Slofstra [aut] (ORCID: <https://orcid.org/0000-0002-0400-0468>),\n  Stuart Wheater [aut],\n  Demetris Avraam [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dsTidyverseClient",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dsTidyverseClient 'DataSHIELD' 'Tidyverse' Clientside Package Implementation of selected 'Tidyverse' functions within 'DataSHIELD', an open-source federated analysis solution in R. Currently, 'DataSHIELD' contains very limited tools for data manipulation, so the aim of this package is to improve the researcher experience by implementing essential functions for data manipulation, including subsetting, filtering, grouping, and renaming variables. This is the clientside package which should be installed locally, and is used in conjuncture with the serverside package 'dsTidyverse' which is installed on the remote server holding the data. For more information, see <https://tidyverse.org/> and <https://datashield.org/>.  "
  },
  {
    "id": 602,
    "package_name": "geeLite",
    "title": "Building and Managing Local Databases from 'Google Earth Engine'",
    "description": "Simplifies the creation, management, and updating of local databases using data extracted from 'Google Earth Engine' ('GEE'). It integrates with 'GEE' to store, aggregate, and process spatio-temporal data, leveraging 'SQLite' for efficient, serverless storage. The 'geeLite' package provides utilities for data transformation and supports real-time monitoring and analysis of geospatial features, making it suitable for researchers and practitioners in geospatial science. For details, see Kurbucz and Andr\u00e9e (2025) \"Building and Managing Local Databases from Google Earth Engine with the geeLite R Package\" <https://hdl.handle.net/10986/43165>.",
    "version": "1.0.6",
    "maintainer": "Marcell T. Kurbucz <m.kurbucz@ucl.ac.uk>",
    "author": "Marcell T. Kurbucz [aut, cre],\n  Bo Pieter Johannes Andr\u00e9e [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=geeLite",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geeLite Building and Managing Local Databases from 'Google Earth Engine' Simplifies the creation, management, and updating of local databases using data extracted from 'Google Earth Engine' ('GEE'). It integrates with 'GEE' to store, aggregate, and process spatio-temporal data, leveraging 'SQLite' for efficient, serverless storage. The 'geeLite' package provides utilities for data transformation and supports real-time monitoring and analysis of geospatial features, making it suitable for researchers and practitioners in geospatial science. For details, see Kurbucz and Andr\u00e9e (2025) \"Building and Managing Local Databases from Google Earth Engine with the geeLite R Package\" <https://hdl.handle.net/10986/43165>.  "
  },
  {
    "id": 608,
    "package_name": "geoflow",
    "title": "Orchestrate Geospatial (Meta)Data Management Workflows and\nManage FAIR Services",
    "description": "An engine to facilitate the orchestration and execution of metadata-driven data management workflows, in compliance with 'FAIR' \n  (Findable, Accessible, Interoperable and Reusable) data management principles. By means of a pivot metadata model, relying on the 'DublinCore' standard (<https://dublincore.org/>), \n  a unique source of metadata can be used to operate multiple and inter-connected data management actions. Users can also customise their own workflows by creating specific actions \n  but the library comes with a set of native actions targeting common geographic information and data management, in particular actions oriented to the publication on the web of metadata \n  and data resources to provide standard discovery and access services. At first, default actions of the library were meant to focus on providing turn-key actions for geospatial (meta)data: \n  1) by creating manage geospatial (meta)data complying with 'ISO/TC211' (<https://committee.iso.org/home/tc211>) and 'OGC' (<https://www.ogc.org/standards/>) geographic information standards \n  (eg 19115/19119/19110/19139) and related best practices (eg. 'INSPIRE'); and 2) by facilitating extraction, reading and publishing of standard geospatial (meta)data within widely used software \n  that compound a Spatial Data Infrastructure ('SDI'), including spatial databases (eg. 'PostGIS'), metadata catalogues (eg. 'GeoNetwork', 'CSW' servers), data servers (eg. 'GeoServer'). The library was \n  then extended to actions for other domains: 1) biodiversity (meta)data standard management including handling of 'EML' metadata, and their management with 'DataOne' servers, 2) in situ sensors, \n  remote sensing and model outputs  (meta)data standard management by handling part of 'CF' conventions, 'NetCDF' data format and 'OPeNDAP' access protocol, and their management with 'Thredds' servers, \n  3) generic / domain agnostic (meta)data standard managers ('DublinCore', 'DataCite'), to facilitate the publication of data within (meta)data repositories such as 'Zenodo' (<https://zenodo.org>) \n  or DataVerse (<https://dataverse.org/>). The execution of several actions will then allow to cross-reference (meta)data resources in each action performed, offering a way to bind resources \n  between each other (eg. reference 'Zenodo' 'DOI' in 'GeoNetwork'/'GeoServer' metadata, or vice versa reference 'GeoNetwork'/'GeoServer' links in 'Zenodo' or 'EML' metadata). The use of\n  standardized configuration files ('JSON' or 'YAML' formats) allow fully reproducible workflows to facilitate the work of data and information managers.",
    "version": "1.1.0",
    "maintainer": "Emmanuel Blondel <emmanuel.blondel1@gmail.com>",
    "author": "Emmanuel Blondel [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5870-5762>),\n  Julien, Barde [aut] (ORCID: <https://orcid.org/0000-0002-3519-6141>),\n  Wilfried Heintz [aut] (ORCID: <https://orcid.org/0000-0002-9244-9766>),\n  Alexandre Bennici [ctb],\n  Sylvain Poulain [ctb],\n  Bastien Grasset [ctb],\n  Mathias Rouan [ctb],\n  Emilie Lerigoleur [ctb],\n  Yvan Le Bras [ctb],\n  Jeroen Ooms [ctb]",
    "url": "https://github.com/r-geoflow/geoflow",
    "bug_reports": "https://github.com/r-geoflow/geoflow/issues",
    "repository": "https://cran.r-project.org/package=geoflow",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "geoflow Orchestrate Geospatial (Meta)Data Management Workflows and\nManage FAIR Services An engine to facilitate the orchestration and execution of metadata-driven data management workflows, in compliance with 'FAIR' \n  (Findable, Accessible, Interoperable and Reusable) data management principles. By means of a pivot metadata model, relying on the 'DublinCore' standard (<https://dublincore.org/>), \n  a unique source of metadata can be used to operate multiple and inter-connected data management actions. Users can also customise their own workflows by creating specific actions \n  but the library comes with a set of native actions targeting common geographic information and data management, in particular actions oriented to the publication on the web of metadata \n  and data resources to provide standard discovery and access services. At first, default actions of the library were meant to focus on providing turn-key actions for geospatial (meta)data: \n  1) by creating manage geospatial (meta)data complying with 'ISO/TC211' (<https://committee.iso.org/home/tc211>) and 'OGC' (<https://www.ogc.org/standards/>) geographic information standards \n  (eg 19115/19119/19110/19139) and related best practices (eg. 'INSPIRE'); and 2) by facilitating extraction, reading and publishing of standard geospatial (meta)data within widely used software \n  that compound a Spatial Data Infrastructure ('SDI'), including spatial databases (eg. 'PostGIS'), metadata catalogues (eg. 'GeoNetwork', 'CSW' servers), data servers (eg. 'GeoServer'). The library was \n  then extended to actions for other domains: 1) biodiversity (meta)data standard management including handling of 'EML' metadata, and their management with 'DataOne' servers, 2) in situ sensors, \n  remote sensing and model outputs  (meta)data standard management by handling part of 'CF' conventions, 'NetCDF' data format and 'OPeNDAP' access protocol, and their management with 'Thredds' servers, \n  3) generic / domain agnostic (meta)data standard managers ('DublinCore', 'DataCite'), to facilitate the publication of data within (meta)data repositories such as 'Zenodo' (<https://zenodo.org>) \n  or DataVerse (<https://dataverse.org/>). The execution of several actions will then allow to cross-reference (meta)data resources in each action performed, offering a way to bind resources \n  between each other (eg. reference 'Zenodo' 'DOI' in 'GeoNetwork'/'GeoServer' metadata, or vice versa reference 'GeoNetwork'/'GeoServer' links in 'Zenodo' or 'EML' metadata). The use of\n  standardized configuration files ('JSON' or 'YAML' formats) allow fully reproducible workflows to facilitate the work of data and information managers.  "
  },
  {
    "id": 755,
    "package_name": "lessR",
    "title": "Less Code with More Comprehensive Results",
    "description": "Each function replaces multiple standard R functions. For example,\n    two function calls, Read() and CountAll(), generate summary statistics for\n    all variables in the data frame, plus histograms and bar charts. Other\n    functions provide data aggregation via pivot tables; comprehensive\n    regression, ANOVA, and t-test; visualizations including integrated\n    Violin/Box/Scatter plot for a numerical variable, bar chart, histogram,\n    box plot, density curves, calibrated power curve; reading multiple data\n    formats with the same call; variable labels; time series with aggregation\n    and forecasting; color themes; and Trellis (facet) graphics. Also includes\n    a confirmatory factor analysis of multiple-indicator measurement models,\n    pedagogical routines for data simulation (e.g., Central Limit Theorem),\n    generation and rendering of regression instructions for interpretative output,\n    and both interactive construction of visualizations and interactive\n    visualizations with plotly.",
    "version": "4.5",
    "maintainer": "David W. Gerbing <gerbing@pdx.edu>",
    "author": "David W. Gerbing [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6998-8350>, Affiliation: School of\n    Business, Portland State University)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=lessR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lessR Less Code with More Comprehensive Results Each function replaces multiple standard R functions. For example,\n    two function calls, Read() and CountAll(), generate summary statistics for\n    all variables in the data frame, plus histograms and bar charts. Other\n    functions provide data aggregation via pivot tables; comprehensive\n    regression, ANOVA, and t-test; visualizations including integrated\n    Violin/Box/Scatter plot for a numerical variable, bar chart, histogram,\n    box plot, density curves, calibrated power curve; reading multiple data\n    formats with the same call; variable labels; time series with aggregation\n    and forecasting; color themes; and Trellis (facet) graphics. Also includes\n    a confirmatory factor analysis of multiple-indicator measurement models,\n    pedagogical routines for data simulation (e.g., Central Limit Theorem),\n    generation and rendering of regression instructions for interpretative output,\n    and both interactive construction of visualizations and interactive\n    visualizations with plotly.  "
  },
  {
    "id": 1030,
    "package_name": "pxmake",
    "title": "Make PX-Files in R",
    "description": "Create PX-files from scratch or read and modify existing ones. \n    Includes a function for every PX keyword, making metadata manipulation\n    simple and human-readable.",
    "version": "0.19.0",
    "maintainer": "Johan Ejstrud <johan@ejstrud.com>",
    "author": "Johan Ejstrud [cre, aut],\n  Lars Pedersen [aut],\n  Statistics Greenland [cph] (https://stat.gl/)",
    "url": "https://github.com/StatisticsGreenland/pxmake,\nhttps://statisticsgreenland.github.io/pxmake/",
    "bug_reports": "https://github.com/StatisticsGreenland/pxmake/issues",
    "repository": "https://cran.r-project.org/package=pxmake",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pxmake Make PX-Files in R Create PX-files from scratch or read and modify existing ones. \n    Includes a function for every PX keyword, making metadata manipulation\n    simple and human-readable.  "
  },
  {
    "id": 1300,
    "package_name": "tall",
    "title": "Text Analysis for All",
    "description": "An R 'shiny' app designed for diverse text analysis tasks, offering a wide range of methodologies tailored to Natural Language Processing (NLP) needs. \n             It is a versatile, general-purpose tool for analyzing textual data. \n             'tall' features a comprehensive workflow, including data cleaning, preprocessing, statistical analysis, and visualization, all integrated for effective text analysis.",
    "version": "0.5.1",
    "maintainer": "Massimo Aria <aria@unina.it>",
    "author": "Massimo Aria [aut, cre, cph] (0000-0002-8517-9411),\n  Maria Spano [aut] (ORCID: <https://orcid.org/0000-0002-3103-2342>),\n  Luca D'Aniello [aut] (ORCID: <https://orcid.org/0000-0003-1019-9212>),\n  Corrado Cuccurullo [ctb] (ORCID:\n    <https://orcid.org/0000-0002-7401-8575>),\n  Michelangelo Misuraca [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8794-966X>)",
    "url": "https://github.com/massimoaria/tall, https://www.k-synth.com/tall/",
    "bug_reports": "https://github.com/massimoaria/tall/issues",
    "repository": "https://cran.r-project.org/package=tall",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tall Text Analysis for All An R 'shiny' app designed for diverse text analysis tasks, offering a wide range of methodologies tailored to Natural Language Processing (NLP) needs. \n             It is a versatile, general-purpose tool for analyzing textual data. \n             'tall' features a comprehensive workflow, including data cleaning, preprocessing, statistical analysis, and visualization, all integrated for effective text analysis.  "
  },
  {
    "id": 1340,
    "package_name": "tidyfst",
    "title": "Tidy Verbs for Fast Data Manipulation",
    "description": "A toolkit of tidy data manipulation verbs with 'data.table' as the backend.\n  Combining the merits of syntax elegance from 'dplyr' and computing performance from 'data.table', \n  'tidyfst' intends to provide users with state-of-the-art data manipulation tools with least pain.\n  This package is an extension of 'data.table'. While enjoying a tidy syntax, \n  it also wraps combinations of efficient functions to facilitate frequently-used data operations.  ",
    "version": "1.8.3",
    "maintainer": "Tian-Yuan Huang <huang.tian-yuan@qq.com>",
    "author": "Tian-Yuan Huang [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4151-3764>)",
    "url": "https://github.com/hope-data-science/tidyfst,\nhttps://hope-data-science.github.io/tidyfst/",
    "bug_reports": "https://github.com/hope-data-science/tidyfst/issues",
    "repository": "https://cran.r-project.org/package=tidyfst",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyfst Tidy Verbs for Fast Data Manipulation A toolkit of tidy data manipulation verbs with 'data.table' as the backend.\n  Combining the merits of syntax elegance from 'dplyr' and computing performance from 'data.table', \n  'tidyfst' intends to provide users with state-of-the-art data manipulation tools with least pain.\n  This package is an extension of 'data.table'. While enjoying a tidy syntax, \n  it also wraps combinations of efficient functions to facilitate frequently-used data operations.    "
  },
  {
    "id": 1565,
    "package_name": "AOV1R",
    "title": "Inference in the Balanced One-Way ANOVA Model with Random Factor",
    "description": "Provides functions to perform statistical inference in the balanced one-way ANOVA model with a random factor: confidence intervals, prediction interval, and Weerahandi generalized pivotal quantities. References: Burdick & Graybill (1992, ISBN-13: 978-0824786441); Weerahandi (1995) <doi:10.1007/978-1-4612-0825-9>; Lin & Liao (2008) <doi:10.1016/j.jspi.2008.01.001>.",
    "version": "0.1.0",
    "maintainer": "St\u00e9phane Laurent <laurent_step@outlook.fr>",
    "author": "St\u00e9phane Laurent",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AOV1R",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AOV1R Inference in the Balanced One-Way ANOVA Model with Random Factor Provides functions to perform statistical inference in the balanced one-way ANOVA model with a random factor: confidence intervals, prediction interval, and Weerahandi generalized pivotal quantities. References: Burdick & Graybill (1992, ISBN-13: 978-0824786441); Weerahandi (1995) <doi:10.1007/978-1-4612-0825-9>; Lin & Liao (2008) <doi:10.1016/j.jspi.2008.01.001>.  "
  },
  {
    "id": 1637,
    "package_name": "AdIsMF",
    "title": "Adsorption Isotherm Model Fitting",
    "description": "The Langmuir and Freundlich adsorption isotherms are pivotal in characterizing adsorption processes, essential across various scientific disciplines. Proper interpretation of adsorption isotherms involves robust fitting of data to the models, accurate estimation of parameters, and efficiency evaluation of the models, both in linear and non-linear forms. For researchers and practitioners in the fields of chemistry, environmental science, soil science, and engineering, a comprehensive package that satisfies all these requirements would be ideal for accurate and efficient analysis of adsorption data, precise model selection and validation for rigorous scientific inquiry and real-world applications. Details can be found in Langmuir (1918) <doi:10.1021/ja02242a004> and Giles (1973) <doi:10.1111/j.1478-4408.1973.tb03158.x>.",
    "version": "0.1.1",
    "maintainer": "Debopam Rakshit <rakshitdebopam@yahoo.com>",
    "author": "Debopam Rakshit [aut, cre],\n  Arkaprava Roy [aut],\n  K. M. Manjaiah [aut],\n  Siba Prasad Datta [aut],\n  Ritwika Das [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=AdIsMF",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "AdIsMF Adsorption Isotherm Model Fitting The Langmuir and Freundlich adsorption isotherms are pivotal in characterizing adsorption processes, essential across various scientific disciplines. Proper interpretation of adsorption isotherms involves robust fitting of data to the models, accurate estimation of parameters, and efficiency evaluation of the models, both in linear and non-linear forms. For researchers and practitioners in the fields of chemistry, environmental science, soil science, and engineering, a comprehensive package that satisfies all these requirements would be ideal for accurate and efficient analysis of adsorption data, precise model selection and validation for rigorous scientific inquiry and real-world applications. Details can be found in Langmuir (1918) <doi:10.1021/ja02242a004> and Giles (1973) <doi:10.1111/j.1478-4408.1973.tb03158.x>.  "
  },
  {
    "id": 1937,
    "package_name": "BalancedSampling",
    "title": "Balanced and Spatially Balanced Sampling",
    "description": "Select balanced and spatially balanced probability samples in multi-dimensional spaces\n    with any prescribed inclusion probabilities. It contains fast (C++ via Rcpp) implementations of\n    the included sampling methods. The local pivotal method by Grafstr\u00f6m, Lundstr\u00f6m and Schelin (2012)\n    <doi:10.1111/j.1541-0420.2011.01699.x> and spatially correlated Poisson sampling by Grafstr\u00f6m (2012)\n    <doi:10.1016/j.jspi.2011.07.003> are included. Also the cube method (for balanced sampling) and\n    the local cube method (for doubly balanced sampling) are included, see Grafstr\u00f6m and Till\u00e9 (2013)\n    <doi:10.1002/env.2194>.",
    "version": "2.1.1",
    "maintainer": "Anton Grafstr\u00f6m <anton.grafstrom@gmail.com>",
    "author": "Anton Grafstr\u00f6m [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-4345-4024>),\n  Wilmer Prentius [aut] (ORCID: <https://orcid.org/0000-0002-3561-290X>),\n  Jonathan Lisic [ctb]",
    "url": "https://www.envisim.se/,\nhttps://github.com/envisim/BalancedSampling/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BalancedSampling",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BalancedSampling Balanced and Spatially Balanced Sampling Select balanced and spatially balanced probability samples in multi-dimensional spaces\n    with any prescribed inclusion probabilities. It contains fast (C++ via Rcpp) implementations of\n    the included sampling methods. The local pivotal method by Grafstr\u00f6m, Lundstr\u00f6m and Schelin (2012)\n    <doi:10.1111/j.1541-0420.2011.01699.x> and spatially correlated Poisson sampling by Grafstr\u00f6m (2012)\n    <doi:10.1016/j.jspi.2011.07.003> are included. Also the cube method (for balanced sampling) and\n    the local cube method (for doubly balanced sampling) are included, see Grafstr\u00f6m and Till\u00e9 (2013)\n    <doi:10.1002/env.2194>.  "
  },
  {
    "id": 1968,
    "package_name": "BayesDissolution",
    "title": "Bayesian Models for Dissolution Testing",
    "description": "Fits Bayesian models (amongst others) to dissolution data sets that can be used for dissolution testing. The package was originally constructed to include only the Bayesian models outlined in Pourmohamad et al. (2022) <doi:10.1111/rssc.12535>. However, additional Bayesian and non-Bayesian models (based on bootstrapping and generalized pivotal quanties) have also been added. More models may be added over time.",
    "version": "0.2.1",
    "maintainer": "Tony Pourmohamad <tpourmohamad@gmail.com>",
    "author": "Tony Pourmohamad [aut, cre],\n  Steven Novick [aut],\n  Robert Richardson [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=BayesDissolution",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BayesDissolution Bayesian Models for Dissolution Testing Fits Bayesian models (amongst others) to dissolution data sets that can be used for dissolution testing. The package was originally constructed to include only the Bayesian models outlined in Pourmohamad et al. (2022) <doi:10.1111/rssc.12535>. However, additional Bayesian and non-Bayesian models (based on bootstrapping and generalized pivotal quanties) have also been added. More models may be added over time.  "
  },
  {
    "id": 2037,
    "package_name": "BeeBDC",
    "title": "Occurrence Data Cleaning",
    "description": "Flags and checks occurrence data that are in Darwin Core\n    format.  The package includes generic functions and data as well as\n    some that are specific to bees. This package is meant to build upon\n    and be complimentary to other excellent occurrence cleaning packages,\n    including 'bdc' and 'CoordinateCleaner'.  This package uses datasets\n    from several sources and particularly from the Discover Life Website,\n    created by Ascher and Pickering (2020).  For further information,\n    please see the original publication and package website.  Publication\n    - Dorey et al. (2023) <doi:10.1101/2023.06.30.547152> and package\n    website - Dorey et al. (2023) <https://github.com/jbdorey/BeeBDC>.",
    "version": "1.3.1",
    "maintainer": "James B. Dorey <jbdorey@me.com>",
    "author": "James B. Dorey [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0003-2721-3842>),\n  Robert L. O'Reilly [aut] (ORCID:\n    <https://orcid.org/0000-0001-5291-7396>),\n  Silas Bossert [aut] (ORCID: <https://orcid.org/0000-0002-3620-5468>),\n  Erica E. Fischer [aut] (ORCID: <https://orcid.org/0000-0002-8202-158X>)",
    "url": "https://jbdorey.github.io/BeeBDC/\nhttps://github.com/jbdorey/BeeBDC",
    "bug_reports": "https://github.com/jbdorey/BeeBDC/issues",
    "repository": "https://cran.r-project.org/package=BeeBDC",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "BeeBDC Occurrence Data Cleaning Flags and checks occurrence data that are in Darwin Core\n    format.  The package includes generic functions and data as well as\n    some that are specific to bees. This package is meant to build upon\n    and be complimentary to other excellent occurrence cleaning packages,\n    including 'bdc' and 'CoordinateCleaner'.  This package uses datasets\n    from several sources and particularly from the Discover Life Website,\n    created by Ascher and Pickering (2020).  For further information,\n    please see the original publication and package website.  Publication\n    - Dorey et al. (2023) <doi:10.1101/2023.06.30.547152> and package\n    website - Dorey et al. (2023) <https://github.com/jbdorey/BeeBDC>.  "
  },
  {
    "id": 2192,
    "package_name": "CATAcode",
    "title": "Explore and Code Responses to Check-All-that-Apply Survey Items",
    "description": "Analyzing responses to check-all-that-apply survey items often requires \n    data transformations and subjective decisions for combining categories. 'CATAcode'\n    contains tools for exploring response patterns, facilitating data transformations, \n    applying a set of decision rules for coding responses, and summarizing response frequencies.",
    "version": "1.0.0",
    "maintainer": "Kyle Nickodem <kyle.nickodem@gmail.com>",
    "author": "Kyle Nickodem [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-4976-3378>),\n  Gabriel J. Merrin [aut]",
    "url": "https://github.com/knickodem/CATAcode",
    "bug_reports": "https://github.com/knickodem/CATAcode/issues",
    "repository": "https://cran.r-project.org/package=CATAcode",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CATAcode Explore and Code Responses to Check-All-that-Apply Survey Items Analyzing responses to check-all-that-apply survey items often requires \n    data transformations and subjective decisions for combining categories. 'CATAcode'\n    contains tools for exploring response patterns, facilitating data transformations, \n    applying a set of decision rules for coding responses, and summarizing response frequencies.  "
  },
  {
    "id": 2210,
    "package_name": "CCAMLRGIS",
    "title": "Antarctic Spatial Data Manipulation",
    "description": "Loads and creates spatial data, including layers and tools that are relevant\n    to the activities of the Commission for the Conservation of Antarctic Marine Living \n    Resources. Provides two categories of functions: load functions and create functions.\n    Load functions are used to import existing spatial layers from the online CCAMLR GIS\n    such as the ASD boundaries. Create functions are used to create layers from user data\n    such as polygons and grids.",
    "version": "4.2.1",
    "maintainer": "Stephane Thanassekos <stephane.thanassekos@ccamlr.org>",
    "author": "Stephane Thanassekos [aut, cre],\n  Keith Reid [aut],\n  Lucy Robinson [aut],\n  Michael D. Sumner [ctb],\n  Roger Bivand [ctb]",
    "url": "https://github.com/ccamlr/CCAMLRGIS#ccamlrgis-r-package",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CCAMLRGIS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CCAMLRGIS Antarctic Spatial Data Manipulation Loads and creates spatial data, including layers and tools that are relevant\n    to the activities of the Commission for the Conservation of Antarctic Marine Living \n    Resources. Provides two categories of functions: load functions and create functions.\n    Load functions are used to import existing spatial layers from the online CCAMLR GIS\n    such as the ASD boundaries. Create functions are used to create layers from user data\n    such as polygons and grids.  "
  },
  {
    "id": 2291,
    "package_name": "CMIP6VisR",
    "title": "Visualization and Analysis of Coupled Model Intercomparison\nProject, Phase-6 (CMIP6) Hydroclimatic Data",
    "description": "Data manipulation for Coupled Model Intercomparison Project, Phase-6 (CMIP6) hydroclimatic data. The files are archived in the Federated Research Data Repository (FRDR) (Rajulapati et al, 2024, <doi:10.20383/103.0829>). The data set is described in Abdelmoaty et al. (2025, <doi:10.1038/s41597-025-04396-z>).",
    "version": "1.0.0",
    "maintainer": "Kevin Shook <kshook@kshook.ca>",
    "author": "Simon Michael Papalexiou [aut],\n  Heba Abdelmoaty [aut],\n  Konstantinos Andreadis [aut],\n  Kevin Shook [ctb, cre]",
    "url": "https://github.com/TycheLab/CMIP6VisR",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CMIP6VisR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CMIP6VisR Visualization and Analysis of Coupled Model Intercomparison\nProject, Phase-6 (CMIP6) Hydroclimatic Data Data manipulation for Coupled Model Intercomparison Project, Phase-6 (CMIP6) hydroclimatic data. The files are archived in the Federated Research Data Repository (FRDR) (Rajulapati et al, 2024, <doi:10.20383/103.0829>). The data set is described in Abdelmoaty et al. (2025, <doi:10.1038/s41597-025-04396-z>).  "
  },
  {
    "id": 2326,
    "package_name": "COveR",
    "title": "Clustering with Overlaps",
    "description": "Provide functions for overlaps clustering, fuzzy clustering and interval-valued data manipulation. The package implement the following algorithms:\n\tOKM (Overlapping Kmeans) from Cleuziou, G. (2007) <doi:10.1109/icpr.2008.4761079> ;\n\tNEOKM (Non-exhaustive overlapping Kmeans) from Whang, J. J., Dhillon, I. S., and Gleich, D. F. (2015) <doi:10.1137/1.9781611974010.105> ;\n\tFuzzy Cmeans from Bezdek, J. C. (1981) <doi:10.1007/978-1-4757-0450-1> ;\n\tFuzzy I-Cmeans from de A.T. De Carvalho, F. (2005) <doi:10.1016/j.patrec.2006.08.014>.",
    "version": "1.1.0",
    "maintainer": "Nicolas Hiot <nicolas.hiot@univ-orleans.fr>",
    "author": "Guillaume Cleuziou [aut],\n  Nicolas Hiot [cre] (ORCID: <https://orcid.org/0000-0003-4318-4906>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=COveR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "COveR Clustering with Overlaps Provide functions for overlaps clustering, fuzzy clustering and interval-valued data manipulation. The package implement the following algorithms:\n\tOKM (Overlapping Kmeans) from Cleuziou, G. (2007) <doi:10.1109/icpr.2008.4761079> ;\n\tNEOKM (Non-exhaustive overlapping Kmeans) from Whang, J. J., Dhillon, I. S., and Gleich, D. F. (2015) <doi:10.1137/1.9781611974010.105> ;\n\tFuzzy Cmeans from Bezdek, J. C. (1981) <doi:10.1007/978-1-4757-0450-1> ;\n\tFuzzy I-Cmeans from de A.T. De Carvalho, F. (2005) <doi:10.1016/j.patrec.2006.08.014>.  "
  },
  {
    "id": 2352,
    "package_name": "CSHShydRology",
    "title": "Canadian Hydrological Analyses",
    "description": "A collection of user-submitted functions to aid in the analysis of hydrological data, particularly for users in Canada. The functions focus on the use of Canadian data sets, and are suited to Canadian hydrology, such as the important cold region hydrological processes and will work with Canadian hydrological models. The functions are grouped into several themes, currently including Statistical hydrology, Basic data manipulations, Visualization, and Spatial hydrology. Functions developed by the Floodnet project are also included. CSHShydRology has been developed with the assistance of the Canadian Society for Hydrological Sciences (CSHS) which is an affiliated society of the Canadian Water Resources Association (CWRA). As of version 1.2.6, functions now fail gracefully when attempting to download data from a url which is unavailable.",
    "version": "1.4.4",
    "maintainer": "Kevin Shook <kevin.shook@usask.ca>",
    "author": "Kevin Shook [cre, aut],\n  Paul Whitfield [aut],\n  Robert Chlumsky [aut],\n  Daniel Moore [aut],\n  Martin Durocher [aut],\n  Matthew Lemieux [ctb],\n  Jason Chiang [ctb],\n  Joel Trubilowicz [ctb],\n  SJ Kim [ctb]",
    "url": "https://github.com/CSHS-hydRology/CSHShydRology",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=CSHShydRology",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "CSHShydRology Canadian Hydrological Analyses A collection of user-submitted functions to aid in the analysis of hydrological data, particularly for users in Canada. The functions focus on the use of Canadian data sets, and are suited to Canadian hydrology, such as the important cold region hydrological processes and will work with Canadian hydrological models. The functions are grouped into several themes, currently including Statistical hydrology, Basic data manipulations, Visualization, and Spatial hydrology. Functions developed by the Floodnet project are also included. CSHShydRology has been developed with the assistance of the Canadian Society for Hydrological Sciences (CSHS) which is an affiliated society of the Canadian Water Resources Association (CWRA). As of version 1.2.6, functions now fail gracefully when attempting to download data from a url which is unavailable.  "
  },
  {
    "id": 2722,
    "package_name": "DCEmgmt",
    "title": "DCE Data Reshaping and Processing",
    "description": "Prepare the results of a DCE to be analysed through choice models.'DCEmgmt' reshapes DCE data from wide to long format considering the special characteristics of a DCE. 'DCEmgmt' includes the function 'DCEestm' which estimates choice models once the database has been reshaped with 'DCEmgmt'.",
    "version": "0.0.1",
    "maintainer": "Daniel Perez-Troncoso <danielperez@ugr.es>",
    "author": "Daniel Perez-Troncoso [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DCEmgmt",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DCEmgmt DCE Data Reshaping and Processing Prepare the results of a DCE to be analysed through choice models.'DCEmgmt' reshapes DCE data from wide to long format considering the special characteristics of a DCE. 'DCEmgmt' includes the function 'DCEestm' which estimates choice models once the database has been reshaped with 'DCEmgmt'.  "
  },
  {
    "id": 2799,
    "package_name": "DImodels",
    "title": "Diversity-Interactions (DI) Models",
    "description": "The 'DImodels' package is suitable for analysing data from biodiversity and ecosystem function studies using the Diversity-Interactions (DI) modelling approach introduced by Kirwan et al. (2009) <doi:10.1890/08-1684.1>. Suitable data will contain proportions for each species and a community-level response variable, and may also include additional factors, such as blocks or treatments. The package can perform data manipulation tasks, such as computing pairwise interactions (the DI_data() function), can perform an automated model selection process (the autoDI() function) and has the flexibility to fit a wide range of user-defined DI models (the DI() function).",
    "version": "1.3.3",
    "maintainer": "Rafael de Andrade Moral <rafael.deandrademoral@mu.ie>",
    "author": "Rafael de Andrade Moral [aut, cre],\n  John Connolly [aut],\n  Rishabh Vishwakarma [ctb],\n  Caroline Brophy [aut]",
    "url": "https://dimodels.com/, https://github.com/rafamoral/DImodels",
    "bug_reports": "https://github.com/rafamoral/DImodels/issues",
    "repository": "https://cran.r-project.org/package=DImodels",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DImodels Diversity-Interactions (DI) Models The 'DImodels' package is suitable for analysing data from biodiversity and ecosystem function studies using the Diversity-Interactions (DI) modelling approach introduced by Kirwan et al. (2009) <doi:10.1890/08-1684.1>. Suitable data will contain proportions for each species and a community-level response variable, and may also include additional factors, such as blocks or treatments. The package can perform data manipulation tasks, such as computing pairwise interactions (the DI_data() function), can perform an automated model selection process (the autoDI() function) and has the flexibility to fit a wide range of user-defined DI models (the DI() function).  "
  },
  {
    "id": 2903,
    "package_name": "DataClean",
    "title": "Data Cleaning",
    "description": "Includes functions that researchers or practitioners may use to clean\n    raw data, transferring html, xlsx, txt data file into other formats. And it\n    also can be used to manipulate text variables, extract numeric variables from\n    text variables and other variable cleaning processes. It is originated from a\n    author's project which focuses on creative performance in online education\n    environment. The resulting paper of that study will be published soon.",
    "version": "1.0",
    "maintainer": "Xiaorui(Jeremy) Zhu <zhuxiaorui1989@gmail.com>",
    "author": "Xiaorui(Jeremy) Zhu",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=DataClean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "DataClean Data Cleaning Includes functions that researchers or practitioners may use to clean\n    raw data, transferring html, xlsx, txt data file into other formats. And it\n    also can be used to manipulate text variables, extract numeric variables from\n    text variables and other variable cleaning processes. It is originated from a\n    author's project which focuses on creative performance in online education\n    environment. The resulting paper of that study will be published soon.  "
  },
  {
    "id": 2931,
    "package_name": "Deducer",
    "title": "A Data Analysis GUI for R",
    "description": "An intuitive, cross-platform graphical data analysis system. It uses menus and dialogs to guide the user efficiently through the data manipulation and analysis process, and has an excel like spreadsheet for easy data frame visualization and editing. Deducer works best when used with the Java based R GUI JGR, but the dialogs can be called from the command line. Dialogs have also been integrated into the Windows Rgui.",
    "version": "0.9-1",
    "maintainer": "Ian Fellows <ian@fellstat.com>",
    "author": "Ian Fellows [aut, cre]",
    "url": "https://www.deducer.org/ https://www.fellstat.com",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Deducer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Deducer A Data Analysis GUI for R An intuitive, cross-platform graphical data analysis system. It uses menus and dialogs to guide the user efficiently through the data manipulation and analysis process, and has an excel like spreadsheet for easy data frame visualization and editing. Deducer works best when used with the Java based R GUI JGR, but the dialogs can be called from the command line. Dialogs have also been integrated into the Windows Rgui.  "
  },
  {
    "id": 3081,
    "package_name": "EDOtrans",
    "title": "Euclidean Distance-Optimized Data Transformation",
    "description": "A data transformation method which takes into account the special property of scale non-invariance with a breakpoint at 1 of the Euclidean distance.",
    "version": "0.2.5",
    "maintainer": "Jorn Lotsch <j.lotsch@em.uni-frankfurt.de>",
    "author": "Jorn Lotsch[aut,cre], Alfred Ultsch[aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=EDOtrans",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "EDOtrans Euclidean Distance-Optimized Data Transformation A data transformation method which takes into account the special property of scale non-invariance with a breakpoint at 1 of the Euclidean distance.  "
  },
  {
    "id": 3453,
    "package_name": "FastKRR",
    "title": "Kernel Ridge Regression using 'RcppArmadillo'",
    "description": "Provides core computational operations in C++ via 'RcppArmadillo', enabling faster performance than pure R, improved numerical stability, and parallel execution with OpenMP where available. On systems without OpenMP support, the package automatically falls back to single-threaded execution with no user configuration required. For efficient model selection, it integrates with 'CVST' to provide sequential-testing cross-validation that identifies competitive hyperparameters without exhaustive grid search. The package offers a unified interface for exact kernel ridge regression and three scalable approximations\u2014Nystr\u00f6m, Pivoted Cholesky, and Random Fourier Features\u2014allowing analyses with substantially larger sample sizes than are feasible with exact KRR. It also integrates with the 'tidymodels' ecosystem via the 'parsnip' model specification 'krr_reg', and the S3 method tunable.krr_reg(). To understand the theoretical background, one can refer to Wainwright (2019) <doi:10.1017/9781108627771>.",
    "version": "0.1.2",
    "maintainer": "Kwan-Young Bak <kybak@sungshin.ac.kr>",
    "author": "Gyeongmin Kim [aut] (Sungshin Women's University),\n  Seyoung Lee [aut] (Sungshin Women's University),\n  Miyoung Jang [aut] (Sungshin Women's University),\n  Kwan-Young Bak [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-4541-160X>, Sungshin Women's\n    University)",
    "url": "https://github.com/kybak90/FastKRR, https://www.tidymodels.org",
    "bug_reports": "https://github.com/kybak90/FastKRR/issues",
    "repository": "https://cran.r-project.org/package=FastKRR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "FastKRR Kernel Ridge Regression using 'RcppArmadillo' Provides core computational operations in C++ via 'RcppArmadillo', enabling faster performance than pure R, improved numerical stability, and parallel execution with OpenMP where available. On systems without OpenMP support, the package automatically falls back to single-threaded execution with no user configuration required. For efficient model selection, it integrates with 'CVST' to provide sequential-testing cross-validation that identifies competitive hyperparameters without exhaustive grid search. The package offers a unified interface for exact kernel ridge regression and three scalable approximations\u2014Nystr\u00f6m, Pivoted Cholesky, and Random Fourier Features\u2014allowing analyses with substantially larger sample sizes than are feasible with exact KRR. It also integrates with the 'tidymodels' ecosystem via the 'parsnip' model specification 'krr_reg', and the S3 method tunable.krr_reg(). To understand the theoretical background, one can refer to Wainwright (2019) <doi:10.1017/9781108627771>.  "
  },
  {
    "id": 3600,
    "package_name": "GCalcium",
    "title": "A Data Manipulation and Analysis Package for Calcium Indicator\nData",
    "description": "Provides shortcuts in extracting useful data points and summarizing waveform data. It is optimized for speed to work efficiently with large data sets so you can get to the analysis phase more quickly. It also utilizes a user-friendly format for use by both beginners and seasoned R users.",
    "version": "1.0.0",
    "maintainer": "Andrew Tamalunas <atamalu@g.clemson.edu>",
    "author": "Andrew Tamalunas",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GCalcium",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GCalcium A Data Manipulation and Analysis Package for Calcium Indicator\nData Provides shortcuts in extracting useful data points and summarizing waveform data. It is optimized for speed to work efficiently with large data sets so you can get to the analysis phase more quickly. It also utilizes a user-friendly format for use by both beginners and seasoned R users.  "
  },
  {
    "id": 3661,
    "package_name": "GISINTEGRATION",
    "title": "GIS Integration",
    "description": "Designed to facilitate the preprocessing and linking of GIS (Geographic Information System) databases\n  <https://www.sciencedirect.com/topics/computer-science/gis-database>,\n  the R package 'GISINTEGRATION' offers a robust solution for efficiently preparing  GIS data for advanced \n  spatial analyses. This package excels in simplifying intrica  procedures like data cleaning, normalization, \n  and format conversion, ensuring that the data are optimally primed for precise and thorough analysis.",
    "version": "1.0",
    "maintainer": "Leila Marvian Mashhad <Leila.marveian@gmail.com>",
    "author": "Hossein Hassani [aut],\n  Leila Marvian Mashhad [aut, cre],\n  Sara Stewart [aut],\n  Steve Macfeelys [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GISINTEGRATION",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GISINTEGRATION GIS Integration Designed to facilitate the preprocessing and linking of GIS (Geographic Information System) databases\n  <https://www.sciencedirect.com/topics/computer-science/gis-database>,\n  the R package 'GISINTEGRATION' offers a robust solution for efficiently preparing  GIS data for advanced \n  spatial analyses. This package excels in simplifying intrica  procedures like data cleaning, normalization, \n  and format conversion, ensuring that the data are optimally primed for precise and thorough analysis.  "
  },
  {
    "id": 3663,
    "package_name": "GISTools",
    "title": "Further Capabilities in Geographic Information Science",
    "description": "Mapping and spatial data manipulation tools - in particular\n    drawing thematic maps with nice looking legends,  and aggregation of point\n    data to polygons.",
    "version": "1.0-2",
    "maintainer": "Binbin Lu <binbinlu@whu.edu.cn>",
    "author": "Chris Brunsdon [aut],\n  Binbin Lu [aut, cre],\n  Hongyan Chen [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GISTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GISTools Further Capabilities in Geographic Information Science Mapping and spatial data manipulation tools - in particular\n    drawing thematic maps with nice looking legends,  and aggregation of point\n    data to polygons.  "
  },
  {
    "id": 3767,
    "package_name": "GVS",
    "title": "'Geocoordinate Validation Service'",
    "description": "The 'Geocoordinate Validation Service' (GVS) runs checks of coordinates in latitude/longitude format. It returns annotated coordinates with additional flags and metadata that can be used in data cleaning.  Additionally, the package has functions related to attribution and metadata information. More information can be found at <https://github.com/ojalaquellueva/gvs/tree/master/api>.",
    "version": "0.0.1",
    "maintainer": "Brian Maitner <bmaitner@gmail.com>",
    "author": "Brian Maitner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2118-9880>),\n  Brad Boyle [aut],\n  Rethvick Sriram Yugendra Babu [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GVS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GVS 'Geocoordinate Validation Service' The 'Geocoordinate Validation Service' (GVS) runs checks of coordinates in latitude/longitude format. It returns annotated coordinates with additional flags and metadata that can be used in data cleaning.  Additionally, the package has functions related to attribution and metadata information. More information can be found at <https://github.com/ojalaquellueva/gvs/tree/master/api>.  "
  },
  {
    "id": 3835,
    "package_name": "GeodRegr",
    "title": "Geodesic Regression",
    "description": "Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.",
    "version": "0.2.0",
    "maintainer": "Ha-Young Shin <hayoung.shin@gmail.com>",
    "author": "Ha-Young Shin [aut, cre],\n  Hee-Seok Oh [aut]",
    "url": "https://github.com/hayoungshin1/GeodRegr",
    "bug_reports": "https://github.com/hayoungshin1/GeodRegr/issues",
    "repository": "https://cran.r-project.org/package=GeodRegr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GeodRegr Geodesic Regression Provides a gradient descent algorithm to find a geodesic relationship between real-valued independent variables and a manifold-valued dependent variable (i.e. geodesic regression). Available manifolds are Euclidean space, the sphere, hyperbolic space, and Kendall's 2-dimensional shape space. Besides the standard least-squares loss, the least absolute deviations, Huber, and Tukey biweight loss functions can also be used to perform robust geodesic regression. Functions to help choose appropriate cutoff parameters to maintain high efficiency for the Huber and Tukey biweight estimators are included, as are functions for generating random tangent vectors from the Riemannian normal distributions on the sphere and hyperbolic space. The n-sphere is a n-dimensional manifold: we represent it as a sphere of radius 1 and center 0 embedded in (n+1)-dimensional space. Using the hyperboloid model of hyperbolic space, n-dimensional hyperbolic space is embedded in (n+1)-dimensional Minkowski space as the upper sheet of a hyperboloid of two sheets. Kendall's 2D shape space with K landmarks is of real dimension 2K-4; preshapes are represented as complex K-vectors with mean 0 and magnitude 1. Details are described in Shin, H.-Y. and Oh, H.-S. (2020) <arXiv:2007.04518>. Also see Fletcher, P. T. (2013) <doi:10.1007/s11263-012-0591-y>.  "
  },
  {
    "id": 3901,
    "package_name": "GsymPoint",
    "title": "Estimation of the Generalized Symmetry Point, an Optimal\nCutpoint in Continuous Diagnostic Tests",
    "description": "Estimation of the cutpoint defined by the Generalized Symmetry point in a binary classification setting based on a continuous diagnostic test or marker. Two methods have been implemented to construct confidence intervals for this optimal cutpoint, one based on the Generalized Pivotal Quantity and the other based on Empirical Likelihood. Numerical and graphical outputs for these two methods are easily obtained.",
    "version": "1.1.2",
    "maintainer": "M\u00f3nica L\u00f3pez-Rat\u00f3n <monica.lopez.raton@gmail.com>",
    "author": "M\u00f3nica L\u00f3pez-Rat\u00f3n [aut, cre],\n  Carmen Cadarso-Su\u00e1rez [aut],\n  Elisa M. Molanes-L\u00f3pez [aut],\n  Emilio Let\u00f3n [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=GsymPoint",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "GsymPoint Estimation of the Generalized Symmetry Point, an Optimal\nCutpoint in Continuous Diagnostic Tests Estimation of the cutpoint defined by the Generalized Symmetry point in a binary classification setting based on a continuous diagnostic test or marker. Two methods have been implemented to construct confidence intervals for this optimal cutpoint, one based on the Generalized Pivotal Quantity and the other based on Empirical Likelihood. Numerical and graphical outputs for these two methods are easily obtained.  "
  },
  {
    "id": 4117,
    "package_name": "IDPmisc",
    "title": "'Utilities of Institute of Data Analyses and Process Design\n(www.zhaw.ch/idp)'",
    "description": "Different high-level graphics functions for displaying large datasets, displaying circular data in a very flexible way, finding local maxima, brewing color ramps, drawing nice arrows, zooming 2D-plots, creating figures with differently colored margin and plot region.  In addition, the package contains auxiliary functions for data manipulation like omitting observations with irregular values or selecting data by logical vectors, which include NAs. Other functions are especially useful in spectroscopy and analyses of environmental data: robust baseline fitting, finding peaks in spectra, converting humidity measures.",
    "version": "1.1.21",
    "maintainer": "Christoph Hofer <christoph.hofer@zhaw.ch>",
    "author": "Christoph Hofer [cre],\n  Rene Locher [aut],\n  Andreas Ruckstuhl [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=IDPmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IDPmisc 'Utilities of Institute of Data Analyses and Process Design\n(www.zhaw.ch/idp)' Different high-level graphics functions for displaying large datasets, displaying circular data in a very flexible way, finding local maxima, brewing color ramps, drawing nice arrows, zooming 2D-plots, creating figures with differently colored margin and plot region.  In addition, the package contains auxiliary functions for data manipulation like omitting observations with irregular values or selecting data by logical vectors, which include NAs. Other functions are especially useful in spectroscopy and analyses of environmental data: robust baseline fitting, finding peaks in spectra, converting humidity measures.  "
  },
  {
    "id": 4171,
    "package_name": "IPEDSuploadables",
    "title": "Transforms Institutional Data into Text Files for IPEDS\nAutomated Import/Upload",
    "description": "Starting from user-supplied institutional data, these scripts \n    transform, aggregate, and reshape the information to produce \n    key-value pair data files that are able to be uploaded to IPEDS (Integrated Postsecondary Education Data System) \n    through their submission portal <https://surveys.nces.ed.gov/ipeds/>. Starting data specifications can be found in the vignettes.  \n    Final files are saved locally to a location of the user's choice. \n    User-friendly readable files can also be produced for purposes of data review and validation. ",
    "version": "3.0.0",
    "maintainer": "Alison Lanski <alanski@nd.edu>",
    "author": "Alison Lanski [aut, cre],\n  Shiloh Fling [aut],\n  Edwin Welch [aut]",
    "url": "https://github.com/AlisonLanski/IPEDSuploadables,\nhttps://alisonlanski.github.io/IPEDSuploadables/",
    "bug_reports": "https://github.com/AlisonLanski/IPEDSuploadables/issues",
    "repository": "https://cran.r-project.org/package=IPEDSuploadables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "IPEDSuploadables Transforms Institutional Data into Text Files for IPEDS\nAutomated Import/Upload Starting from user-supplied institutional data, these scripts \n    transform, aggregate, and reshape the information to produce \n    key-value pair data files that are able to be uploaded to IPEDS (Integrated Postsecondary Education Data System) \n    through their submission portal <https://surveys.nces.ed.gov/ipeds/>. Starting data specifications can be found in the vignettes.  \n    Final files are saved locally to a location of the user's choice. \n    User-friendly readable files can also be produced for purposes of data review and validation.   "
  },
  {
    "id": 4360,
    "package_name": "KOFM",
    "title": "Test the Kronecker Product Structure in Tensor Factor Models",
    "description": "To test if a tensor time series following a Tucker-decomposition factor model has a Kronecker product structure. Supplementary functions for tensor reshape and its reversal are also included.",
    "version": "0.1.1",
    "maintainer": "Zetai Cen <z.cen@lse.ac.uk>",
    "author": "Zetai Cen [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=KOFM",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "KOFM Test the Kronecker Product Structure in Tensor Factor Models To test if a tensor time series following a Tucker-decomposition factor model has a Kronecker product structure. Supplementary functions for tensor reshape and its reversal are also included.  "
  },
  {
    "id": 4466,
    "package_name": "LLMAgentR",
    "title": "Language Model Agents in R for AI Workflows and Research",
    "description": "Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.",
    "version": "0.3.0",
    "maintainer": "Kwadwo Daddy Nyame Owusu Boakye <kwadwo.owusuboakye@outlook.com>",
    "author": "Kwadwo Daddy Nyame Owusu Boakye [aut, cre]",
    "url": "https://github.com/knowusuboaky/LLMAgentR,\nhttps://knowusuboaky.github.io/LLMAgentR/",
    "bug_reports": "https://github.com/knowusuboaky/LLMAgentR/issues",
    "repository": "https://cran.r-project.org/package=LLMAgentR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "LLMAgentR Language Model Agents in R for AI Workflows and Research Provides modular, graph-based agents powered by large language models (LLMs) for intelligent task execution in R. \n      Supports structured workflows for tasks such as forecasting, data visualization, feature engineering, data wrangling, data cleaning, 'SQL', code generation, weather reporting, and research-driven question answering. \n      Each agent performs iterative reasoning: recommending steps, generating R code, executing, debugging, and explaining results. \n      Includes built-in support for packages such as 'tidymodels', 'modeltime', 'plotly', 'ggplot2', and 'prophet'. Designed for analysts, developers, and teams building intelligent, reproducible AI workflows in R. \n      Compatible with LLM providers such as 'OpenAI', 'Anthropic', 'Groq', and 'Ollama'. Inspired by the Python package 'langagent'.  "
  },
  {
    "id": 4771,
    "package_name": "MIMER",
    "title": "Data Wrangling for Antimicrobial Resistance Studies",
    "description": "Designed for analyzing the Medical Information Mart for Intensive Care(MIMIC) dataset, \n  a repository of freely accessible electronic health records. MIMER(MIMIC-enabled Research) package, offers\n  a suite of data wrangling functions tailored specifically for preparing the dataset for research purposes,\n  particularly in antimicrobial resistance(AMR) studies. It simplifies complex data manipulation tasks, allowing\n  researchers to focus on their primary inquiries without being bogged down by wrangling complexities.",
    "version": "1.0.4",
    "maintainer": "Anoop Velluva <anoop.velluva@liverpool.ac.uk>",
    "author": "Anoop Velluva [aut, cre] (ORCID:\n    <https://orcid.org/0009-0001-6198-6016>),\n  Alessandro Gerada [ctb] (ORCID:\n    <https://orcid.org/0000-0002-6743-4271>),\n  Alexander Howard [ctb] (ORCID: <https://orcid.org/0000-0002-4195-6821>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=MIMER",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "MIMER Data Wrangling for Antimicrobial Resistance Studies Designed for analyzing the Medical Information Mart for Intensive Care(MIMIC) dataset, \n  a repository of freely accessible electronic health records. MIMER(MIMIC-enabled Research) package, offers\n  a suite of data wrangling functions tailored specifically for preparing the dataset for research purposes,\n  particularly in antimicrobial resistance(AMR) studies. It simplifies complex data manipulation tasks, allowing\n  researchers to focus on their primary inquiries without being bogged down by wrangling complexities.  "
  },
  {
    "id": 5575,
    "package_name": "PDtoolkit",
    "title": "Collection of Tools for PD Rating Model Development and\nValidation",
    "description": "The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.",
    "version": "1.2.0",
    "maintainer": "Andrija Djurovic <djandrija@gmail.com>",
    "author": "Andrija Djurovic [aut, cre]",
    "url": "https://github.com/andrija-djurovic/PDtoolkit",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PDtoolkit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PDtoolkit Collection of Tools for PD Rating Model Development and\nValidation The goal of this package is to cover the most common steps in probability of default (PD) rating model development and validation. \n\t     The main procedures available are those that refer to univariate, bivariate, multivariate analysis, calibration and validation. \n\t     Along with accompanied 'monobin' and 'monobinShiny' packages, 'PDtoolkit' provides functions which are suitable for different \n\t     data transformation and modeling tasks such as: \n\t     imputations, monotonic binning of numeric risk factors, binning of categorical risk factors, weights of evidence (WoE) and \n\t     information value (IV) calculations, WoE coding (replacement of risk factors modalities with WoE values), risk factor clustering, \n\t     area under curve (AUC) calculation and others. Additionally, package provides set of validation functions for testing homogeneity, \n\t     heterogeneity, discriminatory and predictive power of the model.  "
  },
  {
    "id": 5619,
    "package_name": "PLMIX",
    "title": "Bayesian Analysis of Finite Mixture of Plackett-Luce Models",
    "description": "Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi:10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi:10.1002/sim.6224>.",
    "version": "2.2.0",
    "maintainer": "Cristina Mollica <cristina.mollica@uniroma1.it>",
    "author": "Cristina Mollica [aut, cre],\n  Luca Tardella [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PLMIX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PLMIX Bayesian Analysis of Finite Mixture of Plackett-Luce Models Fit finite mixtures of Plackett-Luce models for partial top rankings/orderings within the Bayesian framework. It provides MAP point estimates via EM algorithm and posterior MCMC simulations via Gibbs Sampling. It also fits MLE as a special case of the noninformative Bayesian analysis with vague priors. In addition to inferential techniques, the package assists other fundamental phases of a model-based analysis for partial rankings/orderings, by including functions for data manipulation, simulation, descriptive summary, model selection and goodness-of-fit evaluation. Main references on the methods are Mollica and Tardella (2017) <doi:10.1007/s11336-016-9530-0> and Mollica and Tardella (2014) <doi:10.1002/sim.6224>.  "
  },
  {
    "id": 5824,
    "package_name": "PhoneValidator",
    "title": "Client for 'GenderAPI.io' Phone Number Validation and Formatter\nAPI",
    "description": "Provides an interface to the 'GenderAPI.io' Phone Number Validation & Formatter API (<https://www.genderapi.io>) for validating international phone numbers, detecting number type (mobile, landline, Voice over Internet Protocol (VoIP)), retrieving region and country metadata, and formatting numbers to E.164 or national format. Designed to simplify integration into R workflows for data validation, Customer Relationship Management (CRM) data cleaning, and analytics tasks. Full documentation is available at <https://www.genderapi.io/docs-phone-validation-formatter-api>.",
    "version": "1.0.1",
    "maintainer": "Onur Ozturk <onurozturk1980@gmail.com>",
    "author": "Onur Ozturk [aut, cre]",
    "url": "https://github.com/GenderAPI/PhoneValidator-R",
    "bug_reports": "https://github.com/GenderAPI/PhoneValidator-R/issues",
    "repository": "https://cran.r-project.org/package=PhoneValidator",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PhoneValidator Client for 'GenderAPI.io' Phone Number Validation and Formatter\nAPI Provides an interface to the 'GenderAPI.io' Phone Number Validation & Formatter API (<https://www.genderapi.io>) for validating international phone numbers, detecting number type (mobile, landline, Voice over Internet Protocol (VoIP)), retrieving region and country metadata, and formatting numbers to E.164 or national format. Designed to simplify integration into R workflows for data validation, Customer Relationship Management (CRM) data cleaning, and analytics tasks. Full documentation is available at <https://www.genderapi.io/docs-phone-validation-formatter-api>.  "
  },
  {
    "id": 5963,
    "package_name": "PubMedMining",
    "title": "Text-Mining of the 'PubMed' Repository",
    "description": "Easy function for text-mining the 'PubMed' repository based on defined sets of terms.\n    The relationship between fix-terms (related to your research topic) and pub-terms (terms which pivot around your research focus)\n    is calculated using the pointwise mutual information algorithm ('PMI'). Church, Kenneth Ward and Hanks, Patrick (1990) <https://www.aclweb.org/anthology/J90-1003/>\n    A text file is generated with the 'PMI'-scores for each fix-term. Then for each collocation pairs (a fix-term + a pub-term),\n    a text file is generated with related article titles and publishing years. Additional Author section will follow in the next version updates.",
    "version": "1.0.0",
    "maintainer": "Jeff DIDIER <jeff.didier.001@student.uni.lu>",
    "author": "Jeff DIDIER [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=PubMedMining",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "PubMedMining Text-Mining of the 'PubMed' Repository Easy function for text-mining the 'PubMed' repository based on defined sets of terms.\n    The relationship between fix-terms (related to your research topic) and pub-terms (terms which pivot around your research focus)\n    is calculated using the pointwise mutual information algorithm ('PMI'). Church, Kenneth Ward and Hanks, Patrick (1990) <https://www.aclweb.org/anthology/J90-1003/>\n    A text file is generated with the 'PMI'-scores for each fix-term. Then for each collocation pairs (a fix-term + a pub-term),\n    a text file is generated with related article titles and publishing years. Additional Author section will follow in the next version updates.  "
  },
  {
    "id": 6003,
    "package_name": "QR",
    "title": "QR Factorization without Pivoting",
    "description": "This function performs QR factorization without pivoting to a real or complex matrix. It is based on Anderson. E. and ten others (1999) \"LAPACK Users' Guide\". Third Edition. SIAM.",
    "version": "0.1.3",
    "maintainer": "Juan Claramunt Gonzalez <j.claramunt.gonzalez@fsw.leidenuniv.nl>",
    "author": "Juan Claramunt Gonzalez [aut, cre, cph]",
    "url": "http://www.netlib.org/lapack/explore-html/df/dc5/group__variants_g_ecomputational_ga3766ea903391b5cf9008132f7440ec7b.html",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=QR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "QR QR Factorization without Pivoting This function performs QR factorization without pivoting to a real or complex matrix. It is based on Anderson. E. and ten others (1999) \"LAPACK Users' Guide\". Third Edition. SIAM.  "
  },
  {
    "id": 6101,
    "package_name": "RATest",
    "title": "Randomization Tests",
    "description": "A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter.",
    "version": "0.1.10",
    "maintainer": "Mauricio Olivares <mau.olivarego@gmail.com>",
    "author": "Mauricio Olivares [aut, cre],\n  Ignacio Sarmiento-Barbieri [aut]",
    "url": "",
    "bug_reports": "https://github.com/ignaciomsarmiento/RATest/issues",
    "repository": "https://cran.r-project.org/package=RATest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RATest Randomization Tests A collection of randomization tests, data sets and examples. The current version focuses on five testing problems and their implementation in empirical work. First, it facilitates the empirical researcher to test for particular hypotheses, such as comparisons of means, medians, and variances from k populations using robust permutation tests, which asymptotic validity holds under very weak assumptions, while retaining the exact rejection probability in finite samples when the underlying distributions are identical. Second, the description and implementation of a permutation test for testing the continuity assumption of the baseline covariates in the sharp regression discontinuity design (RDD) as in Canay and Kamat (2018) <https://goo.gl/UZFqt7>. More specifically, it allows the user to select a set of covariates and test the aforementioned hypothesis using a permutation test based on the Cramer-von Misses test statistic. Graphical inspection of the empirical CDF and histograms for the variables of interest is also supported in the package. Third, it provides the practitioner with an effortless implementation of a permutation test based on the martingale decomposition of the empirical process for testing for heterogeneous treatment effects in the presence of an estimated nuisance parameter as in Chung and Olivares (2021) <doi:10.1016/j.jeconom.2020.09.015>. Fourth, this version considers the two-sample goodness-of-fit testing problem under covariate adaptive randomization and implements a permutation test based on a prepivoted Kolmogorov-Smirnov test statistic. Lastly, it implements an asymptotically valid permutation test based on the quantile process for the hypothesis of constant quantile treatment effects in the presence of an estimated nuisance parameter.  "
  },
  {
    "id": 6165,
    "package_name": "REDCapCAST",
    "title": "REDCap Metadata Casting and Castellated Data Handling",
    "description": "Casting metadata for REDCap database creation and handling of \n    castellated data using repeated instruments and longitudinal projects in \n    'REDCap'. Keeps a focused data export approach, by allowing to only export \n    required data from the database. Also for casting new REDCap databases based \n    on datasets from other sources.\n    Originally forked from the R part of 'REDCapRITS' by Paul Egeler. \n    See <https://github.com/pegeler/REDCapRITS>.\n    'REDCap' (Research Electronic Data Capture) is a secure, web-based software\n    platform designed to support data capture for research studies, providing\n    1) an intuitive interface for validated data capture; 2) audit trails for\n    tracking data manipulation and export procedures; 3) automated export\n    procedures for seamless data downloads to common statistical packages; and\n    4) procedures for data integration and interoperability with external \n    sources (Harris et al (2009) <doi:10.1016/j.jbi.2008.08.010>; \n    Harris et al (2019) <doi:10.1016/j.jbi.2019.103208>).",
    "version": "25.3.2",
    "maintainer": "Andreas Gammelgaard Damsbo <agdamsbo@clin.au.dk>",
    "author": "Andreas Gammelgaard Damsbo [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7559-1154>),\n  Paul Egeler [aut] (ORCID: <https://orcid.org/0000-0001-6948-9498>)",
    "url": "https://github.com/agdamsbo/REDCapCAST,\nhttps://agdamsbo.github.io/REDCapCAST/",
    "bug_reports": "https://github.com/agdamsbo/REDCapCAST/issues",
    "repository": "https://cran.r-project.org/package=REDCapCAST",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "REDCapCAST REDCap Metadata Casting and Castellated Data Handling Casting metadata for REDCap database creation and handling of \n    castellated data using repeated instruments and longitudinal projects in \n    'REDCap'. Keeps a focused data export approach, by allowing to only export \n    required data from the database. Also for casting new REDCap databases based \n    on datasets from other sources.\n    Originally forked from the R part of 'REDCapRITS' by Paul Egeler. \n    See <https://github.com/pegeler/REDCapRITS>.\n    'REDCap' (Research Electronic Data Capture) is a secure, web-based software\n    platform designed to support data capture for research studies, providing\n    1) an intuitive interface for validated data capture; 2) audit trails for\n    tracking data manipulation and export procedures; 3) automated export\n    procedures for seamless data downloads to common statistical packages; and\n    4) procedures for data integration and interoperability with external \n    sources (Harris et al (2009) <doi:10.1016/j.jbi.2008.08.010>; \n    Harris et al (2019) <doi:10.1016/j.jbi.2019.103208>).  "
  },
  {
    "id": 6252,
    "package_name": "RJafroc",
    "title": "Artificial Intelligence Systems and Observer Performance",
    "description": "Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.",
    "version": "2.1.2",
    "maintainer": "Dev Chakraborty <dpc10ster@gmail.com>",
    "author": "Dev Chakraborty [cre, aut, cph],\n  Peter Phillips [ctb],\n  Xuetong Zhai [aut]",
    "url": "https://dpc10ster.github.io/RJafroc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=RJafroc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "RJafroc Artificial Intelligence Systems and Observer Performance Analyzing the performance of artificial intelligence\n (AI) systems/algorithms characterized by a 'search-and-report'\n strategy. Historically observer performance has dealt with\n measuring radiologists' performances in search tasks, e.g., searching\n for lesions in medical images and reporting them, but the implicit\n location information has been ignored. The implemented methods apply\n to analyzing the absolute and relative performances of AI systems,\n comparing AI performance to a group of human readers or optimizing the\n reporting threshold of an AI system. In addition to performing historical\n receiver operating receiver operating characteristic (ROC) analysis\n (localization information ignored), the software also performs\n free-response receiver operating characteristic (FROC)\n analysis, where lesion localization information is used. A book\n using the software has been published: Chakraborty DP: Observer\n Performance Methods for Diagnostic Imaging - Foundations, Modeling,\n and Applications with R-Based Examples, Taylor-Francis LLC; 2017:\n <https://www.routledge.com/Observer-Performance-Methods-for-Diagnostic-Imaging-Foundations-Modeling/Chakraborty/p/book/9781482214840>.\n Online updates to this book, which use the software, are at\n <https://dpc10ster.github.io/RJafrocQuickStart/>,\n <https://dpc10ster.github.io/RJafrocRocBook/> and at\n <https://dpc10ster.github.io/RJafrocFrocBook/>. Supported data\n collection paradigms are the ROC, FROC and the location ROC (LROC).\n ROC data consists of single ratings per images, where a rating is\n the perceived confidence level that the image is that of a diseased\n patient. An ROC curve is a plot of true positive fraction vs. false\n positive fraction. FROC data consists of a variable number (zero or\n more) of mark-rating pairs per image, where a mark is the location\n of a reported suspicious region and the rating is the confidence\n level that it is a real lesion. LROC data consists of a rating and a\n location of the most suspicious region, for every image. Four models\n of observer performance, and curve-fitting software, are implemented:\n the binormal model (BM), the contaminated binormal model (CBM), the\n correlated contaminated binormal model (CORCBM), and the radiological\n search model (RSM). Unlike the binormal model, CBM, CORCBM and RSM\n predict 'proper' ROC curves that do not inappropriately cross the\n chance diagonal. Additionally, RSM parameters are related to search\n performance (not measured in conventional ROC analysis) and\n classification performance. Search performance refers to finding\n lesions, i.e., true positives, while simultaneously not finding false\n positive locations. Classification performance measures the ability to\n distinguish between true and false positive locations. Knowing these\n separate performances allows principled optimization of reader or AI\n system performance. This package supersedes Windows JAFROC (jackknife\n alternative FROC) software V4.2.1,\n <https://github.com/dpc10ster/WindowsJafroc>. Package functions are\n organized as follows. Data file related function names are preceded\n by 'Df', curve fitting functions by 'Fit', included data sets by 'dataset',\n plotting functions by 'Plot', significance testing functions by 'St',\n sample size related functions by 'Ss', data simulation functions by\n 'Simulate' and utility functions by 'Util'. Implemented are figures of\n merit (FOMs) for quantifying performance and functions for visualizing\n empirical or fitted operating characteristics: e.g., ROC, FROC, alternative\n FROC (AFROC) and weighted AFROC (wAFROC) curves. For fully crossed study\n designs significance testing of reader-averaged FOM differences between\n modalities is implemented via either Dorfman-Berbaum-Metz or the\n Obuchowski-Rockette methods. Also implemented is single treatment analysis,\n which allows comparison of performance of a group of radiologists to a\n specified value, or comparison of AI to a group of radiologists interpreting\n the same cases. Crossed-modality analysis is implemented wherein there are\n two crossed treatment factors and the aim is to determined performance in\n each treatment factor averaged over all levels of the second factor. Sample\n size estimation tools are provided for ROC and FROC studies; these use\n estimates of the relevant variances from a pilot study to predict required\n numbers of readers and cases in a pivotal study to achieve the desired power.\n Utility and data file manipulation functions allow data to be read in any of\n the currently used input formats, including Excel, and the results of the\n analysis can be viewed in text or Excel output files. The methods are\n illustrated with several included datasets from the author's collaborations.\n This update includes improvements to the code, some as a result of\n user-reported bugs and new feature requests, and others discovered during\n ongoing testing and code simplification.  "
  },
  {
    "id": 6624,
    "package_name": "Rdta",
    "title": "Data Transforming Augmentation for Linear Mixed Models",
    "description": "We provide a toolbox to fit univariate and multivariate linear mixed models via data transforming augmentation. Users can also fit these models via typical data augmentation for a comparison. It returns either maximum likelihood estimates of unknown model parameters (hyper-parameters) via an EM algorithm or posterior samples of those parameters via MCMC. Also see Tak et al. (2019) <doi:10.1080/10618600.2019.1704295>.",
    "version": "1.0.1",
    "maintainer": "Hyungsuk Tak <hyungsuk.tak@gmail.com>",
    "author": "Hyungsuk Tak, Kisung You, Sujit K. Ghosh, and Bingyue Su",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Rdta",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Rdta Data Transforming Augmentation for Linear Mixed Models We provide a toolbox to fit univariate and multivariate linear mixed models via data transforming augmentation. Users can also fit these models via typical data augmentation for a comparison. It returns either maximum likelihood estimates of unknown model parameters (hyper-parameters) via an EM algorithm or posterior samples of those parameters via MCMC. Also see Tak et al. (2019) <doi:10.1080/10618600.2019.1704295>.  "
  },
  {
    "id": 6966,
    "package_name": "SGDinference",
    "title": "Inference with Stochastic Gradient Descent",
    "description": "Estimation and inference methods for large-scale mean and quantile regression models via stochastic (sub-)gradient descent (S-subGD) algorithms. \n    The inference procedure handles cross-sectional data sequentially: \n    (i) updating the parameter estimate with each incoming \"new observation\", \n    (ii) aggregating it as a Polyak-Ruppert average, and \n    (iii) computing an asymptotically pivotal statistic for inference through random scaling. \n    The methodology used in the 'SGDinference' package is described in detail in the following papers: \n    (i) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2022) <doi:10.1609/aaai.v36i7.20701> \"Fast and robust online inference with stochastic gradient descent via random scaling\".\n    (ii) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2023) <arXiv:2209.14502> \"Fast Inference for Quantile Regression with Tens of Millions of Observations\". ",
    "version": "0.1.0",
    "maintainer": "Youngki Shin <shiny11@mcmaster.ca>",
    "author": "Sokbae Lee [aut],\n  Yuan Liao [aut],\n  Myung Hwan Seo [aut],\n  Youngki Shin [aut, cre]",
    "url": "https://github.com/SGDinference-Lab/SGDinference/",
    "bug_reports": "https://github.com/SGDinference-Lab/SGDinference/issues",
    "repository": "https://cran.r-project.org/package=SGDinference",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SGDinference Inference with Stochastic Gradient Descent Estimation and inference methods for large-scale mean and quantile regression models via stochastic (sub-)gradient descent (S-subGD) algorithms. \n    The inference procedure handles cross-sectional data sequentially: \n    (i) updating the parameter estimate with each incoming \"new observation\", \n    (ii) aggregating it as a Polyak-Ruppert average, and \n    (iii) computing an asymptotically pivotal statistic for inference through random scaling. \n    The methodology used in the 'SGDinference' package is described in detail in the following papers: \n    (i) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2022) <doi:10.1609/aaai.v36i7.20701> \"Fast and robust online inference with stochastic gradient descent via random scaling\".\n    (ii) Lee, S., Liao, Y., Seo, M.H. and Shin, Y. (2023) <arXiv:2209.14502> \"Fast Inference for Quantile Regression with Tens of Millions of Observations\".   "
  },
  {
    "id": 7118,
    "package_name": "SSBtools",
    "title": "Algorithms and Tools for Tabular Statistics and Hierarchical\nComputations",
    "description": "Includes general data manipulation functions, algorithms for statistical disclosure control (Langsrud, 2024) <doi:10.1007/978-3-031-69651-0_6> and functions for hierarchical computations by sparse model matrices (Langsrud, 2023) <doi:10.32614/RJ-2023-088>. ",
    "version": "1.8.6",
    "maintainer": "\u00d8yvind Langsrud <oyl@ssb.no>",
    "author": "\u00d8yvind Langsrud [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1380-4396>),\n  Daniel Lupp [aut] (ORCID: <https://orcid.org/0000-0003-3575-1691>),\n  Bj\u00f8rn-Helge Mevik [ctb],\n  Vidar Norstein Klungre [rev] (ORCID:\n    <https://orcid.org/0000-0003-1925-5911>),\n  Statistics Norway [cph]",
    "url": "https://github.com/statisticsnorway/ssb-ssbtools,\nhttps://statisticsnorway.github.io/ssb-ssbtools/",
    "bug_reports": "https://github.com/statisticsnorway/ssb-ssbtools/issues",
    "repository": "https://cran.r-project.org/package=SSBtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "SSBtools Algorithms and Tools for Tabular Statistics and Hierarchical\nComputations Includes general data manipulation functions, algorithms for statistical disclosure control (Langsrud, 2024) <doi:10.1007/978-3-031-69651-0_6> and functions for hierarchical computations by sparse model matrices (Langsrud, 2023) <doi:10.32614/RJ-2023-088>.   "
  },
  {
    "id": 7779,
    "package_name": "Tmisc",
    "title": "Turner Miscellaneous",
    "description": "Miscellaneous utility functions for data manipulation,\n    data tidying, and working with gene expression data.",
    "version": "1.0.1",
    "maintainer": "Stephen Turner <vustephen@gmail.com>",
    "author": "Stephen Turner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9140-9028>)",
    "url": "https://github.com/stephenturner/Tmisc,\nhttps://stephenturner.github.io/Tmisc/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Tmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tmisc Turner Miscellaneous Miscellaneous utility functions for data manipulation,\n    data tidying, and working with gene expression data.  "
  },
  {
    "id": 7790,
    "package_name": "Tplyr",
    "title": "A Traceability Focused Grammar of Clinical Data Summary",
    "description": "A traceability focused tool created to simplify the data manipulation necessary to create clinical summaries.",
    "version": "1.2.1",
    "maintainer": "Mike Stackhouse <mike.stackhouse@atorusresearch.com>",
    "author": "Eli Miller [aut] (ORCID: <https://orcid.org/0000-0002-2127-9456>),\n  Mike Stackhouse [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6030-723X>),\n  Ashley Tarasiewicz [aut],\n  Nathan Kosiba [ctb] (ORCID: <https://orcid.org/0000-0001-5359-4234>),\n  Sadchla Mascary [ctb],\n  Andrew Bates [ctb],\n  Shiyu Chen [ctb],\n  Oleksii Mikryukov [ctb],\n  Atorus Research LLC [cph]",
    "url": "https://github.com/atorus-research/Tplyr",
    "bug_reports": "https://github.com/atorus-research/Tplyr/issues",
    "repository": "https://cran.r-project.org/package=Tplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Tplyr A Traceability Focused Grammar of Clinical Data Summary A traceability focused tool created to simplify the data manipulation necessary to create clinical summaries.  "
  },
  {
    "id": 8010,
    "package_name": "WGCNA",
    "title": "Weighted Correlation Network Analysis",
    "description": "Functions necessary to perform Weighted Correlation Network Analysis on high-dimensional data as originally described in Horvath and Zhang (2005) <doi:10.2202/1544-6115.1128> and Langfelder and Horvath (2008) <doi:10.1186/1471-2105-9-559>. Includes functions for rudimentary data cleaning, construction of correlation networks, module identification, summarization, and relating of variables and modules to sample traits. Also includes a number of utility functions for data manipulation and visualization.",
    "version": "1.73",
    "maintainer": "Peter Langfelder <Peter.Langfelder@gmail.com>",
    "author": "Peter Langfelder [aut, cre],\n  Steve Horvath [aut],\n  Chaochao Cai [aut],\n  Jun Dong [aut],\n  Jeremy Miller [aut],\n  Lin Song [aut],\n  Andy Yip [aut],\n  Bin Zhang [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WGCNA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WGCNA Weighted Correlation Network Analysis Functions necessary to perform Weighted Correlation Network Analysis on high-dimensional data as originally described in Horvath and Zhang (2005) <doi:10.2202/1544-6115.1128> and Langfelder and Horvath (2008) <doi:10.1186/1471-2105-9-559>. Includes functions for rudimentary data cleaning, construction of correlation networks, module identification, summarization, and relating of variables and modules to sample traits. Also includes a number of utility functions for data manipulation and visualization.  "
  },
  {
    "id": 8092,
    "package_name": "WrightMap",
    "title": "IRT Item-Person Map with 'ConQuest' Integration",
    "description": "A powerful yet simple graphical tool available in the field of psychometrics is the Wright Map (also known as item maps or item-person maps), which presents the location of both respondents and items on the same scale. Wright Maps are commonly used to present the results of dichotomous or polytomous item response models. The 'WrightMap' package provides functions to create these plots from item parameters and person estimates stored as R objects. Although the package can be used in conjunction with any software used to estimate the IRT model (e.g. 'TAM', 'mirt', 'eRm' or 'IRToys' in 'R', or 'Stata', 'Mplus', etc.),  'WrightMap' features special integration with 'ConQuest' to facilitate reading and plotting its output directly.The 'wrightMap' function creates Wright Maps based on person estimates and item parameters produced by an item response analysis. The 'CQmodel' function reads output files created using 'ConQuest' software and creates a set of data frames for easy data manipulation, bundled in a 'CQmodel' object. The 'wrightMap' function can take a 'CQmodel' object as input or it can be used to create Wright Maps directly from data frames of person and item parameters.",
    "version": "1.4",
    "maintainer": "David Torres Irribarra <david@torresirribarra.me>",
    "author": "David Torres Irribarra [aut, cre],\n  Rebecca Freund [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=WrightMap",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "WrightMap IRT Item-Person Map with 'ConQuest' Integration A powerful yet simple graphical tool available in the field of psychometrics is the Wright Map (also known as item maps or item-person maps), which presents the location of both respondents and items on the same scale. Wright Maps are commonly used to present the results of dichotomous or polytomous item response models. The 'WrightMap' package provides functions to create these plots from item parameters and person estimates stored as R objects. Although the package can be used in conjunction with any software used to estimate the IRT model (e.g. 'TAM', 'mirt', 'eRm' or 'IRToys' in 'R', or 'Stata', 'Mplus', etc.),  'WrightMap' features special integration with 'ConQuest' to facilitate reading and plotting its output directly.The 'wrightMap' function creates Wright Maps based on person estimates and item parameters produced by an item response analysis. The 'CQmodel' function reads output files created using 'ConQuest' software and creates a set of data frames for easy data manipulation, bundled in a 'CQmodel' object. The 'wrightMap' function can take a 'CQmodel' object as input or it can be used to create Wright Maps directly from data frames of person and item parameters.  "
  },
  {
    "id": 8116,
    "package_name": "YRmisc",
    "title": "Y&R Miscellaneous R Functions",
    "description": "Miscellaneous functions for data analysis, portfolio management, graphics, data manipulation, statistical investigation, including descriptive statistics, creating leading and lagging variables, portfolio return analysis, time series difference and percentage change calculation, stacking data for higher efficient analysis.",
    "version": "0.1.6",
    "maintainer": "Xuanhua (Peter) Yin <peteryin.sju@hotmail.com>",
    "author": "Manuel Russon <RUSSONM@stjohns.edu>, Xuanhua (Peter) Yin <peteryin.sju@hotmail.com>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=YRmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "YRmisc Y&R Miscellaneous R Functions Miscellaneous functions for data analysis, portfolio management, graphics, data manipulation, statistical investigation, including descriptive statistics, creating leading and lagging variables, portfolio return analysis, time series difference and percentage change calculation, stacking data for higher efficient analysis.  "
  },
  {
    "id": 8122,
    "package_name": "Ymisc",
    "title": "Miscellaneous Functions",
    "description": "The Author's personal R Package that contains miscellaneous functions. \n    The current version of package contains miscellaneous functions for brain data \n    to compute Asymmetry Index (AI) and bilateral (L+R) measures and reshape the data.",
    "version": "0.1.0",
    "maintainer": "Yoo Ri Hwang <yrhwang89@gmail.com>",
    "author": "Yoo Ri Hwang [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=Ymisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "Ymisc Miscellaneous Functions The Author's personal R Package that contains miscellaneous functions. \n    The current version of package contains miscellaneous functions for brain data \n    to compute Asymmetry Index (AI) and bilateral (L+R) measures and reshape the data.  "
  },
  {
    "id": 8144,
    "package_name": "aIc",
    "title": "Testing for Compositional Pathologies in Datasets",
    "description": "A set of tests for compositional pathologies. Tests for coherence of correlations with aIc.coherent() as suggested by (Erb et al. (2020) <doi:10.1016/j.acags.2020.100026>),  compositional dominance of distance with aIc.dominant(), compositional perturbation invariance with aIc.perturb() as suggested by (Aitchison (1992) <doi:10.1007/BF00891269>) and singularity of the covariation matrix with aIc.singular(). Currently tests five data transformations: prop, clr, TMM, TMMwsp, and RLE from the R packages 'ALDEx2', 'edgeR' and 'DESeq2' (Fernandes et al (2014) <doi:10.1186/2049-2618-2-15>, Anders et al. (2013)<doi:10.1038/nprot.2013.099>).",
    "version": "1.0",
    "maintainer": "Greg Gloor <ggloor@uwo.ca>",
    "author": "Greg Gloor",
    "url": "https://github.com/ggloor/aIc",
    "bug_reports": "https://github.com/ggloor/aIc/issues",
    "repository": "https://cran.r-project.org/package=aIc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "aIc Testing for Compositional Pathologies in Datasets A set of tests for compositional pathologies. Tests for coherence of correlations with aIc.coherent() as suggested by (Erb et al. (2020) <doi:10.1016/j.acags.2020.100026>),  compositional dominance of distance with aIc.dominant(), compositional perturbation invariance with aIc.perturb() as suggested by (Aitchison (1992) <doi:10.1007/BF00891269>) and singularity of the covariation matrix with aIc.singular(). Currently tests five data transformations: prop, clr, TMM, TMMwsp, and RLE from the R packages 'ALDEx2', 'edgeR' and 'DESeq2' (Fernandes et al (2014) <doi:10.1186/2049-2618-2-15>, Anders et al. (2013)<doi:10.1038/nprot.2013.099>).  "
  },
  {
    "id": 8187,
    "package_name": "academictwitteR",
    "title": "Access the Twitter Academic Research Product Track V2 API\nEndpoint",
    "description": "Package to query the Twitter Academic Research Product Track,\n    providing access to full-archive search and other v2 API endpoints. Functions\n    are written with academic research in mind. They provide flexibility in how \n    the user wishes to store collected data, and encourage regular storage of data\n    to mitigate loss when collecting large volumes of tweets. They also provide\n    workarounds to manage and reshape the format in which data is provided on\n    the client side.",
    "version": "0.3.1",
    "maintainer": "Christopher Barrie <christopher.barrie@ed.ac.uk>",
    "author": "Christopher Barrie [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9156-990X>),\n  Justin Chun-ting Ho [aut] (ORCID:\n    <https://orcid.org/0000-0002-7884-1059>),\n  Chung-hong Chan [ctb] (ORCID: <https://orcid.org/0000-0002-6232-7530>),\n  Noelia Rico [ctb] (ORCID: <https://orcid.org/0000-0002-6169-4523>),\n  Tim K\u00f6nig [ctb] (ORCID: <https://orcid.org/0000-0002-2852-2690>),\n  Thomas Davidson [ctb] (ORCID: <https://orcid.org/0000-0002-5947-7490>)",
    "url": "https://github.com/cjbarrie/academictwitteR",
    "bug_reports": "https://github.com/cjbarrie/academictwitteR/issues",
    "repository": "https://cran.r-project.org/package=academictwitteR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "academictwitteR Access the Twitter Academic Research Product Track V2 API\nEndpoint Package to query the Twitter Academic Research Product Track,\n    providing access to full-archive search and other v2 API endpoints. Functions\n    are written with academic research in mind. They provide flexibility in how \n    the user wishes to store collected data, and encourage regular storage of data\n    to mitigate loss when collecting large volumes of tweets. They also provide\n    workarounds to manage and reshape the format in which data is provided on\n    the client side.  "
  },
  {
    "id": 8210,
    "package_name": "acledR",
    "title": "Manipulate ACLED Data",
    "description": "Tools working with data from ACLED (Armed Conflict Location and Event Data). Functions include simplified access to ACLED's API (<https://apidocs.acleddata.com/>), methods for keeping local versions of ACLED data up-to-date, and functions for common ACLED data transformations.",
    "version": "1.0.1",
    "maintainer": "Trey Billing <t.billing@acleddata.com>",
    "author": "Armed Conflict Location and Event Data ACLED [cph],\n  Trey Billing [aut, cre],\n  Lucas Fagliano [aut],\n  Katayoun Kishi [ctb]",
    "url": "https://dtacled.github.io/acledR/",
    "bug_reports": "https://github.com/dtacled/acledR/issues",
    "repository": "https://cran.r-project.org/package=acledR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "acledR Manipulate ACLED Data Tools working with data from ACLED (Armed Conflict Location and Event Data). Functions include simplified access to ACLED's API (<https://apidocs.acleddata.com/>), methods for keeping local versions of ACLED data up-to-date, and functions for common ACLED data transformations.  "
  },
  {
    "id": 8749,
    "package_name": "autodb",
    "title": "Automatic Database Normalisation for Data Frames",
    "description": "Automatic normalisation of a data frame to third normal form, with\n  the intention of easing the process of data cleaning. (Usage to design your\n  actual database for you is not advised.)\n  Originally inspired by the 'AutoNormalize' library for 'Python' by 'Alteryx'\n  (<https://github.com/alteryx/autonormalize>), with various changes and\n  improvements. Automatic discovery of functional or approximate dependencies,\n  normalisation based on those, and plotting of the resulting \"database\" via\n  'Graphviz', with options to exclude some attributes at discovery time, or\n  remove discovered dependencies at normalisation time.",
    "version": "3.2.4",
    "maintainer": "Mark Webster <markwebster204@yahoo.co.uk>",
    "author": "Mark Webster [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3351-0686>)",
    "url": "https://charnelmouse.github.io/autodb/,\nhttps://github.com/CharnelMouse/autodb",
    "bug_reports": "https://github.com/CharnelMouse/autodb/issues",
    "repository": "https://cran.r-project.org/package=autodb",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "autodb Automatic Database Normalisation for Data Frames Automatic normalisation of a data frame to third normal form, with\n  the intention of easing the process of data cleaning. (Usage to design your\n  actual database for you is not advised.)\n  Originally inspired by the 'AutoNormalize' library for 'Python' by 'Alteryx'\n  (<https://github.com/alteryx/autonormalize>), with various changes and\n  improvements. Automatic discovery of functional or approximate dependencies,\n  normalisation based on those, and plotting of the resulting \"database\" via\n  'Graphviz', with options to exclude some attributes at discovery time, or\n  remove discovered dependencies at normalisation time.  "
  },
  {
    "id": 8994,
    "package_name": "bdc",
    "title": "Biodiversity Data Cleaning",
    "description": "It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.",
    "version": "1.1.5",
    "maintainer": "Bruno Ribeiro <ribeiro.brr@gmail.com>",
    "author": "Bruno Ribeiro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7755-6715>),\n  Santiago Velazco [aut] (ORCID: <https://orcid.org/0000-0002-7527-0967>),\n  Karlo Guidoni-Martins [aut] (ORCID:\n    <https://orcid.org/0000-0002-8458-8467>),\n  Geiziane Tessarolo [aut] (ORCID:\n    <https://orcid.org/0000-0003-1361-0062>),\n  Lucas Jardim [aut] (ORCID: <https://orcid.org/0000-0003-2602-5575>),\n  Steven Bachman [ctb] (ORCID: <https://orcid.org/0000-0003-1085-6075>),\n  Rafael Loyola [ctb] (ORCID: <https://orcid.org/0000-0001-5323-2735>)",
    "url": "https://brunobrr.github.io/bdc/ (website)\nhttps://github.com/brunobrr/bdc",
    "bug_reports": "https://github.com/brunobrr/bdc/issues",
    "repository": "https://cran.r-project.org/package=bdc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bdc Biodiversity Data Cleaning It brings together several aspects of biodiversity\n    data-cleaning in one place. 'bdc' is organized in thematic modules\n    related to different biodiversity dimensions, including 1) Merge\n    datasets: standardization and integration of different datasets; 2)\n    Pre-filter: flagging and removal of invalid or non-interpretable\n    information, followed by data amendments; 3) Taxonomy: cleaning,\n    parsing, and harmonization of scientific names from several taxonomic\n    groups against taxonomic databases locally stored through the\n    application of exact and partial matching algorithms; 4) Space:\n    flagging of erroneous, suspect, and low-precision geographic\n    coordinates; and 5) Time: flagging and, whenever possible, correction\n    of inconsistent collection date. In addition, it contains\n    features to visualize, document, and report data quality \u2013 which is\n    essential for making data quality assessment transparent and\n    reproducible. The reference for the methodology is Bruno et al. (2022)\n    <doi:10.1111/2041-210X.13868>.  "
  },
  {
    "id": 9121,
    "package_name": "bigmds",
    "title": "Multidimensional Scaling for Big Data",
    "description": "MDS is a statistic tool for reduction of dimensionality, using as input a distance\n    matrix of dimensions n \u00d7 n. When n is large, classical algorithms suffer from\n    computational problems and MDS configuration can not be obtained.\n    With this package, we address these problems by means of six algorithms, being two of them \n    original proposals:\n        - Landmark MDS proposed by De Silva V. and JB. Tenenbaum (2004).\n        - Interpolation MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Reduced MDS proposed by Paradis E (2018).\n        - Pivot MDS proposed by Brandes U. and C. Pich (2007)\n        - Divide-and-conquer MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Fast MDS, proposed by Yang, T., J. Liu, L. McMillan and W. Wang (2006).",
    "version": "3.0.0",
    "maintainer": "Cristian Pach\u00f3n Garc\u00eda <cc.pachon@gmail.com>",
    "author": "Cristian Pach\u00f3n Garc\u00eda [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9518-4874>),\n  Pedro Delicado [aut] (ORCID: <https://orcid.org/0000-0003-3933-4852>)",
    "url": "https://github.com/pachoning/bigmds",
    "bug_reports": "https://github.com/pachoning/bigmds/issues",
    "repository": "https://cran.r-project.org/package=bigmds",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "bigmds Multidimensional Scaling for Big Data MDS is a statistic tool for reduction of dimensionality, using as input a distance\n    matrix of dimensions n \u00d7 n. When n is large, classical algorithms suffer from\n    computational problems and MDS configuration can not be obtained.\n    With this package, we address these problems by means of six algorithms, being two of them \n    original proposals:\n        - Landmark MDS proposed by De Silva V. and JB. Tenenbaum (2004).\n        - Interpolation MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Reduced MDS proposed by Paradis E (2018).\n        - Pivot MDS proposed by Brandes U. and C. Pich (2007)\n        - Divide-and-conquer MDS proposed by Delicado P. and C. Pach\u00f3n-Garc\u00eda (2021)\n        <arXiv:2007.11919> (original proposal).\n        - Fast MDS, proposed by Yang, T., J. Liu, L. McMillan and W. Wang (2006).  "
  },
  {
    "id": 9254,
    "package_name": "blit",
    "title": "Bioinformatics Library for Integrated Tools",
    "description": "An all-encompassing R toolkit designed to streamline the\n    process of calling various bioinformatics software and then performing data\n    analysis and visualization in R. With 'blit', users can easily integrate a\n    wide array of bioinformatics command line tools into their workflows,\n    leveraging the power of R for sophisticated data manipulation and graphical\n    representation.",
    "version": "0.2.0",
    "maintainer": "Yun Peng <yunyunp96@163.com>",
    "author": "Yun Peng [aut, cre] (ORCID: <https://orcid.org/0000-0003-2801-3332>),\n  Shixiang Wang [aut] (ORCID: <https://orcid.org/0000-0001-9855-7357>),\n  Jennifer Lu [cph] (Author of the included scripts from Kraken2 and\n    KrakenTools libraries),\n  Li Song [cph] (Author of included scripts from TRUST4 library),\n  X. Shirley Liu [cph] (Author of included scripts from TRUST4 library)",
    "url": "https://github.com/WangLabCSU/blit",
    "bug_reports": "https://github.com/WangLabCSU/blit/issues",
    "repository": "https://cran.r-project.org/package=blit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blit Bioinformatics Library for Integrated Tools An all-encompassing R toolkit designed to streamline the\n    process of calling various bioinformatics software and then performing data\n    analysis and visualization in R. With 'blit', users can easily integrate a\n    wide array of bioinformatics command line tools into their workflows,\n    leveraging the power of R for sophisticated data manipulation and graphical\n    representation.  "
  },
  {
    "id": 9268,
    "package_name": "blockr.core",
    "title": "Graphical Web-Framework for Data Manipulation and Visualization",
    "description": "A framework for data manipulation and visualization using a\n    web-based point and click user interface where analysis pipelines are decomposed into re-usable and parameterizable blocks.",
    "version": "0.1.1",
    "maintainer": "Nicolas Bennett <nicolas@cynkra.com>",
    "author": "Nicolas Bennett [aut, cre],\n  David Granjon [aut],\n  Christoph Sax [aut],\n  Karma Tarap [ctb],\n  John Coene [ctb],\n  Bristol Myers Squibb [fnd]",
    "url": "https://bristolmyerssquibb.github.io/blockr.core/",
    "bug_reports": "https://github.com/BristolMyersSquibb/blockr.core/issues",
    "repository": "https://cran.r-project.org/package=blockr.core",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "blockr.core Graphical Web-Framework for Data Manipulation and Visualization A framework for data manipulation and visualization using a\n    web-based point and click user interface where analysis pipelines are decomposed into re-usable and parameterizable blocks.  "
  },
  {
    "id": 9578,
    "package_name": "cancerradarr",
    "title": "Cancer RADAR Project Tool",
    "description": "Cancer RADAR is a project which aim is to develop an\n    infrastructure that allows quantifying the risk of cancer by migration\n    background across Europe.  This package contains a set of functions\n    cancer registries partners should use to reshape 5 year-age group\n    cancer incidence data into a set of summary statistics (see Boyle &\n    Parkin (1991, ISBN:978-92-832-1195-2)) in lines with Cancer RADAR data\n    protections rules.",
    "version": "2.1.0",
    "maintainer": "Damien Georges <georgesd@iarc.who.int>",
    "author": "Nienke Alberts [aut],\n  Damien Georges [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2425-7591>),\n  Stefano Rosso [aut],\n  Iacopo Baussano [aut] (ORCID: <https://orcid.org/0000-0002-7322-1862>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cancerradarr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cancerradarr Cancer RADAR Project Tool Cancer RADAR is a project which aim is to develop an\n    infrastructure that allows quantifying the risk of cancer by migration\n    background across Europe.  This package contains a set of functions\n    cancer registries partners should use to reshape 5 year-age group\n    cancer incidence data into a set of summary statistics (see Boyle &\n    Parkin (1991, ISBN:978-92-832-1195-2)) in lines with Cancer RADAR data\n    protections rules.  "
  },
  {
    "id": 9687,
    "package_name": "cba",
    "title": "Clustering for Business Analytics",
    "description": "Implements clustering techniques such as Proximus and Rock, utility functions for efficient computation of cross distances and data manipulation. ",
    "version": "0.2-25",
    "maintainer": "Christian Buchta <christian.buchta@wu.ac.at>",
    "author": "Christian Buchta [aut, cre],\n  Michael Hahsler [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cba",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cba Clustering for Business Analytics Implements clustering techniques such as Proximus and Rock, utility functions for efficient computation of cross distances and data manipulation.   "
  },
  {
    "id": 9723,
    "package_name": "cdata",
    "title": "Fluid Data Transformations",
    "description": "Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.",
    "version": "1.2.1",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Nina Zumel [aut],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/cdata/,\nhttps://winvector.github.io/cdata/",
    "bug_reports": "https://github.com/WinVector/cdata/issues",
    "repository": "https://cran.r-project.org/package=cdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cdata Fluid Data Transformations Supplies higher-order coordinatized data specification and fluid transform operators that include pivot and anti-pivot as special cases. \n    The methodology is describe in 'Zumel', 2018, \"Fluid data reshaping with 'cdata'\", <https://winvector.github.io/FluidData/FluidDataReshapingWithCdata.html> , <DOI:10.5281/zenodo.1173299> .\n    This package introduces the idea of explicit control table specification of data transforms.\n    Works on in-memory data or on remote data using 'rquery' and 'SQL' database interfaces.  "
  },
  {
    "id": 9834,
    "package_name": "checkthat",
    "title": "Intuitive Unit Testing Tools for Data Manipulation",
    "description": "Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.",
    "version": "0.1.0",
    "maintainer": "Ian Cero <ian_cero@urmc.rochester.edu>",
    "author": "Ian Cero [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-2862-0450>)",
    "url": "https://github.com/iancero/checkthat,\nhttps://iancero.github.io/checkthat/",
    "bug_reports": "https://github.com/iancero/checkthat/issues",
    "repository": "https://cran.r-project.org/package=checkthat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "checkthat Intuitive Unit Testing Tools for Data Manipulation Provides a lightweight data validation and testing toolkit for R. \n    Its guiding philosophy is that adding code-based data checks to users' \n    existing workflow should be both quick and intuitive. The suite of \n    functions included therefore mirror the common data checks many users \n    already perform by hand or by eye. Additionally, the 'checkthat' package is \n    optimized to work within 'tidyverse' data manipulation pipelines.  "
  },
  {
    "id": 9837,
    "package_name": "cheese",
    "title": "Tools for Working with Data During Statistical Analysis",
    "description": "Contains tools for working with data during statistical analysis, promoting flexible, intuitive, and reproducible workflows. There are functions designated for specific statistical tasks such building a custom univariate descriptive table, computing pairwise association statistics, etc. These are built on a collection of data manipulation tools designed for general use that are motivated by the functional programming concept.",
    "version": "0.1.3",
    "maintainer": "Alex Zajichek <alexzajichek@gmail.com>",
    "author": "Alex Zajichek [aut, cre]",
    "url": "https://zajichek.github.io/cheese/,\nhttps://github.com/zajichek/cheese/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=cheese",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cheese Tools for Working with Data During Statistical Analysis Contains tools for working with data during statistical analysis, promoting flexible, intuitive, and reproducible workflows. There are functions designated for specific statistical tasks such building a custom univariate descriptive table, computing pairwise association statistics, etc. These are built on a collection of data manipulation tools designed for general use that are motivated by the functional programming concept.  "
  },
  {
    "id": 9956,
    "package_name": "clean",
    "title": "Fast and Easy Data Cleaning",
    "description": "A wrapper around the new 'cleaner' package, that allows\n  data cleaning functions for classes 'logical', 'factor', 'numeric', \n  'character', 'currency' and 'Date' to make data cleaning fast and\n  easy. Relying on very few dependencies, it provides smart guessing,\n  but with user options to override anything if needed.",
    "version": "2.0.0",
    "maintainer": "Matthijs S. Berends <m.s.berends@umcg.nl>",
    "author": "Matthijs S. Berends [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7620-1800>)",
    "url": "https://github.com/msberends/cleaner",
    "bug_reports": "https://github.com/msberends/cleaner/issues",
    "repository": "https://cran.r-project.org/package=clean",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clean Fast and Easy Data Cleaning A wrapper around the new 'cleaner' package, that allows\n  data cleaning functions for classes 'logical', 'factor', 'numeric', \n  'character', 'currency' and 'Date' to make data cleaning fast and\n  easy. Relying on very few dependencies, it provides smart guessing,\n  but with user options to override anything if needed.  "
  },
  {
    "id": 9960,
    "package_name": "cleaner",
    "title": "Fast and Easy Data Cleaning",
    "description": "Data cleaning functions for classes logical,\n  factor, numeric, character, currency and Date to make\n  data cleaning fast and easy. Relying on very few dependencies, it \n  provides smart guessing, but with user options to override \n  anything if needed.",
    "version": "1.5.5",
    "maintainer": "Matthijs S. Berends <m.s.berends@umcg.nl>",
    "author": "Matthijs S. Berends [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7620-1800>)",
    "url": "https://msberends.github.io/cleaner/,\nhttps://github.com/msberends/cleaner",
    "bug_reports": "https://github.com/msberends/cleaner/issues",
    "repository": "https://cran.r-project.org/package=cleaner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "cleaner Fast and Easy Data Cleaning Data cleaning functions for classes logical,\n  factor, numeric, character, currency and Date to make\n  data cleaning fast and easy. Relying on very few dependencies, it \n  provides smart guessing, but with user options to override \n  anything if needed.  "
  },
  {
    "id": 9994,
    "package_name": "clinpubr",
    "title": "Clinical Publication",
    "description": "Accelerate the process from clinical data to medical publication,\n    including clinical data cleaning, significant result screening, and the\n    generation of publish-ready tables and figures.",
    "version": "1.1.0",
    "maintainer": "Yue Niu <niuyuesam@163.com>",
    "author": "Yue Niu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6843-3548>),\n  Keyun Wang [aut]",
    "url": "https://github.com/yotasama/clinpubr,\nhttps://gitee.com/yotasama/clinpubr",
    "bug_reports": "https://github.com/yotasama/clinpubr/issues",
    "repository": "https://cran.r-project.org/package=clinpubr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "clinpubr Clinical Publication Accelerate the process from clinical data to medical publication,\n    including clinical data cleaning, significant result screening, and the\n    generation of publish-ready tables and figures.  "
  },
  {
    "id": 10166,
    "package_name": "collapse",
    "title": "Advanced and Fast Data Transformation",
    "description": "A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.",
    "version": "2.1.5",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6212-5229>),\n  Matt Dowle [ctb],\n  Arun Srinivasan [ctb],\n  Morgan Jacob [ctb],\n  Dirk Eddelbuettel [ctb],\n  Laurent Berge [ctb],\n  Kevin Tappe [ctb],\n  Alina Cherkas [ctb],\n  R Core Team and contributors worldwide [ctb],\n  Martyn Plummer [cph],\n  1999-2016 The R Core Team [cph]",
    "url": "https://sebkrantz.github.io/collapse/,\nhttps://github.com/SebKrantz/collapse",
    "bug_reports": "https://github.com/SebKrantz/collapse/issues",
    "repository": "https://cran.r-project.org/package=collapse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "collapse Advanced and Fast Data Transformation A large C/C++-based package for advanced data transformation and \n    statistical computing in R that is extremely fast, class-agnostic, robust, and \n    programmer friendly. Core functionality includes a rich set of S3 generic grouped \n    and weighted statistical functions for vectors, matrices and data frames, which \n    provide efficient low-level vectorizations, OpenMP multithreading, and skip missing \n    values by default. These are integrated with fast grouping and ordering algorithms \n    (also callable from C), and efficient data manipulation functions. The package also \n    provides a flexible and rigorous approach to time series and panel data in R, fast \n    functions for data transformation and common statistical procedures, detailed \n    (grouped, weighted) summary statistics, powerful tools to work with nested data, \n    fast data object conversions, functions for memory efficient R programming, and \n    helpers to effectively deal with variable labels, attributes, and missing data. It \n    seamlessly supports base R objects/classes as well as 'units', 'integer64', 'xts'/\n    'zoo', 'tibble', 'grouped_df', 'data.table', 'sf', and 'pseries'/'pdata.frame'.  "
  },
  {
    "id": 10249,
    "package_name": "comperes",
    "title": "Manage Competition Results",
    "description": "Tools for storing and managing competition results.\n    Competition is understood as a set of games in which players gain some\n    abstract scores.  There are two ways for storing results: in long (one\n    row per game-player) and wide (one row per game with fixed amount of\n    players) formats. This package provides functions for creation and\n    conversion between them. Also there are functions for computing their\n    summary and Head-to-Head values for players. They leverage grammar of\n    data manipulation from 'dplyr'.",
    "version": "0.2.7",
    "maintainer": "Evgeni Chasnovski <evgeni.chasnovski@gmail.com>",
    "author": "Evgeni Chasnovski [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-1617-4019>)",
    "url": "https://github.com/echasnovski/comperes",
    "bug_reports": "https://github.com/echasnovski/comperes/issues",
    "repository": "https://cran.r-project.org/package=comperes",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "comperes Manage Competition Results Tools for storing and managing competition results.\n    Competition is understood as a set of games in which players gain some\n    abstract scores.  There are two ways for storing results: in long (one\n    row per game-player) and wide (one row per game with fixed amount of\n    players) formats. This package provides functions for creation and\n    conversion between them. Also there are functions for computing their\n    summary and Head-to-Head values for players. They leverage grammar of\n    data manipulation from 'dplyr'.  "
  },
  {
    "id": 10405,
    "package_name": "corkscrew",
    "title": "Preprocessor for Data Modeling",
    "description": "Includes binning categorical variables into lesser number of categories based on t-test, converting categorical variables into continuous features \n\tusing the mean of the response variable for the respective categories, understanding the relationship between the response variable and predictor variables \n\tusing data transformations.",
    "version": "1.1",
    "maintainer": "Santhosh Sasanapuri <santhosh458@gmail.com>",
    "author": "Navin Loganathan [aut], \n\tMohan Manivannan [aut], \n\tSanthosh Sasanapuri [aut, cre], \n\tLatentView Analytics [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=corkscrew",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "corkscrew Preprocessor for Data Modeling Includes binning categorical variables into lesser number of categories based on t-test, converting categorical variables into continuous features \n\tusing the mean of the response variable for the respective categories, understanding the relationship between the response variable and predictor variables \n\tusing data transformations.  "
  },
  {
    "id": 10426,
    "package_name": "correspondenceTables",
    "title": "Creating Correspondence Tables Between Two Statistical\nClassifications",
    "description": "\n    A candidate correspondence table between two classifications can be created when there are correspondence tables leading from the first classification to the second one via intermediate 'pivot' classifications. \n    The correspondence table between two statistical classifications can be updated when one of the classifications gets updated to a new version.",
    "version": "0.7.4",
    "maintainer": "M\u00e1ty\u00e1s M\u00e9sz\u00e1ros <matyas.meszaros@ec.europa.eu>",
    "author": "Vasilis Chasiotis [aut] (Department of Statistics, Athens University of\n    Economics and Business),\n  Photis Stavropoulos [aut] (Quantos S.A. Statistics and Information\n    Systems),\n  Martin Karlberg [aut],\n  M\u00e1ty\u00e1s M\u00e9sz\u00e1ros [cre]",
    "url": "https://github.com/eurostat/correspondenceTables",
    "bug_reports": "https://github.com/eurostat/correspondenceTables/issues",
    "repository": "https://cran.r-project.org/package=correspondenceTables",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "correspondenceTables Creating Correspondence Tables Between Two Statistical\nClassifications \n    A candidate correspondence table between two classifications can be created when there are correspondence tables leading from the first classification to the second one via intermediate 'pivot' classifications. \n    The correspondence table between two statistical classifications can be updated when one of the classifications gets updated to a new version.  "
  },
  {
    "id": 10637,
    "package_name": "csodata",
    "title": "Download Data from the CSO 'PxStat' API",
    "description": "Imports 'PxStat' data in JSON-stat format and (optionally) reshapes it into wide\n\tformat. The Central Statistics Office (CSO) is the national statistical institute of Ireland\n\tand 'PxStat' is the CSOs online database of Official Statistics. This database contains current\n\tand historical data series compiled from CSO statistical releases and is accessed at\n\t<https://data.cso.ie>.\n\tThe CSO 'PxStat' Application Programming Interface (API), which is accessed in this package, provides\n\taccess to 'PxStat' data in JSON-stat format at <https://data.cso.ie>.\n\tThis dissemination tool allows developers machine to machine access to CSO 'PxStat' data.",
    "version": "1.5.1",
    "maintainer": "Conor Crowley <conor.crowley@cso.ie>",
    "author": "Eoin Horgan [aut] (ORCID: <https://orcid.org/0000-0002-3446-6154>),\n  Conor Crowley [aut, cre],\n  Vytas Vaiciulis [aut],\n  Mervyn O'Luing [aut],\n  James O'Rourke [aut]",
    "url": "https://github.com/CSOIreland/csodata",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=csodata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "csodata Download Data from the CSO 'PxStat' API Imports 'PxStat' data in JSON-stat format and (optionally) reshapes it into wide\n\tformat. The Central Statistics Office (CSO) is the national statistical institute of Ireland\n\tand 'PxStat' is the CSOs online database of Official Statistics. This database contains current\n\tand historical data series compiled from CSO statistical releases and is accessed at\n\t<https://data.cso.ie>.\n\tThe CSO 'PxStat' Application Programming Interface (API), which is accessed in this package, provides\n\taccess to 'PxStat' data in JSON-stat format at <https://data.cso.ie>.\n\tThis dissemination tool allows developers machine to machine access to CSO 'PxStat' data.  "
  },
  {
    "id": 10813,
    "package_name": "dat",
    "title": "Tools for Data Manipulation",
    "description": "An implementation of common higher order functions with syntactic\n    sugar for anonymous function. Provides also a link to 'dplyr' and\n    'data.table' for common transformations on data frames to work around non\n    standard evaluation by default.",
    "version": "0.5.0",
    "maintainer": "Sebastian Warnholz <wahani@gmail.com>",
    "author": "Sebastian Warnholz [aut, cre]",
    "url": "",
    "bug_reports": "https://github.com/wahani/dat/issues",
    "repository": "https://cran.r-project.org/package=dat",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dat Tools for Data Manipulation An implementation of common higher order functions with syntactic\n    sugar for anonymous function. Provides also a link to 'dplyr' and\n    'data.table' for common transformations on data frames to work around non\n    standard evaluation by default.  "
  },
  {
    "id": 10823,
    "package_name": "dataMojo",
    "title": "Reshape Data Table",
    "description": "A grammar of data manipulation with 'data.table', providing a consistent a series of utility functions that help you solve the most common data manipulation challenges.",
    "version": "1.0.0",
    "maintainer": "Jiena McLellan <jienagu90@gmail.com>",
    "author": "Jiena McLellan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5578-088X>),\n  Michael Condouris [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8862-4250>),\n  Brittney Zykan [ctb],\n  Sai Im [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dataMojo",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataMojo Reshape Data Table A grammar of data manipulation with 'data.table', providing a consistent a series of utility functions that help you solve the most common data manipulation challenges.  "
  },
  {
    "id": 10828,
    "package_name": "datacleanr",
    "title": "Interactive and Reproducible Data Cleaning",
    "description": "Flexible and efficient cleaning of data with interactivity.\n  'datacleanr' facilitates best practices in data analyses and reproducibility with built-in features and by translating interactive/manual operations to code. \n  The package is designed for interoperability, and so seamlessly fits into reproducible analyses pipelines in 'R'.",
    "version": "1.0.5",
    "maintainer": "Alexander Hurley <agl.hurley@gmail.com>",
    "author": "Alexander Hurley [cre, aut, cph] (ORCID:\n    <https://orcid.org/0000-0002-9641-2805>),\n  Richard Peters [ctb] (ORCID: <https://orcid.org/0000-0002-7441-1297>),\n  Christoforos Pappas [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5721-557X>)",
    "url": "https://github.com/the-Hull/datacleanr",
    "bug_reports": "https://github.com/the-Hull/datacleanr/issues",
    "repository": "https://cran.r-project.org/package=datacleanr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "datacleanr Interactive and Reproducible Data Cleaning Flexible and efficient cleaning of data with interactivity.\n  'datacleanr' facilitates best practices in data analyses and reproducibility with built-in features and by translating interactive/manual operations to code. \n  The package is designed for interoperability, and so seamlessly fits into reproducible analyses pipelines in 'R'.  "
  },
  {
    "id": 10834,
    "package_name": "dataframeexplorer",
    "title": "Familiarity with Dataframes Before Data Manipulation",
    "description": "Real life data is muddy, fuzzy and unpredictable. This makes data manipulations tedious and bringing the data to right shape alone is a major chunk of work. Functions in this package help us get an understanding of dataframes to dramatically reduces data coding time.",
    "version": "1.0.2",
    "maintainer": "Ashrith Reddy <ashrithssreddy@gmail.com>",
    "author": "Ashrith Reddy [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=dataframeexplorer",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataframeexplorer Familiarity with Dataframes Before Data Manipulation Real life data is muddy, fuzzy and unpredictable. This makes data manipulations tedious and bringing the data to right shape alone is a major chunk of work. Functions in this package help us get an understanding of dataframes to dramatically reduces data coding time.  "
  },
  {
    "id": 10858,
    "package_name": "dataviewR",
    "title": "An Interactive and Feature-Rich Data Viewer",
    "description": "Provides an interactive viewer for 'data.frame' and 'tibble' objects using 'shiny' <https://shiny.posit.co/> and 'DT' <https://rstudio.github.io/DT/>. It supports complex filtering, column selection, and automatic generation of reproducible 'dplyr' <https://dplyr.tidyverse.org/> code for data manipulation. The package is designed for ease of use in data exploration and reporting workflows.",
    "version": "0.1.1",
    "maintainer": "Madhan Kumar N <madhanmanoj1999@gmail.com>",
    "author": "Madhan Kumar N [aut, cre]",
    "url": "https://github.com/madhankumarnagaraji/dataviewR",
    "bug_reports": "https://github.com/madhankumarnagaraji/dataviewR/issues",
    "repository": "https://cran.r-project.org/package=dataviewR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dataviewR An Interactive and Feature-Rich Data Viewer Provides an interactive viewer for 'data.frame' and 'tibble' objects using 'shiny' <https://shiny.posit.co/> and 'DT' <https://rstudio.github.io/DT/>. It supports complex filtering, column selection, and automatic generation of reproducible 'dplyr' <https://dplyr.tidyverse.org/> code for data manipulation. The package is designed for ease of use in data exploration and reporting workflows.  "
  },
  {
    "id": 10909,
    "package_name": "dcmodify",
    "title": "Modify Data Using Externally Defined Modification Rules",
    "description": "Data cleaning scripts typically contain a lot of 'if this change that'\n    type of statements. Such statements are typically condensed expert knowledge.\n    With this package, such 'data modifying rules' are taken out of the code and\n    become in stead parameters to the work flow. This allows one to maintain, document,\n    and reason about data modification rules as separate entities.",
    "version": "0.9.0",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo [cre, aut] (ORCID:\n    <https://orcid.org/0000-0002-9807-4686>),\n  Edwin de Jonge [aut] (ORCID: <https://orcid.org/0000-0002-6580-4718>),\n  Sjabbo Schaveling [ctb],\n  Floris Ruijter [ctb]",
    "url": "https://github.com/data-cleaning/dcmodify",
    "bug_reports": "https://github.com/data-cleaning/dcmodify/issues",
    "repository": "https://cran.r-project.org/package=dcmodify",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dcmodify Modify Data Using Externally Defined Modification Rules Data cleaning scripts typically contain a lot of 'if this change that'\n    type of statements. Such statements are typically condensed expert knowledge.\n    With this package, such 'data modifying rules' are taken out of the code and\n    become in stead parameters to the work flow. This allows one to maintain, document,\n    and reason about data modification rules as separate entities.  "
  },
  {
    "id": 10957,
    "package_name": "deducorrect",
    "title": "Deductive Correction, Deductive Imputation, and Deterministic\nCorrection",
    "description": "A collection of methods for automated data cleaning where all actions are logged.",
    "version": "1.3.7",
    "maintainer": "Mark van der Loo <mark.vanderloo@gmail.com>",
    "author": "Mark van der Loo, Edwin de Jonge, and Sander\n    Scholtus",
    "url": "https://github.com/data-cleaning/deducorrect",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=deducorrect",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "deducorrect Deductive Correction, Deductive Imputation, and Deterministic\nCorrection A collection of methods for automated data cleaning where all actions are logged.  "
  },
  {
    "id": 11094,
    "package_name": "dformula",
    "title": "Data Manipulation using Formula",
    "description": "A tool for manipulating data using the generic formula. A single formula allows to easily add, replace and remove variables before running the analysis. ",
    "version": "1.0",
    "maintainer": "Alessio Serafini <srf.alessio@gmail.com>",
    "author": "Alessio Serafini [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8579-5695>)",
    "url": "https://github.com/serafinialessio/dformula",
    "bug_reports": "https://github.com/serafinialessio/dformula/issues",
    "repository": "https://cran.r-project.org/package=dformula",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dformula Data Manipulation using Formula A tool for manipulating data using the generic formula. A single formula allows to easily add, replace and remove variables before running the analysis.   "
  },
  {
    "id": 11184,
    "package_name": "discSurv",
    "title": "Discrete Time Survival Analysis",
    "description": "Provides data transformations, estimation utilities,\n    predictive evaluation measures and simulation functions for discrete time\n    survival analysis.",
    "version": "2.0.0",
    "maintainer": "Thomas Welchowski <welchow@imbie.meb.uni-bonn.de>",
    "author": "Thomas Welchowski <welchow@imbie.meb.uni-bonn.de> and Moritz Berger <moritz.berger@imbie.uni-bonn.de> and David Koehler <koehler@imbie.uni-bonn.de> and Matthias Schmid <matthias.schmid@imbie.uni-bonn.de> ",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=discSurv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "discSurv Discrete Time Survival Analysis Provides data transformations, estimation utilities,\n    predictive evaluation measures and simulation functions for discrete time\n    survival analysis.  "
  },
  {
    "id": 11203,
    "package_name": "disk.frame",
    "title": "Larger-than-RAM Disk-Based Data Manipulation Framework",
    "description": "A disk-based data manipulation tool for working with \n  large-than-RAM datasets. Aims to lower the barrier-to-entry for \n  manipulating large datasets by adhering closely to popular and \n  familiar data manipulation paradigms like 'dplyr' verbs and \n  'data.table' syntax.",
    "version": "0.8.3",
    "maintainer": "Dai ZJ <zhuojia.dai@gmail.com>",
    "author": "Dai ZJ [aut, cre],\n  Jacky Poon [ctb]",
    "url": "https://diskframe.com",
    "bug_reports": "https://github.com/DiskFrame/disk.frame/issues",
    "repository": "https://cran.r-project.org/package=disk.frame",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "disk.frame Larger-than-RAM Disk-Based Data Manipulation Framework A disk-based data manipulation tool for working with \n  large-than-RAM datasets. Aims to lower the barrier-to-entry for \n  manipulating large datasets by adhering closely to popular and \n  familiar data manipulation paradigms like 'dplyr' verbs and \n  'data.table' syntax.  "
  },
  {
    "id": 11267,
    "package_name": "dlookr",
    "title": "Tools for Data Diagnosis, Exploration, Transformation",
    "description": "A collection of tools that support data diagnosis, exploration, and transformation. \n    Data diagnostics provides information and visualization of missing values, outliers, and unique \n    and negative values to help you understand the distribution and quality of your data. \n    Data exploration provides information and visualization of the descriptive statistics of \n    univariate variables, normality tests and outliers, correlation of two variables, \n    and the relationship between the target variable and predictor. Data transformation supports binning \n    for categorizing continuous variables, imputes missing values and outliers, and resolves skewness. \n    And it creates automated reports that support these three tasks.",
    "version": "0.6.5",
    "maintainer": "Choonghyun Ryu <choonghyun.ryu@gmail.com>",
    "author": "Choonghyun Ryu [aut, cre]",
    "url": "https://github.com/choonghyunryu/dlookr/,\nhttps://choonghyunryu.github.io/dlookr/",
    "bug_reports": "https://github.com/choonghyunryu/dlookr/issues",
    "repository": "https://cran.r-project.org/package=dlookr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dlookr Tools for Data Diagnosis, Exploration, Transformation A collection of tools that support data diagnosis, exploration, and transformation. \n    Data diagnostics provides information and visualization of missing values, outliers, and unique \n    and negative values to help you understand the distribution and quality of your data. \n    Data exploration provides information and visualization of the descriptive statistics of \n    univariate variables, normality tests and outliers, correlation of two variables, \n    and the relationship between the target variable and predictor. Data transformation supports binning \n    for categorizing continuous variables, imputes missing values and outliers, and resolves skewness. \n    And it creates automated reports that support these three tasks.  "
  },
  {
    "id": 11279,
    "package_name": "dmtools",
    "title": "Tools for Clinical Data Management",
    "description": "For checking the dataset from EDC(Electronic Data Capture) in clinical trials.\n             'dmtools' reshape your dataset in a tidy view and check events.\n             You can reshape the dataset and choose your target to check, for example, the laboratory reference range.",
    "version": "0.2.6",
    "maintainer": "Konstantin Ryabov <chachabooms@gmail.com>",
    "author": "Konstantin Ryabov [aut, cre]",
    "url": "https://github.com/KonstantinRyabov/dmtools",
    "bug_reports": "https://github.com/KonstantinRyabov/dmtools/issues",
    "repository": "https://cran.r-project.org/package=dmtools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dmtools Tools for Clinical Data Management For checking the dataset from EDC(Electronic Data Capture) in clinical trials.\n             'dmtools' reshape your dataset in a tidy view and check events.\n             You can reshape the dataset and choose your target to check, for example, the laboratory reference range.  "
  },
  {
    "id": 11286,
    "package_name": "do",
    "title": "Data Operator",
    "description": "Flexibly convert data between long and wide format using just two\n    functions: reshape_toLong() and reshape_toWide().",
    "version": "2.0.0.1",
    "maintainer": "Jing Zhang <zj391120@163.com>",
    "author": "Jing Zhang [aut, cre],\n  Zhi Jin [aut],\n  CRAN Team [ctb]",
    "url": "https://github.com/yikeshu0611/do",
    "bug_reports": "https://github.com/yikeshu0611/do/issues",
    "repository": "https://cran.r-project.org/package=do",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "do Data Operator Flexibly convert data between long and wide format using just two\n    functions: reshape_toLong() and reshape_toWide().  "
  },
  {
    "id": 11359,
    "package_name": "dplyrAssist",
    "title": "RStudio Addin for Teaching and Learning Data Manipulation Using\n'dplyr'",
    "description": "An RStudio addin for teaching and learning data manipulation using the 'dplyr' package.\n    You can learn each steps of data manipulation by clicking your mouse without coding.\n    You can get resultant data (as a 'tibble') and the code for data manipulation.",
    "version": "0.1.0",
    "maintainer": "Keon-Woong Moon <cardiomoon@gmail.com>",
    "author": "Keon-Woong Moon [aut, cre]",
    "url": "https://github.com/cardiomoon/dplyrAssist",
    "bug_reports": "https://github.com/cardiomoon/dplyrAssist/issues",
    "repository": "https://cran.r-project.org/package=dplyrAssist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "dplyrAssist RStudio Addin for Teaching and Learning Data Manipulation Using\n'dplyr' An RStudio addin for teaching and learning data manipulation using the 'dplyr' package.\n    You can learn each steps of data manipulation by clicking your mouse without coding.\n    You can get resultant data (as a 'tibble') and the code for data manipulation.  "
  },
  {
    "id": 11539,
    "package_name": "easy.utils",
    "title": "Frequently Used Functions for Easy R Programming",
    "description": "Some utility functions for validation and data manipulation. These functions can be helpful to reduce internal codes everywhere in package development.",
    "version": "0.1.0",
    "maintainer": "Yuchen Li <ycli1995@outlook.com>",
    "author": "Yuchen Li [aut, cre]",
    "url": "https://github.com/ycli1995/easy.utils",
    "bug_reports": "https://github.com/ycli1995/easy.utils/issues",
    "repository": "https://cran.r-project.org/package=easy.utils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "easy.utils Frequently Used Functions for Easy R Programming Some utility functions for validation and data manipulation. These functions can be helpful to reduce internal codes everywhere in package development.  "
  },
  {
    "id": 11573,
    "package_name": "eatTools",
    "title": "Miscellaneous Functions for the Analysis of Educational\nAssessments",
    "description": "\n   Miscellaneous functions for data cleaning and data analysis of educational assessments. Includes functions for descriptive \n   analyses, character vector manipulations and weighted statistics. Mainly a lightweight dependency for the packages 'eatRep', \n   'eatGADS', 'eatPrep' and 'eatModel' (which will be subsequently submitted to 'CRAN').\n   The function for defining (weighted) contrasts in weighted effect coding refers to\n   te Grotenhuis et al. (2017) <doi:10.1007/s00038-016-0901-1>.\n   Functions for weighted statistics refer to\n   Wolter (2007) <doi:10.1007/978-0-387-35099-8>.",
    "version": "0.7.9",
    "maintainer": "Sebastian Weirich <sebastian.weirich@iqb.hu-berlin.de>",
    "author": "Sebastian Weirich [aut, cre],\n  Martin Hecht [aut],\n  Karoline Sachse [aut],\n  Benjamin Becker [aut],\n  Nicole Mahler [aut],\n  Edna Grewers [ctb]",
    "url": "https://github.com/weirichs/eatTools,\nhttps://weirichs.github.io/eatTools/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=eatTools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "eatTools Miscellaneous Functions for the Analysis of Educational\nAssessments \n   Miscellaneous functions for data cleaning and data analysis of educational assessments. Includes functions for descriptive \n   analyses, character vector manipulations and weighted statistics. Mainly a lightweight dependency for the packages 'eatRep', \n   'eatGADS', 'eatPrep' and 'eatModel' (which will be subsequently submitted to 'CRAN').\n   The function for defining (weighted) contrasts in weighted effect coding refers to\n   te Grotenhuis et al. (2017) <doi:10.1007/s00038-016-0901-1>.\n   Functions for weighted statistics refer to\n   Wolter (2007) <doi:10.1007/978-0-387-35099-8>.  "
  },
  {
    "id": 11672,
    "package_name": "editrules",
    "title": "Parsing, Applying, and Manipulating Data Cleaning Rules",
    "description": "Please note: active development has moved to packages 'validate'\n    and 'errorlocate'. Facilitates reading and manipulating (multivariate) data\n    restrictions (edit rules) on numerical and categorical data. Rules can be\n    defined with common R syntax and parsed to an internal (matrix-like format).\n    Rules can be manipulated with variable elimination and value substitution\n    methods, allowing for feasibility checks and more. Data can be tested against\n    the rules and erroneous fields can be found based on Fellegi and Holt's\n    generalized principle. Rules dependencies can be visualized with using the\n    'igraph' package. ",
    "version": "2.9.6",
    "maintainer": "Edwin de Jonge <edwindjonge@gmail.com>",
    "author": "Edwin de Jonge [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-6580-4718>),\n  Mark van der Loo [aut]",
    "url": "https://github.com/data-cleaning/editrules",
    "bug_reports": "https://github.com/data-cleaning/editrules/issues",
    "repository": "https://cran.r-project.org/package=editrules",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "editrules Parsing, Applying, and Manipulating Data Cleaning Rules Please note: active development has moved to packages 'validate'\n    and 'errorlocate'. Facilitates reading and manipulating (multivariate) data\n    restrictions (edit rules) on numerical and categorical data. Rules can be\n    defined with common R syntax and parsed to an internal (matrix-like format).\n    Rules can be manipulated with variable elimination and value substitution\n    methods, allowing for feasibility checks and more. Data can be tested against\n    the rules and erroneous fields can be found based on Fellegi and Holt's\n    generalized principle. Rules dependencies can be visualized with using the\n    'igraph' package.   "
  },
  {
    "id": 11717,
    "package_name": "einops",
    "title": "Flexible, Powerful, and Readable Tensor Operations",
    "description": "Perform tensor operations using a concise yet expressive syntax inspired by the Python library of the same name.\n    Reshape, rearrange, and combine multidimensional arrays for scientific computing, machine learning, and data analysis.\n    Einops simplifies complex manipulations, making code more maintainable and intuitive.\n    The original implementation is demonstrated in Rogozhnikov (2022) <https://openreview.net/forum?id=oapKSVM2bcj>.",
    "version": "0.2.1",
    "maintainer": "Qile Yang <qile.yang@berkeley.edu>",
    "author": "Qile Yang [cre, aut, cph] (ORCID:\n    <https://orcid.org/0009-0005-0148-2499>)",
    "url": "https://github.com/Qile0317/einops,\nhttps://qile0317.github.io/einops/",
    "bug_reports": "https://github.com/Qile0317/einops/issues",
    "repository": "https://cran.r-project.org/package=einops",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "einops Flexible, Powerful, and Readable Tensor Operations Perform tensor operations using a concise yet expressive syntax inspired by the Python library of the same name.\n    Reshape, rearrange, and combine multidimensional arrays for scientific computing, machine learning, and data analysis.\n    Einops simplifies complex manipulations, making code more maintainable and intuitive.\n    The original implementation is demonstrated in Rogozhnikov (2022) <https://openreview.net/forum?id=oapKSVM2bcj>.  "
  },
  {
    "id": 12066,
    "package_name": "expss",
    "title": "Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics",
    "description": "Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.",
    "version": "0.11.7",
    "maintainer": "Gregory Demin <gdemin@gmail.com>",
    "author": "Gregory Demin [aut, cre],\n  Sebastian Jeworutzki [ctb] (ORCID:\n    <https://orcid.org/0000-0002-2671-5253>),\n  Dan Chaltiel [ctb],\n  John Williams [ctb],\n  Tom Elliott [ctb]",
    "url": "https://gdemin.github.io/expss/",
    "bug_reports": "https://github.com/gdemin/expss/issues",
    "repository": "https://cran.r-project.org/package=expss",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "expss Tables, Labels and Some Useful Functions from Spreadsheets and\n'SPSS' Statistics Package computes and displays tables with support for 'SPSS'-style \n        labels, multiple and nested banners, weights, multiple-response variables \n        and significance testing. There are facilities for nice output of tables \n        in 'knitr', 'Shiny', '*.xlsx' files, R and 'Jupyter' notebooks. Methods \n        for labelled variables add value labels support to base R functions and to \n        some functions from other packages. Additionally, the package brings \n        popular data transformation functions from 'SPSS' Statistics and 'Excel': \n        'RECODE', 'COUNT', 'COUNTIF', 'VLOOKUP' and etc. \n        These functions are very useful for data processing in marketing research \n        surveys. Package intended to help people to move data \n        processing from 'Excel' and 'SPSS' to R.  "
  },
  {
    "id": 12124,
    "package_name": "fMRIscrub",
    "title": "Scrubbing and Other Data Cleaning Routines for fMRI",
    "description": "Data-driven fMRI denoising with projection scrubbing (Pham et al \n    (2022) <doi:10.1016/j.neuroimage.2023.119972>). Also includes routines for \n    DVARS (Derivatives VARianceS) (Afyouni and Nichols (2018) \n    <doi:10.1016/j.neuroimage.2017.12.098>), motion scrubbing (Power et al \n    (2012) <doi:10.1016/j.neuroimage.2011.10.018>), aCompCor (anatomical \n    Components Correction) (Muschelli et al (2014)\n    <doi:10.1016/j.neuroimage.2014.03.028>), detrending, and nuisance\n    regression. Projection scrubbing is also applicable to other\n    outlier detection tasks involving high-dimensional data.",
    "version": "0.14.5",
    "maintainer": "Amanda Mejia <mandy.mejia@gmail.com>",
    "author": "Amanda Mejia [aut, cre],\n  John Muschelli [aut] (ORCID: <https://orcid.org/0000-0001-6469-1750>),\n  Damon Pham [aut] (ORCID: <https://orcid.org/0000-0001-7563-4727>),\n  Daniel McDonald [ctb],\n  Fatma Parlak [ctb]",
    "url": "https://github.com/mandymejia/fMRIscrub",
    "bug_reports": "https://github.com/mandymejia/fMRIscrub/issues",
    "repository": "https://cran.r-project.org/package=fMRIscrub",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fMRIscrub Scrubbing and Other Data Cleaning Routines for fMRI Data-driven fMRI denoising with projection scrubbing (Pham et al \n    (2022) <doi:10.1016/j.neuroimage.2023.119972>). Also includes routines for \n    DVARS (Derivatives VARianceS) (Afyouni and Nichols (2018) \n    <doi:10.1016/j.neuroimage.2017.12.098>), motion scrubbing (Power et al \n    (2012) <doi:10.1016/j.neuroimage.2011.10.018>), aCompCor (anatomical \n    Components Correction) (Muschelli et al (2014)\n    <doi:10.1016/j.neuroimage.2014.03.028>), detrending, and nuisance\n    regression. Projection scrubbing is also applicable to other\n    outlier detection tasks involving high-dimensional data.  "
  },
  {
    "id": 12135,
    "package_name": "fabR",
    "title": "Wrapper Functions Collection Used in Data Pipelines",
    "description": "The goal of this package is to provide wrapper functions in the \n    data cleaning and cleansing processes. These function helps in messages and \n    interaction with the user, keep track of information in pipelines, help in \n    the wrangling, munging, assessment and visualization of data frame-like \n    material.",
    "version": "2.1.1",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre],\n  Maelstrom-Research [fnd]",
    "url": "https://github.com/GuiFabre/fabR",
    "bug_reports": "https://github.com/GuiFabre/fabR/issues",
    "repository": "https://cran.r-project.org/package=fabR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fabR Wrapper Functions Collection Used in Data Pipelines The goal of this package is to provide wrapper functions in the \n    data cleaning and cleansing processes. These function helps in messages and \n    interaction with the user, keep track of information in pipelines, help in \n    the wrangling, munging, assessment and visualization of data frame-like \n    material.  "
  },
  {
    "id": 12180,
    "package_name": "fakir",
    "title": "Generate Fake Datasets for Prototyping and Teaching",
    "description": "Create fake datasets that can be used for prototyping and teaching.\n    This package provides a set of functions to generate fake data\n    for a variety of data types, such as dates, addresses,\n    and names. It can be used for prototyping (notably in 'shiny') or as a tool\n    to teach data manipulation and data visualization.",
    "version": "1.0.0",
    "maintainer": "Colin Fay <contact@colinfay.me>",
    "author": "Colin Fay [aut, cre] (ORCID: <https://orcid.org/0000-0001-7343-1846>),\n  Sebastien Rochette [aut] (ORCID:\n    <https://orcid.org/0000-0002-1565-9313>),\n  ThinkR [cph]",
    "url": "https://github.com/Thinkr-open/fakir",
    "bug_reports": "https://github.com/Thinkr-open/fakir/issues",
    "repository": "https://cran.r-project.org/package=fakir",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fakir Generate Fake Datasets for Prototyping and Teaching Create fake datasets that can be used for prototyping and teaching.\n    This package provides a set of functions to generate fake data\n    for a variety of data types, such as dates, addresses,\n    and names. It can be used for prototyping (notably in 'shiny') or as a tool\n    to teach data manipulation and data visualization.  "
  },
  {
    "id": 12254,
    "package_name": "fastplyr",
    "title": "Fast Alternatives to 'tidyverse' Functions",
    "description": "A full set of fast data manipulation tools with a tidy\n    front-end and a fast back-end using 'collapse' and 'cheapr'.",
    "version": "0.9.91",
    "maintainer": "Nick Christofides <nick.christofides.r@gmail.com>",
    "author": "Nick Christofides [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-9743-7342>)",
    "url": "",
    "bug_reports": "https://github.com/NicChr/fastplyr/issues",
    "repository": "https://cran.r-project.org/package=fastplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastplyr Fast Alternatives to 'tidyverse' Functions A full set of fast data manipulation tools with a tidy\n    front-end and a fast back-end using 'collapse' and 'cheapr'.  "
  },
  {
    "id": 12267,
    "package_name": "fastverse",
    "title": "A Suite of High-Performance Packages for Statistics and Data\nManipulation",
    "description": "Easy installation, loading and management, of high-performance packages \n             for statistical computing and data manipulation in R. \n             The core 'fastverse' consists of 4 packages: 'data.table', 'collapse', \n             'kit' and 'magrittr', that jointly only depend on 'Rcpp'. \n             The 'fastverse' can be freely and permanently extended with \n             additional packages, both globally or for individual projects. \n             Separate package verses can also be created. Fast packages \n             for many common tasks such as time series, dates and times, strings, \n             spatial data, statistics, data serialization, larger-than-memory \n             processing, and compilation of R code are listed in the README file: \n             <https://github.com/fastverse/fastverse#suggested-extensions>.",
    "version": "0.3.4",
    "maintainer": "Sebastian Krantz <sebastian.krantz@graduateinstitute.ch>",
    "author": "Sebastian Krantz [aut, cre],\n  Hadley Wickham [ctb]",
    "url": "https://fastverse.github.io/fastverse/,\nhttps://fastverse.r-universe.dev/, https://github.com/fastverse",
    "bug_reports": "https://github.com/fastverse/fastverse/issues",
    "repository": "https://cran.r-project.org/package=fastverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fastverse A Suite of High-Performance Packages for Statistics and Data\nManipulation Easy installation, loading and management, of high-performance packages \n             for statistical computing and data manipulation in R. \n             The core 'fastverse' consists of 4 packages: 'data.table', 'collapse', \n             'kit' and 'magrittr', that jointly only depend on 'Rcpp'. \n             The 'fastverse' can be freely and permanently extended with \n             additional packages, both globally or for individual projects. \n             Separate package verses can also be created. Fast packages \n             for many common tasks such as time series, dates and times, strings, \n             spatial data, statistics, data serialization, larger-than-memory \n             processing, and compilation of R code are listed in the README file: \n             <https://github.com/fastverse/fastverse#suggested-extensions>.  "
  },
  {
    "id": 12466,
    "package_name": "fixr",
    "title": "Fixing Data Made Easy for Statistical Analysis",
    "description": "A set of functions that facilitate basic data manipulation and cleaning for statistical analysis including functions for finding and fixing duplicate rows and columns, missing values, outliers, and special characters in column and row names and functions for checking data consistency, distribution, quality, reliability, and structure.",
    "version": "0.2.0",
    "maintainer": "Ambu Vijayan <ambuvjyn@gmail.com>",
    "author": "Ambu Vijayan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8924-5685>),\n  J. Sreekumar [aut] (ORCID: <https://orcid.org/0000-0002-4253-6378>,\n    Principal Scientist, ICAR - Central Tuber Crops Research Institute)",
    "url": "https://github.com/ambuvjyn/fixr",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=fixr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "fixr Fixing Data Made Easy for Statistical Analysis A set of functions that facilitate basic data manipulation and cleaning for statistical analysis including functions for finding and fixing duplicate rows and columns, missing values, outliers, and special characters in column and row names and functions for checking data consistency, distribution, quality, reliability, and structure.  "
  },
  {
    "id": 12486,
    "package_name": "flattabler",
    "title": "Obtaining a Flat Table from Pivot Tables",
    "description": "Transformations that allow obtaining a flat table from\n    reports in text or Excel format that contain data in the form of pivot\n    tables. They can be defined for a single report and applied to a set\n    of reports.",
    "version": "2.1.2",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/flattabler/,\nhttps://github.com/josesamos/flattabler",
    "bug_reports": "https://github.com/josesamos/flattabler/issues",
    "repository": "https://cran.r-project.org/package=flattabler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "flattabler Obtaining a Flat Table from Pivot Tables Transformations that allow obtaining a flat table from\n    reports in text or Excel format that contain data in the form of pivot\n    tables. They can be defined for a single report and applied to a set\n    of reports.  "
  },
  {
    "id": 12612,
    "package_name": "forestry",
    "title": "Reshape Data Tree",
    "description": "A series of utility functions to help with \n    reshaping hierarchy of data tree, and reform the structure of data tree. ",
    "version": "0.1.1",
    "maintainer": "Jiena McLellan <jienagu90@gmail.com>",
    "author": "Jiena McLellan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5578-088X>),\n  Michael Condouris [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8862-4250>),\n  Sai Im [ctb] (ORCID: <https://orcid.org/0000-0003-1990-5179>)",
    "url": "",
    "bug_reports": "https://github.com/jienagu/forestry/issues",
    "repository": "https://cran.r-project.org/package=forestry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "forestry Reshape Data Tree A series of utility functions to help with \n    reshaping hierarchy of data tree, and reform the structure of data tree.   "
  },
  {
    "id": 12771,
    "package_name": "funprog",
    "title": "Functional Programming",
    "description": "\n    High-order functions for data manipulation : sort or group data, given one\n    or more auxiliary functions. Functions are inspired by other pure\n    functional programming languages ('Haskell' mainly). The package also \n    provides built-in function operators for creating compact anonymous\n    functions, as well as the possibility to use the 'purrr' package syntax.",
    "version": "0.3.0",
    "maintainer": "Pierre-Yves Berrard <pyb@gmx.com>",
    "author": "Pierre-Yves Berrard [aut, cre]",
    "url": "https://py_b.gitlab.io/funprog, https://gitlab.com/py_b/funprog",
    "bug_reports": "https://gitlab.com/py_b/funprog/-/issues",
    "repository": "https://cran.r-project.org/package=funprog",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "funprog Functional Programming \n    High-order functions for data manipulation : sort or group data, given one\n    or more auxiliary functions. Functions are inspired by other pure\n    functional programming languages ('Haskell' mainly). The package also \n    provides built-in function operators for creating compact anonymous\n    functions, as well as the possibility to use the 'purrr' package syntax.  "
  },
  {
    "id": 12947,
    "package_name": "gdata",
    "title": "Various R Programming Tools for Data Manipulation",
    "description": "Various R programming tools for data manipulation, including\n  medical unit conversions, combining objects, character vector operations,\n  factor manipulation, obtaining information about R objects, generating\n  fixed-width format files, extracting components of date & time objects,\n  operations on columns of data frames, matrix operations, operations on\n  vectors, operations on data frames, value of last evaluated expression, and a\n  resample() wrapper for sample() that ensures consistent behavior for both\n  scalar and vector arguments.",
    "version": "3.0.1",
    "maintainer": "Arni Magnusson <thisisarni@gmail.com>",
    "author": "Gregory R. Warnes [aut],\n  Gregor Gorjanc [aut],\n  Arni Magnusson [aut, cre],\n  Liviu Andronic [aut],\n  Jim Rogers [aut],\n  Don MacQueen [aut],\n  Ales Korosec [aut],\n  Ben Bolker [ctb],\n  Michael Chirico [ctb],\n  Gabor Grothendieck [ctb],\n  Thomas Lumley [ctb],\n  Brian Ripley [ctb],\n  inoui llc [fnd]",
    "url": "https://github.com/r-gregmisc/gdata",
    "bug_reports": "https://github.com/r-gregmisc/gdata/issues",
    "repository": "https://cran.r-project.org/package=gdata",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gdata Various R Programming Tools for Data Manipulation Various R programming tools for data manipulation, including\n  medical unit conversions, combining objects, character vector operations,\n  factor manipulation, obtaining information about R objects, generating\n  fixed-width format files, extracting components of date & time objects,\n  operations on columns of data frames, matrix operations, operations on\n  vectors, operations on data frames, value of last evaluated expression, and a\n  resample() wrapper for sample() that ensures consistent behavior for both\n  scalar and vector arguments.  "
  },
  {
    "id": 12986,
    "package_name": "gen5helper",
    "title": "Processing 'Gen5' 2.06 Exported Data",
    "description": "A collection of functions for processing 'Gen5' 2.06 exported data.\n    'Gen5' is an essential data analysis software for BioTek plate readers <https://www.biotek.com/products/software-robotics-software/gen5-microplate-reader-and-imager-software/>. This package contains functions for data cleaning, \n    modeling and plotting using exported data from 'Gen5' version 2.06. It exports\n    technically correct data defined in (Edwin de Jonge and Mark van der Loo \n    (2013) <https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf>) for customized analysis. It \n    contains Boltzmann fitting for general kinetic analysis.  \n    See <https://www.github.com/yanxianUCSB/gen5helper> for more information,\n    documentation and examples.",
    "version": "1.0.1",
    "maintainer": "Yanxian Lin <yanxian00@gmail.com>",
    "author": "Yanxian Lin [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gen5helper",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gen5helper Processing 'Gen5' 2.06 Exported Data A collection of functions for processing 'Gen5' 2.06 exported data.\n    'Gen5' is an essential data analysis software for BioTek plate readers <https://www.biotek.com/products/software-robotics-software/gen5-microplate-reader-and-imager-software/>. This package contains functions for data cleaning, \n    modeling and plotting using exported data from 'Gen5' version 2.06. It exports\n    technically correct data defined in (Edwin de Jonge and Mark van der Loo \n    (2013) <https://cran.r-project.org/doc/contrib/de_Jonge+van_der_Loo-Introduction_to_data_cleaning_with_R.pdf>) for customized analysis. It \n    contains Boltzmann fitting for general kinetic analysis.  \n    See <https://www.github.com/yanxianUCSB/gen5helper> for more information,\n    documentation and examples.  "
  },
  {
    "id": 12999,
    "package_name": "genderapi",
    "title": "Client for 'GenderAPI.io'",
    "description": "Provides an interface to the 'GenderAPI.io' web service (<https://www.genderapi.io>) for determining gender from personal names, email addresses, or social media usernames. Functions are available to submit single or batch queries and retrieve additional information such as accuracy scores and country-specific gender predictions. This package simplifies integration of 'GenderAPI.io' into R workflows for data cleaning, user profiling, and analytics tasks.",
    "version": "1.0.3",
    "maintainer": "Onur Ozturk <onurozturk1980@gmail.com>",
    "author": "Onur Ozturk [aut, cre]",
    "url": "https://github.com/GenderAPI/genderapi-R",
    "bug_reports": "https://github.com/GenderAPI/genderapi-R/issues",
    "repository": "https://cran.r-project.org/package=genderapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "genderapi Client for 'GenderAPI.io' Provides an interface to the 'GenderAPI.io' web service (<https://www.genderapi.io>) for determining gender from personal names, email addresses, or social media usernames. Functions are available to submit single or batch queries and retrieve additional information such as accuracy scores and country-specific gender predictions. This package simplifies integration of 'GenderAPI.io' into R workflows for data cleaning, user profiling, and analytics tasks.  "
  },
  {
    "id": 13647,
    "package_name": "gridOT",
    "title": "Approximate Optimal Transport Between Two-Dimensional Grids",
    "description": "Can be used for optimal transport between two-dimensional grids with respect to separable cost functions of l^p form. It utilizes the Frank-Wolfe algorithm to approximate so-called pivot measures: One-dimensional transport plans that fully describe the full transport, see G. Auricchio (2023) <doi:10.4171/RLM/1026>. For these, it offers methods for visualization and to extract the corresponding transport plans and costs. Additionally, related functions for one-dimensional optimal transport are available.",
    "version": "1.0.2",
    "maintainer": "Michel Groppe <michel.groppe@uni-goettingen.de>",
    "author": "Michel Groppe [aut, cre],\n  Nicholas Bonneel [ctb],\n  Egerv\u00e1ry Research Group on Combinatorial Optimization [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=gridOT",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "gridOT Approximate Optimal Transport Between Two-Dimensional Grids Can be used for optimal transport between two-dimensional grids with respect to separable cost functions of l^p form. It utilizes the Frank-Wolfe algorithm to approximate so-called pivot measures: One-dimensional transport plans that fully describe the full transport, see G. Auricchio (2023) <doi:10.4171/RLM/1026>. For these, it offers methods for visualization and to extract the corresponding transport plans and costs. Additionally, related functions for one-dimensional optimal transport are available.  "
  },
  {
    "id": 13671,
    "package_name": "groupr",
    "title": "Groups with Inapplicable Values",
    "description": "The 'groupr' package provides a more powerful version of grouped\n  tibbles from 'dplyr'. It allows groups to be marked inapplicable,\n  which is a simple but widely useful way to express structure in a dataset.\n  It also provides powerful pivoting and other group manipulation functions.",
    "version": "0.1.2",
    "maintainer": "Nicholas Griffiths <ngriffiths21@gmail.com>",
    "author": "Nicholas Griffiths [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-8166-9045>)",
    "url": "https://github.com/ngriffiths21/groupr",
    "bug_reports": "https://github.com/ngriffiths21/groupr/issues",
    "repository": "https://cran.r-project.org/package=groupr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "groupr Groups with Inapplicable Values The 'groupr' package provides a more powerful version of grouped\n  tibbles from 'dplyr'. It allows groups to be marked inapplicable,\n  which is a simple but widely useful way to express structure in a dataset.\n  It also provides powerful pivoting and other group manipulation functions.  "
  },
  {
    "id": 14329,
    "package_name": "image2data",
    "title": "Turn Images into Data Sets",
    "description": "The goal of 'image2data' is to extract images and return\n    them into a data set, \n    especially for teaching data manipulation and data visualization. \n    Basically, the eponymous function takes an \n    image file ('png', 'tiff', 'jpeg', 'bmp') and turn it into a data set,\n    pixels being rows (subjects) and columns (variables) being their coordinate positions (x- and y-axis) and their respective color (in hex codes). \n    The function can return a complete image or a range of color (i.e., contour, silhouette).\n    The data can then be manipulated as would any data set by either creating other related variables (to hide the image) or as a genuine toy data set.",
    "version": "1.0.1",
    "maintainer": "P.-O. Caron <pocaron19@gmail.com>",
    "author": "P.-O. Caron [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0001-6346-5583>),\n  Alexandre Dufresne [aut, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=image2data",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "image2data Turn Images into Data Sets The goal of 'image2data' is to extract images and return\n    them into a data set, \n    especially for teaching data manipulation and data visualization. \n    Basically, the eponymous function takes an \n    image file ('png', 'tiff', 'jpeg', 'bmp') and turn it into a data set,\n    pixels being rows (subjects) and columns (variables) being their coordinate positions (x- and y-axis) and their respective color (in hex codes). \n    The function can return a complete image or a range of color (i.e., contour, silhouette).\n    The data can then be manipulated as would any data set by either creating other related variables (to hide the image) or as a genuine toy data set.  "
  },
  {
    "id": 14647,
    "package_name": "itraxR",
    "title": "Itrax Data Analysis Tools",
    "description": "Parse, trim, join, visualise and analyse data from Itrax sediment core multi-parameter \n    scanners manufactured by Cox Analytical Systems, Sweden. Functions are provided for parsing \n    XRF-peak area files, line-scan optical images, and radiographic images, alongside accompanying metadata. \n    A variety of data wrangling tasks like trimming, joining and reducing XRF-peak area data are simplified. \n    Multivariate methods are implemented with appropriate data transformation. ",
    "version": "1.12.2",
    "maintainer": "Thomas Bishop <tombishopemail@gmail.com>",
    "author": "Thomas Bishop",
    "url": "https://github.com/tombishop1/itraxR/",
    "bug_reports": "https://github.com/tombishop1/itraxR/issues",
    "repository": "https://cran.r-project.org/package=itraxR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "itraxR Itrax Data Analysis Tools Parse, trim, join, visualise and analyse data from Itrax sediment core multi-parameter \n    scanners manufactured by Cox Analytical Systems, Sweden. Functions are provided for parsing \n    XRF-peak area files, line-scan optical images, and radiographic images, alongside accompanying metadata. \n    A variety of data wrangling tasks like trimming, joining and reducing XRF-peak area data are simplified. \n    Multivariate methods are implemented with appropriate data transformation.   "
  },
  {
    "id": 14892,
    "package_name": "kit",
    "title": "Data Manipulation Functions Implemented in C",
    "description": "Basic functions, implemented in C, for large data manipulation. Fast vectorised ifelse()/nested if()/switch() functions, psum()/pprod() functions equivalent to pmin()/pmax() plus others which are missing from base R. Most of these functions are callable at C level.",
    "version": "0.0.20",
    "maintainer": "Morgan Jacob <morgan.emailbox@gmail.com>",
    "author": "Morgan Jacob [aut, cre, cph],\n  Sebastian Krantz [ctb]",
    "url": "",
    "bug_reports": "https://github.com/2005m/kit/issues",
    "repository": "https://cran.r-project.org/package=kit",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "kit Data Manipulation Functions Implemented in C Basic functions, implemented in C, for large data manipulation. Fast vectorised ifelse()/nested if()/switch() functions, psum()/pprod() functions equivalent to pmin()/pmax() plus others which are missing from base R. Most of these functions are callable at C level.  "
  },
  {
    "id": 15131,
    "package_name": "lehuynh",
    "title": "Le-Huynh Truc-Ly's R Code and Templates",
    "description": "Miscellaneous R functions (for graphics, data import, \n    data transformation, and general utilities) and templates (for exploratory \n    analysis, Bayesian modeling, and crafting scientific manuscripts). ",
    "version": "0.1.1",
    "maintainer": "Truc-Ly Le-Huynh <trucly.lehuynh@gmail.com>",
    "author": "Truc-Ly Le-Huynh [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-5227-2185>)",
    "url": "https://github.com/le-huynh/lehuynh,\nhttps://le-huynh.github.io/lehuynh/",
    "bug_reports": "https://github.com/le-huynh/lehuynh/issues",
    "repository": "https://cran.r-project.org/package=lehuynh",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lehuynh Le-Huynh Truc-Ly's R Code and Templates Miscellaneous R functions (for graphics, data import, \n    data transformation, and general utilities) and templates (for exploratory \n    analysis, Bayesian modeling, and crafting scientific manuscripts).   "
  },
  {
    "id": 15138,
    "package_name": "lenses",
    "title": "Elegant Data Manipulation with Lenses",
    "description": "Provides tools for creating and using lenses to simplify data manipulation. Lenses are composable getter/setter pairs for working with data in a purely functional way. Inspired by the 'Haskell' library 'lens' (Kmett, 2012) <https://hackage.haskell.org/package/lens>. For a fairly comprehensive (and highly technical) history of lenses please see the 'lens' wiki <https://github.com/ekmett/lens/wiki/History-of-Lenses>.",
    "version": "0.0.3",
    "maintainer": "Chris Hammill <cfhammill@gmail.com>",
    "author": "Chris Hammill [aut, cre, trl],\n  Ben Darwin [aut, trl]",
    "url": "http://cfhammill.github.io/lenses,\nhttps://github.com/cfhammill/lenses",
    "bug_reports": "https://github.com/cfhammill/lenses/issues",
    "repository": "https://cran.r-project.org/package=lenses",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lenses Elegant Data Manipulation with Lenses Provides tools for creating and using lenses to simplify data manipulation. Lenses are composable getter/setter pairs for working with data in a purely functional way. Inspired by the 'Haskell' library 'lens' (Kmett, 2012) <https://hackage.haskell.org/package/lens>. For a fairly comprehensive (and highly technical) history of lenses please see the 'lens' wiki <https://github.com/ekmett/lens/wiki/History-of-Lenses>.  "
  },
  {
    "id": 15185,
    "package_name": "lidR",
    "title": "Airborne LiDAR Data Manipulation and Visualization for Forestry\nApplications",
    "description": "Airborne LiDAR (Light Detection and Ranging) interface for data\n    manipulation and visualization. Read/write 'las' and 'laz' files, computation\n    of metrics in area based approach, point filtering, artificial point reduction,\n    classification from geographic data, normalization, individual tree segmentation\n    and other manipulations.",
    "version": "4.2.2",
    "maintainer": "Jean-Romain Roussel <info@r-lidar.com>",
    "author": "Jean-Romain Roussel [aut, cre, cph],\n  David Auty [aut, ctb] (Reviews the documentation),\n  Florian De Boissieu [ctb] (Fixed bugs and improved catalog features),\n  Andrew S\u00e1nchez Meador [ctb] (Implemented wing2015() for\n    segment_snags()),\n  Bourdon Jean-Fran\u00e7ois [ctb] (Contributed to Roussel2020() for\n    track_sensor()),\n  Gatziolis Demetrios [ctb] (Implemented Gatziolis2019() for\n    track_sensor()),\n  Leon Steinmeier [ctb] (Contributed to parallelization management),\n  Stanislaw Adaszewski [cph] (Author of the C++ concaveman code),\n  Beno\u00eet St-Onge [cph] (Author of the 'chm_prep' function)",
    "url": "https://github.com/r-lidar/lidR",
    "bug_reports": "https://github.com/r-lidar/lidR/issues",
    "repository": "https://cran.r-project.org/package=lidR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lidR Airborne LiDAR Data Manipulation and Visualization for Forestry\nApplications Airborne LiDAR (Light Detection and Ranging) interface for data\n    manipulation and visualization. Read/write 'las' and 'laz' files, computation\n    of metrics in area based approach, point filtering, artificial point reduction,\n    classification from geographic data, normalization, individual tree segmentation\n    and other manipulations.  "
  },
  {
    "id": 15248,
    "package_name": "listarrays",
    "title": "A Toolbox for Working with R Arrays in a Functional Programming\nStyle",
    "description": "A toolbox for R arrays. Flexibly split, bind, reshape, modify, \n    subset and name arrays.",
    "version": "0.4.0",
    "maintainer": "Tomasz Kalinowski <kalinowskit@gmail.com>",
    "author": "Tomasz Kalinowski [aut, cre]",
    "url": "https://github.com/t-kalinowski/listarrays,\nhttps://t-kalinowski.github.io/listarrays/",
    "bug_reports": "https://github.com/t-kalinowski/listarrays/issues",
    "repository": "https://cran.r-project.org/package=listarrays",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "listarrays A Toolbox for Working with R Arrays in a Functional Programming\nStyle A toolbox for R arrays. Flexibly split, bind, reshape, modify, \n    subset and name arrays.  "
  },
  {
    "id": 15264,
    "package_name": "liver",
    "title": "Toolkit and Datasets for Data Science",
    "description": "Provides a collection of helper functions and illustrative datasets to support learning and teaching of data science with R. The package is designed as a companion to the book <https://book-data-science-r.netlify.app>, making key data science techniques accessible to individuals with minimal coding experience. Functions include tools for data partitioning, performance evaluation, and data transformations (e.g., z-score and min-max scaling). The included datasets are curated to highlight practical applications in data exploration, modeling, and multivariate analysis. An early inspiration for the package came from an ancient Persian idiom about \"eating the liveR,\" symbolizing deep and immersive engagement with knowledge.",
    "version": "1.26",
    "maintainer": "Reza Mohammadi <a.mohammadi@uva.nl>",
    "author": "Reza Mohammadi [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-9538-0648>),\n  Kevin Burke [aut]",
    "url": "https://book-data-science-r.netlify.app",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=liver",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "liver Toolkit and Datasets for Data Science Provides a collection of helper functions and illustrative datasets to support learning and teaching of data science with R. The package is designed as a companion to the book <https://book-data-science-r.netlify.app>, making key data science techniques accessible to individuals with minimal coding experience. Functions include tools for data partitioning, performance evaluation, and data transformations (e.g., z-score and min-max scaling). The included datasets are curated to highlight practical applications in data exploration, modeling, and multivariate analysis. An early inspiration for the package came from an ancient Persian idiom about \"eating the liveR,\" symbolizing deep and immersive engagement with knowledge.  "
  },
  {
    "id": 15443,
    "package_name": "lsr",
    "title": "Companion to \"Learning Statistics with R\"",
    "description": "A collection of tools intended to make introductory \n    statistics easier to teach, including wrappers for common \n    hypothesis tests and basic data manipulation. It accompanies \n    Navarro, D. J. (2015). Learning Statistics with R: A Tutorial \n    for Psychology Students and Other Beginners, Version 0.6. ",
    "version": "0.5.2",
    "maintainer": "Danielle Navarro <djnavarro@protonmail.com>",
    "author": "Danielle Navarro [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7648-6578>)",
    "url": "https://github.com/djnavarro/lsr",
    "bug_reports": "https://github.com/djnavarro/lsr/issues",
    "repository": "https://cran.r-project.org/package=lsr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "lsr Companion to \"Learning Statistics with R\" A collection of tools intended to make introductory \n    statistics easier to teach, including wrappers for common \n    hypothesis tests and basic data manipulation. It accompanies \n    Navarro, D. J. (2015). Learning Statistics with R: A Tutorial \n    for Psychology Students and Other Beginners, Version 0.6.   "
  },
  {
    "id": 15479,
    "package_name": "m61r",
    "title": "Package About Data Manipulation in Pure Base R",
    "description": "Data manipulation in one package and in base R.\n  Minimal. No dependencies.\n  'dplyr' and 'tidyr'-like in one place.\n  Nothing else than base R to build the package.",
    "version": "0.0.3",
    "maintainer": "Jean-Marie Lepioufle <pv71u98h1@gmail.com>",
    "author": "Jean-Marie Lepioufle [aut, cre]",
    "url": "https://github.com/pv71u98h1/m61r/",
    "bug_reports": "https://github.com/pv71u98h1/m61r/issues/",
    "repository": "https://cran.r-project.org/package=m61r",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "m61r Package About Data Manipulation in Pure Base R Data manipulation in one package and in base R.\n  Minimal. No dependencies.\n  'dplyr' and 'tidyr'-like in one place.\n  Nothing else than base R to build the package.  "
  },
  {
    "id": 15520,
    "package_name": "madshapR",
    "title": "Functions to Support Data Management and Processing Using the\nMaelstrom Research Approach",
    "description": "Functions to support data cleaning, evaluation, and description, \n     developed for integration with Maelstrom Research software tools. 'madshapR' \n     provides functions primarily to evaluate and manipulate datasets and \n     data dictionaries in preparation for data harmonization with the package \n     'Rmonize' and to facilitate integration and transfer between RStudio servers \n     and secure Opal environments. 'madshapR' functions can be used independently\n     but are optimized in conjunction with \u2018Rmonize\u2019 functions for streamlined \n     and coherent harmonization processing.",
    "version": "2.0.0",
    "maintainer": "Guillaume Fabre <guijoseph.fabre@gmail.com>",
    "author": "Guillaume Fabre [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-0124-9970>),\n  Maelstrom Research [aut, fnd, cph]",
    "url": "https://github.com/maelstrom-research/madshapR",
    "bug_reports": "https://github.com/maelstrom-research/madshapR/issues",
    "repository": "https://cran.r-project.org/package=madshapR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "madshapR Functions to Support Data Management and Processing Using the\nMaelstrom Research Approach Functions to support data cleaning, evaluation, and description, \n     developed for integration with Maelstrom Research software tools. 'madshapR' \n     provides functions primarily to evaluate and manipulate datasets and \n     data dictionaries in preparation for data harmonization with the package \n     'Rmonize' and to facilitate integration and transfer between RStudio servers \n     and secure Opal environments. 'madshapR' functions can be used independently\n     but are optimized in conjunction with \u2018Rmonize\u2019 functions for streamlined \n     and coherent harmonization processing.  "
  },
  {
    "id": 15524,
    "package_name": "magclass",
    "title": "Data Class and Tools for Handling Spatial-Temporal Data",
    "description": "Data class for increased interoperability working with\n    spatial-temporal data together with corresponding functions and\n    methods (conversions, basic calculations and basic data manipulation).\n    The class distinguishes between spatial, temporal and other dimensions\n    to facilitate the development and interoperability of tools build for\n    it. Additional features are name-based addressing of data and internal\n    consistency checks (e.g. checking for the right data order in\n    calculations).",
    "version": "6.13.2",
    "maintainer": "Jan Philipp Dietrich <dietrich@pik-potsdam.de>",
    "author": "Jan Philipp Dietrich [aut, cre],\n  Benjamin Leon Bodirsky [aut],\n  Markus Bonsch [aut],\n  Florian Humpenoeder [aut],\n  Stephen Bi [aut],\n  Kristine Karstens [aut],\n  Debbora Leip [aut],\n  Pascal Sauer [aut],\n  Lavinia Baumstark [ctb],\n  Christoph Bertram [ctb],\n  Anastasis Giannousakis [ctb],\n  David Klein [ctb],\n  Ina Neher [ctb],\n  Michaja Pehl [ctb],\n  Anselm Schultes [ctb],\n  Miodrag Stevanovic [ctb],\n  Xiaoxi Wang [ctb],\n  Felicitas Beier [ctb],\n  Mika Pfl\u00fcger [ctb],\n  Oliver Richters [ctb]",
    "url": "https://github.com/pik-piam/magclass,\nhttps://doi.org/10.5281/zenodo.1158580",
    "bug_reports": "https://github.com/pik-piam/magclass/issues",
    "repository": "https://cran.r-project.org/package=magclass",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "magclass Data Class and Tools for Handling Spatial-Temporal Data Data class for increased interoperability working with\n    spatial-temporal data together with corresponding functions and\n    methods (conversions, basic calculations and basic data manipulation).\n    The class distinguishes between spatial, temporal and other dimensions\n    to facilitate the development and interoperability of tools build for\n    it. Additional features are name-based addressing of data and internal\n    consistency checks (e.g. checking for the right data order in\n    calculations).  "
  },
  {
    "id": 15711,
    "package_name": "mbX",
    "title": "A Comprehensive Microbiome Data Processing Pipeline",
    "description": "\n    Provides tools for cleaning, processing, and preparing microbiome \n    sequencing data (e.g., 16S rRNA) for downstream analysis. Supports CSV, \n    TXT, and Excel file formats. The main function, ezclean(), automates \n    microbiome data transformation, including format validation, \n    transposition, numeric conversion, and metadata integration. It also \n    handles taxonomic levels efficiently, resolves duplicated taxa entries, \n    and outputs a well-structured, analysis-ready dataset. The companion \n    functions ezstat() run statistical tests and summarize results, while \n    ezviz() produces publication-ready visualizations.",
    "version": "0.2.0",
    "maintainer": "Utsav Lamichhane <utsav.lamichhane@gmail.com>",
    "author": "Utsav Lamichhane [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mbX",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mbX A Comprehensive Microbiome Data Processing Pipeline \n    Provides tools for cleaning, processing, and preparing microbiome \n    sequencing data (e.g., 16S rRNA) for downstream analysis. Supports CSV, \n    TXT, and Excel file formats. The main function, ezclean(), automates \n    microbiome data transformation, including format validation, \n    transposition, numeric conversion, and metadata integration. It also \n    handles taxonomic levels efficiently, resolves duplicated taxa entries, \n    and outputs a well-structured, analysis-ready dataset. The companion \n    functions ezstat() run statistical tests and summarize results, while \n    ezviz() produces publication-ready visualizations.  "
  },
  {
    "id": 15913,
    "package_name": "metamicrobiomeR",
    "title": "Microbiome Data Analysis & Meta-Analysis with GAMLSS-BEZI &\nRandom Effects",
    "description": "Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> . ",
    "version": "1.2",
    "maintainer": "Nhan Ho <nhanhocumc@gmail.com>",
    "author": "Nhan Ho [aut, cre]",
    "url": "https://github.com/nhanhocu/metamicrobiomeR",
    "bug_reports": "https://github.com/nhanhocu/metamicrobiomeR/issues",
    "repository": "https://cran.r-project.org/package=metamicrobiomeR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "metamicrobiomeR Microbiome Data Analysis & Meta-Analysis with GAMLSS-BEZI &\nRandom Effects Generalized Additive Model for Location, Scale and Shape (GAMLSS) \n    with zero inflated beta (BEZI) family for analysis of microbiome relative abundance data \n    (with various options for data transformation/normalization to address compositional effects) and \n    random effects meta-analysis models for meta-analysis pooling estimates across microbiome studies \n    are implemented. \n    Random Forest model to predict microbiome age based on relative abundances of  \n    shared bacterial genera with the Bangladesh data (Subramanian et al 2014), \n    comparison of multiple diversity indexes using linear/linear mixed effect models \n    and some data display/visualization are also implemented.\n    The reference paper is published by \n    Ho NT, Li F, Wang S, Kuhn L (2019) <doi:10.1186/s12859-019-2744-2> .   "
  },
  {
    "id": 15943,
    "package_name": "meteor",
    "title": "Meteorological Data Manipulation",
    "description": "A set of functions for weather and climate data manipulation, and other helper functions, to support dynamic ecological modeling, particularly crop and crop disease modeling.",
    "version": "0.4-5",
    "maintainer": "Robert J. Hijmans <r.hijmans@gmail.com>",
    "author": "Robert J. Hijmans [cre, aut],\n  Gerald Nelson [ctb],\n  Maarten Waterloo [ctb]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=meteor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "meteor Meteorological Data Manipulation A set of functions for weather and climate data manipulation, and other helper functions, to support dynamic ecological modeling, particularly crop and crop disease modeling.  "
  },
  {
    "id": 15986,
    "package_name": "mi",
    "title": "Missing Data Imputation and Model Checking",
    "description": "The mi package provides functions for data manipulation, imputing missing values in an approximate Bayesian framework, diagnostics of the models used to generate the imputations, confidence-building mechanisms to validate some of the assumptions of the imputation algorithm, and functions to analyze multiply imputed data sets with the appropriate degree of sampling uncertainty.",
    "version": "1.2",
    "maintainer": "Ben Goodrich <benjamin.goodrich@columbia.edu>",
    "author": "Andrew Gelman [ctb],\n  Jennifer Hill [ctb],\n  Yu-Sung Su [aut],\n  Masanao Yajima [ctb],\n  Maria Pittau [ctb],\n  Ben Goodrich [cre, aut],\n  Yajuan Si [ctb],\n  Jon Kropko [aut]",
    "url": "https://sites.stat.columbia.edu/gelman/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=mi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "mi Missing Data Imputation and Model Checking The mi package provides functions for data manipulation, imputing missing values in an approximate Bayesian framework, diagnostics of the models used to generate the imputations, confidence-building mechanisms to validate some of the assumptions of the imputation algorithm, and functions to analyze multiply imputed data sets with the appropriate degree of sampling uncertainty.  "
  },
  {
    "id": 16361,
    "package_name": "monitOS",
    "title": "Monitoring Overall Survival in Pivotal Trials in Indolent\nCancers",
    "description": "These guidelines are meant to provide a pragmatic, yet rigorous, help to drug developers and decision makers, since they are shaped by three fundamental ingredients: the clinically determined margin of detriment on OS that is unacceptably high (delta null); the benefit on OS that is plausible given the mechanism of action of the novel intervention (delta alt); and the quantity of information (i.e. survival events) it is feasible to accrue given the clinical and drug development setting. The proposed guidelines facilitate transparent discussions between stakeholders focusing on the risks of erroneous decisions and what might be an acceptable trade-off between power and the false positive error rate.",
    "version": "0.1.6",
    "maintainer": "Thibaud Coroller <thibaud.coroller@novartis.com>",
    "author": "Thomas Fleming [ctb],\n  Lisa Hampson [aut],\n  Bharani Bharani-Dharan [ctb],\n  Frank Bretz [ctb],\n  Arunava Chakravartty [ctb],\n  Thibaud Coroller [aut, cre],\n  Evanthia Koukouli [aut],\n  Janet Wittes [ctb],\n  Nigel Yateman [ctb],\n  Emmanuel Zuber [ctb],\n  Novartis Pharma AG [cph]",
    "url": "https://opensource.nibr.com/monitOS/,\nhttps://github.com/Novartis/monitOS",
    "bug_reports": "https://github.com/Novartis/monitOS/issues",
    "repository": "https://cran.r-project.org/package=monitOS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "monitOS Monitoring Overall Survival in Pivotal Trials in Indolent\nCancers These guidelines are meant to provide a pragmatic, yet rigorous, help to drug developers and decision makers, since they are shaped by three fundamental ingredients: the clinically determined margin of detriment on OS that is unacceptably high (delta null); the benefit on OS that is plausible given the mechanism of action of the novel intervention (delta alt); and the quantity of information (i.e. survival events) it is feasible to accrue given the clinical and drug development setting. The proposed guidelines facilitate transparent discussions between stakeholders focusing on the risks of erroneous decisions and what might be an acceptable trade-off between power and the false positive error rate.  "
  },
  {
    "id": 16468,
    "package_name": "msSPChelpR",
    "title": "Helper Functions for Second Primary Cancer Analyses",
    "description": "A collection of helper functions for analyzing Second Primary Cancer data, \n    including functions to reshape data, to calculate patient states and analyze cancer incidence.",
    "version": "0.9.1",
    "maintainer": "Marian Eberl <marian.eberl@tum.de>",
    "author": "Marian Eberl [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-6584-3197>)",
    "url": "https://marianschmidt.github.io/msSPChelpR/",
    "bug_reports": "https://github.com/marianschmidt/msSPChelpR/issues",
    "repository": "https://cran.r-project.org/package=msSPChelpR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "msSPChelpR Helper Functions for Second Primary Cancer Analyses A collection of helper functions for analyzing Second Primary Cancer data, \n    including functions to reshape data, to calculate patient states and analyze cancer incidence.  "
  },
  {
    "id": 16572,
    "package_name": "multilevel",
    "title": "Multilevel Functions",
    "description": "Tools used by organizational researchers for the analysis of multilevel data. Includes four broad sets of tools. First, functions for estimating within-group agreement and reliability indices. Second, functions for manipulating multilevel and longitudinal (panel) data. Third, simulations for estimating power and generating multilevel data. Fourth, miscellaneous functions for estimating reliability and performing simple calculations and data transformations.  ",
    "version": "2.7.1",
    "maintainer": "Paul Bliese <pdbliese@gmail.com>",
    "author": "Paul Bliese [aut, cre],\n  Gilad Chen [ctb],\n  Patrick Downes [ctb],\n  Donald Schepker [ctb],\n  Jonas Lang [ctb]",
    "url": "https://www.r-project.org",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=multilevel",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "multilevel Multilevel Functions Tools used by organizational researchers for the analysis of multilevel data. Includes four broad sets of tools. First, functions for estimating within-group agreement and reliability indices. Second, functions for manipulating multilevel and longitudinal (panel) data. Third, simulations for estimating power and generating multilevel data. Fourth, miscellaneous functions for estimating reliability and performing simple calculations and data transformations.    "
  },
  {
    "id": 17058,
    "package_name": "notebookutils",
    "title": "Dummy R APIs Used in 'Azure Synapse Analytics' for Local\nDevelopments",
    "description": "This is a pure dummy interfaces package which mirrors 'MsSparkUtils' APIs <https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-r> of 'Azure Synapse Analytics' <https://learn.microsoft.com/en-us/azure/synapse-analytics/> for R users,\n    customer of Azure Synapse can download this package from CRAN for local development.",
    "version": "1.5.3",
    "maintainer": "runtimeexp <runtimeexpdg@microsoft.com>",
    "author": "runtimeexp [aut, cre],\n  Microsoft [cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=notebookutils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "notebookutils Dummy R APIs Used in 'Azure Synapse Analytics' for Local\nDevelopments This is a pure dummy interfaces package which mirrors 'MsSparkUtils' APIs <https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-r> of 'Azure Synapse Analytics' <https://learn.microsoft.com/en-us/azure/synapse-analytics/> for R users,\n    customer of Azure Synapse can download this package from CRAN for local development.  "
  },
  {
    "id": 17091,
    "package_name": "nplyr",
    "title": "A Grammar of Nested Data Manipulation",
    "description": "Provides functions for manipulating nested data frames in a \n    list-column using 'dplyr' <https://dplyr.tidyverse.org/> syntax. Rather than \n    unnesting, then manipulating a data frame, 'nplyr' allows users to \n    manipulate each nested data frame directly. 'nplyr' is a wrapper for 'dplyr'\n    functions that provide tools for common data manipulation steps: filtering \n    rows, selecting columns, summarising grouped data, among others.",
    "version": "0.3.0",
    "maintainer": "Bol\u00edvar Aponte Rol\u00f3n <bolaponte@pm.me>",
    "author": "Mark Rieke [aut],\n  Bol\u00edvar Aponte Rol\u00f3n [cre] (ORCID:\n    <https://orcid.org/0000-0002-2544-4551>),\n  Joran Elias [ctb]",
    "url": "https://github.com/jibarozzo/nplyr,\nhttps://jibarozzo.github.io/nplyr/",
    "bug_reports": "https://github.com/jibarozzo/nplyr/issues",
    "repository": "https://cran.r-project.org/package=nplyr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "nplyr A Grammar of Nested Data Manipulation Provides functions for manipulating nested data frames in a \n    list-column using 'dplyr' <https://dplyr.tidyverse.org/> syntax. Rather than \n    unnesting, then manipulating a data frame, 'nplyr' allows users to \n    manipulate each nested data frame directly. 'nplyr' is a wrapper for 'dplyr'\n    functions that provide tools for common data manipulation steps: filtering \n    rows, selecting columns, summarising grouped data, among others.  "
  },
  {
    "id": 17209,
    "package_name": "oeli",
    "title": "Some Utilities for Developing Data Science Software",
    "description": "A collection of general-purpose helper functions that I (and maybe \n    others) find useful when developing data science software. Includes tools \n    for simulation, data transformation, input validation, and more.",
    "version": "0.7.5",
    "maintainer": "Lennart Oelschl\u00e4ger <oelschlaeger.lennart@gmail.com>",
    "author": "Lennart Oelschl\u00e4ger [aut, cre]",
    "url": "https://github.com/loelschlaeger/oeli,\nhttp://loelschlaeger.de/oeli/",
    "bug_reports": "https://github.com/loelschlaeger/oeli/issues",
    "repository": "https://cran.r-project.org/package=oeli",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "oeli Some Utilities for Developing Data Science Software A collection of general-purpose helper functions that I (and maybe \n    others) find useful when developing data science software. Includes tools \n    for simulation, data transformation, input validation, and more.  "
  },
  {
    "id": 17257,
    "package_name": "onemapsgapi",
    "title": "R Wrapper for the 'OneMap.Sg API'",
    "description": "An R wrapper for the 'OneMap.Sg' API <https://www.onemap.gov.sg/docs/>. \n    Functions help users query data from the API and return raw JSON data\n    in \"tidy\" formats. Support is also available for users to retrieve data from multiple API calls \n    and integrate results into single dataframes, without\n    needing to clean and merge the data themselves. This package is best suited for users who would \n    like to perform analyses with Singapore's spatial data without having to perform excessive data cleaning.",
    "version": "2.0.0",
    "maintainer": "Jolene Lim <jolene.lim14@gmail.com>",
    "author": "Jolene Lim [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=onemapsgapi",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "onemapsgapi R Wrapper for the 'OneMap.Sg API' An R wrapper for the 'OneMap.Sg' API <https://www.onemap.gov.sg/docs/>. \n    Functions help users query data from the API and return raw JSON data\n    in \"tidy\" formats. Support is also available for users to retrieve data from multiple API calls \n    and integrate results into single dataframes, without\n    needing to clean and merge the data themselves. This package is best suited for users who would \n    like to perform analyses with Singapore's spatial data without having to perform excessive data cleaning.  "
  },
  {
    "id": 17295,
    "package_name": "openVA",
    "title": "Automated Method for Verbal Autopsy",
    "description": "Implements multiple existing open-source algorithms for coding cause of death from verbal autopsies. The methods implemented include 'InterVA4' by Byass et al (2012) <doi:10.3402/gha.v5i0.19281>, 'InterVA5' by Byass at al (2019) <doi:10.1186/s12916-019-1333-6>, 'InSilicoVA' by McCormick et al (2016) <doi:10.1080/01621459.2016.1152191>, 'NBC' by Miasnikof et al (2015) <doi:10.1186/s12916-015-0521-2>, and a replication of 'Tariff' method by James et al (2011) <doi:10.1186/1478-7954-9-31> and Serina, et al. (2015) <doi:10.1186/s12916-015-0527-9>. It also provides tools for data manipulation tasks commonly used in Verbal Autopsy analysis and implements easy graphical visualization of individual and population level statistics. The 'NBC' method is implemented by the 'nbc4va' package that can be installed from <https://github.com/rrwen/nbc4va>. Note that this package was not developed by authors affiliated with the Institute for Health Metrics and Evaluation and thus unintentional discrepancies may exist in the implementation of the 'Tariff' method.",
    "version": "1.2.0",
    "maintainer": "Zehang Richard Li <lizehang@gmail.com>",
    "author": "Zehang Richard Li [aut, cre],\n  Jason Thomas [aut],\n  Tyler H. McCormick [aut],\n  Samuel J. Clark [aut]",
    "url": "https://github.com/verbal-autopsy-software/openVA",
    "bug_reports": "https://github.com/verbal-autopsy-software/openVA/issues",
    "repository": "https://cran.r-project.org/package=openVA",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "openVA Automated Method for Verbal Autopsy Implements multiple existing open-source algorithms for coding cause of death from verbal autopsies. The methods implemented include 'InterVA4' by Byass et al (2012) <doi:10.3402/gha.v5i0.19281>, 'InterVA5' by Byass at al (2019) <doi:10.1186/s12916-019-1333-6>, 'InSilicoVA' by McCormick et al (2016) <doi:10.1080/01621459.2016.1152191>, 'NBC' by Miasnikof et al (2015) <doi:10.1186/s12916-015-0521-2>, and a replication of 'Tariff' method by James et al (2011) <doi:10.1186/1478-7954-9-31> and Serina, et al. (2015) <doi:10.1186/s12916-015-0527-9>. It also provides tools for data manipulation tasks commonly used in Verbal Autopsy analysis and implements easy graphical visualization of individual and population level statistics. The 'NBC' method is implemented by the 'nbc4va' package that can be installed from <https://github.com/rrwen/nbc4va>. Note that this package was not developed by authors affiliated with the Institute for Health Metrics and Evaluation and thus unintentional discrepancies may exist in the implementation of the 'Tariff' method.  "
  },
  {
    "id": 17488,
    "package_name": "paar",
    "title": "Precision Agriculture Data Analysis",
    "description": "Precision agriculture spatial data\n depuration and homogeneous zones (management zone) delineation.\n The package includes functions that performs protocols for data cleaning\n management zone delineation and zone comparison; protocols are described in \n Paccioretti et al., (2020) <doi:10.1016/j.compag.2020.105556>.",
    "version": "1.0.1",
    "maintainer": "Pablo Paccioretti <pablopaccioretti@agro.unc.edu.ar>",
    "author": "Pablo Paccioretti [aut, cre, cph],\n  Mariano C\u00f3rdoba [aut],\n  Franca Giannini-Kurina [aut],\n  M\u00f3nica Balzarini [aut]",
    "url": "https://ppaccioretti.github.io/paar/,\nhttps://github.com/PPaccioretti/paar",
    "bug_reports": "https://github.com/PPaccioretti/paar/issues",
    "repository": "https://cran.r-project.org/package=paar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "paar Precision Agriculture Data Analysis Precision agriculture spatial data\n depuration and homogeneous zones (management zone) delineation.\n The package includes functions that performs protocols for data cleaning\n management zone delineation and zone comparison; protocols are described in \n Paccioretti et al., (2020) <doi:10.1016/j.compag.2020.105556>.  "
  },
  {
    "id": 17522,
    "package_name": "palaeoverse",
    "title": "Prepare and Explore Data for Palaeobiological Analyses",
    "description": "Provides functionality to support data preparation and exploration for\n  palaeobiological analyses, improving code reproducibility and accessibility. The\n  wider aim of 'palaeoverse' is to bring the palaeobiological community together \n  to establish agreed standards. The package currently includes functionality for \n  data cleaning, binning (time and space), exploration, summarisation and \n  visualisation. Reference datasets (i.e. Geological Time Scales <https://stratigraphy.org/chart>)\n  and auxiliary functions are also provided. Details can be found in:\n  Jones et al., (2023) <doi: 10.1111/2041-210X.14099>.",
    "version": "1.4.0",
    "maintainer": "Lewis A. Jones <LewisA.Jones@outlook.com>",
    "author": "Lewis A. Jones [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-3902-8986>),\n  William Gearty [aut] (ORCID: <https://orcid.org/0000-0003-0076-3262>),\n  Bethany J. Allen [aut] (ORCID: <https://orcid.org/0000-0003-0282-6407>),\n  Kilian Eichenseer [aut] (ORCID:\n    <https://orcid.org/0000-0002-0477-8878>),\n  Christopher D. Dean [aut] (ORCID:\n    <https://orcid.org/0000-0001-6471-6903>),\n  Sofia Galvan [ctb] (ORCID: <https://orcid.org/0000-0002-3092-4314>),\n  Miranta Kouvari [ctb] (ORCID: <https://orcid.org/0000-0002-5442-6221>),\n  Pedro L. Godoy [ctb] (ORCID: <https://orcid.org/0000-0003-4519-5094>),\n  Cecily Nicholl [ctb] (ORCID: <https://orcid.org/0000-0003-2860-2604>),\n  Lucas Buffan [ctb] (ORCID: <https://orcid.org/0000-0002-2353-1432>),\n  Erin M. Dillon [ctb] (ORCID: <https://orcid.org/0000-0003-0249-027X>),\n  Joseph T. Flannery-Sutherland [aut] (ORCID:\n    <https://orcid.org/0000-0001-8232-6773>),\n  A. Alessandro Chiarenza [ctb] (ORCID:\n    <https://orcid.org/0000-0001-5525-6730>)",
    "url": "https://palaeoverse.palaeoverse.org,\nhttps://github.com/palaeoverse/palaeoverse,\nhttps://palaeoverse.org",
    "bug_reports": "https://github.com/palaeoverse/palaeoverse/issues",
    "repository": "https://cran.r-project.org/package=palaeoverse",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "palaeoverse Prepare and Explore Data for Palaeobiological Analyses Provides functionality to support data preparation and exploration for\n  palaeobiological analyses, improving code reproducibility and accessibility. The\n  wider aim of 'palaeoverse' is to bring the palaeobiological community together \n  to establish agreed standards. The package currently includes functionality for \n  data cleaning, binning (time and space), exploration, summarisation and \n  visualisation. Reference datasets (i.e. Geological Time Scales <https://stratigraphy.org/chart>)\n  and auxiliary functions are also provided. Details can be found in:\n  Jones et al., (2023) <doi: 10.1111/2041-210X.14099>.  "
  },
  {
    "id": 17782,
    "package_name": "pepe",
    "title": "Data Manipulation",
    "description": "Is designed to make easier printing summary statistics (for continues and factor level) tables in Latex, and plotting by factor. ",
    "version": "1.2.0",
    "maintainer": "Seyma Kalay <seymakalay@hotmail.com>",
    "author": "Seyma Kalay",
    "url": "https://github.com/seymakalay/pepe",
    "bug_reports": "https://github.com/seymakalay/pepe/issues",
    "repository": "https://cran.r-project.org/package=pepe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pepe Data Manipulation Is designed to make easier printing summary statistics (for continues and factor level) tables in Latex, and plotting by factor.   "
  },
  {
    "id": 17902,
    "package_name": "phsmethods",
    "title": "Standard Methods for Use in Public Health Scotland",
    "description": "A collection of methods for commonly undertaken analytical\n    tasks, primarily developed for Public Health Scotland (PHS) analysts,\n    but the package is also generally useful to others working in the\n    healthcare space, particularly since it has functions for working with\n    Community Health Index (CHI) numbers. The package can help to make\n    data manipulation and analysis more efficient and reproducible.",
    "version": "1.0.2",
    "maintainer": "Tina Fu <Yuyan.Fu2@phs.scot>",
    "author": "Public Health Scotland [cph],\n  David Caldwell [aut],\n  Lucinda Lawrie [rev],\n  Jack Hannah [aut],\n  Tina Fu [aut, cre],\n  Ciara Gribben [aut],\n  Chris Deans [aut],\n  Jaime Villacampa [aut],\n  Graeme Gowans [aut],\n  Alice Byers [ctb],\n  Alan Yeung [ctb],\n  James McMahon [aut] (ORCID: <https://orcid.org/0000-0002-5380-2029>),\n  Nicolaos Christofidis [aut]",
    "url": "https://github.com/Public-Health-Scotland/phsmethods,\nhttps://public-health-scotland.github.io/phsmethods/",
    "bug_reports": "https://github.com/Public-Health-Scotland/phsmethods/issues",
    "repository": "https://cran.r-project.org/package=phsmethods",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "phsmethods Standard Methods for Use in Public Health Scotland A collection of methods for commonly undertaken analytical\n    tasks, primarily developed for Public Health Scotland (PHS) analysts,\n    but the package is also generally useful to others working in the\n    healthcare space, particularly since it has functions for working with\n    Community Health Index (CHI) numbers. The package can help to make\n    data manipulation and analysis more efficient and reproducible.  "
  },
  {
    "id": 17963,
    "package_name": "pipeliner",
    "title": "Machine Learning Pipelines for R",
    "description": "A framework for defining 'pipelines' of functions for applying data transformations, \n  model estimation and inverse-transformations, resulting in predicted value generation (or \n  model-scoring) functions that automatically apply the entire pipeline of functions required to go\n  from input to predicted output.",
    "version": "0.1.1",
    "maintainer": "Alex Ioannides <alex.ioannides@yahoo.co.uk>",
    "author": "Alex Ioannides",
    "url": "https://github.com/alexioannides/pipeliner",
    "bug_reports": "https://github.com/alexioannides/pipeliner/issues",
    "repository": "https://cran.r-project.org/package=pipeliner",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pipeliner Machine Learning Pipelines for R A framework for defining 'pipelines' of functions for applying data transformations, \n  model estimation and inverse-transformations, resulting in predicted value generation (or \n  model-scoring) functions that automatically apply the entire pipeline of functions required to go\n  from input to predicted output.  "
  },
  {
    "id": 17969,
    "package_name": "pivmet",
    "title": "Pivotal Methods for Bayesian Relabelling and k-Means Clustering",
    "description": "Collection of pivotal algorithms \n             for: relabelling the MCMC chains in order to undo the label \n             switching problem in Bayesian mixture models;\n             fitting sparse finite mixtures;\n             initializing the centers of the classical k-means algorithm \n             in order to obtain a better clustering solution. \n             For further details see\n             Egidi, Pappad\u00e0, Pauli and Torelli (2018b)<ISBN:9788891910233>.",
    "version": "0.6.0",
    "maintainer": "Leonardo Egidi <legidi@units.it>",
    "author": "Leonardo Egidi[aut, cre], \n        Roberta Pappad\u00e0[aut], \n        Francesco Pauli[aut], \n        Nicola Torelli[aut] ",
    "url": "https://github.com/leoegidi/pivmet",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pivmet",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pivmet Pivotal Methods for Bayesian Relabelling and k-Means Clustering Collection of pivotal algorithms \n             for: relabelling the MCMC chains in order to undo the label \n             switching problem in Bayesian mixture models;\n             fitting sparse finite mixtures;\n             initializing the centers of the classical k-means algorithm \n             in order to obtain a better clustering solution. \n             For further details see\n             Egidi, Pappad\u00e0, Pauli and Torelli (2018b)<ISBN:9788891910233>.  "
  },
  {
    "id": 17970,
    "package_name": "pivotaltrackR",
    "title": "A Client for the 'Pivotal Tracker' API",
    "description": "'Pivotal Tracker' <https://www.pivotaltracker.com> is a project\n    management software-as-a-service that provides a REST API. This package\n    provides an R interface to that API, allowing you to query it and work with\n    its responses.",
    "version": "0.2.0",
    "maintainer": "Neal Richardson <neal.p.richardson@gmail.com>",
    "author": "Neal Richardson [aut, cre]",
    "url": "https://enpiar.com/r/pivotaltrackR/,\nhttps://github.com/nealrichardson/pivotaltrackR/",
    "bug_reports": "https://github.com/nealrichardson/pivotaltrackR/issues",
    "repository": "https://cran.r-project.org/package=pivotaltrackR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pivotaltrackR A Client for the 'Pivotal Tracker' API 'Pivotal Tracker' <https://www.pivotaltracker.com> is a project\n    management software-as-a-service that provides a REST API. This package\n    provides an R interface to that API, allowing you to query it and work with\n    its responses.  "
  },
  {
    "id": 17971,
    "package_name": "pivotea",
    "title": "Create Pivot Table Easily",
    "description": "Pivot easily by specifying rows, columns, values and split.",
    "version": "1.0.2",
    "maintainer": "Toshikazu Matsumura <matutosi@gmail.com>",
    "author": "Toshikazu Matsumura [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2306-6355>)",
    "url": "https://github.com/matutosi/pivotea,\nhttps://matutosi.github.io/pivotea/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=pivotea",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pivotea Create Pivot Table Easily Pivot easily by specifying rows, columns, values and split.  "
  },
  {
    "id": 17972,
    "package_name": "pivottabler",
    "title": "Create Pivot Tables",
    "description": "Create regular pivot tables with just a few lines of R.  \n    More complex pivot tables can also be created, e.g. pivot tables\n    with irregular layouts, multiple calculations and/or derived \n    calculations based on multiple data frames.  Pivot tables are\n    constructed using R only and can be written to a range of\n    output formats (plain text, 'HTML', 'Latex' and 'Excel'), \n    including with styling/formatting.",
    "version": "1.5.6",
    "maintainer": "Christopher Bailiss <cbailiss@gmail.com>",
    "author": "Christopher Bailiss [aut, cre]",
    "url": "http://www.pivottabler.org.uk/,\nhttps://github.com/cbailiss/pivottabler",
    "bug_reports": "https://github.com/cbailiss/pivottabler/issues",
    "repository": "https://cran.r-project.org/package=pivottabler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pivottabler Create Pivot Tables Create regular pivot tables with just a few lines of R.  \n    More complex pivot tables can also be created, e.g. pivot tables\n    with irregular layouts, multiple calculations and/or derived \n    calculations based on multiple data frames.  Pivot tables are\n    constructed using R only and can be written to a range of\n    output formats (plain text, 'HTML', 'Latex' and 'Excel'), \n    including with styling/formatting.  "
  },
  {
    "id": 18083,
    "package_name": "pmartR",
    "title": "Panomics Marketplace - Quality Control and Statistical Analysis\nfor Panomics Data",
    "description": "Provides functionality for quality control processing and statistical analysis of mass spectrometry (MS) omics data, in particular proteomic (either at the peptide or the protein level), lipidomic, and metabolomic data, as well as RNA-seq based count data and nuclear magnetic resonance (NMR) data. This includes data transformation, specification of groups that are to be compared against each other, filtering of features and/or samples, data normalization, data summarization (correlation, PCA), and statistical comparisons between defined groups.  Implements methods described in:  Webb-Robertson et al. (2014) <doi:10.1074/mcp.M113.030932>.  Webb-Robertson et al. (2011) <doi:10.1002/pmic.201100078>.  Matzke et al. (2011) <doi:10.1093/bioinformatics/btr479>.  Matzke et al. (2013) <doi:10.1002/pmic.201200269>.  Polpitiya et al. (2008) <doi:10.1093/bioinformatics/btn217>.  Webb-Robertson et al. (2010) <doi:10.1021/pr1005247>.  ",
    "version": "2.5.1",
    "maintainer": "Lisa Bramer <lisa.bramer@pnnl.gov>",
    "author": "Lisa Bramer [aut, cre],\n  Kelly Stratton [aut],\n  Daniel Claborne [aut],\n  Evan Glasscock [ctb],\n  Rachel Richardson [ctb],\n  David Degnan [ctb],\n  Evan Martin [ctb]",
    "url": "https://pmartr.github.io/pmartR/, https://github.com/pmartR/pmartR",
    "bug_reports": "https://github.com/pmartR/pmartR/issues",
    "repository": "https://cran.r-project.org/package=pmartR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "pmartR Panomics Marketplace - Quality Control and Statistical Analysis\nfor Panomics Data Provides functionality for quality control processing and statistical analysis of mass spectrometry (MS) omics data, in particular proteomic (either at the peptide or the protein level), lipidomic, and metabolomic data, as well as RNA-seq based count data and nuclear magnetic resonance (NMR) data. This includes data transformation, specification of groups that are to be compared against each other, filtering of features and/or samples, data normalization, data summarization (correlation, PCA), and statistical comparisons between defined groups.  Implements methods described in:  Webb-Robertson et al. (2014) <doi:10.1074/mcp.M113.030932>.  Webb-Robertson et al. (2011) <doi:10.1002/pmic.201100078>.  Matzke et al. (2011) <doi:10.1093/bioinformatics/btr479>.  Matzke et al. (2013) <doi:10.1002/pmic.201200269>.  Polpitiya et al. (2008) <doi:10.1093/bioinformatics/btn217>.  Webb-Robertson et al. (2010) <doi:10.1021/pr1005247>.    "
  },
  {
    "id": 18198,
    "package_name": "poputils",
    "title": "Demographic Analysis and Data Manipulation",
    "description": "Perform tasks commonly encountered when\n    preparing and analysing demographic data.\n    Some functions are intended for end users, and\n    others for developers. Includes functions for\n    working with life tables.",
    "version": "0.4.2",
    "maintainer": "John Bryant <john@bayesiandemography.com>",
    "author": "John Bryant [aut, cre],\n  Bayesian Demography Limited [cph]",
    "url": "https://bayesiandemography.github.io/poputils/,\nhttps://github.com/bayesiandemography/poputils",
    "bug_reports": "https://github.com/bayesiandemography/poputils/issues",
    "repository": "https://cran.r-project.org/package=poputils",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "poputils Demographic Analysis and Data Manipulation Perform tasks commonly encountered when\n    preparing and analysing demographic data.\n    Some functions are intended for end users, and\n    others for developers. Includes functions for\n    working with life tables.  "
  },
  {
    "id": 18351,
    "package_name": "priceR",
    "title": "Economics and Pricing Tools",
    "description": "Functions to aid in micro and macro economic analysis and handling of price and\n    currency data. Includes extraction of relevant inflation and exchange rate data from World Bank\n    API, data cleaning/parsing, and standardisation. Inflation adjustment\n    calculations as found in Principles of Macroeconomics by Gregory Mankiw et al (2014). Current\n    and historical end of day exchange rates for 171 currencies from the European Central Bank\n    Statistical Data Warehouse (2020).",
    "version": "1.0.3",
    "maintainer": "Steve Condylios <steve.condylios@gmail.com>",
    "author": "Steve Condylios [aut, cre],\n  Bruno Mioto [ctb],\n  Bryan Shalloway [ctb]",
    "url": "https://github.com/stevecondylios/priceR",
    "bug_reports": "https://github.com/stevecondylios/priceR/issues",
    "repository": "https://cran.r-project.org/package=priceR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "priceR Economics and Pricing Tools Functions to aid in micro and macro economic analysis and handling of price and\n    currency data. Includes extraction of relevant inflation and exchange rate data from World Bank\n    API, data cleaning/parsing, and standardisation. Inflation adjustment\n    calculations as found in Principles of Macroeconomics by Gregory Mankiw et al (2014). Current\n    and historical end of day exchange rates for 171 currencies from the European Central Bank\n    Statistical Data Warehouse (2020).  "
  },
  {
    "id": 18510,
    "package_name": "psycCleaning",
    "title": "Data Cleaning for Psychological Analyses",
    "description": "Useful for preparing and cleaning data. It includes functions to center data, reverse coding, dummy code and effect code data, and more.",
    "version": "0.1.1",
    "maintainer": "Jason Moy <jason.moyhj@gmail.com>",
    "author": "Jason Moy [aut, cre] (ORCID: <https://orcid.org/0000-0001-8795-3311>)",
    "url": "https://jasonmoy28.github.io/psycCleaning/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=psycCleaning",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "psycCleaning Data Cleaning for Psychological Analyses Useful for preparing and cleaning data. It includes functions to center data, reverse coding, dummy code and effect code data, and more.  "
  },
  {
    "id": 18754,
    "package_name": "quest",
    "title": "Prepare Questionnaire Data for Analysis",
    "description": "Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an \"s\" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an \"s\" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.",
    "version": "0.2.1",
    "maintainer": "David Disabato <ddisab01@gmail.com>",
    "author": "David Disabato [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7094-4996>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=quest",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "quest Prepare Questionnaire Data for Analysis Offers a suite of functions to prepare questionnaire data for analysis (perhaps other types of data as well). By data preparation, I mean data analytic tasks to get your raw data ready for statistical modeling (e.g., regression). There are functions to investigate missing data, reshape data, validate responses, recode variables, score questionnaires, center variables, aggregate by groups, shift scores (i.e., leads or lags), etc. It provides functions for both single level and multilevel (i.e., grouped) data. With a few exceptions (e.g., ncases()), functions without an \"s\" at the end of their primary word (e.g., center_by()) act on atomic vectors, while functions with an \"s\" at the end of their primary word (e.g., centers_by()) act on multiple columns of a data.frame.  "
  },
  {
    "id": 18782,
    "package_name": "r02pro",
    "title": "R Programming: Zero to Pro",
    "description": "This is a companion package of the book \"R Programming: Zero to Pro\"  <https://r02pro.github.io/>. It contains the datasets used in the book and provides interactive exercises corresponding to the book. It covers a wide range of topics including visualization, data transformation, tidying data, data input and output.",
    "version": "0.2",
    "maintainer": "Yang Feng <yangfengstat@gmail.com>",
    "author": "Yang Feng [aut, cre],\n  Jianan Zhu [aut]",
    "url": "https://r02pro.github.io/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=r02pro",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r02pro R Programming: Zero to Pro This is a companion package of the book \"R Programming: Zero to Pro\"  <https://r02pro.github.io/>. It contains the datasets used in the book and provides interactive exercises corresponding to the book. It covers a wide range of topics including visualization, data transformation, tidying data, data input and output.  "
  },
  {
    "id": 18785,
    "package_name": "r2dictionary",
    "title": "A Mini-Dictionary for 'R', 'shiny' and 'rmarkdown' Documents",
    "description": "Despite the predominant use of R for data manipulation and various robust statistical calculations, in recent years, more people from various disciplines are beginning to use R for other purposes. In doing this seemlessly, further tools are needed users to easily and freely write in R for all kinds of purposes. The r2dictionary introduces a means for users to directly search for definitions of terms within the R environment.",
    "version": "0.3",
    "maintainer": "Obinna Obianom <idonshayo@gmail.com>",
    "author": "Obinna Obianom [aut, cre]",
    "url": "https://r2dictionary.obi.obianom.com",
    "bug_reports": "https://github.com/oobianom/r2dictionary/issues",
    "repository": "https://cran.r-project.org/package=r2dictionary",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r2dictionary A Mini-Dictionary for 'R', 'shiny' and 'rmarkdown' Documents Despite the predominant use of R for data manipulation and various robust statistical calculations, in recent years, more people from various disciplines are beginning to use R for other purposes. In doing this seemlessly, further tools are needed users to easily and freely write in R for all kinds of purposes. The r2dictionary introduces a means for users to directly search for definitions of terms within the R environment.  "
  },
  {
    "id": 18817,
    "package_name": "r6qualitytools",
    "title": "R6-Based Statistical Methods for Quality Science",
    "description": "A comprehensive suite of statistical tools for Quality Management, designed around the Define, Measure, Analyze, Improve, and Control (DMAIC) cycle used in Six Sigma methodology. Based on the discontinued CRAN package 'qualitytools', this package refactors its original design by incorporating 'R6' object-oriented programming for increased flexibility and performance. It replaces traditional graphics with modern, interactive visualizations using 'ggplot2' and 'plotly'. Built on 'tidyverse' principles, it simplifies data manipulation and visualization, offering an intuitive approach to quality science.",
    "version": "1.0.1",
    "maintainer": "Fabian Encarnacion <fab.encarnacion@outlook.com>",
    "author": "Andrea Barahona [aut],\n  Fabian Encarnacion [aut, cre, cph],\n  Miguel Flores [aut],\n  Javier Tarrio-Saavedra [ctb],\n  Salvador Naya [ctb]",
    "url": "https://github.com/Fabianenc/r6qualitytools",
    "bug_reports": "https://github.com/Fabianenc/r6qualitytools/issues",
    "repository": "https://cran.r-project.org/package=r6qualitytools",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "r6qualitytools R6-Based Statistical Methods for Quality Science A comprehensive suite of statistical tools for Quality Management, designed around the Define, Measure, Analyze, Improve, and Control (DMAIC) cycle used in Six Sigma methodology. Based on the discontinued CRAN package 'qualitytools', this package refactors its original design by incorporating 'R6' object-oriented programming for increased flexibility and performance. It replaces traditional graphics with modern, interactive visualizations using 'ggplot2' and 'plotly'. Built on 'tidyverse' principles, it simplifies data manipulation and visualization, offering an intuitive approach to quality science.  "
  },
  {
    "id": 18894,
    "package_name": "rSPARCS",
    "title": "Sites, Population, and Records Cleaning Skills",
    "description": "Data cleaning including 1) generating datasets for time-series and case-crossover analyses based on raw hospital records, 2) linking individuals to an areal map, 3) picking out cases living within a buffer of certain size surrounding a site, etc. For more information, please refer to Zhang W,etc. (2018) <doi:10.1016/j.envpol.2018.08.030>. ",
    "version": "0.1.1",
    "maintainer": "Wangjian Zhang <zhangwj227@mail.sysu.edu.cn>",
    "author": "Wangjian Zhang [aut, cre],\n  Zhicheng Du [aut],\n  Xinlei Deng [aut],\n  Ziqiang Lin [aut],\n  Bo Ye [aut],\n  Jijin Yao [aut],\n  Yanan Jin [aut],\n  Wayne Lawrence [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rSPARCS",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rSPARCS Sites, Population, and Records Cleaning Skills Data cleaning including 1) generating datasets for time-series and case-crossover analyses based on raw hospital records, 2) linking individuals to an areal map, 3) picking out cases living within a buffer of certain size surrounding a site, etc. For more information, please refer to Zhang W,etc. (2018) <doi:10.1016/j.envpol.2018.08.030>.   "
  },
  {
    "id": 19342,
    "package_name": "renz",
    "title": "R-Enzymology",
    "description": "Contains utilities for the analysis of Michaelian kinetic data. Beside the classical linearization \n\tmethods (Lineweaver-Burk, Eadie-Hofstee, Hanes-Woolf and Eisenthal-Cornish-Bowden), features include the \n\tability to carry out weighted regression analysis that, in most cases, substantially improves the estimation \n\tof kinetic parameters (Aledo (2021) <doi:10.1002/bmb.21522>). To avoid data transformation and the potential\n\tbiases introduced by them, the package also offers functions to directly fitting data to the Michaelis-Menten \n\tequation, either using ([S], v) or (time, [S]) data. Utilities to simulate substrate progress-curves (making \n\tuse of the Lambert W function) are also provided. The package is accompanied of vignettes that aim to orientate\n\tthe user in the choice of the most suitable method to estimate the kinetic parameter of an Michaelian enzyme.",
    "version": "0.2.1",
    "maintainer": "Juan Carlos Aledo <caledo@uma.es>",
    "author": "Juan Carlos Aledo",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=renz",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "renz R-Enzymology Contains utilities for the analysis of Michaelian kinetic data. Beside the classical linearization \n\tmethods (Lineweaver-Burk, Eadie-Hofstee, Hanes-Woolf and Eisenthal-Cornish-Bowden), features include the \n\tability to carry out weighted regression analysis that, in most cases, substantially improves the estimation \n\tof kinetic parameters (Aledo (2021) <doi:10.1002/bmb.21522>). To avoid data transformation and the potential\n\tbiases introduced by them, the package also offers functions to directly fitting data to the Michaelis-Menten \n\tequation, either using ([S], v) or (time, [S]) data. Utilities to simulate substrate progress-curves (making \n\tuse of the Lambert W function) are also provided. The package is accompanied of vignettes that aim to orientate\n\tthe user in the choice of the most suitable method to estimate the kinetic parameter of an Michaelian enzyme.  "
  },
  {
    "id": 19370,
    "package_name": "reproj",
    "title": "Coordinate System Transformations for Generic Map Data",
    "description": "Transform coordinates from a specified source to a specified \n target map projection. This uses the 'PROJ' library directly, by wrapping the \n 'PROJ' package which leverages 'libproj', otherwise the 'proj4' package. The 'reproj()' \n function is generic, methods may be added to remove the need for an explicit \n source definition. If 'proj4' is in use 'reproj()' handles the requirement for \n conversion of angular units where necessary. This is for use primarily to \n transform generic data formats and direct leverage of the underlying\n 'PROJ' library. (There are transformations that aren't possible with 'PROJ' and\n that are provided by the 'GDAL' library, a limitation which users of this \n package should be aware of.) The 'PROJ' library is available at \n <https://proj.org/>.  ",
    "version": "0.7.0",
    "maintainer": "Michael D. Sumner <mdsumner@gmail.com>",
    "author": "Michael D. Sumner [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2471-7511>)",
    "url": "https://github.com/hypertidy/reproj,\nhttps://hypertidy.github.io/reproj/",
    "bug_reports": "https://github.com/hypertidy/reproj/issues",
    "repository": "https://cran.r-project.org/package=reproj",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reproj Coordinate System Transformations for Generic Map Data Transform coordinates from a specified source to a specified \n target map projection. This uses the 'PROJ' library directly, by wrapping the \n 'PROJ' package which leverages 'libproj', otherwise the 'proj4' package. The 'reproj()' \n function is generic, methods may be added to remove the need for an explicit \n source definition. If 'proj4' is in use 'reproj()' handles the requirement for \n conversion of angular units where necessary. This is for use primarily to \n transform generic data formats and direct leverage of the underlying\n 'PROJ' library. (There are transformations that aren't possible with 'PROJ' and\n that are provided by the 'GDAL' library, a limitation which users of this \n package should be aware of.) The 'PROJ' library is available at \n <https://proj.org/>.    "
  },
  {
    "id": 19391,
    "package_name": "reshape",
    "title": "Flexibly Reshape Data",
    "description": "Flexibly restructure and aggregate data using \n  just two functions: melt and cast.",
    "version": "0.8.10",
    "maintainer": "Hadley Wickham <hadley@rstudio.com>",
    "author": "Hadley Wickham [aut, cre]",
    "url": "http://had.co.nz/reshape",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=reshape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reshape Flexibly Reshape Data Flexibly restructure and aggregate data using \n  just two functions: melt and cast.  "
  },
  {
    "id": 19392,
    "package_name": "reshape2",
    "title": "Flexibly Reshape Data: A Reboot of the Reshape Package",
    "description": "Flexibly restructure and aggregate data using just two\n    functions: melt and 'dcast' (or 'acast').",
    "version": "1.4.5",
    "maintainer": "Hadley Wickham <hadley@posit.co>",
    "author": "Hadley Wickham [aut, cre]",
    "url": "https://github.com/hadley/reshape",
    "bug_reports": "https://github.com/hadley/reshape/issues",
    "repository": "https://cran.r-project.org/package=reshape2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "reshape2 Flexibly Reshape Data: A Reboot of the Reshape Package Flexibly restructure and aggregate data using just two\n    functions: melt and 'dcast' (or 'acast').  "
  },
  {
    "id": 19421,
    "package_name": "retroharmonize",
    "title": "Ex Post Survey Data Harmonization",
    "description": "Assist in reproducible retrospective (ex-post) harmonization\n    of data, particularly individual level survey data, by providing tools\n    for organizing metadata, standardizing the coding of variables, and\n    variable names and value labels, including missing values, and\n    documenting the data transformations, with the help of comprehensive\n    s3 classes.",
    "version": "0.2.0",
    "maintainer": "Daniel Antal <daniel.antal@ceemid.eu>",
    "author": "Daniel Antal [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-7513-6760>),\n  Marta Kolczynska [ctb] (ORCID: <https://orcid.org/0000-0003-4981-0437>),\n  Pyry Kantanen [ctb] (ORCID: <https://orcid.org/0000-0003-2853-2765>),\n  Diego Hernang\u00f3mez Herrero [ctb] (ORCID:\n    <https://orcid.org/0000-0001-8457-4658>)",
    "url": "https://retroharmonize.dataobservatory.eu/,\nhttps://ropengov.github.io/retroharmonize/,\nhttps://github.com/rOpenGov/retroharmonize",
    "bug_reports": "https://github.com/rOpenGov/retroharmonize/issues",
    "repository": "https://cran.r-project.org/package=retroharmonize",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "retroharmonize Ex Post Survey Data Harmonization Assist in reproducible retrospective (ex-post) harmonization\n    of data, particularly individual level survey data, by providing tools\n    for organizing metadata, standardizing the coding of variables, and\n    variable names and value labels, including missing values, and\n    documenting the data transformations, with the help of comprehensive\n    s3 classes.  "
  },
  {
    "id": 19447,
    "package_name": "rfishnet2",
    "title": "Exploratory Data Analysis for FishNet2 Data",
    "description": "Provides data processing and summarization of data from FishNet2.net\n  in text and graphical outputs. Allows efficient filtering of information and\n  data cleaning.",
    "version": "0.2.0",
    "maintainer": "Kennedy Dorsey <kadorsey97@gmail.com>",
    "author": "Margaux Armfield email = margaux.armfield@gmail.com [aut],\n  Kennedy Dorsey [aut, cre]",
    "url": "https://github.com/kdors/rfishnet2",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfishnet2",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfishnet2 Exploratory Data Analysis for FishNet2 Data Provides data processing and summarization of data from FishNet2.net\n  in text and graphical outputs. Allows efficient filtering of information and\n  data cleaning.  "
  },
  {
    "id": 19454,
    "package_name": "rfriend",
    "title": "Provides Batch Functions and Visualisation for Basic Statistical\nProcedures",
    "description": "Designed to streamline data analysis and statistical testing, reducing the length of R \n    scripts while generating well-formatted outputs in 'pdf', 'Microsoft Word', and 'Microsoft Excel' \n    formats. In essence, the package contains functions which are sophisticated wrappers around \n    existing R functions that are called by using 'f_' (user f_riendly) prefix followed by the normal \n    function name. This first version of the 'rfriend' package focuses  primarily on data exploration, \n    including tools for creating summary tables, f_summary(), performing data transformations, \n    f_boxcox() in part based on 'MASS/boxcox' and 'rcompanion', and f_bestNormalize() \n    which wraps and extends functionality from the 'bestNormalize' package. Furthermore, 'rfriend' \n    can automatically (or on request) generate visualizations such as boxplots, f_boxplot(), \n    QQ-plots, f_qqnorm(), histograms f_hist(), and density plots. Additionally, the package includes \n    four statistical test functions: f_aov(), f_kruskal_test(), f_glm(), f_chisq_test for sequential \n    testing and visualisation of the 'stats' functions: aov(), kruskal.test(), glm() and chisq.test. \n    These functions support testing multiple response variables and predictors, while also handling \n    assumption checks, data transformations, and post hoc tests. Post hoc results are automatically \n    summarized in a table using the compact letter display (cld) format for easy interpretation. The \n    package also provides a function to do model comparison, f_model_comparison(), and several \n    utility functions to simplify common R tasks. For example, f_clear() clears the workspace and \n    restarts R with a single command; f_setwd() sets the working directory to match the directory \n    of the current script; f_theme() quickly changes 'RStudio' themes; and f_factors() converts \n    multiple columns of a data frame to factors, and much more. If you encounter any issues or have \n    feature requests, please feel free to contact me via email.",
    "version": "2.0.0",
    "maintainer": "Sander H. van Delden <plantmind@proton.me>",
    "author": "Sander H. van Delden [aut, cre]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rfriend",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rfriend Provides Batch Functions and Visualisation for Basic Statistical\nProcedures Designed to streamline data analysis and statistical testing, reducing the length of R \n    scripts while generating well-formatted outputs in 'pdf', 'Microsoft Word', and 'Microsoft Excel' \n    formats. In essence, the package contains functions which are sophisticated wrappers around \n    existing R functions that are called by using 'f_' (user f_riendly) prefix followed by the normal \n    function name. This first version of the 'rfriend' package focuses  primarily on data exploration, \n    including tools for creating summary tables, f_summary(), performing data transformations, \n    f_boxcox() in part based on 'MASS/boxcox' and 'rcompanion', and f_bestNormalize() \n    which wraps and extends functionality from the 'bestNormalize' package. Furthermore, 'rfriend' \n    can automatically (or on request) generate visualizations such as boxplots, f_boxplot(), \n    QQ-plots, f_qqnorm(), histograms f_hist(), and density plots. Additionally, the package includes \n    four statistical test functions: f_aov(), f_kruskal_test(), f_glm(), f_chisq_test for sequential \n    testing and visualisation of the 'stats' functions: aov(), kruskal.test(), glm() and chisq.test. \n    These functions support testing multiple response variables and predictors, while also handling \n    assumption checks, data transformations, and post hoc tests. Post hoc results are automatically \n    summarized in a table using the compact letter display (cld) format for easy interpretation. The \n    package also provides a function to do model comparison, f_model_comparison(), and several \n    utility functions to simplify common R tasks. For example, f_clear() clears the workspace and \n    restarts R with a single command; f_setwd() sets the working directory to match the directory \n    of the current script; f_theme() quickly changes 'RStudio' themes; and f_factors() converts \n    multiple columns of a data frame to factors, and much more. If you encounter any issues or have \n    feature requests, please feel free to contact me via email.  "
  },
  {
    "id": 19554,
    "package_name": "rjdworkspace",
    "title": "Manipulate 'JDemetra+' Workspaces",
    "description": "Set of tools to manipulate the 'JDemetra+' workspaces.  Based\n    on the 'RJDemetra' package (which interfaces with version 2 of the\n    'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>), the seasonal\n    adjustment software officially recommended to the members of the\n    European Statistical System (ESS) and the European System of Central\n    Banks).  This package provides access to additional workspace\n    manipulation functions such as metadata manipulation, raw paths and\n    wrangling of several workspaces simultaneously.  These additional\n    functionalities are useful as part of a CVS data production chain.",
    "version": "1.2.0",
    "maintainer": "Tanguy Barthelemy <tanguy.barthelemy@insee.fr>",
    "author": "Tanguy Barthelemy [aut, cre, art],\n  Alain Quartier-la-Tente [aut] (ORCID:\n    <https://orcid.org/0000-0001-7890-3857>),\n  Institut national de la statistique et des \u00e9tudes \u00e9conomiques [cph]\n    (https://www.insee.fr/),\n  Anna Smyk [aut]",
    "url": "https://github.com/InseeFrLab/rjdworkspace,\nhttps://inseefrlab.github.io/rjdworkspace/",
    "bug_reports": "https://github.com/InseeFrLab/rjdworkspace/issues",
    "repository": "https://cran.r-project.org/package=rjdworkspace",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjdworkspace Manipulate 'JDemetra+' Workspaces Set of tools to manipulate the 'JDemetra+' workspaces.  Based\n    on the 'RJDemetra' package (which interfaces with version 2 of the\n    'JDemetra+' (<https://github.com/jdemetra/jdemetra-app>), the seasonal\n    adjustment software officially recommended to the members of the\n    European Statistical System (ESS) and the European System of Central\n    Banks).  This package provides access to additional workspace\n    manipulation functions such as metadata manipulation, raw paths and\n    wrangling of several workspaces simultaneously.  These additional\n    functionalities are useful as part of a CVS data production chain.  "
  },
  {
    "id": 19558,
    "package_name": "rjsoncons",
    "title": "Query, Pivot, Patch, and Validate 'JSON' and 'NDJSON'",
    "description": "Functions to query (filter or transform), pivot (convert\n    from array-of-objects to object-of-arrays, for easy import as 'R'\n    data frame), search, patch (edit), and validate (against 'JSON Schema')\n    'JSON' and 'NDJSON' strings, files, or URLs. Query and\n    pivot support 'JSONpointer', 'JSONpath' or 'JMESpath'\n    expressions. The implementation uses the 'jsoncons'\n    <https://danielaparker.github.io/jsoncons/> header-only library;\n    the library is easily linked to other packages for direct access\n    to 'C++' functionality not implemented here.",
    "version": "1.3.2",
    "maintainer": "Martin Morgan <mtmorgan.xyz@gmail.com>",
    "author": "Martin Morgan [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5874-8148>),\n  Marcel Ramos [aut] (ORCID: <https://orcid.org/0000-0002-3242-0582>),\n  Daniel Parker [aut, cph] (jsoncons C++ library maintainer)",
    "url": "https://mtmorgan.github.io/rjsoncons/",
    "bug_reports": "https://github.com/mtmorgan/rjsoncons/issues",
    "repository": "https://cran.r-project.org/package=rjsoncons",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rjsoncons Query, Pivot, Patch, and Validate 'JSON' and 'NDJSON' Functions to query (filter or transform), pivot (convert\n    from array-of-objects to object-of-arrays, for easy import as 'R'\n    data frame), search, patch (edit), and validate (against 'JSON Schema')\n    'JSON' and 'NDJSON' strings, files, or URLs. Query and\n    pivot support 'JSONpointer', 'JSONpath' or 'JMESpath'\n    expressions. The implementation uses the 'jsoncons'\n    <https://danielaparker.github.io/jsoncons/> header-only library;\n    the library is easily linked to other packages for direct access\n    to 'C++' functionality not implemented here.  "
  },
  {
    "id": 19578,
    "package_name": "rlist",
    "title": "A Toolbox for Non-Tabular Data Manipulation",
    "description": "Provides a set of functions for data manipulation with\n    list objects, including mapping, filtering, grouping, sorting,\n    updating, searching, and other useful functions. Most functions\n    are designed to be pipeline friendly so that data processing with\n    lists can be chained.",
    "version": "0.4.6.2",
    "maintainer": "Kun Ren <ken@renkun.me>",
    "author": "Kun Ren <ken@renkun.me>",
    "url": "https://renkun-ken.github.io/rlist/,\nhttps://github.com/renkun-ken/rlist,\nhttps://renkun-ken.github.io/rlist-tutorial/",
    "bug_reports": "https://github.com/renkun-ken/rlist/issues",
    "repository": "https://cran.r-project.org/package=rlist",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rlist A Toolbox for Non-Tabular Data Manipulation Provides a set of functions for data manipulation with\n    list objects, including mapping, filtering, grouping, sorting,\n    updating, searching, and other useful functions. Most functions\n    are designed to be pipeline friendly so that data processing with\n    lists can be chained.  "
  },
  {
    "id": 19783,
    "package_name": "rpivotTable",
    "title": "Build Powerful Pivot Tables and Dynamically Slice & Dice your\nData",
    "description": "Build powerful pivot tables (aka Pivot Grid, Pivot Chart, Cross-Tab)\n    and dynamically slice & dice / drag 'n' drop your data. 'rpivotTable' is a\n    wrapper of 'pivottable', a powerful open-source Pivot Table library implemented\n    in 'JavaScript' by Nicolas Kruchten. Aligned to 'pivottable' v2.19.0.",
    "version": "0.3.0",
    "maintainer": "Enzo Martoglio  <enzo@smartinsightsfromdata.com>",
    "author": "Enzo Martoglio [aut, cre],\n  Nicolas Kruchten [ctb, cph],\n  Nagarajan Chinnasamy [ctb, cph],\n  Kenton Russell [ctb]",
    "url": "https://github.com/smartinsightsfromdata,\nhttp://smartinsightsfromdata.github.io",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=rpivotTable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rpivotTable Build Powerful Pivot Tables and Dynamically Slice & Dice your\nData Build powerful pivot tables (aka Pivot Grid, Pivot Chart, Cross-Tab)\n    and dynamically slice & dice / drag 'n' drop your data. 'rpivotTable' is a\n    wrapper of 'pivottable', a powerful open-source Pivot Table library implemented\n    in 'JavaScript' by Nicolas Kruchten. Aligned to 'pivottable' v2.19.0.  "
  },
  {
    "id": 19805,
    "package_name": "rqdatatable",
    "title": "'rquery' for 'data.table'",
    "description": "Implements the 'rquery' piped Codd-style query algebra using 'data.table'.  This allows\n   for a high-speed in memory implementation of Codd-style data manipulation tools.",
    "version": "1.3.3",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rqdatatable/,\nhttps://winvector.github.io/rqdatatable/",
    "bug_reports": "https://github.com/WinVector/rqdatatable/issues",
    "repository": "https://cran.r-project.org/package=rqdatatable",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rqdatatable 'rquery' for 'data.table' Implements the 'rquery' piped Codd-style query algebra using 'data.table'.  This allows\n   for a high-speed in memory implementation of Codd-style data manipulation tools.  "
  },
  {
    "id": 19807,
    "package_name": "rquery",
    "title": "Relational Query Generator for Data Manipulation at Scale",
    "description": "A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.",
    "version": "1.4.99",
    "maintainer": "John Mount <jmount@win-vector.com>",
    "author": "John Mount [aut, cre],\n  Win-Vector LLC [cph]",
    "url": "https://github.com/WinVector/rquery/,\nhttps://winvector.github.io/rquery/",
    "bug_reports": "https://github.com/WinVector/rquery/issues",
    "repository": "https://cran.r-project.org/package=rquery",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rquery Relational Query Generator for Data Manipulation at Scale A piped query generator based on Edgar F. Codd's relational\n    algebra, and on production experience using 'SQL' and 'dplyr' at big data\n    scale.  The design represents an attempt to make 'SQL' more teachable by\n    denoting composition by a sequential pipeline notation instead of nested\n    queries or functions.   The implementation delivers reliable high \n    performance data processing on large data systems such as 'Spark',\n    databases, and 'data.table'. Package features include: data processing trees\n    or pipelines as observable objects (able to report both columns\n    produced and columns used), optimized 'SQL' generation as an explicit\n    user visible table modeling step, plus explicit query reasoning and checking.  "
  },
  {
    "id": 19822,
    "package_name": "rrefine",
    "title": "r Client for OpenRefine API",
    "description": "'OpenRefine' (formerly 'Google Refine') is a popular, open source data cleaning software. This package enables users to programmatically trigger data transfer between R and 'OpenRefine'. Available functionality includes project import, export and deletion.",
    "version": "2.1.0",
    "maintainer": "VP Nagraj <nagraj@nagraj.net>",
    "author": "VP Nagraj [aut, cre]",
    "url": "https://github.com/vpnagraj/rrefine",
    "bug_reports": "https://github.com/vpnagraj/rrefine/issues",
    "repository": "https://cran.r-project.org/package=rrefine",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rrefine r Client for OpenRefine API 'OpenRefine' (formerly 'Google Refine') is a popular, open source data cleaning software. This package enables users to programmatically trigger data transfer between R and 'OpenRefine'. Available functionality includes project import, export and deletion.  "
  },
  {
    "id": 19836,
    "package_name": "rsamplr",
    "title": "Sampling Algorithms and Spatially Balanced Sampling",
    "description": "\n    Fast tools for unequal probability sampling in multi-dimensional spaces, implemented in Rust for high performance.\n    The package offers a wide range of methods, including Sampford (Sampford, 1967, <doi:10.1093/biomet/54.3-4.499>) and correlated Poisson sampling (Bondesson and Thorburn, 2008, <doi:10.1111/j.1467-9469.2008.00596.x>), pivotal sampling (Deville and Till\u00e9, 1998, <doi:10.1093/biomet/91.4.893>), and balanced sampling such as the cube method (Deville and Till\u00e9, 2004, <doi:10.1093/biomet/91.4.893>) to ensure auxiliary totals are respected.\n    Spatially balanced approaches, including the local pivotal method (Grafstr\u00f6m et al., 2012, <doi:10.1111/j.1541-0420.2011.01699.x>), spatially correlated Poisson sampling (Grafstr\u00f6m, 2012, <doi:10.1016/j.jspi.2011.07.003>), and locally correlated Poisson sampling (Prentius, 2024, <doi:10.1002/env.2832>), provide efficient designs when the target variable is linked to auxiliary information.",
    "version": "0.1.1",
    "maintainer": "Wilmer Prentius <wilmer.prentius@slu.se>",
    "author": "Wilmer Prentius [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3561-290X>),\n  Anton Grafstr\u00f6m [ctb] (ORCID: <https://orcid.org/0000-0002-4345-4024>),\n  Authors of the dependent Rust crates [aut] (see inst/AUTHORS file)",
    "url": "https://www.envisim.se/, https://github.com/envisim/rust-samplr/",
    "bug_reports": "https://github.com/envisim/rust-samplr/issues",
    "repository": "https://cran.r-project.org/package=rsamplr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "rsamplr Sampling Algorithms and Spatially Balanced Sampling \n    Fast tools for unequal probability sampling in multi-dimensional spaces, implemented in Rust for high performance.\n    The package offers a wide range of methods, including Sampford (Sampford, 1967, <doi:10.1093/biomet/54.3-4.499>) and correlated Poisson sampling (Bondesson and Thorburn, 2008, <doi:10.1111/j.1467-9469.2008.00596.x>), pivotal sampling (Deville and Till\u00e9, 1998, <doi:10.1093/biomet/91.4.893>), and balanced sampling such as the cube method (Deville and Till\u00e9, 2004, <doi:10.1093/biomet/91.4.893>) to ensure auxiliary totals are respected.\n    Spatially balanced approaches, including the local pivotal method (Grafstr\u00f6m et al., 2012, <doi:10.1111/j.1541-0420.2011.01699.x>), spatially correlated Poisson sampling (Grafstr\u00f6m, 2012, <doi:10.1016/j.jspi.2011.07.003>), and locally correlated Poisson sampling (Prentius, 2024, <doi:10.1002/env.2832>), provide efficient designs when the target variable is linked to auxiliary information.  "
  },
  {
    "id": 19939,
    "package_name": "ruminate",
    "title": "A Pharmacometrics Data Transformation and Analysis Tool",
    "description": "Exploration of pharmacometrics data involves both general tools (transformation and plotting) and specific techniques (non-compartmental analysis). This kind of exploration is generally accomplished by utilizing different packages. The purpose of 'ruminate' is to create a 'shiny' interface to make these tools more broadly available while creating reproducible results.",
    "version": "0.3.2",
    "maintainer": "John Harrold <john.m.harrold@gmail.com>",
    "author": "John Harrold [aut, cre] (ORCID:\n    <https://orcid.org/0000-0003-2052-4373>)",
    "url": "https://ruminate.ubiquity.tools/",
    "bug_reports": "https://github.com/john-harrold/ruminate/issues",
    "repository": "https://cran.r-project.org/package=ruminate",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "ruminate A Pharmacometrics Data Transformation and Analysis Tool Exploration of pharmacometrics data involves both general tools (transformation and plotting) and specific techniques (non-compartmental analysis). This kind of exploration is generally accomplished by utilizing different packages. The purpose of 'ruminate' is to create a 'shiny' interface to make these tools more broadly available while creating reproducible results.  "
  },
  {
    "id": 20073,
    "package_name": "sampleVADIR",
    "title": "Draw Stratified Samples from the VADIR Database",
    "description": "Affords researchers the ability to draw stratified samples from the U.S. Department of Veteran's Affairs/Department of Defense Identity Repository (VADIR) database according to a variety of population characteristics. The VADIR database contains information for all veterans who were separated from the military after 1980. The central utility of the present package is to integrate data cleaning and formatting for the VADIR database with the stratification methods described by Mahto (2019) <https://CRAN.R-project.org/package=splitstackshape>. Data from VADIR are not provided as part of this package.",
    "version": "1.0.0",
    "maintainer": "Trevor Swanson <trevorswanson222@gmail.com>",
    "author": "Trevor Swanson [aut, cre],\n  Kelsie Forbush [aut],\n  Joanna Wiese [ctb],\n  Melinda Gaddy [ctb],\n  Mary Oehlert [ctb]",
    "url": "https://github.com/tswanson222/sampleVADIR",
    "bug_reports": "https://github.com/tswanson222/sampleVADIR/issues",
    "repository": "https://cran.r-project.org/package=sampleVADIR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sampleVADIR Draw Stratified Samples from the VADIR Database Affords researchers the ability to draw stratified samples from the U.S. Department of Veteran's Affairs/Department of Defense Identity Repository (VADIR) database according to a variety of population characteristics. The VADIR database contains information for all veterans who were separated from the military after 1980. The central utility of the present package is to integrate data cleaning and formatting for the VADIR database with the stratification methods described by Mahto (2019) <https://CRAN.R-project.org/package=splitstackshape>. Data from VADIR are not provided as part of this package.  "
  },
  {
    "id": 20297,
    "package_name": "sdtm.oak",
    "title": "SDTM Data Transformation Engine",
    "description": "An Electronic Data Capture system (EDC) and Data Standard agnostic \n    solution that enables the pharmaceutical programming community to develop \n    Clinical Data Interchange Standards Consortium (CDISC)\n    Study Data Tabulation Model (SDTM) datasets in R. The reusable algorithms \n    concept in 'sdtm.oak' provides a framework for modular programming and can \n    potentially automate the conversion of raw clinical data to SDTM through \n    standardized SDTM specifications. SDTM is one of the required standards for data \n    submission to the Food and Drug Administration (FDA) in the United States and \n    Pharmaceuticals and Medical Devices Agency (PMDA) in Japan. SDTM standards \n    are implemented following the SDTM Implementation Guide as defined by \n    CDISC <https://www.cdisc.org/standards/foundational/sdtmig>.",
    "version": "0.2.0",
    "maintainer": "Rammprasad Ganapathy <ganapathy.rammprasad@gene.com>",
    "author": "Rammprasad Ganapathy [aut, cre],\n  Adam Forys [aut],\n  Edgar Manukyan [aut],\n  Rosemary Li [aut],\n  Preetesh Parikh [aut],\n  Lisa Houterloot [aut],\n  Yogesh Gupta [aut],\n  Omar Garcia [aut],\n  Ramiro Magno [aut] (ORCID: <https://orcid.org/0000-0001-5226-3441>),\n  Kamil Sijko [aut] (ORCID: <https://orcid.org/0000-0002-2203-1065>),\n  Shiyu Chen [aut],\n  Pattern Institute [cph, fnd],\n  F. Hoffmann-La Roche AG [cph, fnd],\n  Pfizer Inc [cph, fnd],\n  Mohsin Uzzama [aut],\n  Transition Technologies Science [cph, fnd]",
    "url": "https://pharmaverse.github.io/sdtm.oak/,\nhttps://github.com/pharmaverse/sdtm.oak",
    "bug_reports": "https://github.com/pharmaverse/sdtm.oak/issues",
    "repository": "https://cran.r-project.org/package=sdtm.oak",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sdtm.oak SDTM Data Transformation Engine An Electronic Data Capture system (EDC) and Data Standard agnostic \n    solution that enables the pharmaceutical programming community to develop \n    Clinical Data Interchange Standards Consortium (CDISC)\n    Study Data Tabulation Model (SDTM) datasets in R. The reusable algorithms \n    concept in 'sdtm.oak' provides a framework for modular programming and can \n    potentially automate the conversion of raw clinical data to SDTM through \n    standardized SDTM specifications. SDTM is one of the required standards for data \n    submission to the Food and Drug Administration (FDA) in the United States and \n    Pharmaceuticals and Medical Devices Agency (PMDA) in Japan. SDTM standards \n    are implemented following the SDTM Implementation Guide as defined by \n    CDISC <https://www.cdisc.org/standards/foundational/sdtmig>.  "
  },
  {
    "id": 20312,
    "package_name": "secr",
    "title": "Spatially Explicit Capture-Recapture",
    "description": "Functions to estimate the density and size of a spatially \n  distributed animal population sampled with an array of passive detectors, \n  such as traps, or by searching polygons or transects. Models incorporating \n  distance-dependent detection are fitted by maximizing the likelihood. \n  Tools are included for data manipulation and model selection.",
    "version": "5.3.0",
    "maintainer": "Murray Efford <murray.efford@otago.ac.nz>",
    "author": "Murray Efford [aut, cre] (ORCID:\n    <https://orcid.org/0000-0001-5231-5184>),\n  Philipp Jund [ctb] (faster transect search and spacing),\n  David Fletcher [ctb] (overdispersion),\n  Yan Ru Choo [ctb] (ORCID: <https://orcid.org/0000-0002-8852-7178>,\n    goodness-of-fit)",
    "url": "https://www.otago.ac.nz/density/,\nhttps://github.com/MurrayEfford/secr/",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=secr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "secr Spatially Explicit Capture-Recapture Functions to estimate the density and size of a spatially \n  distributed animal population sampled with an array of passive detectors, \n  such as traps, or by searching polygons or transects. Models incorporating \n  distance-dependent detection are fitted by maximizing the likelihood. \n  Tools are included for data manipulation and model selection.  "
  },
  {
    "id": 20628,
    "package_name": "shinymrp",
    "title": "Interface for Multilevel Regression and Poststratification",
    "description": "Dual interfaces, graphical and programmatic, designed for\n    intuitive applications of Multilevel Regression and Poststratification (MRP).\n    Users can apply the method to a variety of datasets, from electronic health records\n    to sample survey data, through an end-to-end Bayesian data analysis workflow.\n    The package provides robust tools for data cleaning, exploratory analysis,\n    flexible model building, and insightful result visualization. For more details, see\n    Si et al. (2020) <https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2020002/article/00003-eng.pdf?st=iF1_Fbrh>\n    and Si (2025) <doi:10.1214/24-STS932>.",
    "version": "0.10.0",
    "maintainer": "Toan Tran <trannttoan97@gmail.com>",
    "author": "Toan Tran [cre, aut, cph],\n  Jonah Gabry [aut, cph],\n  Yajuan Si [aut, cph]",
    "url": "https://mrp-interface.github.io/shinymrp/",
    "bug_reports": "https://github.com/mrp-interface/shinymrp/issues",
    "repository": "https://cran.r-project.org/package=shinymrp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinymrp Interface for Multilevel Regression and Poststratification Dual interfaces, graphical and programmatic, designed for\n    intuitive applications of Multilevel Regression and Poststratification (MRP).\n    Users can apply the method to a variety of datasets, from electronic health records\n    to sample survey data, through an end-to-end Bayesian data analysis workflow.\n    The package provides robust tools for data cleaning, exploratory analysis,\n    flexible model building, and insightful result visualization. For more details, see\n    Si et al. (2020) <https://www150.statcan.gc.ca/n1/en/pub/12-001-x/2020002/article/00003-eng.pdf?st=iF1_Fbrh>\n    and Si (2025) <doi:10.1214/24-STS932>.  "
  },
  {
    "id": 20633,
    "package_name": "shinypivottabler",
    "title": "Shiny Module to Create Pivot Tables",
    "description": "Shiny Module to create, visualize, customize and export Excel-like pivot table.",
    "version": "1.2",
    "maintainer": "Benoit Thieurmel <bthieurmel@gmail.com>",
    "author": "Benoit Thieurmel [aut, cre],\n  Thibaut Dubois [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=shinypivottabler",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "shinypivottabler Shiny Module to Create Pivot Tables Shiny Module to create, visualize, customize and export Excel-like pivot table.  "
  },
  {
    "id": 20690,
    "package_name": "sigugr",
    "title": "Workflow for Geographic Data",
    "description": "Streamlines geographic data transformation, storage and\n    publication, simplifying data preparation and enhancing\n    interoperability across formats and platforms.",
    "version": "1.0.0",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/sigugr/,\nhttps://github.com/josesamos/sigugr",
    "bug_reports": "https://github.com/josesamos/sigugr/issues",
    "repository": "https://cran.r-project.org/package=sigugr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sigugr Workflow for Geographic Data Streamlines geographic data transformation, storage and\n    publication, simplifying data preparation and enhancing\n    interoperability across formats and platforms.  "
  },
  {
    "id": 20716,
    "package_name": "simTargetCov",
    "title": "Data Transformation or Simulation with Empirical Covariance\nMatrix",
    "description": "Transforms or simulates data with a target empirical covariance matrix supplied\n             by the user. The method to obtain the data with the target empirical covariance\n             matrix is described in Section 5.1 of Christidis, Van Aelst and Zamar (2019)\n             <arXiv:1812.05678>.",
    "version": "1.0.1",
    "maintainer": "Anthony Christidis <anthony.christidis@stat.ubc.ca>",
    "author": "Anthony Christidis <anthony.christidis@stat.ubc.ca>,\n    Stefan Van Aelst <stefan.vanaelst@kuleuven.be>,\n    Ruben Zamar <ruben@stat.ubc.ca>",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=simTargetCov",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "simTargetCov Data Transformation or Simulation with Empirical Covariance\nMatrix Transforms or simulates data with a target empirical covariance matrix supplied\n             by the user. The method to obtain the data with the target empirical covariance\n             matrix is described in Section 5.1 of Christidis, Van Aelst and Zamar (2019)\n             <arXiv:1812.05678>.  "
  },
  {
    "id": 20821,
    "package_name": "sjmisc",
    "title": "Data and Variable Transformation Functions",
    "description": "Collection of miscellaneous utility functions, supporting data\n    transformation tasks like recoding, dichotomizing or grouping variables,\n    setting and replacing missing values. The data transformation functions\n    also support labelled data, and all integrate seamlessly into a\n    'tidyverse'-workflow.",
    "version": "2.8.11",
    "maintainer": "Daniel L\u00fcdecke <d.luedecke@uke.de>",
    "author": "Daniel L\u00fcdecke [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-8895-3206>),\n  Iago Gin\u00e9-V\u00e1zquez [ctb],\n  Alexander Bartel [ctb] (ORCID: <https://orcid.org/0000-0002-1280-6138>)",
    "url": "https://strengejacke.github.io/sjmisc/",
    "bug_reports": "https://github.com/strengejacke/sjmisc/issues",
    "repository": "https://cran.r-project.org/package=sjmisc",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "sjmisc Data and Variable Transformation Functions Collection of miscellaneous utility functions, supporting data\n    transformation tasks like recoding, dichotomizing or grouping variables,\n    setting and replacing missing values. The data transformation functions\n    also support labelled data, and all integrate seamlessly into a\n    'tidyverse'-workflow.  "
  },
  {
    "id": 21116,
    "package_name": "spatialEco",
    "title": "Spatial Analysis and Modelling Utilities",
    "description": "Utilities to support spatial data manipulation, query, sampling\n    and modelling in ecological applications. Functions include models for species \n\tpopulation density, spatial smoothing, multivariate separability, point process \n\tmodel for creating pseudo- absences and sub-sampling, Quadrant-based sampling and \n\tanalysis, auto-logistic modeling, sampling models, cluster optimization, statistical \n\texploratory tools and raster-based metrics.",
    "version": "2.0-3",
    "maintainer": "Jeffrey S. Evans <jeffrey_evans@tnc.org>",
    "author": "Jeffrey S. Evans [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-5533-7044>),\n  Melanie A. Murphy [ctb],\n  Karthik Ram [ctb]",
    "url": "https://github.com/jeffreyevans/spatialEco,\nhttps://jeffreyevans.github.io/spatialEco/",
    "bug_reports": "https://github.com/jeffreyevans/spatialEco/issues",
    "repository": "https://cran.r-project.org/package=spatialEco",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "spatialEco Spatial Analysis and Modelling Utilities Utilities to support spatial data manipulation, query, sampling\n    and modelling in ecological applications. Functions include models for species \n\tpopulation density, spatial smoothing, multivariate separability, point process \n\tmodel for creating pseudo- absences and sub-sampling, Quadrant-based sampling and \n\tanalysis, auto-logistic modeling, sampling models, cluster optimization, statistical \n\texploratory tools and raster-based metrics.  "
  },
  {
    "id": 21232,
    "package_name": "splitstackshape",
    "title": "Stack and Reshape Datasets After Splitting Concatenated Values",
    "description": "Online data collection tools like Google Forms often export\n    multiple-response questions with data concatenated in cells. The\n    concat.split (cSplit) family of functions splits such data into separate \n    cells. The package also includes functions to stack groups of columns and \n    to reshape wide data, even when the data are \"unbalanced\"---something \n    which reshape (from base R) does not handle, and which melt and dcast from \n    reshape2 do not easily handle.",
    "version": "1.4.8",
    "maintainer": "Ananda Mahto <mrdwab@gmail.com>",
    "author": "Ananda Mahto",
    "url": "http://github.com/mrdwab/splitstackshape",
    "bug_reports": "http://github.com/mrdwab/splitstackshape/issues",
    "repository": "https://cran.r-project.org/package=splitstackshape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "splitstackshape Stack and Reshape Datasets After Splitting Concatenated Values Online data collection tools like Google Forms often export\n    multiple-response questions with data concatenated in cells. The\n    concat.split (cSplit) family of functions splits such data into separate \n    cells. The package also includes functions to stack groups of columns and \n    to reshape wide data, even when the data are \"unbalanced\"---something \n    which reshape (from base R) does not handle, and which melt and dcast from \n    reshape2 do not easily handle.  "
  },
  {
    "id": 21381,
    "package_name": "starschemar",
    "title": "Obtaining Stars from Flat Tables",
    "description": "Data in multidimensional systems is obtained from operational\n    systems and is transformed to adapt it to the new structure.\n    Frequently, the operations to be performed aim to transform a flat\n    table into a star schema. Transformations can be carried out using\n    professional extract, transform and load tools or tools intended for\n    data transformation for end users. With the tools mentioned, this\n    transformation can be carried out, but it requires a lot of work. The\n    main objective of this package is to define transformations that allow\n    obtaining stars from flat tables easily. In addition, it includes\n    basic data cleaning, dimension enrichment, incremental data refresh\n    and query operations, adapted to this context.",
    "version": "1.2.5",
    "maintainer": "Jose Samos <jsamos@ugr.es>",
    "author": "Jose Samos [aut, cre] (ORCID: <https://orcid.org/0000-0002-4457-3439>),\n  Universidad de Granada [cph]",
    "url": "https://josesamos.github.io/starschemar/,\nhttps://github.com/josesamos/starschemar",
    "bug_reports": "https://github.com/josesamos/starschemar/issues",
    "repository": "https://cran.r-project.org/package=starschemar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "starschemar Obtaining Stars from Flat Tables Data in multidimensional systems is obtained from operational\n    systems and is transformed to adapt it to the new structure.\n    Frequently, the operations to be performed aim to transform a flat\n    table into a star schema. Transformations can be carried out using\n    professional extract, transform and load tools or tools intended for\n    data transformation for end users. With the tools mentioned, this\n    transformation can be carried out, but it requires a lot of work. The\n    main objective of this package is to define transformations that allow\n    obtaining stars from flat tables easily. In addition, it includes\n    basic data cleaning, dimension enrichment, incremental data refresh\n    and query operations, adapted to this context.  "
  },
  {
    "id": 21800,
    "package_name": "table.express",
    "title": "Build 'data.table' Expressions with Data Manipulation Verbs",
    "description": "A specialization of 'dplyr' data manipulation verbs that parse and build expressions\n    which are ultimately evaluated by 'data.table', letting it handle all optimizations. A set of\n    additional verbs is also provided to facilitate some common operations on a subset of the data.",
    "version": "0.4.2",
    "maintainer": "Alexis Sarda-Espinosa <alexis.sarda@gmail.com>",
    "author": "Alexis Sarda-Espinosa [cre, aut]",
    "url": "https://asardaes.github.io/table.express/,\nhttps://github.com/asardaes/table.express",
    "bug_reports": "https://github.com/asardaes/table.express/issues",
    "repository": "https://cran.r-project.org/package=table.express",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "table.express Build 'data.table' Expressions with Data Manipulation Verbs A specialization of 'dplyr' data manipulation verbs that parse and build expressions\n    which are ultimately evaluated by 'data.table', letting it handle all optimizations. A set of\n    additional verbs is also provided to facilitate some common operations on a subset of the data.  "
  },
  {
    "id": 21819,
    "package_name": "tabshiftr",
    "title": "Reshape Disorganised Messy Data",
    "description": "Helps the user to build and register schema descriptions of \n    disorganised (messy) tables. Disorganised tables are tables that are \n    not in a topologically coherent form, where packages such as 'tidyr' could \n    be used for reshaping. The schema description documents the arrangement of \n    input tables and is used to reshape them into a standardised (tidy) output \n    format.",
    "version": "0.4.1",
    "maintainer": "Steffen Ehrmann <steffen.ehrmann@posteo.de>",
    "author": "Steffen Ehrmann [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2958-0796>),\n  Tsvetelina Tomova [ctb],\n  Carsten Meyer [aut] (ORCID: <https://orcid.org/0000-0003-3927-5856>),\n  Abdualmaged Alhemiary [ctb],\n  Amelie Haas [ctb],\n  Annika Ertel [ctb],\n  Arne R\u00fcmmler [ctb] (ORCID: <https://orcid.org/0000-0001-8637-9071>),\n  Caroline Busse [ctb]",
    "url": "https://github.com/luckinet/tabshiftr",
    "bug_reports": "https://github.com/luckinet/tabshiftr/issues",
    "repository": "https://cran.r-project.org/package=tabshiftr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tabshiftr Reshape Disorganised Messy Data Helps the user to build and register schema descriptions of \n    disorganised (messy) tables. Disorganised tables are tables that are \n    not in a topologically coherent form, where packages such as 'tidyr' could \n    be used for reshaping. The schema description documents the arrangement of \n    input tables and is used to reshape them into a standardised (tidy) output \n    format.  "
  },
  {
    "id": 21996,
    "package_name": "textshape",
    "title": "Tools for Reshaping Text",
    "description": "Tools that can be used to reshape and restructure text data.",
    "version": "1.7.5",
    "maintainer": "Tyler Rinker <tyler.rinker@gmail.com>",
    "author": "Tyler Rinker [aut, cre],\n  Joran Elias [ctb],\n  Matthew Flickinger [ctb],\n  Paul Foster [ctb]",
    "url": "https://github.com/trinker/textshape",
    "bug_reports": "https://github.com/trinker/textshape/issues",
    "repository": "https://cran.r-project.org/package=textshape",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "textshape Tools for Reshaping Text Tools that can be used to reshape and restructure text data.  "
  },
  {
    "id": 22085,
    "package_name": "tidyfast",
    "title": "Fast Tidying of Data",
    "description": "Tidying functions built on 'data.table'\n    to provide quick and efficient data manipulation with\n    minimal overhead.",
    "version": "0.4.0",
    "maintainer": "Tyson Barrett <t.barrett88@gmail.com>",
    "author": "Tyson Barrett [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-2137-1391>),\n  Mark Fairbanks [ctb],\n  Ivan Leung [ctb],\n  Indrajeet Patil [ctb] (ORCID: <https://orcid.org/0000-0003-1995-6531>,\n    Twitter: @patilindrajeets)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidyfast",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyfast Fast Tidying of Data Tidying functions built on 'data.table'\n    to provide quick and efficient data manipulation with\n    minimal overhead.  "
  },
  {
    "id": 22090,
    "package_name": "tidygapminder",
    "title": "Easily Tidy Gapminder Datasets",
    "description": "A toolset that allows you to easily import and tidy data sheets retrieved\n    from Gapminder data web tools. It will therefore contribute to reduce the time\n    used in data cleaning of Gapminder indicator data sheets as they are very messy.",
    "version": "0.1.1",
    "maintainer": "Anicet Ebou <anicet.ebou@gmail.com>",
    "author": "Anicet Ebou [aut, cre] (ORCID: <https://orcid.org/0000-0003-4005-177X>)",
    "url": "https://ebedthan.github.io/tidygapminder",
    "bug_reports": "https://github.com/ebedthan/tidygapminder/issues",
    "repository": "https://cran.r-project.org/package=tidygapminder",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidygapminder Easily Tidy Gapminder Datasets A toolset that allows you to easily import and tidy data sheets retrieved\n    from Gapminder data web tools. It will therefore contribute to reduce the time\n    used in data cleaning of Gapminder indicator data sheets as they are very messy.  "
  },
  {
    "id": 22105,
    "package_name": "tidymodlr",
    "title": "An R6 Class to Perform Analysis on Long Tidy Data",
    "description": "Transforms long data into a matrix form to allow for ease of input into modelling packages for regression, principal components, imputation or machine learning. It does this by pivoting on user defined columns, generating a key-value table for variable names to ensure one-to-one mappings are preserved. It is particularly useful when the indicator names in the columns are long descriptive strings, for example \"Energy imports, net (% of energy use)\". High level analysis wrapper functions for correlation and principal components analysis are provided.",
    "version": "1.0.0",
    "maintainer": "David Hammond <anotherdavidhammond@gmail.com>",
    "author": "David Hammond [aut, cre]",
    "url": "https://github.com/david-hammond/tidymodlr",
    "bug_reports": "https://github.com/david-hammond/tidymodlr/issues",
    "repository": "https://cran.r-project.org/package=tidymodlr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidymodlr An R6 Class to Perform Analysis on Long Tidy Data Transforms long data into a matrix form to allow for ease of input into modelling packages for regression, principal components, imputation or machine learning. It does this by pivoting on user defined columns, generating a key-value table for variable names to ensure one-to-one mappings are preserved. It is particularly useful when the indicator names in the columns are long descriptive strings, for example \"Energy imports, net (% of energy use)\". High level analysis wrapper functions for correlation and principal components analysis are provided.  "
  },
  {
    "id": 22116,
    "package_name": "tidyrhrv",
    "title": "Read, Iteratively Filter, and Analyze Multiple ECG Datasets",
    "description": "Allows users to quickly load multiple patients' electrocardiographic \n    (ECG) data at once and conduct relevant time analysis of heart rate variability \n    (HRV) without manual edits from a physician or data cleaning specialist. \n    The package provides the unique ability to iteratively filter, plot, \n    and store time analysis results in a data frame while writing plots to a \n    predefined folder. This streamlines the workflow for HRV analysis across \n    multiple datasets. Methods are based on Rodr\u00edguez-Li\u00f1ares et al. (2011) \n    <doi:10.1016/j.cmpb.2010.05.012>. Examples of applications using this \n    package include Kwon et al. (2022) <doi:10.1007/s10286-022-00865-2> and \n    Lawrence et al. (2023) <doi:10.1016/j.autneu.2022.103056>.",
    "version": "1.1.0",
    "maintainer": "Steven Lawrence <stevenlawrence.r@gmail.com>",
    "author": "Steven Lawrence [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-7595-8323>)",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=tidyrhrv",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyrhrv Read, Iteratively Filter, and Analyze Multiple ECG Datasets Allows users to quickly load multiple patients' electrocardiographic \n    (ECG) data at once and conduct relevant time analysis of heart rate variability \n    (HRV) without manual edits from a physician or data cleaning specialist. \n    The package provides the unique ability to iteratively filter, plot, \n    and store time analysis results in a data frame while writing plots to a \n    predefined folder. This streamlines the workflow for HRV analysis across \n    multiple datasets. Methods are based on Rodr\u00edguez-Li\u00f1ares et al. (2011) \n    <doi:10.1016/j.cmpb.2010.05.012>. Examples of applications using this \n    package include Kwon et al. (2022) <doi:10.1007/s10286-022-00865-2> and \n    Lawrence et al. (2023) <doi:10.1016/j.autneu.2022.103056>.  "
  },
  {
    "id": 22121,
    "package_name": "tidyspec",
    "title": "Spectroscopy Analysis Using the Tidy Data Philosophy",
    "description": "Enables the analysis of spectroscopy data such as infrared ('IR'), Raman, and nuclear magnetic resonance ('NMR') using the tidy data framework from the 'tidyverse'. The 'tidyspec' package provides functions for data transformation, normalization, baseline correction, smoothing, derivatives, and both interactive and static visualization. It promotes structured, reproducible workflows for spectral data exploration and preprocessing. Implemented methods include Savitzky and Golay (1964) \"Smoothing and Differentiation of Data by Simplified Least Squares Procedures\" <doi:10.1021/ac60214a047>, Sternberg (1983) \"Biomedical Image Processing\" <https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1654163>, Zimmermann and Kohler (1996) \"Baseline correction using the rolling ball algorithm\" <doi:10.1016/0168-583X(95)00908-6>, Beattie and Esmonde-White (2021) \"Exploration of Principal Component Analysis: Deriving Principal Component Analysis Visually Using Spectra\" <doi:10.1177/0003702820987847>, Wickham et al. (2019) \"Welcome to the tidyverse\" <doi:10.21105/joss.01686>, and Kuhn, Wickham and Hvitfeldt (2024) \"recipes: Preprocessing and Feature Engineering Steps for Modeling\" <https://CRAN.R-project.org/package=recipes>.",
    "version": "0.1.0",
    "maintainer": "Marcel Ferreira <marcel.ferreira@unesp.br>",
    "author": "Marcel Ferreira [aut, cre] (ORCID:\n    <https://orcid.org/0000-0002-3445-0945>),\n  Willian Zambuzzi [dtc] (ORCID: <https://orcid.org/0000-0002-4149-5965>),\n  Julia Moraes [dtc, ctb] (ORCID:\n    <https://orcid.org/0000-0003-4241-5389>),\n  Emerson Araujo Alves dos Santos [ctb] (ORCID:\n    <https://orcid.org/0000-0002-8200-0906>)",
    "url": "https://github.com/marceelrf/tidyspec",
    "bug_reports": "https://github.com/marceelrf/tidyspec/issues",
    "repository": "https://cran.r-project.org/package=tidyspec",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidyspec Spectroscopy Analysis Using the Tidy Data Philosophy Enables the analysis of spectroscopy data such as infrared ('IR'), Raman, and nuclear magnetic resonance ('NMR') using the tidy data framework from the 'tidyverse'. The 'tidyspec' package provides functions for data transformation, normalization, baseline correction, smoothing, derivatives, and both interactive and static visualization. It promotes structured, reproducible workflows for spectral data exploration and preprocessing. Implemented methods include Savitzky and Golay (1964) \"Smoothing and Differentiation of Data by Simplified Least Squares Procedures\" <doi:10.1021/ac60214a047>, Sternberg (1983) \"Biomedical Image Processing\" <https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=1654163>, Zimmermann and Kohler (1996) \"Baseline correction using the rolling ball algorithm\" <doi:10.1016/0168-583X(95)00908-6>, Beattie and Esmonde-White (2021) \"Exploration of Principal Component Analysis: Deriving Principal Component Analysis Visually Using Spectra\" <doi:10.1177/0003702820987847>, Wickham et al. (2019) \"Welcome to the tidyverse\" <doi:10.21105/joss.01686>, and Kuhn, Wickham and Hvitfeldt (2024) \"recipes: Preprocessing and Feature Engineering Steps for Modeling\" <https://CRAN.R-project.org/package=recipes>.  "
  },
  {
    "id": 22135,
    "package_name": "tidytree",
    "title": "A Tidy Tool for Phylogenetic Tree Data Manipulation",
    "description": "Phylogenetic tree generally contains multiple components including node, edge, branch and associated data. 'tidytree' provides an approach to convert tree object to tidy data frame as well as provides tidy interfaces to manipulate tree data.",
    "version": "0.4.6",
    "maintainer": "Guangchuang Yu <guangchuangyu@gmail.com>",
    "author": "Guangchuang Yu [aut, cre, cph] (ORCID:\n    <https://orcid.org/0000-0002-6485-8781>),\n  Bradley Jones [ctb],\n  Zebulun Arendsee [ctb]",
    "url": "https://www.amazon.com/Integration-Manipulation-Visualization-Phylogenetic-Computational-ebook/dp/B0B5NLZR1Z/",
    "bug_reports": "https://github.com/YuLab-SMU/tidytree/issues",
    "repository": "https://cran.r-project.org/package=tidytree",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "tidytree A Tidy Tool for Phylogenetic Tree Data Manipulation Phylogenetic tree generally contains multiple components including node, edge, branch and associated data. 'tidytree' provides an approach to convert tree object to tidy data frame as well as provides tidy interfaces to manipulate tree data.  "
  },
  {
    "id": 22563,
    "package_name": "twowaytests",
    "title": "Two-Way Tests in Independent Groups Designs",
    "description": "Performs two-way tests in independent groups designs. These are two-way ANOVA, two-way ANOVA under heteroscedasticity: parametric bootstrap based generalized test and generalized pivotal quantity based generalized test, two-way ANOVA for medians, trimmed means, M-estimators. The package performs descriptive statistics and graphical approaches. Moreover, it assesses variance homogeneity and normality of data in each group via tests and plots. All 'twowaytests' functions are designed for two-way layout  (Dag et al., 2024, <doi:10.1016/j.softx.2024.101862>).",
    "version": "1.5",
    "maintainer": "Osman Dag <osman.dag@outlook.com>",
    "author": "Osman Dag [aut, cre],\n  Sam Weerahandi [aut],\n  Malwane Ananda [aut],\n  Muhammed Ali Yilmaz [aut],\n  Merve Kasikci [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=twowaytests",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "twowaytests Two-Way Tests in Independent Groups Designs Performs two-way tests in independent groups designs. These are two-way ANOVA, two-way ANOVA under heteroscedasticity: parametric bootstrap based generalized test and generalized pivotal quantity based generalized test, two-way ANOVA for medians, trimmed means, M-estimators. The package performs descriptive statistics and graphical approaches. Moreover, it assesses variance homogeneity and normality of data in each group via tests and plots. All 'twowaytests' functions are designed for two-way layout  (Dag et al., 2024, <doi:10.1016/j.softx.2024.101862>).  "
  },
  {
    "id": 22578,
    "package_name": "uavRmp",
    "title": "UAV Mission Planner",
    "description": "The Unmanned Aerial Vehicle Mission Planner provides an easy to use work flow for planning autonomous obstacle avoiding surveys of ready to fly unmanned aerial vehicles to retrieve aerial or spot related data. It creates either intermediate flight control files for the DJI-Litchi supported series or ready to upload control files for the pixhawk-based flight controller. Additionally it contains some useful tools for digitizing and data manipulation.",
    "version": "0.7",
    "maintainer": "Chris Reudenbach <reudenbach@uni-marburg.de>",
    "author": "Chris Reudenbach [cre, aut],\n  Marvin Ludwig [ctb],\n  Sebastian Richter [ctb],\n  Florian Detsch [ctb],\n  Hanna Meyer [ctb]",
    "url": "https://github.com/gisma/uavRmp",
    "bug_reports": "https://github.com/gisma/uavRmp/issues",
    "repository": "https://cran.r-project.org/package=uavRmp",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uavRmp UAV Mission Planner The Unmanned Aerial Vehicle Mission Planner provides an easy to use work flow for planning autonomous obstacle avoiding surveys of ready to fly unmanned aerial vehicles to retrieve aerial or spot related data. It creates either intermediate flight control files for the DJI-Litchi supported series or ready to upload control files for the pixhawk-based flight controller. Additionally it contains some useful tools for digitizing and data manipulation.  "
  },
  {
    "id": 22587,
    "package_name": "udpipe",
    "title": "Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit",
    "description": "This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.",
    "version": "0.8.15",
    "maintainer": "Jan Wijffels <jwijffels@bnosac.be>",
    "author": "Jan Wijffels [aut, cre, cph] (R wrapper),\n  BNOSAC [cph] (R wrapper),\n  Institute of Formal and Applied Linguistics, Faculty of Mathematics and\n    Physics, Charles University in Prague, Czech Republic [cph]\n    (src/udpipe.cpp & src/udpipe.h),\n  Milan Straka [aut, cph] (src/udpipe.cpp & src/udpipe.h),\n  Jana Strakov\u00e1 [ctb, cph] (src/udpipe.cpp & src/udpipe.h)",
    "url": "https://bnosac.github.io/udpipe/en/index.html,\nhttps://github.com/bnosac/udpipe",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=udpipe",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "udpipe Tokenization, Parts of Speech Tagging, Lemmatization and\nDependency Parsing with the 'UDPipe' 'NLP' Toolkit This natural language processing toolkit provides language-agnostic\n    'tokenization', 'parts of speech tagging', 'lemmatization' and 'dependency\n    parsing' of raw text. Next to text parsing, the package also allows you to train\n    annotation models based on data of 'treebanks' in 'CoNLL-U' format as provided\n    at <https://universaldependencies.org/format.html>. The techniques are explained\n    in detail in the paper: 'Tokenizing, POS Tagging, Lemmatizing and Parsing UD 2.0\n    with UDPipe', available at <doi:10.18653/v1/K17-3009>. \n    The toolkit also contains functionalities for commonly used data manipulations on texts \n    which are enriched with the output of the parser. Namely functionalities and algorithms \n    for collocations, token co-occurrence, document term matrix handling, \n    term frequency inverse document frequency calculations,\n    information retrieval metrics (Okapi BM25), handling of multi-word expressions,\n    keyword detection (Rapid Automatic Keyword Extraction, noun phrase extraction, syntactical patterns) \n    sentiment scoring and semantic similarity analysis.  "
  },
  {
    "id": 22625,
    "package_name": "uni.shrinkage",
    "title": "Shrinkage Estimation for Univariate Normal Mean",
    "description": "Implement a shrinkage estimation for the univariate normal mean based on a preliminary test (pretest) estimator. This package also provides the confidence interval based on pivoting the cumulative density function. The methodologies are published in Taketomi et al.(2024) <doi:10.1007/s42081-023-00221-2> and Taketomi et al.(2024-)(under review).",
    "version": "1.0.0",
    "maintainer": "Nanami Taketomi <nnmamikrn@gmail.com>",
    "author": "Nanami Taketomi [aut, cre],\n  Jia-Han Shih [aut],\n  Takeshi Emura [aut]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=uni.shrinkage",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "uni.shrinkage Shrinkage Estimation for Univariate Normal Mean Implement a shrinkage estimation for the univariate normal mean based on a preliminary test (pretest) estimator. This package also provides the confidence interval based on pivoting the cumulative density function. The methodologies are published in Taketomi et al.(2024) <doi:10.1007/s42081-023-00221-2> and Taketomi et al.(2024-)(under review).  "
  },
  {
    "id": 22654,
    "package_name": "unpivotr",
    "title": "Unpivot Complex and Irregular Data Layouts",
    "description": "Tools for converting data from complex or irregular layouts to a\n    columnar structure.  For example, tables with multilevel column or row\n    headers, or spreadsheets.  Header and data cells are selected by their\n    contents and position, as well as formatting and comments where available,\n    and are associated with one other by their proximity in given directions.\n    Functions for data frames and HTML tables are provided.",
    "version": "0.6.4",
    "maintainer": "Duncan Garmonsway <nacnudus@gmail.com>",
    "author": "Duncan Garmonsway [aut, cre]",
    "url": "https://github.com/nacnudus/unpivotr",
    "bug_reports": "https://github.com/nacnudus/unpivotr/issues",
    "repository": "https://cran.r-project.org/package=unpivotr",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "unpivotr Unpivot Complex and Irregular Data Layouts Tools for converting data from complex or irregular layouts to a\n    columnar structure.  For example, tables with multilevel column or row\n    headers, or spreadsheets.  Header and data cells are selected by their\n    contents and position, as well as formatting and comments where available,\n    and are associated with one other by their proximity in given directions.\n    Functions for data frames and HTML tables are provided.  "
  },
  {
    "id": 22792,
    "package_name": "vcfppR",
    "title": "Rapid Manipulation of the Variant Call Format (VCF)",
    "description": "The 'vcfpp.h' (<https://github.com/Zilong-Li/vcfpp>) provides an easy-to-use 'C++' 'API' of 'htslib', offering full functionality for manipulating Variant Call Format (VCF) files. The 'vcfppR' package serves as the R bindings of the 'vcfpp.h' library, enabling rapid processing of both compressed and uncompressed VCF files. Explore a range of powerful features for efficient VCF data manipulation.",
    "version": "0.8.2",
    "maintainer": "Zilong Li <zilong.dk@gmail.com>",
    "author": "Zilong Li [aut, cre] (ORCID: <https://orcid.org/0000-0001-5859-2078>),\n  Bonfield, James K and Marshall, John and Danecek, Petr and Li, Heng and\n    Ohan, Valeriu and Whitwham, Andrew and Keane, Thomas and Davies,\n    Robert M [cph] (Authors of included htslib library)",
    "url": "https://github.com/Zilong-Li/vcfppR",
    "bug_reports": "https://github.com/Zilong-Li/vcfppR/issues",
    "repository": "https://cran.r-project.org/package=vcfppR",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vcfppR Rapid Manipulation of the Variant Call Format (VCF) The 'vcfpp.h' (<https://github.com/Zilong-Li/vcfpp>) provides an easy-to-use 'C++' 'API' of 'htslib', offering full functionality for manipulating Variant Call Format (VCF) files. The 'vcfppR' package serves as the R bindings of the 'vcfpp.h' library, enabling rapid processing of both compressed and uncompressed VCF files. Explore a range of powerful features for efficient VCF data manipulation.  "
  },
  {
    "id": 22954,
    "package_name": "vvconverter",
    "title": "Apply Transformations to Data",
    "description": "Provides a set of functions for data transformations.\n    Transformations are performed on character and numeric data. As the scope of the package is\n    within Student Analytics, there are functions focused around the academic year.",
    "version": "0.7.0",
    "maintainer": "Tomer Iwan <t.iwan@vu.nl>",
    "author": "Tomer Iwan [aut, cre, cph]",
    "url": "https://vusaverse.github.io/vvconverter/,\nhttps://github.com/vusaverse/vvconverter",
    "bug_reports": "https://github.com/vusaverse/vvconverter/issues",
    "repository": "https://cran.r-project.org/package=vvconverter",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vvconverter Apply Transformations to Data Provides a set of functions for data transformations.\n    Transformations are performed on character and numeric data. As the scope of the package is\n    within Student Analytics, there are functions focused around the academic year.  "
  },
  {
    "id": 22958,
    "package_name": "vvsculptor",
    "title": "Apply Manipulations to Data Frames",
    "description": "Provides a set of functions for manipulating data frames in accordance with specific business rules. \n    In addition, it includes wrapper functions for commonly used functions from the popular 'tidyverse' package, \n    making it easy to integrate these functions into data analysis workflows. \n    The package is designed to streamline data preprocessing and help users quickly and efficiently \n    perform data transformations that are specific to their business needs.",
    "version": "0.4.10",
    "maintainer": "Tomer Iwan <t.iwan@vu.nl>",
    "author": "Tomer Iwan [aut, cre, cph]",
    "url": "",
    "bug_reports": "",
    "repository": "https://cran.r-project.org/package=vvsculptor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "vvsculptor Apply Manipulations to Data Frames Provides a set of functions for manipulating data frames in accordance with specific business rules. \n    In addition, it includes wrapper functions for commonly used functions from the popular 'tidyverse' package, \n    making it easy to integrate these functions into data analysis workflows. \n    The package is designed to streamline data preprocessing and help users quickly and efficiently \n    perform data transformations that are specific to their business needs.  "
  },
  {
    "id": 23026,
    "package_name": "wdpar",
    "title": "Interface to the World Database on Protected Areas",
    "description": "Fetch and clean data from the World Database on Protected\n    Areas (WDPA) and the World Database on Other Effective Area-Based\n    Conservation Measures (WDOECM). Data is obtained from Protected Planet\n    <https://www.protectedplanet.net/en>. To augment data cleaning procedures,\n    users can install the 'prepr' R package (available at\n    <https://github.com/prioritizr/prepr>). For more information on this\n    package, see Hanson (2022) <doi:10.21105/joss.04594>.",
    "version": "1.3.8",
    "maintainer": "Jeffrey O Hanson <jeffrey.hanson@uqconnect.edu.au>",
    "author": "Jeffrey O Hanson [aut, cre]",
    "url": "https://prioritizr.github.io/wdpar/,\nhttps://github.com/prioritizr/wdpar",
    "bug_reports": "https://github.com/prioritizr/wdpar/issues",
    "repository": "https://cran.r-project.org/package=wdpar",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wdpar Interface to the World Database on Protected Areas Fetch and clean data from the World Database on Protected\n    Areas (WDPA) and the World Database on Other Effective Area-Based\n    Conservation Measures (WDOECM). Data is obtained from Protected Planet\n    <https://www.protectedplanet.net/en>. To augment data cleaning procedures,\n    users can install the 'prepr' R package (available at\n    <https://github.com/prioritizr/prepr>). For more information on this\n    package, see Hanson (2022) <doi:10.21105/joss.04594>.  "
  },
  {
    "id": 23121,
    "package_name": "wizaRdry",
    "title": "A Magical Framework for Collaborative & Reproducible Data\nAnalysis",
    "description": "A comprehensive data analysis framework for NIH-funded research that streamlines workflows for both data cleaning and preparing and modifying NIH Data Archive ('NDA') data structures and submission templates. Provides unified access to multiple data sources ('REDCap', 'MongoDB', 'Qualtrics', 'SQL', 'ORACLE') through interfaces to their APIs, with specialized functions for data cleaning, filtering, merging, and parsing. Features automatic validation, field harmonization, and memory-aware processing to enhance reproducibility in multi-site collaborative research as described in Mittal et al. (2021) <doi:10.20900/jpbs.20210011>.",
    "version": "0.5.0",
    "maintainer": "Joshua G. Kenney <joshua.kenney@yale.edu>",
    "author": "Joshua G. Kenney [aut, cre],\n  Trevor F. Williams [aut],\n  Minerva K. Pappu [aut],\n  Michael J. Spilka [aut],\n  Danielle N. Pratt [ctb],\n  Victor J. Pokorny [ctb],\n  Santiago Castiello de Obeso [ctb],\n  Praveen Suthaharan [ctb],\n  Christian R. Horgan [ctb]",
    "url": "https://github.com/belieflab/wizaRdry",
    "bug_reports": "https://github.com/belieflab/wizaRdry/issues",
    "repository": "https://cran.r-project.org/package=wizaRdry",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wizaRdry A Magical Framework for Collaborative & Reproducible Data\nAnalysis A comprehensive data analysis framework for NIH-funded research that streamlines workflows for both data cleaning and preparing and modifying NIH Data Archive ('NDA') data structures and submission templates. Provides unified access to multiple data sources ('REDCap', 'MongoDB', 'Qualtrics', 'SQL', 'ORACLE') through interfaces to their APIs, with specialized functions for data cleaning, filtering, merging, and parsing. Features automatic validation, field harmonization, and memory-aware processing to enhance reproducibility in multi-site collaborative research as described in Mittal et al. (2021) <doi:10.20900/jpbs.20210011>.  "
  },
  {
    "id": 23146,
    "package_name": "wordpredictor",
    "title": "Develop Text Prediction Models Based on N-Grams",
    "description": "A framework for developing n-gram models for text prediction.\n    It provides data cleaning, data sampling, extracting tokens from text,\n    model generation, model evaluation and word prediction. For information on how n-gram models \n    work we referred to: \"Speech and Language Processing\"\n    <https://web.archive.org/web/20240919222934/https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fslp3%2F3.pdf>. For optimizing R code and\n    using R6 classes we referred to \"Advanced R\" \n    <https://adv-r.hadley.nz/r6.html>. For writing R extensions we referred to \n    \"R Packages\", <https://r-pkgs.org/index.html>.",
    "version": "0.0.5",
    "maintainer": "Nadir Latif <pakjiddat@gmail.com>",
    "author": "Nadir Latif [aut, cre] (ORCID: <https://orcid.org/0000-0002-7543-7405>)",
    "url": "https://github.com/pakjiddat/word-predictor,\nhttps://pakjiddat.github.io/word-predictor/",
    "bug_reports": "https://github.com/pakjiddat/word-predictor/issues",
    "repository": "https://cran.r-project.org/package=wordpredictor",
    "exports": [],
    "topics": [],
    "score": "NA",
    "stars": 0,
    "primary_category": "general",
    "source_universe": "cran-direct",
    "search_text": "wordpredictor Develop Text Prediction Models Based on N-Grams A framework for developing n-gram models for text prediction.\n    It provides data cleaning, data sampling, extracting tokens from text,\n    model generation, model evaluation and word prediction. For information on how n-gram models \n    work we referred to: \"Speech and Language Processing\"\n    <https://web.archive.org/web/20240919222934/https%3A%2F%2Fweb.stanford.edu%2F~jurafsky%2Fslp3%2F3.pdf>. For optimizing R code and\n    using R6 classes we referred to \"Advanced R\" \n    <https://adv-r.hadley.nz/r6.html>. For writing R extensions we referred to \n    \"R Packages\", <https://r-pkgs.org/index.html>.  "
  }
]